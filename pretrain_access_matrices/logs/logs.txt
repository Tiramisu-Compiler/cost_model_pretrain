2023-09-27 18:06:29,162:INFO:  Starting experiment lstm autoencoder debug
2023-09-27 18:06:29,163:INFO:  Defining the model
2023-09-27 18:06:29,259:INFO:  Reading the dataset
2023-09-27 18:07:07,542:INFO:  Starting experiment lstm autoencoder debug
2023-09-27 18:07:07,543:INFO:  Defining the model
2023-09-27 18:07:07,584:INFO:  Reading the dataset
2023-09-27 18:07:14,969:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:14,995:INFO:  Epoch 1/500:  train Loss: 108.0895   val Loss: 101.6674   time: 2.18s   best: 101.6674
2023-09-27 18:07:15,271:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:15,477:INFO:  Epoch 2/500:  train Loss: 98.4714   val Loss: 100.0183   time: 0.27s   best: 100.0183
2023-09-27 18:07:15,726:INFO:  Epoch 3/500:  train Loss: 99.5671   val Loss: 100.1448   time: 0.24s   best: 100.0183
2023-09-27 18:07:15,986:INFO:  Epoch 4/500:  train Loss: 99.9113   val Loss: 100.1954   time: 0.26s   best: 100.0183
2023-09-27 18:07:16,234:INFO:  Epoch 5/500:  train Loss: 100.0123   val Loss: 100.2139   time: 0.24s   best: 100.0183
2023-09-27 18:07:16,499:INFO:  Epoch 6/500:  train Loss: 100.0537   val Loss: 100.2132   time: 0.26s   best: 100.0183
2023-09-27 18:07:16,745:INFO:  Epoch 7/500:  train Loss: 99.9406   val Loss: 100.1858   time: 0.24s   best: 100.0183
2023-09-27 18:07:17,023:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:17,069:INFO:  Epoch 8/500:  train Loss: 99.5130   val Loss: 99.3953   time: 0.27s   best: 99.3953
2023-09-27 18:07:17,335:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:17,530:INFO:  Epoch 9/500:  train Loss: 98.3199   val Loss: 97.1976   time: 0.26s   best: 97.1976
2023-09-27 18:07:17,782:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:17,956:INFO:  Epoch 10/500:  train Loss: 96.8741   val Loss: 95.8836   time: 0.25s   best: 95.8836
2023-09-27 18:07:18,204:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:18,226:INFO:  Epoch 11/500:  train Loss: 94.9908   val Loss: 91.4279   time: 0.24s   best: 91.4279
2023-09-27 18:07:18,486:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:18,664:INFO:  Epoch 12/500:  train Loss: 94.3640   val Loss: 91.0326   time: 0.26s   best: 91.0326
2023-09-27 18:07:18,923:INFO:  Epoch 13/500:  train Loss: 94.0329   val Loss: 93.0508   time: 0.26s   best: 91.0326
2023-09-27 18:07:19,183:INFO:  Epoch 14/500:  train Loss: 94.1791   val Loss: 94.6434   time: 0.26s   best: 91.0326
2023-09-27 18:07:19,448:INFO:  Epoch 15/500:  train Loss: 94.7503   val Loss: 94.6218   time: 0.26s   best: 91.0326
2023-09-27 18:07:19,696:INFO:  Epoch 16/500:  train Loss: 94.7527   val Loss: 94.6234   time: 0.24s   best: 91.0326
2023-09-27 18:07:19,964:INFO:  Epoch 17/500:  train Loss: 94.7647   val Loss: 94.4896   time: 0.26s   best: 91.0326
2023-09-27 18:07:20,211:INFO:  Epoch 18/500:  train Loss: 94.2164   val Loss: 94.2854   time: 0.24s   best: 91.0326
2023-09-27 18:07:20,477:INFO:  Epoch 19/500:  train Loss: 94.2726   val Loss: 94.2280   time: 0.26s   best: 91.0326
2023-09-27 18:07:20,725:INFO:  Epoch 20/500:  train Loss: 94.0563   val Loss: 94.1480   time: 0.24s   best: 91.0326
2023-09-27 18:07:21,003:INFO:  Epoch 21/500:  train Loss: 94.3881   val Loss: 94.1126   time: 0.27s   best: 91.0326
2023-09-27 18:07:21,254:INFO:  Epoch 22/500:  train Loss: 93.8388   val Loss: 93.9672   time: 0.25s   best: 91.0326
2023-09-27 18:07:21,514:INFO:  Epoch 23/500:  train Loss: 93.7476   val Loss: 93.8482   time: 0.26s   best: 91.0326
2023-09-27 18:07:21,765:INFO:  Epoch 24/500:  train Loss: 93.6535   val Loss: 93.8097   time: 0.24s   best: 91.0326
2023-09-27 18:07:22,037:INFO:  Epoch 25/500:  train Loss: 94.0241   val Loss: 93.8249   time: 0.27s   best: 91.0326
2023-09-27 18:07:22,284:INFO:  Epoch 26/500:  train Loss: 93.7270   val Loss: 93.8400   time: 0.24s   best: 91.0326
2023-09-27 18:07:22,548:INFO:  Epoch 27/500:  train Loss: 93.5200   val Loss: 93.9157   time: 0.26s   best: 91.0326
2023-09-27 18:07:22,796:INFO:  Epoch 28/500:  train Loss: 93.6757   val Loss: 93.8921   time: 0.24s   best: 91.0326
2023-09-27 18:07:23,070:INFO:  Epoch 29/500:  train Loss: 93.6148   val Loss: 93.8443   time: 0.27s   best: 91.0326
2023-09-27 18:07:23,321:INFO:  Epoch 30/500:  train Loss: 93.8650   val Loss: 93.8223   time: 0.25s   best: 91.0326
2023-09-27 18:07:23,587:INFO:  Epoch 31/500:  train Loss: 93.6107   val Loss: 93.7567   time: 0.26s   best: 91.0326
2023-09-27 18:07:23,839:INFO:  Epoch 32/500:  train Loss: 93.3946   val Loss: 93.6468   time: 0.25s   best: 91.0326
2023-09-27 18:07:24,105:INFO:  Epoch 33/500:  train Loss: 93.9822   val Loss: 93.4258   time: 0.26s   best: 91.0326
2023-09-27 18:07:24,347:INFO:  Epoch 34/500:  train Loss: 93.1608   val Loss: 93.0717   time: 0.24s   best: 91.0326
2023-09-27 18:07:24,597:INFO:  Epoch 35/500:  train Loss: 92.5915   val Loss: 92.7681   time: 0.25s   best: 91.0326
2023-09-27 18:07:24,862:INFO:  Epoch 36/500:  train Loss: 92.7155   val Loss: 92.4779   time: 0.26s   best: 91.0326
2023-09-27 18:07:25,135:INFO:  Epoch 37/500:  train Loss: 92.5736   val Loss: 92.1929   time: 0.25s   best: 91.0326
2023-09-27 18:07:25,390:INFO:  Epoch 38/500:  train Loss: 91.9940   val Loss: 91.7982   time: 0.25s   best: 91.0326
2023-09-27 18:07:25,635:INFO:  Epoch 39/500:  train Loss: 91.0920   val Loss: 91.3013   time: 0.24s   best: 91.0326
2023-09-27 18:07:25,908:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:25,930:INFO:  Epoch 40/500:  train Loss: 90.9996   val Loss: 90.8484   time: 0.27s   best: 90.8484
2023-09-27 18:07:26,198:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:26,389:INFO:  Epoch 41/500:  train Loss: 90.0303   val Loss: 89.7314   time: 0.26s   best: 89.7314
2023-09-27 18:07:26,637:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:26,755:INFO:  Epoch 42/500:  train Loss: 89.3583   val Loss: 88.9522   time: 0.24s   best: 88.9522
2023-09-27 18:07:27,013:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:27,038:INFO:  Epoch 43/500:  train Loss: 89.2353   val Loss: 88.8051   time: 0.25s   best: 88.8051
2023-09-27 18:07:27,303:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:27,388:INFO:  Epoch 44/500:  train Loss: 89.3518   val Loss: 88.7414   time: 0.26s   best: 88.7414
2023-09-27 18:07:27,638:INFO:  Epoch 45/500:  train Loss: 89.1539   val Loss: 89.2348   time: 0.25s   best: 88.7414
2023-09-27 18:07:27,910:INFO:  Epoch 46/500:  train Loss: 89.5165   val Loss: 89.4373   time: 0.27s   best: 88.7414
2023-09-27 18:07:28,160:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:28,324:INFO:  Epoch 47/500:  train Loss: 89.1231   val Loss: 88.1917   time: 0.25s   best: 88.1917
2023-09-27 18:07:28,572:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:28,595:INFO:  Epoch 48/500:  train Loss: 88.6711   val Loss: 87.6991   time: 0.24s   best: 87.6991
2023-09-27 18:07:28,855:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:29,065:INFO:  Epoch 49/500:  train Loss: 87.8965   val Loss: 87.5959   time: 0.26s   best: 87.5959
2023-09-27 18:07:29,326:INFO:  Epoch 50/500:  train Loss: 88.7391   val Loss: 88.8846   time: 0.26s   best: 87.5959
2023-09-27 18:07:29,576:INFO:  Epoch 51/500:  train Loss: 89.0403   val Loss: 88.5637   time: 0.25s   best: 87.5959
2023-09-27 18:07:29,846:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:30,003:INFO:  Epoch 52/500:  train Loss: 87.7295   val Loss: 87.0889   time: 0.26s   best: 87.0889
2023-09-27 18:07:30,251:INFO:  Epoch 53/500:  train Loss: 88.1609   val Loss: 87.2991   time: 0.24s   best: 87.0889
2023-09-27 18:07:30,513:INFO:  Epoch 54/500:  train Loss: 88.9849   val Loss: 89.4321   time: 0.26s   best: 87.0889
2023-09-27 18:07:30,760:INFO:  Epoch 55/500:  train Loss: 89.6860   val Loss: 89.5002   time: 0.24s   best: 87.0889
2023-09-27 18:07:31,030:INFO:  Epoch 56/500:  train Loss: 89.0222   val Loss: 87.8221   time: 0.27s   best: 87.0889
2023-09-27 18:07:31,283:INFO:  Epoch 57/500:  train Loss: 87.7846   val Loss: 87.4355   time: 0.25s   best: 87.0889
2023-09-27 18:07:31,547:INFO:  Epoch 58/500:  train Loss: 87.7574   val Loss: 87.1546   time: 0.26s   best: 87.0889
2023-09-27 18:07:31,800:INFO:  Epoch 59/500:  train Loss: 87.6762   val Loss: 87.4618   time: 0.25s   best: 87.0889
2023-09-27 18:07:32,068:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:32,120:INFO:  Epoch 60/500:  train Loss: 87.7331   val Loss: 86.9990   time: 0.26s   best: 86.9990
2023-09-27 18:07:32,369:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:32,580:INFO:  Epoch 61/500:  train Loss: 88.2103   val Loss: 86.8400   time: 0.24s   best: 86.8400
2023-09-27 18:07:32,827:INFO:  Epoch 62/500:  train Loss: 88.7502   val Loss: 89.0350   time: 0.24s   best: 86.8400
2023-09-27 18:07:33,096:INFO:  Epoch 63/500:  train Loss: 88.4525   val Loss: 87.0640   time: 0.27s   best: 86.8400
2023-09-27 18:07:33,348:INFO:  Epoch 64/500:  train Loss: 87.9424   val Loss: 87.4674   time: 0.25s   best: 86.8400
2023-09-27 18:07:33,613:INFO:  Epoch 65/500:  train Loss: 88.1160   val Loss: 88.2533   time: 0.26s   best: 86.8400
2023-09-27 18:07:33,865:INFO:  Epoch 66/500:  train Loss: 88.6928   val Loss: 88.7607   time: 0.25s   best: 86.8400
2023-09-27 18:07:34,132:INFO:  Epoch 67/500:  train Loss: 88.5473   val Loss: 87.2621   time: 0.26s   best: 86.8400
2023-09-27 18:07:34,380:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:34,402:INFO:  Epoch 68/500:  train Loss: 87.0161   val Loss: 86.5880   time: 0.24s   best: 86.5880
2023-09-27 18:07:34,666:INFO:  Epoch 69/500:  train Loss: 87.3359   val Loss: 86.6886   time: 0.26s   best: 86.5880
2023-09-27 18:07:34,914:INFO:  Epoch 70/500:  train Loss: 87.0158   val Loss: 86.8376   time: 0.24s   best: 86.5880
2023-09-27 18:07:35,195:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:35,471:INFO:  Epoch 71/500:  train Loss: 86.7545   val Loss: 86.2195   time: 0.27s   best: 86.2195
2023-09-27 18:07:35,730:INFO:  Epoch 72/500:  train Loss: 86.8042   val Loss: 86.4054   time: 0.25s   best: 86.2195
2023-09-27 18:07:35,984:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:36,107:INFO:  Epoch 73/500:  train Loss: 86.2410   val Loss: 85.5851   time: 0.25s   best: 85.5851
2023-09-27 18:07:36,356:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:36,557:INFO:  Epoch 74/500:  train Loss: 87.6635   val Loss: 85.2766   time: 0.24s   best: 85.2766
2023-09-27 18:07:36,803:INFO:  Epoch 75/500:  train Loss: 86.1416   val Loss: 86.0644   time: 0.24s   best: 85.2766
2023-09-27 18:07:37,080:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:37,157:INFO:  Epoch 76/500:  train Loss: 86.0097   val Loss: 84.9923   time: 0.27s   best: 84.9923
2023-09-27 18:07:37,409:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:37,543:INFO:  Epoch 77/500:  train Loss: 85.5511   val Loss: 84.7449   time: 0.25s   best: 84.7449
2023-09-27 18:07:37,804:INFO:  Epoch 78/500:  train Loss: 85.9895   val Loss: 85.8923   time: 0.25s   best: 84.7449
2023-09-27 18:07:38,053:INFO:  Epoch 79/500:  train Loss: 86.3987   val Loss: 85.7658   time: 0.24s   best: 84.7449
2023-09-27 18:07:38,319:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:38,340:INFO:  Epoch 80/500:  train Loss: 85.6070   val Loss: 84.7166   time: 0.26s   best: 84.7166
2023-09-27 18:07:38,589:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:38,751:INFO:  Epoch 81/500:  train Loss: 85.6363   val Loss: 84.3066   time: 0.24s   best: 84.3066
2023-09-27 18:07:39,003:INFO:  Epoch 82/500:  train Loss: 85.6104   val Loss: 85.5313   time: 0.25s   best: 84.3066
2023-09-27 18:07:39,273:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:39,379:INFO:  Epoch 83/500:  train Loss: 85.0070   val Loss: 84.2100   time: 0.27s   best: 84.2100
2023-09-27 18:07:39,631:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:39,683:INFO:  Epoch 84/500:  train Loss: 85.5667   val Loss: 84.1717   time: 0.25s   best: 84.1717
2023-09-27 18:07:39,936:INFO:  Epoch 85/500:  train Loss: 85.3426   val Loss: 84.7371   time: 0.25s   best: 84.1717
2023-09-27 18:07:40,203:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:40,289:INFO:  Epoch 86/500:  train Loss: 84.9744   val Loss: 83.2527   time: 0.26s   best: 83.2527
2023-09-27 18:07:40,573:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:40,693:INFO:  Epoch 87/500:  train Loss: 84.1633   val Loss: 83.1141   time: 0.28s   best: 83.1141
2023-09-27 18:07:40,951:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:41,099:INFO:  Epoch 88/500:  train Loss: 83.5536   val Loss: 82.9332   time: 0.25s   best: 82.9332
2023-09-27 18:07:41,554:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:42,128:INFO:  Epoch 89/500:  train Loss: 83.4344   val Loss: 82.5806   time: 0.45s   best: 82.5806
2023-09-27 18:07:42,380:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:42,403:INFO:  Epoch 90/500:  train Loss: 83.5874   val Loss: 82.5156   time: 0.25s   best: 82.5156
2023-09-27 18:07:42,650:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:42,929:INFO:  Epoch 91/500:  train Loss: 83.0901   val Loss: 82.5110   time: 0.24s   best: 82.5110
2023-09-27 18:07:43,183:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:43,301:INFO:  Epoch 92/500:  train Loss: 84.3438   val Loss: 81.4712   time: 0.25s   best: 81.4712
2023-09-27 18:07:43,548:INFO:  Epoch 93/500:  train Loss: 82.6828   val Loss: 82.1584   time: 0.24s   best: 81.4712
2023-09-27 18:07:43,816:INFO:  Epoch 94/500:  train Loss: 82.4878   val Loss: 82.3621   time: 0.26s   best: 81.4712
2023-09-27 18:07:44,065:INFO:  Epoch 95/500:  train Loss: 84.0653   val Loss: 84.3681   time: 0.24s   best: 81.4712
2023-09-27 18:07:44,332:INFO:  Epoch 96/500:  train Loss: 84.1256   val Loss: 83.5813   time: 0.26s   best: 81.4712
2023-09-27 18:07:44,578:INFO:  Epoch 97/500:  train Loss: 84.6228   val Loss: 82.8366   time: 0.24s   best: 81.4712
2023-09-27 18:07:44,843:INFO:  Epoch 98/500:  train Loss: 83.4824   val Loss: 82.2524   time: 0.26s   best: 81.4712
2023-09-27 18:07:45,095:INFO:  Epoch 99/500:  train Loss: 82.9090   val Loss: 81.5335   time: 0.25s   best: 81.4712
2023-09-27 18:07:45,481:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:45,516:INFO:  Epoch 100/500:  train Loss: 81.9339   val Loss: 80.9504   time: 0.38s   best: 80.9504
2023-09-27 18:07:45,781:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:46,189:INFO:  Epoch 101/500:  train Loss: 81.9800   val Loss: 80.3684   time: 0.26s   best: 80.3684
2023-09-27 18:07:46,439:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:46,938:INFO:  Epoch 102/500:  train Loss: 81.1881   val Loss: 80.3090   time: 0.25s   best: 80.3090
2023-09-27 18:07:47,190:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:47,288:INFO:  Epoch 103/500:  train Loss: 81.0892   val Loss: 79.7196   time: 0.25s   best: 79.7196
2023-09-27 18:07:47,546:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:47,970:INFO:  Epoch 104/500:  train Loss: 80.9868   val Loss: 79.2507   time: 0.25s   best: 79.2507
2023-09-27 18:07:48,218:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:48,267:INFO:  Epoch 105/500:  train Loss: 80.2646   val Loss: 79.0807   time: 0.24s   best: 79.0807
2023-09-27 18:07:48,535:INFO:  Epoch 106/500:  train Loss: 81.1371   val Loss: 79.1259   time: 0.25s   best: 79.0807
2023-09-27 18:07:48,784:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:49,412:INFO:  Epoch 107/500:  train Loss: 80.1162   val Loss: 78.7874   time: 0.24s   best: 78.7874
2023-09-27 18:07:49,675:INFO:  Epoch 108/500:  train Loss: 81.1245   val Loss: 80.6568   time: 0.25s   best: 78.7874
2023-09-27 18:07:49,950:INFO:  Epoch 109/500:  train Loss: 81.0384   val Loss: 80.0773   time: 0.26s   best: 78.7874
2023-09-27 18:07:50,205:INFO:  Epoch 110/500:  train Loss: 80.9389   val Loss: 79.1669   time: 0.24s   best: 78.7874
2023-09-27 18:07:50,542:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:50,765:INFO:  Epoch 111/500:  train Loss: 79.6542   val Loss: 78.5758   time: 0.26s   best: 78.5758
2023-09-27 18:07:51,033:INFO:  Epoch 112/500:  train Loss: 80.5105   val Loss: 79.4560   time: 0.25s   best: 78.5758
2023-09-27 18:07:51,290:INFO:  Epoch 113/500:  train Loss: 80.7501   val Loss: 79.7802   time: 0.24s   best: 78.5758
2023-09-27 18:07:51,561:INFO:  Epoch 114/500:  train Loss: 81.1642   val Loss: 80.2599   time: 0.26s   best: 78.5758
2023-09-27 18:07:51,825:INFO:  Epoch 115/500:  train Loss: 81.4063   val Loss: 80.0720   time: 0.25s   best: 78.5758
2023-09-27 18:07:52,095:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:52,655:INFO:  Epoch 116/500:  train Loss: 80.0165   val Loss: 78.5259   time: 0.27s   best: 78.5259
2023-09-27 18:07:52,911:INFO:  Epoch 117/500:  train Loss: 81.4180   val Loss: 81.4117   time: 0.24s   best: 78.5259
2023-09-27 18:07:53,182:INFO:  Epoch 118/500:  train Loss: 80.8312   val Loss: 79.5859   time: 0.26s   best: 78.5259
2023-09-27 18:07:53,439:INFO:  Epoch 119/500:  train Loss: 81.0545   val Loss: 78.8893   time: 0.24s   best: 78.5259
2023-09-27 18:07:53,720:INFO:  Epoch 120/500:  train Loss: 79.2845   val Loss: 78.8329   time: 0.27s   best: 78.5259
2023-09-27 18:07:53,975:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:54,005:INFO:  Epoch 121/500:  train Loss: 79.4410   val Loss: 78.2872   time: 0.25s   best: 78.2872
2023-09-27 18:07:54,270:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:54,678:INFO:  Epoch 122/500:  train Loss: 79.6695   val Loss: 77.8356   time: 0.26s   best: 77.8356
2023-09-27 18:07:54,933:INFO:  Epoch 123/500:  train Loss: 79.1334   val Loss: 78.7501   time: 0.24s   best: 77.8356
2023-09-27 18:07:55,205:INFO:  Epoch 124/500:  train Loss: 78.7404   val Loss: 78.0922   time: 0.26s   best: 77.8356
2023-09-27 18:07:55,463:INFO:  Epoch 125/500:  train Loss: 79.5307   val Loss: 78.3666   time: 0.25s   best: 77.8356
2023-09-27 18:07:55,741:INFO:  Epoch 126/500:  train Loss: 79.2093   val Loss: 78.4872   time: 0.27s   best: 77.8356
2023-09-27 18:07:55,995:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:56,181:INFO:  Epoch 127/500:  train Loss: 78.5597   val Loss: 77.1454   time: 0.25s   best: 77.1454
2023-09-27 18:07:56,436:INFO:  Epoch 128/500:  train Loss: 78.8206   val Loss: 78.2603   time: 0.24s   best: 77.1454
2023-09-27 18:07:56,706:INFO:  Epoch 129/500:  train Loss: 79.7236   val Loss: 79.1834   time: 0.26s   best: 77.1454
2023-09-27 18:07:56,962:INFO:  Epoch 130/500:  train Loss: 79.8683   val Loss: 78.1757   time: 0.24s   best: 77.1454
2023-09-27 18:07:57,242:INFO:  Epoch 131/500:  train Loss: 79.6727   val Loss: 78.4481   time: 0.27s   best: 77.1454
2023-09-27 18:07:57,498:INFO:  Epoch 132/500:  train Loss: 78.3127   val Loss: 77.7154   time: 0.24s   best: 77.1454
2023-09-27 18:07:57,776:INFO:  Epoch 133/500:  train Loss: 79.5463   val Loss: 78.6639   time: 0.26s   best: 77.1454
2023-09-27 18:07:58,033:INFO:  Epoch 134/500:  train Loss: 79.4040   val Loss: 77.9150   time: 0.25s   best: 77.1454
2023-09-27 18:07:58,313:INFO:  Epoch 135/500:  train Loss: 78.8875   val Loss: 77.7645   time: 0.27s   best: 77.1454
2023-09-27 18:07:58,561:INFO:  Epoch 136/500:  train Loss: 78.6287   val Loss: 78.0622   time: 0.24s   best: 77.1454
2023-09-27 18:07:58,835:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:58,984:INFO:  Epoch 137/500:  train Loss: 79.1292   val Loss: 77.1430   time: 0.27s   best: 77.1430
2023-09-27 18:07:59,251:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:07:59,409:INFO:  Epoch 138/500:  train Loss: 79.4502   val Loss: 76.5972   time: 0.26s   best: 76.5972
2023-09-27 18:07:59,668:INFO:  Epoch 139/500:  train Loss: 78.9681   val Loss: 77.5196   time: 0.25s   best: 76.5972
2023-09-27 18:07:59,934:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:00,029:INFO:  Epoch 140/500:  train Loss: 78.2514   val Loss: 76.1129   time: 0.26s   best: 76.1129
2023-09-27 18:08:00,298:INFO:  Epoch 141/500:  train Loss: 78.8528   val Loss: 78.4403   time: 0.26s   best: 76.1129
2023-09-27 18:08:00,553:INFO:  Epoch 142/500:  train Loss: 79.1926   val Loss: 78.2430   time: 0.24s   best: 76.1129
2023-09-27 18:08:00,827:INFO:  Epoch 143/500:  train Loss: 78.1065   val Loss: 76.4010   time: 0.26s   best: 76.1129
2023-09-27 18:08:01,079:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:01,111:INFO:  Epoch 144/500:  train Loss: 77.6003   val Loss: 75.8787   time: 0.25s   best: 75.8787
2023-09-27 18:08:01,384:INFO:  Epoch 145/500:  train Loss: 77.7257   val Loss: 78.6737   time: 0.26s   best: 75.8787
2023-09-27 18:08:01,645:INFO:  Epoch 146/500:  train Loss: 79.8553   val Loss: 78.9220   time: 0.25s   best: 75.8787
2023-09-27 18:08:01,915:INFO:  Epoch 147/500:  train Loss: 79.5920   val Loss: 79.5126   time: 0.26s   best: 75.8787
2023-09-27 18:08:02,178:INFO:  Epoch 148/500:  train Loss: 79.9747   val Loss: 77.9599   time: 0.25s   best: 75.8787
2023-09-27 18:08:02,451:INFO:  Epoch 149/500:  train Loss: 78.2272   val Loss: 76.2897   time: 0.26s   best: 75.8787
2023-09-27 18:08:02,700:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:03,212:INFO:  Epoch 150/500:  train Loss: 77.0308   val Loss: 75.6289   time: 0.24s   best: 75.6289
2023-09-27 18:08:03,472:INFO:  Epoch 151/500:  train Loss: 76.6057   val Loss: 76.6727   time: 0.25s   best: 75.6289
2023-09-27 18:08:03,732:INFO:  Epoch 152/500:  train Loss: 79.1481   val Loss: 76.8464   time: 0.25s   best: 75.6289
2023-09-27 18:08:04,009:INFO:  Epoch 153/500:  train Loss: 77.2923   val Loss: 78.4162   time: 0.26s   best: 75.6289
2023-09-27 18:08:04,264:INFO:  Epoch 154/500:  train Loss: 78.6570   val Loss: 77.1152   time: 0.24s   best: 75.6289
2023-09-27 18:08:04,531:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:04,562:INFO:  Epoch 155/500:  train Loss: 77.3693   val Loss: 75.4162   time: 0.26s   best: 75.4162
2023-09-27 18:08:04,811:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:04,878:INFO:  Epoch 156/500:  train Loss: 76.5867   val Loss: 74.7807   time: 0.24s   best: 74.7807
2023-09-27 18:08:05,157:INFO:  Epoch 157/500:  train Loss: 78.3618   val Loss: 77.0529   time: 0.27s   best: 74.7807
2023-09-27 18:08:05,428:INFO:  Epoch 158/500:  train Loss: 76.7989   val Loss: 75.4098   time: 0.25s   best: 74.7807
2023-09-27 18:08:05,695:INFO:  Epoch 159/500:  train Loss: 78.1061   val Loss: 77.5673   time: 0.25s   best: 74.7807
2023-09-27 18:08:05,972:INFO:  Epoch 160/500:  train Loss: 79.8014   val Loss: 78.8393   time: 0.26s   best: 74.7807
2023-09-27 18:08:06,230:INFO:  Epoch 161/500:  train Loss: 79.3657   val Loss: 79.9401   time: 0.24s   best: 74.7807
2023-09-27 18:08:06,502:INFO:  Epoch 162/500:  train Loss: 80.4560   val Loss: 76.7481   time: 0.26s   best: 74.7807
2023-09-27 18:08:06,759:INFO:  Epoch 163/500:  train Loss: 78.4836   val Loss: 76.0873   time: 0.24s   best: 74.7807
2023-09-27 18:08:07,028:INFO:  Epoch 164/500:  train Loss: 76.7526   val Loss: 75.4087   time: 0.26s   best: 74.7807
2023-09-27 18:08:07,291:INFO:  Epoch 165/500:  train Loss: 76.4401   val Loss: 75.7141   time: 0.25s   best: 74.7807
2023-09-27 18:08:07,567:INFO:  Epoch 166/500:  train Loss: 77.8876   val Loss: 77.1919   time: 0.26s   best: 74.7807
2023-09-27 18:08:07,832:INFO:  Epoch 167/500:  train Loss: 77.6949   val Loss: 75.5683   time: 0.25s   best: 74.7807
2023-09-27 18:08:08,108:INFO:  Epoch 168/500:  train Loss: 76.8796   val Loss: 74.8421   time: 0.26s   best: 74.7807
2023-09-27 18:08:08,358:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:08,394:INFO:  Epoch 169/500:  train Loss: 75.8462   val Loss: 74.4842   time: 0.24s   best: 74.4842
2023-09-27 18:08:08,666:INFO:  Epoch 170/500:  train Loss: 77.2317   val Loss: 75.4285   time: 0.26s   best: 74.4842
2023-09-27 18:08:08,922:INFO:  Epoch 171/500:  train Loss: 77.6545   val Loss: 77.4403   time: 0.24s   best: 74.4842
2023-09-27 18:08:09,199:INFO:  Epoch 172/500:  train Loss: 76.9727   val Loss: 77.0252   time: 0.26s   best: 74.4842
2023-09-27 18:08:09,457:INFO:  Epoch 173/500:  train Loss: 77.8379   val Loss: 76.2659   time: 0.25s   best: 74.4842
2023-09-27 18:08:09,739:INFO:  Epoch 174/500:  train Loss: 76.2101   val Loss: 75.0593   time: 0.27s   best: 74.4842
2023-09-27 18:08:09,993:INFO:  Epoch 175/500:  train Loss: 77.7692   val Loss: 74.8246   time: 0.25s   best: 74.4842
2023-09-27 18:08:10,274:INFO:  Epoch 176/500:  train Loss: 76.3909   val Loss: 75.4083   time: 0.27s   best: 74.4842
2023-09-27 18:08:10,531:INFO:  Epoch 177/500:  train Loss: 75.8804   val Loss: 75.3132   time: 0.24s   best: 74.4842
2023-09-27 18:08:10,804:INFO:  Epoch 178/500:  train Loss: 77.4297   val Loss: 77.1725   time: 0.26s   best: 74.4842
2023-09-27 18:08:11,087:INFO:  Epoch 179/500:  train Loss: 76.4539   val Loss: 77.4085   time: 0.27s   best: 74.4842
2023-09-27 18:08:11,374:INFO:  Epoch 180/500:  train Loss: 77.9059   val Loss: 74.6990   time: 0.27s   best: 74.4842
2023-09-27 18:08:11,629:INFO:  Epoch 181/500:  train Loss: 76.8260   val Loss: 75.6082   time: 0.24s   best: 74.4842
2023-09-27 18:08:12,090:INFO:  Epoch 182/500:  train Loss: 76.0676   val Loss: 75.6069   time: 0.46s   best: 74.4842
2023-09-27 18:08:12,396:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:12,426:INFO:  Epoch 183/500:  train Loss: 77.0748   val Loss: 74.4807   time: 0.30s   best: 74.4807
2023-09-27 18:08:12,681:INFO:  Epoch 184/500:  train Loss: 75.7422   val Loss: 77.7127   time: 0.24s   best: 74.4807
2023-09-27 18:08:12,952:INFO:  Epoch 185/500:  train Loss: 80.7559   val Loss: 79.7954   time: 0.26s   best: 74.4807
2023-09-27 18:08:13,219:INFO:  Epoch 186/500:  train Loss: 79.2139   val Loss: 78.1436   time: 0.26s   best: 74.4807
2023-09-27 18:08:13,494:INFO:  Epoch 187/500:  train Loss: 80.0252   val Loss: 77.0300   time: 0.26s   best: 74.4807
2023-09-27 18:08:13,749:INFO:  Epoch 188/500:  train Loss: 77.7013   val Loss: 75.5723   time: 0.24s   best: 74.4807
2023-09-27 18:08:14,028:INFO:  Epoch 189/500:  train Loss: 77.4078   val Loss: 75.6722   time: 0.27s   best: 74.4807
2023-09-27 18:08:14,304:INFO:  Epoch 190/500:  train Loss: 76.0450   val Loss: 75.3093   time: 0.25s   best: 74.4807
2023-09-27 18:08:14,563:INFO:  Epoch 191/500:  train Loss: 76.1568   val Loss: 74.7909   time: 0.25s   best: 74.4807
2023-09-27 18:08:14,832:INFO:  Epoch 192/500:  train Loss: 75.7042   val Loss: 77.1430   time: 0.24s   best: 74.4807
2023-09-27 18:08:15,096:INFO:  Epoch 193/500:  train Loss: 77.8981   val Loss: 76.9054   time: 0.25s   best: 74.4807
2023-09-27 18:08:15,360:INFO:  Epoch 194/500:  train Loss: 76.8371   val Loss: 76.9470   time: 0.24s   best: 74.4807
2023-09-27 18:08:15,627:INFO:  Epoch 195/500:  train Loss: 77.4753   val Loss: 75.4097   time: 0.26s   best: 74.4807
2023-09-27 18:08:15,878:INFO:  Epoch 196/500:  train Loss: 75.5934   val Loss: 74.5411   time: 0.24s   best: 74.4807
2023-09-27 18:08:16,152:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:16,385:INFO:  Epoch 197/500:  train Loss: 74.6770   val Loss: 73.2836   time: 0.27s   best: 73.2836
2023-09-27 18:08:16,642:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:16,675:INFO:  Epoch 198/500:  train Loss: 75.4597   val Loss: 73.0641   time: 0.25s   best: 73.0641
2023-09-27 18:08:16,938:INFO:  Epoch 199/500:  train Loss: 75.8932   val Loss: 74.7652   time: 0.24s   best: 73.0641
2023-09-27 18:08:17,250:INFO:  Epoch 200/500:  train Loss: 75.7639   val Loss: 74.7996   time: 0.30s   best: 73.0641
2023-09-27 18:08:17,533:INFO:  Epoch 201/500:  train Loss: 74.7239   val Loss: 73.3669   time: 0.27s   best: 73.0641
2023-09-27 18:08:17,788:INFO:  Epoch 202/500:  train Loss: 74.8598   val Loss: 74.7715   time: 0.24s   best: 73.0641
2023-09-27 18:08:18,065:INFO:  Epoch 203/500:  train Loss: 74.4687   val Loss: 73.1861   time: 0.26s   best: 73.0641
2023-09-27 18:08:18,318:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:18,349:INFO:  Epoch 204/500:  train Loss: 76.6884   val Loss: 72.9054   time: 0.25s   best: 72.9054
2023-09-27 18:08:18,620:INFO:  Epoch 205/500:  train Loss: 75.3360   val Loss: 74.9752   time: 0.26s   best: 72.9054
2023-09-27 18:08:18,876:INFO:  Epoch 206/500:  train Loss: 75.9164   val Loss: 76.8409   time: 0.24s   best: 72.9054
2023-09-27 18:08:19,154:INFO:  Epoch 207/500:  train Loss: 76.9864   val Loss: 73.6561   time: 0.26s   best: 72.9054
2023-09-27 18:08:19,411:INFO:  Epoch 208/500:  train Loss: 74.7345   val Loss: 73.7227   time: 0.24s   best: 72.9054
2023-09-27 18:08:19,684:INFO:  Epoch 209/500:  train Loss: 75.5330   val Loss: 76.4527   time: 0.26s   best: 72.9054
2023-09-27 18:08:19,938:INFO:  Epoch 210/500:  train Loss: 81.2339   val Loss: 83.1272   time: 0.25s   best: 72.9054
2023-09-27 18:08:20,224:INFO:  Epoch 211/500:  train Loss: 83.7315   val Loss: 83.7699   time: 0.27s   best: 72.9054
2023-09-27 18:08:20,480:INFO:  Epoch 212/500:  train Loss: 85.6843   val Loss: 84.5925   time: 0.24s   best: 72.9054
2023-09-27 18:08:20,753:INFO:  Epoch 213/500:  train Loss: 85.2177   val Loss: 82.9213   time: 0.26s   best: 72.9054
2023-09-27 18:08:21,011:INFO:  Epoch 214/500:  train Loss: 83.6023   val Loss: 81.8078   time: 0.24s   best: 72.9054
2023-09-27 18:08:21,288:INFO:  Epoch 215/500:  train Loss: 81.9921   val Loss: 79.2029   time: 0.26s   best: 72.9054
2023-09-27 18:08:21,544:INFO:  Epoch 216/500:  train Loss: 79.5257   val Loss: 77.4869   time: 0.24s   best: 72.9054
2023-09-27 18:08:21,817:INFO:  Epoch 217/500:  train Loss: 77.8598   val Loss: 75.3879   time: 0.26s   best: 72.9054
2023-09-27 18:08:22,075:INFO:  Epoch 218/500:  train Loss: 75.8217   val Loss: 73.3494   time: 0.25s   best: 72.9054
2023-09-27 18:08:22,353:INFO:  Epoch 219/500:  train Loss: 74.8661   val Loss: 73.9032   time: 0.27s   best: 72.9054
2023-09-27 18:08:22,609:INFO:  Epoch 220/500:  train Loss: 74.8821   val Loss: 74.4335   time: 0.24s   best: 72.9054
2023-09-27 18:08:22,884:INFO:  Epoch 221/500:  train Loss: 75.5633   val Loss: 73.8715   time: 0.26s   best: 72.9054
2023-09-27 18:08:23,138:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:23,273:INFO:  Epoch 222/500:  train Loss: 74.4104   val Loss: 72.6006   time: 0.25s   best: 72.6006
2023-09-27 18:08:23,530:INFO:  Epoch 223/500:  train Loss: 74.1718   val Loss: 72.9468   time: 0.24s   best: 72.6006
2023-09-27 18:08:23,799:INFO:  Epoch 224/500:  train Loss: 73.9272   val Loss: 72.9219   time: 0.26s   best: 72.6006
2023-09-27 18:08:24,058:INFO:  Epoch 225/500:  train Loss: 73.8892   val Loss: 73.0132   time: 0.25s   best: 72.6006
2023-09-27 18:08:24,328:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:24,400:INFO:  Epoch 226/500:  train Loss: 73.5729   val Loss: 72.4997   time: 0.27s   best: 72.4997
2023-09-27 18:08:24,655:INFO:  Epoch 227/500:  train Loss: 74.1370   val Loss: 73.5185   time: 0.24s   best: 72.4997
2023-09-27 18:08:24,927:INFO:  Epoch 228/500:  train Loss: 74.6389   val Loss: 74.4393   time: 0.26s   best: 72.4997
2023-09-27 18:08:25,188:INFO:  Epoch 229/500:  train Loss: 75.0759   val Loss: 73.7842   time: 0.25s   best: 72.4997
2023-09-27 18:08:25,463:INFO:  Epoch 230/500:  train Loss: 74.3569   val Loss: 73.4802   time: 0.26s   best: 72.4997
2023-09-27 18:08:25,718:INFO:  Epoch 231/500:  train Loss: 75.1059   val Loss: 74.4645   time: 0.24s   best: 72.4997
2023-09-27 18:08:25,992:INFO:  Epoch 232/500:  train Loss: 74.8704   val Loss: 74.7114   time: 0.26s   best: 72.4997
2023-09-27 18:08:26,255:INFO:  Epoch 233/500:  train Loss: 75.2074   val Loss: 73.5853   time: 0.25s   best: 72.4997
2023-09-27 18:08:26,522:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:26,703:INFO:  Epoch 234/500:  train Loss: 74.1504   val Loss: 72.4588   time: 0.26s   best: 72.4588
2023-09-27 18:08:26,962:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:27,504:INFO:  Epoch 235/500:  train Loss: 73.6544   val Loss: 72.0545   time: 0.25s   best: 72.0545
2023-09-27 18:08:27,753:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:27,788:INFO:  Epoch 236/500:  train Loss: 73.2223   val Loss: 71.9301   time: 0.24s   best: 71.9301
2023-09-27 18:08:28,047:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:29,156:INFO:  Epoch 237/500:  train Loss: 72.8124   val Loss: 71.7560   time: 0.25s   best: 71.7560
2023-09-27 18:08:29,418:INFO:  Epoch 238/500:  train Loss: 72.9844   val Loss: 72.5783   time: 0.25s   best: 71.7560
2023-09-27 18:08:29,673:INFO:  Epoch 239/500:  train Loss: 73.7784   val Loss: 72.8076   time: 0.24s   best: 71.7560
2023-09-27 18:08:29,952:INFO:  Epoch 240/500:  train Loss: 73.9857   val Loss: 72.2772   time: 0.27s   best: 71.7560
2023-09-27 18:08:30,209:INFO:  Epoch 241/500:  train Loss: 72.3283   val Loss: 71.8300   time: 0.25s   best: 71.7560
2023-09-27 18:08:30,489:INFO:  Epoch 242/500:  train Loss: 74.6844   val Loss: 75.2619   time: 0.27s   best: 71.7560
2023-09-27 18:08:30,744:INFO:  Epoch 243/500:  train Loss: 75.6046   val Loss: 75.5504   time: 0.24s   best: 71.7560
2023-09-27 18:08:31,018:INFO:  Epoch 244/500:  train Loss: 75.5784   val Loss: 73.2634   time: 0.26s   best: 71.7560
2023-09-27 18:08:31,278:INFO:  Epoch 245/500:  train Loss: 73.5158   val Loss: 72.0276   time: 0.25s   best: 71.7560
2023-09-27 18:08:31,546:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:31,722:INFO:  Epoch 246/500:  train Loss: 73.7624   val Loss: 71.2663   time: 0.26s   best: 71.2663
2023-09-27 18:08:31,992:INFO:  Epoch 247/500:  train Loss: 72.8267   val Loss: 72.0041   time: 0.26s   best: 71.2663
2023-09-27 18:08:32,257:INFO:  Epoch 248/500:  train Loss: 72.8024   val Loss: 71.5861   time: 0.24s   best: 71.2663
2023-09-27 18:08:32,530:INFO:  Epoch 249/500:  train Loss: 72.6831   val Loss: 71.4508   time: 0.26s   best: 71.2663
2023-09-27 18:08:32,785:INFO:  Epoch 250/500:  train Loss: 71.9485   val Loss: 71.7219   time: 0.24s   best: 71.2663
2023-09-27 18:08:33,062:INFO:  Epoch 251/500:  train Loss: 72.8476   val Loss: 71.9973   time: 0.26s   best: 71.2663
2023-09-27 18:08:33,319:INFO:  Epoch 252/500:  train Loss: 73.4816   val Loss: 71.5962   time: 0.24s   best: 71.2663
2023-09-27 18:08:33,592:INFO:  Epoch 253/500:  train Loss: 73.2965   val Loss: 72.2574   time: 0.26s   best: 71.2663
2023-09-27 18:08:33,847:INFO:  Epoch 254/500:  train Loss: 72.8322   val Loss: 72.3307   time: 0.24s   best: 71.2663
2023-09-27 18:08:34,125:INFO:  Epoch 255/500:  train Loss: 74.0609   val Loss: 74.6475   time: 0.27s   best: 71.2663
2023-09-27 18:08:34,389:INFO:  Epoch 256/500:  train Loss: 73.7546   val Loss: 73.6461   time: 0.25s   best: 71.2663
2023-09-27 18:08:34,653:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:34,721:INFO:  Epoch 257/500:  train Loss: 73.5153   val Loss: 71.0184   time: 0.26s   best: 71.0184
2023-09-27 18:08:34,977:INFO:  Epoch 258/500:  train Loss: 74.4272   val Loss: 72.0108   time: 0.24s   best: 71.0184
2023-09-27 18:08:35,252:INFO:  Epoch 259/500:  train Loss: 73.7091   val Loss: 73.5291   time: 0.26s   best: 71.0184
2023-09-27 18:08:35,509:INFO:  Epoch 260/500:  train Loss: 74.7619   val Loss: 73.5583   time: 0.24s   best: 71.0184
2023-09-27 18:08:35,774:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:35,987:INFO:  Epoch 261/500:  train Loss: 73.1297   val Loss: 70.9981   time: 0.26s   best: 70.9981
2023-09-27 18:08:36,255:INFO:  Epoch 262/500:  train Loss: 73.0758   val Loss: 72.5683   time: 0.25s   best: 70.9981
2023-09-27 18:08:36,518:INFO:  Epoch 263/500:  train Loss: 73.0764   val Loss: 73.8630   time: 0.25s   best: 70.9981
2023-09-27 18:08:36,791:INFO:  Epoch 264/500:  train Loss: 74.2332   val Loss: 71.8997   time: 0.26s   best: 70.9981
2023-09-27 18:08:37,040:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:37,210:INFO:  Epoch 265/500:  train Loss: 72.1210   val Loss: 70.4351   time: 0.24s   best: 70.4351
2023-09-27 18:08:37,458:INFO:  Epoch 266/500:  train Loss: 71.6045   val Loss: 70.4950   time: 0.24s   best: 70.4351
2023-09-27 18:08:37,729:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:08:37,778:INFO:  Epoch 267/500:  train Loss: 71.5156   val Loss: 70.2404   time: 0.27s   best: 70.2404
2023-09-27 18:08:38,035:INFO:  Epoch 268/500:  train Loss: 71.2062   val Loss: 70.7919   time: 0.24s   best: 70.2404
2023-09-27 18:08:38,313:INFO:  Epoch 269/500:  train Loss: 70.8117   val Loss: 72.2662   time: 0.27s   best: 70.2404
2023-09-27 18:08:38,570:INFO:  Epoch 270/500:  train Loss: 88.6914   val Loss: 84.6428   time: 0.24s   best: 70.2404
2023-09-27 18:08:38,836:INFO:  Epoch 271/500:  train Loss: 94.2111   val Loss: 96.9179   time: 0.26s   best: 70.2404
2023-09-27 18:08:39,103:INFO:  Epoch 272/500:  train Loss: 95.8976   val Loss: 92.7772   time: 0.25s   best: 70.2404
2023-09-27 18:08:39,377:INFO:  Epoch 273/500:  train Loss: 94.3906   val Loss: 95.5306   time: 0.26s   best: 70.2404
2023-09-27 18:08:39,632:INFO:  Epoch 274/500:  train Loss: 94.9694   val Loss: 91.4425   time: 0.24s   best: 70.2404
2023-09-27 18:08:39,909:INFO:  Epoch 275/500:  train Loss: 93.9871   val Loss: 92.9744   time: 0.26s   best: 70.2404
2023-09-27 18:08:40,168:INFO:  Epoch 276/500:  train Loss: 95.2650   val Loss: 95.4224   time: 0.25s   best: 70.2404
2023-09-27 18:08:40,449:INFO:  Epoch 277/500:  train Loss: 95.0415   val Loss: 94.5230   time: 0.27s   best: 70.2404
2023-09-27 18:08:40,706:INFO:  Epoch 278/500:  train Loss: 93.6076   val Loss: 90.4100   time: 0.24s   best: 70.2404
2023-09-27 18:08:40,982:INFO:  Epoch 279/500:  train Loss: 92.7585   val Loss: 92.2378   time: 0.26s   best: 70.2404
2023-09-27 18:08:41,244:INFO:  Epoch 280/500:  train Loss: 94.2614   val Loss: 94.6287   time: 0.25s   best: 70.2404
2023-09-27 18:08:41,520:INFO:  Epoch 281/500:  train Loss: 95.2397   val Loss: 95.0607   time: 0.26s   best: 70.2404
2023-09-27 18:08:41,775:INFO:  Epoch 282/500:  train Loss: 94.4245   val Loss: 93.6998   time: 0.24s   best: 70.2404
2023-09-27 18:08:42,052:INFO:  Epoch 283/500:  train Loss: 93.7696   val Loss: 93.2490   time: 0.26s   best: 70.2404
2023-09-27 18:08:42,321:INFO:  Epoch 284/500:  train Loss: 93.1527   val Loss: 93.0135   time: 0.27s   best: 70.2404
2023-09-27 18:08:42,788:INFO:  Epoch 285/500:  train Loss: 93.4703   val Loss: 93.0557   time: 0.46s   best: 70.2404
2023-09-27 18:08:43,064:INFO:  Epoch 286/500:  train Loss: 93.1644   val Loss: 93.3717   time: 0.27s   best: 70.2404
2023-09-27 18:08:43,339:INFO:  Epoch 287/500:  train Loss: 93.1831   val Loss: 92.8668   time: 0.26s   best: 70.2404
2023-09-27 18:08:43,617:INFO:  Epoch 288/500:  train Loss: 93.0433   val Loss: 92.7378   time: 0.26s   best: 70.2404
2023-09-27 18:08:43,873:INFO:  Epoch 289/500:  train Loss: 92.8550   val Loss: 92.6056   time: 0.24s   best: 70.2404
2023-09-27 18:08:44,150:INFO:  Epoch 290/500:  train Loss: 92.8581   val Loss: 92.5359   time: 0.26s   best: 70.2404
2023-09-27 18:08:44,406:INFO:  Epoch 291/500:  train Loss: 92.8180   val Loss: 92.6806   time: 0.24s   best: 70.2404
2023-09-27 18:08:44,678:INFO:  Epoch 292/500:  train Loss: 93.0425   val Loss: 92.8705   time: 0.26s   best: 70.2404
2023-09-27 18:08:44,939:INFO:  Epoch 293/500:  train Loss: 92.9945   val Loss: 92.5779   time: 0.25s   best: 70.2404
2023-09-27 18:08:45,219:INFO:  Epoch 294/500:  train Loss: 92.1901   val Loss: 92.0823   time: 0.27s   best: 70.2404
2023-09-27 18:08:45,476:INFO:  Epoch 295/500:  train Loss: 92.5953   val Loss: 92.2398   time: 0.25s   best: 70.2404
2023-09-27 18:08:45,749:INFO:  Epoch 296/500:  train Loss: 92.6703   val Loss: 92.3758   time: 0.26s   best: 70.2404
2023-09-27 18:08:46,006:INFO:  Epoch 297/500:  train Loss: 92.2931   val Loss: 92.1274   time: 0.24s   best: 70.2404
2023-09-27 18:08:46,282:INFO:  Epoch 298/500:  train Loss: 92.5054   val Loss: 92.2565   time: 0.26s   best: 70.2404
2023-09-27 18:08:46,538:INFO:  Epoch 299/500:  train Loss: 92.2963   val Loss: 91.9338   time: 0.24s   best: 70.2404
2023-09-27 18:08:46,859:INFO:  Epoch 300/500:  train Loss: 91.9882   val Loss: 91.5672   time: 0.31s   best: 70.2404
2023-09-27 18:08:47,147:INFO:  Epoch 301/500:  train Loss: 91.8068   val Loss: 91.3056   time: 0.28s   best: 70.2404
2023-09-27 18:08:47,403:INFO:  Epoch 302/500:  train Loss: 91.6396   val Loss: 91.0646   time: 0.24s   best: 70.2404
2023-09-27 18:08:47,678:INFO:  Epoch 303/500:  train Loss: 90.8337   val Loss: 90.5847   time: 0.26s   best: 70.2404
2023-09-27 18:08:47,934:INFO:  Epoch 304/500:  train Loss: 90.7668   val Loss: 90.3223   time: 0.24s   best: 70.2404
2023-09-27 18:08:48,209:INFO:  Epoch 305/500:  train Loss: 90.5250   val Loss: 90.4070   time: 0.26s   best: 70.2404
2023-09-27 18:08:48,465:INFO:  Epoch 306/500:  train Loss: 90.3228   val Loss: 89.6868   time: 0.24s   best: 70.2404
2023-09-27 18:08:48,739:INFO:  Epoch 307/500:  train Loss: 89.5881   val Loss: 88.9665   time: 0.26s   best: 70.2404
2023-09-27 18:08:49,001:INFO:  Epoch 308/500:  train Loss: 89.2404   val Loss: 88.4001   time: 0.25s   best: 70.2404
2023-09-27 18:08:49,279:INFO:  Epoch 309/500:  train Loss: 88.6402   val Loss: 88.1618   time: 0.26s   best: 70.2404
2023-09-27 18:08:49,536:INFO:  Epoch 310/500:  train Loss: 88.2421   val Loss: 87.4894   time: 0.24s   best: 70.2404
2023-09-27 18:08:49,806:INFO:  Epoch 311/500:  train Loss: 87.3902   val Loss: 86.0925   time: 0.26s   best: 70.2404
2023-09-27 18:08:50,060:INFO:  Epoch 312/500:  train Loss: 86.2563   val Loss: 84.6212   time: 0.25s   best: 70.2404
2023-09-27 18:08:50,341:INFO:  Epoch 313/500:  train Loss: 84.8096   val Loss: 82.5333   time: 0.27s   best: 70.2404
2023-09-27 18:08:50,597:INFO:  Epoch 314/500:  train Loss: 82.5317   val Loss: 82.7307   time: 0.24s   best: 70.2404
2023-09-27 18:08:50,880:INFO:  Epoch 315/500:  train Loss: 83.1159   val Loss: 78.7471   time: 0.27s   best: 70.2404
2023-09-27 18:08:51,142:INFO:  Epoch 316/500:  train Loss: 80.5604   val Loss: 79.7127   time: 0.25s   best: 70.2404
2023-09-27 18:08:51,416:INFO:  Epoch 317/500:  train Loss: 79.5938   val Loss: 78.7683   time: 0.26s   best: 70.2404
2023-09-27 18:08:51,673:INFO:  Epoch 318/500:  train Loss: 79.8539   val Loss: 76.8316   time: 0.24s   best: 70.2404
2023-09-27 18:08:51,947:INFO:  Epoch 319/500:  train Loss: 77.7271   val Loss: 76.3109   time: 0.26s   best: 70.2404
2023-09-27 18:08:52,205:INFO:  Epoch 320/500:  train Loss: 75.9118   val Loss: 78.2425   time: 0.24s   best: 70.2404
2023-09-27 18:08:52,478:INFO:  Epoch 321/500:  train Loss: 78.1878   val Loss: 76.0066   time: 0.26s   best: 70.2404
2023-09-27 18:08:52,733:INFO:  Epoch 322/500:  train Loss: 77.3016   val Loss: 75.4826   time: 0.24s   best: 70.2404
2023-09-27 18:08:53,014:INFO:  Epoch 323/500:  train Loss: 75.7246   val Loss: 75.3380   time: 0.27s   best: 70.2404
2023-09-27 18:08:53,274:INFO:  Epoch 324/500:  train Loss: 75.6650   val Loss: 73.8180   time: 0.25s   best: 70.2404
2023-09-27 18:08:53,549:INFO:  Epoch 325/500:  train Loss: 77.1714   val Loss: 73.0726   time: 0.26s   best: 70.2404
2023-09-27 18:08:53,804:INFO:  Epoch 326/500:  train Loss: 75.5759   val Loss: 76.7122   time: 0.24s   best: 70.2404
2023-09-27 18:08:54,081:INFO:  Epoch 327/500:  train Loss: 77.4870   val Loss: 75.4011   time: 0.26s   best: 70.2404
2023-09-27 18:08:54,338:INFO:  Epoch 328/500:  train Loss: 75.7411   val Loss: 74.4360   time: 0.24s   best: 70.2404
2023-09-27 18:08:54,612:INFO:  Epoch 329/500:  train Loss: 75.1962   val Loss: 73.8395   time: 0.26s   best: 70.2404
2023-09-27 18:08:54,875:INFO:  Epoch 330/500:  train Loss: 74.6842   val Loss: 74.9432   time: 0.24s   best: 70.2404
2023-09-27 18:08:55,153:INFO:  Epoch 331/500:  train Loss: 74.7367   val Loss: 75.2953   time: 0.26s   best: 70.2404
2023-09-27 18:08:55,409:INFO:  Epoch 332/500:  train Loss: 75.5348   val Loss: 74.6788   time: 0.24s   best: 70.2404
2023-09-27 18:08:55,682:INFO:  Epoch 333/500:  train Loss: 74.9054   val Loss: 72.9303   time: 0.26s   best: 70.2404
2023-09-27 18:08:55,955:INFO:  Epoch 334/500:  train Loss: 74.8311   val Loss: 73.1192   time: 0.24s   best: 70.2404
2023-09-27 18:08:56,214:INFO:  Epoch 335/500:  train Loss: 73.8667   val Loss: 72.6002   time: 0.25s   best: 70.2404
2023-09-27 18:08:56,478:INFO:  Epoch 336/500:  train Loss: 73.2630   val Loss: 72.0180   time: 0.24s   best: 70.2404
2023-09-27 18:08:56,745:INFO:  Epoch 337/500:  train Loss: 73.0425   val Loss: 72.6425   time: 0.25s   best: 70.2404
2023-09-27 18:08:57,009:INFO:  Epoch 338/500:  train Loss: 73.0872   val Loss: 72.3450   time: 0.25s   best: 70.2404
2023-09-27 18:08:57,286:INFO:  Epoch 339/500:  train Loss: 73.0661   val Loss: 72.1230   time: 0.26s   best: 70.2404
2023-09-27 18:08:57,547:INFO:  Epoch 340/500:  train Loss: 72.6980   val Loss: 71.5047   time: 0.25s   best: 70.2404
2023-09-27 18:08:57,810:INFO:  Epoch 341/500:  train Loss: 72.9848   val Loss: 72.7139   time: 0.26s   best: 70.2404
2023-09-27 18:08:58,064:INFO:  Epoch 342/500:  train Loss: 73.5647   val Loss: 74.2843   time: 0.25s   best: 70.2404
2023-09-27 18:08:58,342:INFO:  Epoch 343/500:  train Loss: 73.7393   val Loss: 72.8874   time: 0.27s   best: 70.2404
2023-09-27 18:08:58,613:INFO:  Epoch 344/500:  train Loss: 73.2243   val Loss: 71.7029   time: 0.27s   best: 70.2404
2023-09-27 18:08:58,888:INFO:  Epoch 345/500:  train Loss: 73.3902   val Loss: 73.5797   time: 0.26s   best: 70.2404
2023-09-27 18:08:59,153:INFO:  Epoch 346/500:  train Loss: 73.4064   val Loss: 74.0274   time: 0.26s   best: 70.2404
2023-09-27 18:08:59,421:INFO:  Epoch 347/500:  train Loss: 74.1213   val Loss: 73.7559   time: 0.26s   best: 70.2404
2023-09-27 18:08:59,691:INFO:  Epoch 348/500:  train Loss: 73.1927   val Loss: 72.3625   time: 0.24s   best: 70.2404
2023-09-27 18:08:59,956:INFO:  Epoch 349/500:  train Loss: 73.5595   val Loss: 71.5861   time: 0.25s   best: 70.2404
2023-09-27 18:09:00,219:INFO:  Epoch 350/500:  train Loss: 73.5787   val Loss: 70.8585   time: 0.24s   best: 70.2404
2023-09-27 18:09:00,478:INFO:  Epoch 351/500:  train Loss: 71.8247   val Loss: 72.2351   time: 0.25s   best: 70.2404
2023-09-27 18:09:00,760:INFO:  Epoch 352/500:  train Loss: 72.7473   val Loss: 71.4357   time: 0.25s   best: 70.2404
2023-09-27 18:09:01,023:INFO:  Epoch 353/500:  train Loss: 72.1804   val Loss: 71.5000   time: 0.25s   best: 70.2404
2023-09-27 18:09:01,299:INFO:  Epoch 354/500:  train Loss: 72.3465   val Loss: 72.4553   time: 0.26s   best: 70.2404
2023-09-27 18:09:01,557:INFO:  Epoch 355/500:  train Loss: 72.6505   val Loss: 73.5753   time: 0.25s   best: 70.2404
2023-09-27 18:09:01,833:INFO:  Epoch 356/500:  train Loss: 73.6905   val Loss: 72.2246   time: 0.26s   best: 70.2404
2023-09-27 18:09:02,090:INFO:  Epoch 357/500:  train Loss: 72.8232   val Loss: 75.6863   time: 0.24s   best: 70.2404
2023-09-27 18:09:02,363:INFO:  Epoch 358/500:  train Loss: 75.6627   val Loss: 76.0484   time: 0.26s   best: 70.2404
2023-09-27 18:09:02,618:INFO:  Epoch 359/500:  train Loss: 75.4686   val Loss: 72.5946   time: 0.24s   best: 70.2404
2023-09-27 18:09:02,897:INFO:  Epoch 360/500:  train Loss: 73.0836   val Loss: 70.2791   time: 0.26s   best: 70.2404
2023-09-27 18:09:03,157:INFO:  Epoch 361/500:  train Loss: 71.4821   val Loss: 70.6788   time: 0.25s   best: 70.2404
2023-09-27 18:09:03,424:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:09:03,454:INFO:  Epoch 362/500:  train Loss: 70.8411   val Loss: 69.7945   time: 0.26s   best: 69.7945
2023-09-27 18:09:03,706:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:09:03,902:INFO:  Epoch 363/500:  train Loss: 71.1106   val Loss: 69.7910   time: 0.25s   best: 69.7910
2023-09-27 18:09:04,160:INFO:  Epoch 364/500:  train Loss: 71.4680   val Loss: 71.2990   time: 0.25s   best: 69.7910
2023-09-27 18:09:04,433:INFO:  Epoch 365/500:  train Loss: 72.0162   val Loss: 70.5750   time: 0.26s   best: 69.7910
2023-09-27 18:09:04,690:INFO:  Epoch 366/500:  train Loss: 71.0837   val Loss: 71.1774   time: 0.24s   best: 69.7910
2023-09-27 18:09:04,970:INFO:  Epoch 367/500:  train Loss: 71.1124   val Loss: 73.4790   time: 0.27s   best: 69.7910
2023-09-27 18:09:05,229:INFO:  Epoch 368/500:  train Loss: 73.6409   val Loss: 75.0802   time: 0.25s   best: 69.7910
2023-09-27 18:09:05,503:INFO:  Epoch 369/500:  train Loss: 73.5021   val Loss: 70.6364   time: 0.26s   best: 69.7910
2023-09-27 18:09:05,753:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:09:05,784:INFO:  Epoch 370/500:  train Loss: 71.7649   val Loss: 69.6955   time: 0.24s   best: 69.6955
2023-09-27 18:09:06,049:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:09:06,203:INFO:  Epoch 371/500:  train Loss: 70.9750   val Loss: 68.7252   time: 0.26s   best: 68.7252
2023-09-27 18:09:06,459:INFO:  Epoch 372/500:  train Loss: 70.5872   val Loss: 68.9315   time: 0.24s   best: 68.7252
2023-09-27 18:09:06,724:INFO:  Epoch 373/500:  train Loss: 70.5922   val Loss: 70.7623   time: 0.26s   best: 68.7252
2023-09-27 18:09:07,009:INFO:  Epoch 374/500:  train Loss: 71.4849   val Loss: 71.1141   time: 0.26s   best: 68.7252
2023-09-27 18:09:07,272:INFO:  Epoch 375/500:  train Loss: 71.3069   val Loss: 72.3257   time: 0.25s   best: 68.7252
2023-09-27 18:09:07,546:INFO:  Epoch 376/500:  train Loss: 71.4249   val Loss: 70.6894   time: 0.24s   best: 68.7252
2023-09-27 18:09:07,805:INFO:  Epoch 377/500:  train Loss: 70.6013   val Loss: 74.0831   time: 0.25s   best: 68.7252
2023-09-27 18:09:08,070:INFO:  Epoch 378/500:  train Loss: 73.5764   val Loss: 73.1662   time: 0.25s   best: 68.7252
2023-09-27 18:09:08,338:INFO:  Epoch 379/500:  train Loss: 71.8063   val Loss: 74.9056   time: 0.25s   best: 68.7252
2023-09-27 18:09:08,607:INFO:  Epoch 380/500:  train Loss: 73.2507   val Loss: 76.6801   time: 0.24s   best: 68.7252
2023-09-27 18:09:08,869:INFO:  Epoch 381/500:  train Loss: 79.8422   val Loss: 84.1051   time: 0.25s   best: 68.7252
2023-09-27 18:09:09,154:INFO:  Epoch 382/500:  train Loss: 86.5192   val Loss: 87.0678   time: 0.27s   best: 68.7252
2023-09-27 18:09:09,411:INFO:  Epoch 383/500:  train Loss: 87.0335   val Loss: 85.8257   time: 0.24s   best: 68.7252
2023-09-27 18:09:09,680:INFO:  Epoch 384/500:  train Loss: 85.5703   val Loss: 84.1357   time: 0.26s   best: 68.7252
2023-09-27 18:09:09,943:INFO:  Epoch 385/500:  train Loss: 84.1227   val Loss: 81.7230   time: 0.25s   best: 68.7252
2023-09-27 18:09:10,221:INFO:  Epoch 386/500:  train Loss: 82.1842   val Loss: 80.0148   time: 0.27s   best: 68.7252
2023-09-27 18:09:10,476:INFO:  Epoch 387/500:  train Loss: 80.4554   val Loss: 78.1903   time: 0.24s   best: 68.7252
2023-09-27 18:09:10,754:INFO:  Epoch 388/500:  train Loss: 78.2388   val Loss: 75.6685   time: 0.27s   best: 68.7252
2023-09-27 18:09:11,018:INFO:  Epoch 389/500:  train Loss: 76.1481   val Loss: 74.8415   time: 0.25s   best: 68.7252
2023-09-27 18:09:11,297:INFO:  Epoch 390/500:  train Loss: 75.0911   val Loss: 74.3592   time: 0.27s   best: 68.7252
2023-09-27 18:09:11,555:INFO:  Epoch 391/500:  train Loss: 73.5689   val Loss: 73.9404   time: 0.25s   best: 68.7252
2023-09-27 18:09:11,830:INFO:  Epoch 392/500:  train Loss: 73.7196   val Loss: 74.5345   time: 0.26s   best: 68.7252
2023-09-27 18:09:12,087:INFO:  Epoch 393/500:  train Loss: 73.8662   val Loss: 74.8944   time: 0.24s   best: 68.7252
2023-09-27 18:09:12,360:INFO:  Epoch 394/500:  train Loss: 73.4500   val Loss: 74.1957   time: 0.26s   best: 68.7252
2023-09-27 18:09:12,616:INFO:  Epoch 395/500:  train Loss: 73.9771   val Loss: 77.0327   time: 0.24s   best: 68.7252
2023-09-27 18:09:12,888:INFO:  Epoch 396/500:  train Loss: 76.5217   val Loss: 74.8409   time: 0.26s   best: 68.7252
2023-09-27 18:09:13,339:INFO:  Epoch 397/500:  train Loss: 74.0457   val Loss: 72.6292   time: 0.45s   best: 68.7252
2023-09-27 18:09:13,658:INFO:  Epoch 398/500:  train Loss: 72.3228   val Loss: 71.5899   time: 0.31s   best: 68.7252
2023-09-27 18:09:13,929:INFO:  Epoch 399/500:  train Loss: 71.8533   val Loss: 72.0563   time: 0.26s   best: 68.7252
2023-09-27 18:09:14,231:INFO:  Epoch 400/500:  train Loss: 71.9744   val Loss: 70.3996   time: 0.29s   best: 68.7252
2023-09-27 18:09:14,514:INFO:  Epoch 401/500:  train Loss: 71.3628   val Loss: 68.9935   time: 0.27s   best: 68.7252
2023-09-27 18:09:14,771:INFO:  Epoch 402/500:  train Loss: 68.8467   val Loss: 71.8748   time: 0.24s   best: 68.7252
2023-09-27 18:09:15,044:INFO:  Epoch 403/500:  train Loss: 72.3605   val Loss: 70.2356   time: 0.26s   best: 68.7252
2023-09-27 18:09:15,304:INFO:  Epoch 404/500:  train Loss: 72.2477   val Loss: 74.3140   time: 0.25s   best: 68.7252
2023-09-27 18:09:15,585:INFO:  Epoch 405/500:  train Loss: 73.7913   val Loss: 74.5607   time: 0.27s   best: 68.7252
2023-09-27 18:09:15,841:INFO:  Epoch 406/500:  train Loss: 73.2879   val Loss: 73.7917   time: 0.24s   best: 68.7252
2023-09-27 18:09:16,116:INFO:  Epoch 407/500:  train Loss: 72.2198   val Loss: 76.6077   time: 0.26s   best: 68.7252
2023-09-27 18:09:16,372:INFO:  Epoch 408/500:  train Loss: 74.5472   val Loss: 70.6996   time: 0.24s   best: 68.7252
2023-09-27 18:09:16,642:INFO:  Epoch 409/500:  train Loss: 71.7705   val Loss: 73.1766   time: 0.26s   best: 68.7252
2023-09-27 18:09:16,898:INFO:  Epoch 410/500:  train Loss: 72.5373   val Loss: 69.9928   time: 0.24s   best: 68.7252
2023-09-27 18:09:17,170:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:09:17,201:INFO:  Epoch 411/500:  train Loss: 70.5232   val Loss: 68.4289   time: 0.27s   best: 68.4289
2023-09-27 18:09:17,450:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:09:17,563:INFO:  Epoch 412/500:  train Loss: 68.3672   val Loss: 67.8682   time: 0.24s   best: 67.8682
2023-09-27 18:09:17,813:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:09:17,859:INFO:  Epoch 413/500:  train Loss: 68.2888   val Loss: 66.3651   time: 0.24s   best: 66.3651
2023-09-27 18:09:18,121:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:09:18,155:INFO:  Epoch 414/500:  train Loss: 67.6138   val Loss: 66.3557   time: 0.26s   best: 66.3557
2023-09-27 18:09:18,410:INFO:  Epoch 415/500:  train Loss: 67.4017   val Loss: 66.6888   time: 0.24s   best: 66.3557
2023-09-27 18:09:18,675:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_8a47.pt
2023-09-27 18:09:18,817:INFO:  Epoch 416/500:  train Loss: 66.5430   val Loss: 65.5169   time: 0.26s   best: 65.5169
2023-09-27 18:09:19,086:INFO:  Epoch 417/500:  train Loss: 66.4285   val Loss: 67.9069   time: 0.26s   best: 65.5169
2023-09-27 18:09:19,345:INFO:  Epoch 418/500:  train Loss: 69.0908   val Loss: 69.4100   time: 0.25s   best: 65.5169
2023-09-27 18:09:19,627:INFO:  Epoch 419/500:  train Loss: 71.2020   val Loss: 79.8667   time: 0.27s   best: 65.5169
2023-09-27 18:09:19,885:INFO:  Epoch 420/500:  train Loss: 85.9202   val Loss: 85.2228   time: 0.24s   best: 65.5169
2023-09-27 18:09:20,162:INFO:  Epoch 421/500:  train Loss: 83.6329   val Loss: 83.1032   time: 0.27s   best: 65.5169
2023-09-27 18:09:20,417:INFO:  Epoch 422/500:  train Loss: 83.4094   val Loss: 81.3056   time: 0.24s   best: 65.5169
2023-09-27 18:09:20,690:INFO:  Epoch 423/500:  train Loss: 80.4088   val Loss: 77.7569   time: 0.26s   best: 65.5169
2023-09-27 18:09:20,948:INFO:  Epoch 424/500:  train Loss: 76.7426   val Loss: 74.4182   time: 0.24s   best: 65.5169
2023-09-27 18:09:21,225:INFO:  Epoch 425/500:  train Loss: 75.6415   val Loss: 75.8071   time: 0.26s   best: 65.5169
2023-09-27 18:09:21,482:INFO:  Epoch 426/500:  train Loss: 76.2628   val Loss: 77.0071   time: 0.24s   best: 65.5169
2023-09-27 18:09:21,763:INFO:  Epoch 427/500:  train Loss: 77.0646   val Loss: 74.2648   time: 0.27s   best: 65.5169
2023-09-27 18:09:22,018:INFO:  Epoch 428/500:  train Loss: 74.0311   val Loss: 73.1510   time: 0.24s   best: 65.5169
2023-09-27 18:09:22,292:INFO:  Epoch 429/500:  train Loss: 72.7534   val Loss: 69.3064   time: 0.26s   best: 65.5169
2023-09-27 18:09:22,547:INFO:  Epoch 430/500:  train Loss: 70.9550   val Loss: 71.2002   time: 0.24s   best: 65.5169
2023-09-27 18:09:22,824:INFO:  Epoch 431/500:  train Loss: 72.0787   val Loss: 71.6916   time: 0.26s   best: 65.5169
2023-09-27 18:09:23,079:INFO:  Epoch 432/500:  train Loss: 71.0521   val Loss: 70.3604   time: 0.24s   best: 65.5169
2023-09-27 18:09:23,356:INFO:  Epoch 433/500:  train Loss: 71.1611   val Loss: 70.6520   time: 0.26s   best: 65.5169
2023-09-27 18:09:23,623:INFO:  Epoch 434/500:  train Loss: 70.7868   val Loss: 70.7650   time: 0.25s   best: 65.5169
2023-09-27 18:09:23,899:INFO:  Epoch 435/500:  train Loss: 70.2108   val Loss: 71.0084   time: 0.26s   best: 65.5169
2023-09-27 18:09:24,157:INFO:  Epoch 436/500:  train Loss: 70.4041   val Loss: 72.2805   time: 0.24s   best: 65.5169
2023-09-27 18:09:24,429:INFO:  Epoch 437/500:  train Loss: 71.0003   val Loss: 71.2339   time: 0.26s   best: 65.5169
2023-09-27 18:09:24,686:INFO:  Epoch 438/500:  train Loss: 72.1253   val Loss: 76.4979   time: 0.24s   best: 65.5169
2023-09-27 18:09:24,960:INFO:  Epoch 439/500:  train Loss: 75.0403   val Loss: 71.4138   time: 0.26s   best: 65.5169
2023-09-27 18:09:25,220:INFO:  Epoch 440/500:  train Loss: 72.0513   val Loss: 70.5929   time: 0.25s   best: 65.5169
2023-09-27 18:09:25,495:INFO:  Epoch 441/500:  train Loss: 71.0021   val Loss: 69.6257   time: 0.26s   best: 65.5169
2023-09-27 18:09:25,759:INFO:  Epoch 442/500:  train Loss: 69.9138   val Loss: 68.6735   time: 0.25s   best: 65.5169
2023-09-27 18:09:26,033:INFO:  Epoch 443/500:  train Loss: 70.1197   val Loss: 70.2941   time: 0.26s   best: 65.5169
2023-09-27 18:09:26,289:INFO:  Epoch 444/500:  train Loss: 70.2402   val Loss: 71.7926   time: 0.24s   best: 65.5169
2023-09-27 18:09:26,563:INFO:  Epoch 445/500:  train Loss: 70.3838   val Loss: 69.8043   time: 0.26s   best: 65.5169
2023-09-27 18:09:26,820:INFO:  Epoch 446/500:  train Loss: 70.2207   val Loss: 73.5879   time: 0.24s   best: 65.5169
2023-09-27 18:09:27,092:INFO:  Epoch 447/500:  train Loss: 72.7577   val Loss: 71.6900   time: 0.26s   best: 65.5169
2023-09-27 18:09:27,351:INFO:  Epoch 448/500:  train Loss: 71.8248   val Loss: 75.0725   time: 0.25s   best: 65.5169
2023-09-27 18:09:27,634:INFO:  Epoch 449/500:  train Loss: 73.3483   val Loss: 70.9152   time: 0.27s   best: 65.5169
2023-09-27 18:09:27,886:INFO:  Epoch 450/500:  train Loss: 72.7620   val Loss: 72.6121   time: 0.25s   best: 65.5169
2023-09-27 18:09:28,162:INFO:  Epoch 451/500:  train Loss: 72.4299   val Loss: 74.3090   time: 0.26s   best: 65.5169
2023-09-27 18:09:28,417:INFO:  Epoch 452/500:  train Loss: 73.1341   val Loss: 78.8563   time: 0.24s   best: 65.5169
2023-09-27 18:09:28,698:INFO:  Epoch 453/500:  train Loss: 75.6341   val Loss: 71.4992   time: 0.27s   best: 65.5169
2023-09-27 18:09:28,955:INFO:  Epoch 454/500:  train Loss: 72.6148   val Loss: 70.8978   time: 0.24s   best: 65.5169
2023-09-27 18:09:29,233:INFO:  Epoch 455/500:  train Loss: 72.1038   val Loss: 71.5118   time: 0.27s   best: 65.5169
2023-09-27 18:09:29,492:INFO:  Epoch 456/500:  train Loss: 71.1010   val Loss: 71.0184   time: 0.24s   best: 65.5169
2023-09-27 18:09:29,773:INFO:  Epoch 457/500:  train Loss: 70.9657   val Loss: 70.0885   time: 0.27s   best: 65.5169
2023-09-27 18:09:30,033:INFO:  Epoch 458/500:  train Loss: 70.4310   val Loss: 73.8599   time: 0.26s   best: 65.5169
2023-09-27 18:09:30,307:INFO:  Epoch 459/500:  train Loss: 75.6371   val Loss: 72.7601   time: 0.26s   best: 65.5169
2023-09-27 18:09:30,569:INFO:  Epoch 460/500:  train Loss: 70.4512   val Loss: 70.8829   time: 0.24s   best: 65.5169
2023-09-27 18:09:30,839:INFO:  Epoch 461/500:  train Loss: 71.7672   val Loss: 70.8966   time: 0.26s   best: 65.5169
2023-09-27 18:09:31,107:INFO:  Epoch 462/500:  train Loss: 70.4717   val Loss: 68.3251   time: 0.24s   best: 65.5169
2023-09-27 18:09:31,372:INFO:  Epoch 463/500:  train Loss: 68.3034   val Loss: 67.8306   time: 0.25s   best: 65.5169
2023-09-27 18:09:31,655:INFO:  Epoch 464/500:  train Loss: 68.1811   val Loss: 68.5556   time: 0.27s   best: 65.5169
2023-09-27 18:09:31,913:INFO:  Epoch 465/500:  train Loss: 68.8181   val Loss: 70.0329   time: 0.25s   best: 65.5169
2023-09-27 18:09:32,187:INFO:  Epoch 466/500:  train Loss: 69.0094   val Loss: 69.8357   time: 0.26s   best: 65.5169
2023-09-27 18:09:32,443:INFO:  Epoch 467/500:  train Loss: 69.8909   val Loss: 73.0998   time: 0.24s   best: 65.5169
2023-09-27 18:09:32,718:INFO:  Epoch 468/500:  train Loss: 72.8272   val Loss: 70.2945   time: 0.26s   best: 65.5169
2023-09-27 18:09:32,975:INFO:  Epoch 469/500:  train Loss: 69.1271   val Loss: 69.5024   time: 0.24s   best: 65.5169
2023-09-27 18:09:33,252:INFO:  Epoch 470/500:  train Loss: 68.9976   val Loss: 67.5855   time: 0.26s   best: 65.5169
2023-09-27 18:09:33,510:INFO:  Epoch 471/500:  train Loss: 68.7592   val Loss: 70.9213   time: 0.25s   best: 65.5169
2023-09-27 18:09:33,821:INFO:  Epoch 472/500:  train Loss: 70.1464   val Loss: 71.4567   time: 0.30s   best: 65.5169
2023-09-27 18:09:34,079:INFO:  Epoch 473/500:  train Loss: 69.3817   val Loss: 74.6119   time: 0.24s   best: 65.5169
2023-09-27 18:09:34,353:INFO:  Epoch 474/500:  train Loss: 72.2734   val Loss: 67.1870   time: 0.26s   best: 65.5169
2023-09-27 18:09:34,609:INFO:  Epoch 475/500:  train Loss: 69.3378   val Loss: 71.4023   time: 0.24s   best: 65.5169
2023-09-27 18:09:34,884:INFO:  Epoch 476/500:  train Loss: 71.2356   val Loss: 71.9709   time: 0.26s   best: 65.5169
2023-09-27 18:09:35,140:INFO:  Epoch 477/500:  train Loss: 69.9481   val Loss: 69.5742   time: 0.24s   best: 65.5169
2023-09-27 18:09:35,415:INFO:  Epoch 478/500:  train Loss: 68.9839   val Loss: 65.9910   time: 0.26s   best: 65.5169
2023-09-27 18:09:35,678:INFO:  Epoch 479/500:  train Loss: 67.5228   val Loss: 68.5947   time: 0.25s   best: 65.5169
2023-09-27 18:09:35,952:INFO:  Epoch 480/500:  train Loss: 68.1740   val Loss: 73.4579   time: 0.26s   best: 65.5169
2023-09-27 18:09:36,211:INFO:  Epoch 481/500:  train Loss: 71.6002   val Loss: 76.4649   time: 0.25s   best: 65.5169
2023-09-27 18:09:36,483:INFO:  Epoch 482/500:  train Loss: 75.7700   val Loss: 67.5292   time: 0.26s   best: 65.5169
2023-09-27 18:09:36,741:INFO:  Epoch 483/500:  train Loss: 69.2495   val Loss: 68.1952   time: 0.24s   best: 65.5169
2023-09-27 18:09:37,015:INFO:  Epoch 484/500:  train Loss: 68.4825   val Loss: 67.7633   time: 0.26s   best: 65.5169
2023-09-27 18:09:37,275:INFO:  Epoch 485/500:  train Loss: 68.5098   val Loss: 66.7329   time: 0.25s   best: 65.5169
2023-09-27 18:09:37,549:INFO:  Epoch 486/500:  train Loss: 66.8334   val Loss: 66.0922   time: 0.26s   best: 65.5169
2023-09-27 18:09:37,813:INFO:  Epoch 487/500:  train Loss: 69.3453   val Loss: 73.3105   time: 0.25s   best: 65.5169
2023-09-27 18:09:38,088:INFO:  Epoch 488/500:  train Loss: 75.7282   val Loss: 75.1608   time: 0.26s   best: 65.5169
2023-09-27 18:09:38,344:INFO:  Epoch 489/500:  train Loss: 72.8094   val Loss: 70.5199   time: 0.24s   best: 65.5169
2023-09-27 18:09:38,618:INFO:  Epoch 490/500:  train Loss: 69.8665   val Loss: 71.5087   time: 0.26s   best: 65.5169
2023-09-27 18:09:38,876:INFO:  Epoch 491/500:  train Loss: 71.7769   val Loss: 73.1133   time: 0.25s   best: 65.5169
2023-09-27 18:09:39,151:INFO:  Epoch 492/500:  train Loss: 70.4886   val Loss: 75.4475   time: 0.26s   best: 65.5169
2023-09-27 18:09:39,410:INFO:  Epoch 493/500:  train Loss: 71.9985   val Loss: 67.8113   time: 0.25s   best: 65.5169
2023-09-27 18:09:39,690:INFO:  Epoch 494/500:  train Loss: 69.5709   val Loss: 70.0356   time: 0.27s   best: 65.5169
2023-09-27 18:09:39,947:INFO:  Epoch 495/500:  train Loss: 69.7117   val Loss: 70.8152   time: 0.24s   best: 65.5169
2023-09-27 18:09:40,226:INFO:  Epoch 496/500:  train Loss: 71.1310   val Loss: 71.7379   time: 0.27s   best: 65.5169
2023-09-27 18:09:40,482:INFO:  Epoch 497/500:  train Loss: 71.7512   val Loss: 73.3595   time: 0.24s   best: 65.5169
2023-09-27 18:09:40,755:INFO:  Epoch 498/500:  train Loss: 71.0693   val Loss: 71.5434   time: 0.26s   best: 65.5169
2023-09-27 18:09:41,016:INFO:  Epoch 499/500:  train Loss: 70.0027   val Loss: 71.0080   time: 0.25s   best: 65.5169
2023-09-27 18:09:41,338:INFO:  Epoch 500/500:  train Loss: 70.3552   val Loss: 69.7556   time: 0.31s   best: 65.5169
2023-09-27 18:09:41,338:INFO:  -----> Training complete in 2m 29s   best validation loss: 65.5169
 
2023-09-27 19:26:59,571:INFO:  Starting experiment lstm autoencoder debug
2023-09-27 19:26:59,584:INFO:  Defining the model
2023-09-27 19:26:59,680:INFO:  Reading the dataset
2023-09-27 19:27:13,011:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:13,035:INFO:  Epoch 1/500:  train Loss: 98.5553   val Loss: 97.5132   time: 4.54s   best: 97.5132
2023-09-27 19:27:13,285:INFO:  Epoch 2/500:  train Loss: 98.7849   val Loss: 97.8214   time: 0.25s   best: 97.5132
2023-09-27 19:27:13,555:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:13,648:INFO:  Epoch 3/500:  train Loss: 98.5063   val Loss: 97.3139   time: 0.26s   best: 97.3139
2023-09-27 19:27:13,915:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:13,939:INFO:  Epoch 4/500:  train Loss: 98.6117   val Loss: 96.6314   time: 0.26s   best: 96.6314
2023-09-27 19:27:14,189:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:14,274:INFO:  Epoch 5/500:  train Loss: 98.7271   val Loss: 96.5897   time: 0.24s   best: 96.5897
2023-09-27 19:27:14,537:INFO:  Epoch 6/500:  train Loss: 98.5063   val Loss: 99.2469   time: 0.26s   best: 96.5897
2023-09-27 19:27:14,800:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:14,885:INFO:  Epoch 7/500:  train Loss: 97.7477   val Loss: 96.2048   time: 0.26s   best: 96.2048
2023-09-27 19:27:15,151:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:15,229:INFO:  Epoch 8/500:  train Loss: 97.4906   val Loss: 95.3214   time: 0.26s   best: 95.3214
2023-09-27 19:27:15,499:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:15,606:INFO:  Epoch 9/500:  train Loss: 97.1154   val Loss: 95.1605   time: 0.26s   best: 95.1605
2023-09-27 19:27:15,855:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:15,944:INFO:  Epoch 10/500:  train Loss: 95.8790   val Loss: 94.5720   time: 0.24s   best: 94.5720
2023-09-27 19:27:16,206:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:16,257:INFO:  Epoch 11/500:  train Loss: 95.1115   val Loss: 93.9316   time: 0.26s   best: 93.9316
2023-09-27 19:27:16,525:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:16,628:INFO:  Epoch 12/500:  train Loss: 94.1593   val Loss: 91.4133   time: 0.26s   best: 91.4133
2023-09-27 19:27:16,890:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:17,004:INFO:  Epoch 13/500:  train Loss: 94.7127   val Loss: 91.0178   time: 0.26s   best: 91.0178
2023-09-27 19:27:17,266:INFO:  Epoch 14/500:  train Loss: 93.7978   val Loss: 93.4672   time: 0.26s   best: 91.0178
2023-09-27 19:27:17,518:INFO:  Epoch 15/500:  train Loss: 94.0320   val Loss: 94.5239   time: 0.25s   best: 91.0178
2023-09-27 19:27:17,790:INFO:  Epoch 16/500:  train Loss: 94.0590   val Loss: 94.3719   time: 0.25s   best: 91.0178
2023-09-27 19:27:18,039:INFO:  Epoch 17/500:  train Loss: 93.7462   val Loss: 93.7244   time: 0.24s   best: 91.0178
2023-09-27 19:27:18,305:INFO:  Epoch 18/500:  train Loss: 93.6391   val Loss: 92.5707   time: 0.26s   best: 91.0178
2023-09-27 19:27:18,553:INFO:  Epoch 19/500:  train Loss: 93.8667   val Loss: 94.1964   time: 0.24s   best: 91.0178
2023-09-27 19:27:18,833:INFO:  Epoch 20/500:  train Loss: 93.8788   val Loss: 94.3904   time: 0.28s   best: 91.0178
2023-09-27 19:27:19,080:INFO:  Epoch 21/500:  train Loss: 93.9865   val Loss: 94.5150   time: 0.24s   best: 91.0178
2023-09-27 19:27:19,350:INFO:  Epoch 22/500:  train Loss: 94.0944   val Loss: 94.5089   time: 0.26s   best: 91.0178
2023-09-27 19:27:19,601:INFO:  Epoch 23/500:  train Loss: 94.0596   val Loss: 94.3770   time: 0.25s   best: 91.0178
2023-09-27 19:27:19,874:INFO:  Epoch 24/500:  train Loss: 94.1004   val Loss: 94.2528   time: 0.27s   best: 91.0178
2023-09-27 19:27:20,124:INFO:  Epoch 25/500:  train Loss: 94.1405   val Loss: 94.2318   time: 0.25s   best: 91.0178
2023-09-27 19:27:20,392:INFO:  Epoch 26/500:  train Loss: 94.1619   val Loss: 94.2658   time: 0.26s   best: 91.0178
2023-09-27 19:27:20,641:INFO:  Epoch 27/500:  train Loss: 94.1772   val Loss: 94.3481   time: 0.25s   best: 91.0178
2023-09-27 19:27:20,921:INFO:  Epoch 28/500:  train Loss: 94.2158   val Loss: 94.2387   time: 0.28s   best: 91.0178
2023-09-27 19:27:21,169:INFO:  Epoch 29/500:  train Loss: 94.0028   val Loss: 94.2012   time: 0.24s   best: 91.0178
2023-09-27 19:27:21,444:INFO:  Epoch 30/500:  train Loss: 93.9353   val Loss: 94.1498   time: 0.27s   best: 91.0178
2023-09-27 19:27:21,693:INFO:  Epoch 31/500:  train Loss: 94.2601   val Loss: 94.1222   time: 0.25s   best: 91.0178
2023-09-27 19:27:21,962:INFO:  Epoch 32/500:  train Loss: 94.0312   val Loss: 94.0817   time: 0.26s   best: 91.0178
2023-09-27 19:27:22,210:INFO:  Epoch 33/500:  train Loss: 93.9017   val Loss: 93.8741   time: 0.24s   best: 91.0178
2023-09-27 19:27:22,477:INFO:  Epoch 34/500:  train Loss: 93.8469   val Loss: 93.7076   time: 0.26s   best: 91.0178
2023-09-27 19:27:22,731:INFO:  Epoch 35/500:  train Loss: 93.3380   val Loss: 93.5818   time: 0.25s   best: 91.0178
2023-09-27 19:27:23,007:INFO:  Epoch 36/500:  train Loss: 93.3505   val Loss: 93.4763   time: 0.27s   best: 91.0178
2023-09-27 19:27:23,254:INFO:  Epoch 37/500:  train Loss: 93.1844   val Loss: 93.3226   time: 0.24s   best: 91.0178
2023-09-27 19:27:23,530:INFO:  Epoch 38/500:  train Loss: 93.1726   val Loss: 93.0318   time: 0.27s   best: 91.0178
2023-09-27 19:27:23,781:INFO:  Epoch 39/500:  train Loss: 92.5432   val Loss: 92.4917   time: 0.24s   best: 91.0178
2023-09-27 19:27:24,048:INFO:  Epoch 40/500:  train Loss: 91.9139   val Loss: 91.9177   time: 0.26s   best: 91.0178
2023-09-27 19:27:24,296:INFO:  Epoch 41/500:  train Loss: 91.1722   val Loss: 91.5311   time: 0.24s   best: 91.0178
2023-09-27 19:27:24,564:INFO:  Epoch 42/500:  train Loss: 91.7679   val Loss: 91.3648   time: 0.26s   best: 91.0178
2023-09-27 19:27:24,826:INFO:  Epoch 43/500:  train Loss: 91.0106   val Loss: 91.3333   time: 0.26s   best: 91.0178
2023-09-27 19:27:25,092:INFO:  Epoch 44/500:  train Loss: 91.2482   val Loss: 91.2115   time: 0.26s   best: 91.0178
2023-09-27 19:27:25,342:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:25,416:INFO:  Epoch 45/500:  train Loss: 91.4052   val Loss: 90.9778   time: 0.24s   best: 90.9778
2023-09-27 19:27:25,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:25,785:INFO:  Epoch 46/500:  train Loss: 91.3435   val Loss: 90.2719   time: 0.24s   best: 90.2719
2023-09-27 19:27:26,048:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:26,091:INFO:  Epoch 47/500:  train Loss: 90.2441   val Loss: 89.4483   time: 0.26s   best: 89.4483
2023-09-27 19:27:26,341:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:26,447:INFO:  Epoch 48/500:  train Loss: 89.2528   val Loss: 88.8520   time: 0.24s   best: 88.8520
2023-09-27 19:27:26,699:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:26,773:INFO:  Epoch 49/500:  train Loss: 88.6301   val Loss: 88.0414   time: 0.25s   best: 88.0414
2023-09-27 19:27:27,039:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:27,095:INFO:  Epoch 50/500:  train Loss: 88.9721   val Loss: 87.6500   time: 0.26s   best: 87.6500
2023-09-27 19:27:27,343:INFO:  Epoch 51/500:  train Loss: 89.2764   val Loss: 89.2795   time: 0.24s   best: 87.6500
2023-09-27 19:27:27,613:INFO:  Epoch 52/500:  train Loss: 89.1051   val Loss: 88.0911   time: 0.27s   best: 87.6500
2023-09-27 19:27:27,865:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:27,911:INFO:  Epoch 53/500:  train Loss: 87.8013   val Loss: 87.1562   time: 0.25s   best: 87.1562
2023-09-27 19:27:28,177:INFO:  Epoch 54/500:  train Loss: 87.5253   val Loss: 87.5782   time: 0.26s   best: 87.1562
2023-09-27 19:27:28,426:INFO:  Epoch 55/500:  train Loss: 88.0863   val Loss: 87.3476   time: 0.24s   best: 87.1562
2023-09-27 19:27:28,695:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:28,818:INFO:  Epoch 56/500:  train Loss: 87.4670   val Loss: 86.7951   time: 0.26s   best: 86.7951
2023-09-27 19:27:29,083:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:29,105:INFO:  Epoch 57/500:  train Loss: 87.2015   val Loss: 86.6032   time: 0.26s   best: 86.6032
2023-09-27 19:27:29,352:INFO:  Epoch 58/500:  train Loss: 88.0727   val Loss: 86.8466   time: 0.24s   best: 86.6032
2023-09-27 19:27:29,625:INFO:  Epoch 59/500:  train Loss: 87.7204   val Loss: 87.3313   time: 0.27s   best: 86.6032
2023-09-27 19:27:29,874:INFO:  Epoch 60/500:  train Loss: 87.6315   val Loss: 87.2295   time: 0.25s   best: 86.6032
2023-09-27 19:27:30,140:INFO:  Epoch 61/500:  train Loss: 87.8214   val Loss: 87.0721   time: 0.26s   best: 86.6032
2023-09-27 19:27:30,391:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:30,518:INFO:  Epoch 62/500:  train Loss: 87.2707   val Loss: 86.3937   time: 0.25s   best: 86.3937
2023-09-27 19:27:30,795:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:30,847:INFO:  Epoch 63/500:  train Loss: 86.8071   val Loss: 86.1565   time: 0.27s   best: 86.1565
2023-09-27 19:27:31,094:INFO:  Epoch 64/500:  train Loss: 86.7503   val Loss: 86.3326   time: 0.24s   best: 86.1565
2023-09-27 19:27:31,358:INFO:  Epoch 65/500:  train Loss: 87.2654   val Loss: 86.6755   time: 0.26s   best: 86.1565
2023-09-27 19:27:31,614:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:31,700:INFO:  Epoch 66/500:  train Loss: 86.4882   val Loss: 85.2600   time: 0.25s   best: 85.2600
2023-09-27 19:27:31,949:INFO:  Epoch 67/500:  train Loss: 86.8280   val Loss: 85.7756   time: 0.24s   best: 85.2600
2023-09-27 19:27:32,212:INFO:  Epoch 68/500:  train Loss: 86.7111   val Loss: 86.5423   time: 0.26s   best: 85.2600
2023-09-27 19:27:32,461:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:32,487:INFO:  Epoch 69/500:  train Loss: 86.2047   val Loss: 85.0879   time: 0.24s   best: 85.0879
2023-09-27 19:27:32,760:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:33,352:INFO:  Epoch 70/500:  train Loss: 85.7955   val Loss: 84.7013   time: 0.27s   best: 84.7013
2023-09-27 19:27:33,607:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:33,892:INFO:  Epoch 71/500:  train Loss: 85.5941   val Loss: 84.3815   time: 0.25s   best: 84.3815
2023-09-27 19:27:34,141:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:34,223:INFO:  Epoch 72/500:  train Loss: 85.2278   val Loss: 84.1750   time: 0.24s   best: 84.1750
2023-09-27 19:27:34,472:INFO:  Epoch 73/500:  train Loss: 86.3717   val Loss: 84.4539   time: 0.25s   best: 84.1750
2023-09-27 19:27:34,744:INFO:  Epoch 74/500:  train Loss: 85.0898   val Loss: 84.6052   time: 0.27s   best: 84.1750
2023-09-27 19:27:35,180:INFO:  Epoch 75/500:  train Loss: 85.2836   val Loss: 84.6305   time: 0.43s   best: 84.1750
2023-09-27 19:27:35,495:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:35,611:INFO:  Epoch 76/500:  train Loss: 85.3846   val Loss: 84.1649   time: 0.31s   best: 84.1649
2023-09-27 19:27:35,874:INFO:  Epoch 77/500:  train Loss: 84.7467   val Loss: 85.2951   time: 0.26s   best: 84.1649
2023-09-27 19:27:36,122:INFO:  Epoch 78/500:  train Loss: 85.2726   val Loss: 84.1807   time: 0.24s   best: 84.1649
2023-09-27 19:27:36,389:INFO:  Epoch 79/500:  train Loss: 84.7476   val Loss: 84.3107   time: 0.26s   best: 84.1649
2023-09-27 19:27:36,637:INFO:  Epoch 80/500:  train Loss: 84.8612   val Loss: 84.2303   time: 0.24s   best: 84.1649
2023-09-27 19:27:36,912:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:36,972:INFO:  Epoch 81/500:  train Loss: 84.3172   val Loss: 82.9830   time: 0.27s   best: 82.9830
2023-09-27 19:27:37,221:INFO:  Epoch 82/500:  train Loss: 85.8394   val Loss: 85.4623   time: 0.24s   best: 82.9830
2023-09-27 19:27:37,496:INFO:  Epoch 83/500:  train Loss: 89.9380   val Loss: 91.7540   time: 0.27s   best: 82.9830
2023-09-27 19:27:37,747:INFO:  Epoch 84/500:  train Loss: 91.8379   val Loss: 90.8418   time: 0.25s   best: 82.9830
2023-09-27 19:27:38,048:INFO:  Epoch 85/500:  train Loss: 89.8309   val Loss: 88.2243   time: 0.30s   best: 82.9830
2023-09-27 19:27:38,296:INFO:  Epoch 86/500:  train Loss: 87.8454   val Loss: 87.6484   time: 0.24s   best: 82.9830
2023-09-27 19:27:38,566:INFO:  Epoch 87/500:  train Loss: 88.1665   val Loss: 87.5911   time: 0.27s   best: 82.9830
2023-09-27 19:27:38,816:INFO:  Epoch 88/500:  train Loss: 87.1248   val Loss: 86.0176   time: 0.25s   best: 82.9830
2023-09-27 19:27:39,062:INFO:  Epoch 89/500:  train Loss: 86.0667   val Loss: 84.7037   time: 0.24s   best: 82.9830
2023-09-27 19:27:39,307:INFO:  Epoch 90/500:  train Loss: 85.0767   val Loss: 84.0284   time: 0.24s   best: 82.9830
2023-09-27 19:27:39,561:INFO:  Epoch 91/500:  train Loss: 84.5534   val Loss: 83.6550   time: 0.25s   best: 82.9830
2023-09-27 19:27:39,807:INFO:  Epoch 92/500:  train Loss: 84.2950   val Loss: 83.3692   time: 0.24s   best: 82.9830
2023-09-27 19:27:40,127:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:40,262:INFO:  Epoch 93/500:  train Loss: 83.8420   val Loss: 82.6012   time: 0.31s   best: 82.6012
2023-09-27 19:27:40,513:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:40,535:INFO:  Epoch 94/500:  train Loss: 83.1253   val Loss: 82.2844   time: 0.25s   best: 82.2844
2023-09-27 19:27:40,804:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:40,943:INFO:  Epoch 95/500:  train Loss: 83.1943   val Loss: 82.2488   time: 0.26s   best: 82.2488
2023-09-27 19:27:41,205:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:41,245:INFO:  Epoch 96/500:  train Loss: 83.3375   val Loss: 82.0534   time: 0.26s   best: 82.0534
2023-09-27 19:27:41,504:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:41,546:INFO:  Epoch 97/500:  train Loss: 82.2426   val Loss: 81.9805   time: 0.25s   best: 81.9805
2023-09-27 19:27:41,814:INFO:  Epoch 98/500:  train Loss: 82.7384   val Loss: 82.7117   time: 0.26s   best: 81.9805
2023-09-27 19:27:42,064:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:42,135:INFO:  Epoch 99/500:  train Loss: 83.3787   val Loss: 81.6879   time: 0.25s   best: 81.6879
2023-09-27 19:27:42,507:INFO:  Epoch 100/500:  train Loss: 82.1899   val Loss: 81.7265   time: 0.36s   best: 81.6879
2023-09-27 19:27:42,786:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:42,872:INFO:  Epoch 101/500:  train Loss: 82.1720   val Loss: 81.1138   time: 0.27s   best: 81.1138
2023-09-27 19:27:43,208:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:43,272:INFO:  Epoch 102/500:  train Loss: 82.8779   val Loss: 81.0185   time: 0.24s   best: 81.0185
2023-09-27 19:27:43,540:INFO:  Epoch 103/500:  train Loss: 81.9558   val Loss: 81.3602   time: 0.26s   best: 81.0185
2023-09-27 19:27:43,811:INFO:  Epoch 104/500:  train Loss: 82.0349   val Loss: 81.1929   time: 0.26s   best: 81.0185
2023-09-27 19:27:44,068:INFO:  Epoch 105/500:  train Loss: 81.7676   val Loss: 81.3220   time: 0.24s   best: 81.0185
2023-09-27 19:27:44,343:INFO:  Epoch 106/500:  train Loss: 82.1317   val Loss: 81.1173   time: 0.26s   best: 81.0185
2023-09-27 19:27:44,600:INFO:  Epoch 107/500:  train Loss: 81.5220   val Loss: 81.1032   time: 0.24s   best: 81.0185
2023-09-27 19:27:44,874:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:44,897:INFO:  Epoch 108/500:  train Loss: 81.9552   val Loss: 80.8013   time: 0.27s   best: 80.8013
2023-09-27 19:27:45,161:INFO:  Epoch 109/500:  train Loss: 81.4289   val Loss: 80.8579   time: 0.25s   best: 80.8013
2023-09-27 19:27:45,437:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:45,639:INFO:  Epoch 110/500:  train Loss: 81.4049   val Loss: 80.3022   time: 0.27s   best: 80.3022
2023-09-27 19:27:45,909:INFO:  Epoch 111/500:  train Loss: 81.8499   val Loss: 81.2726   time: 0.26s   best: 80.3022
2023-09-27 19:27:46,159:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:46,206:INFO:  Epoch 112/500:  train Loss: 82.2835   val Loss: 80.2319   time: 0.24s   best: 80.2319
2023-09-27 19:27:46,481:INFO:  Epoch 113/500:  train Loss: 81.0077   val Loss: 80.4926   time: 0.26s   best: 80.2319
2023-09-27 19:27:46,743:INFO:  Epoch 114/500:  train Loss: 80.8239   val Loss: 80.2627   time: 0.25s   best: 80.2319
2023-09-27 19:27:47,021:INFO:  Epoch 115/500:  train Loss: 81.3237   val Loss: 80.2501   time: 0.27s   best: 80.2319
2023-09-27 19:27:47,278:INFO:  Epoch 116/500:  train Loss: 81.0027   val Loss: 80.3102   time: 0.24s   best: 80.2319
2023-09-27 19:27:47,558:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:47,692:INFO:  Epoch 117/500:  train Loss: 80.8617   val Loss: 79.9768   time: 0.28s   best: 79.9768
2023-09-27 19:27:47,954:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:47,995:INFO:  Epoch 118/500:  train Loss: 80.9323   val Loss: 79.8527   time: 0.26s   best: 79.8527
2023-09-27 19:27:48,247:INFO:  Epoch 119/500:  train Loss: 80.8530   val Loss: 80.1135   time: 0.24s   best: 79.8527
2023-09-27 19:27:48,528:INFO:  Epoch 120/500:  train Loss: 80.5696   val Loss: 80.1855   time: 0.27s   best: 79.8527
2023-09-27 19:27:48,789:INFO:  Epoch 121/500:  train Loss: 80.6838   val Loss: 80.0642   time: 0.25s   best: 79.8527
2023-09-27 19:27:49,066:INFO:  Epoch 122/500:  train Loss: 81.9758   val Loss: 81.3985   time: 0.26s   best: 79.8527
2023-09-27 19:27:49,323:INFO:  Epoch 123/500:  train Loss: 81.4764   val Loss: 80.4793   time: 0.24s   best: 79.8527
2023-09-27 19:27:49,603:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:49,773:INFO:  Epoch 124/500:  train Loss: 81.3924   val Loss: 79.8034   time: 0.27s   best: 79.8034
2023-09-27 19:27:50,035:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:50,177:INFO:  Epoch 125/500:  train Loss: 80.0904   val Loss: 78.8432   time: 0.26s   best: 78.8432
2023-09-27 19:27:50,426:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:50,583:INFO:  Epoch 126/500:  train Loss: 79.5133   val Loss: 78.2400   time: 0.24s   best: 78.2400
2023-09-27 19:27:50,838:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:50,903:INFO:  Epoch 127/500:  train Loss: 80.1933   val Loss: 78.2088   time: 0.25s   best: 78.2088
2023-09-27 19:27:51,172:INFO:  Epoch 128/500:  train Loss: 79.1591   val Loss: 78.8177   time: 0.26s   best: 78.2088
2023-09-27 19:27:51,437:INFO:  Epoch 129/500:  train Loss: 79.6190   val Loss: 78.6508   time: 0.25s   best: 78.2088
2023-09-27 19:27:51,717:INFO:  Epoch 130/500:  train Loss: 79.0171   val Loss: 78.7878   time: 0.27s   best: 78.2088
2023-09-27 19:27:51,973:INFO:  Epoch 131/500:  train Loss: 79.1127   val Loss: 78.2906   time: 0.24s   best: 78.2088
2023-09-27 19:27:52,242:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:52,315:INFO:  Epoch 132/500:  train Loss: 79.5525   val Loss: 77.4271   time: 0.26s   best: 77.4271
2023-09-27 19:27:52,589:INFO:  Epoch 133/500:  train Loss: 78.4302   val Loss: 78.0356   time: 0.26s   best: 77.4271
2023-09-27 19:27:52,853:INFO:  Epoch 134/500:  train Loss: 79.6049   val Loss: 79.5887   time: 0.25s   best: 77.4271
2023-09-27 19:27:53,128:INFO:  Epoch 135/500:  train Loss: 79.7511   val Loss: 78.5722   time: 0.26s   best: 77.4271
2023-09-27 19:27:53,385:INFO:  Epoch 136/500:  train Loss: 79.2105   val Loss: 78.0075   time: 0.24s   best: 77.4271
2023-09-27 19:27:53,660:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:53,715:INFO:  Epoch 137/500:  train Loss: 78.6956   val Loss: 76.9515   time: 0.27s   best: 76.9515
2023-09-27 19:27:53,980:INFO:  Epoch 138/500:  train Loss: 79.1828   val Loss: 77.2582   time: 0.25s   best: 76.9515
2023-09-27 19:27:54,256:INFO:  Epoch 139/500:  train Loss: 77.8409   val Loss: 77.1247   time: 0.26s   best: 76.9515
2023-09-27 19:27:54,515:INFO:  Epoch 140/500:  train Loss: 78.1491   val Loss: 77.1664   time: 0.25s   best: 76.9515
2023-09-27 19:27:54,788:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:54,821:INFO:  Epoch 141/500:  train Loss: 78.1916   val Loss: 76.8754   time: 0.27s   best: 76.8754
2023-09-27 19:27:55,073:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:55,237:INFO:  Epoch 142/500:  train Loss: 78.2767   val Loss: 76.8245   time: 0.25s   best: 76.8245
2023-09-27 19:27:55,503:INFO:  Epoch 143/500:  train Loss: 77.4551   val Loss: 76.8387   time: 0.25s   best: 76.8245
2023-09-27 19:27:55,774:INFO:  Epoch 144/500:  train Loss: 78.2658   val Loss: 77.0715   time: 0.26s   best: 76.8245
2023-09-27 19:27:56,024:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:56,060:INFO:  Epoch 145/500:  train Loss: 78.0249   val Loss: 76.5326   time: 0.24s   best: 76.5326
2023-09-27 19:27:56,328:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:56,455:INFO:  Epoch 146/500:  train Loss: 78.4997   val Loss: 76.0896   time: 0.26s   best: 76.0896
2023-09-27 19:27:56,727:INFO:  Epoch 147/500:  train Loss: 77.7400   val Loss: 76.4614   time: 0.26s   best: 76.0896
2023-09-27 19:27:56,990:INFO:  Epoch 148/500:  train Loss: 77.8639   val Loss: 76.2913   time: 0.25s   best: 76.0896
2023-09-27 19:27:57,265:INFO:  Epoch 149/500:  train Loss: 77.3883   val Loss: 76.3042   time: 0.26s   best: 76.0896
2023-09-27 19:27:57,532:INFO:  Epoch 150/500:  train Loss: 77.3891   val Loss: 76.1098   time: 0.25s   best: 76.0896
2023-09-27 19:27:57,808:INFO:  Epoch 151/500:  train Loss: 77.9536   val Loss: 76.1167   time: 0.26s   best: 76.0896
2023-09-27 19:27:58,065:INFO:  Epoch 152/500:  train Loss: 80.1271   val Loss: 78.8016   time: 0.24s   best: 76.0896
2023-09-27 19:27:58,340:INFO:  Epoch 153/500:  train Loss: 82.4779   val Loss: 84.3888   time: 0.26s   best: 76.0896
2023-09-27 19:27:58,597:INFO:  Epoch 154/500:  train Loss: 82.1759   val Loss: 78.8033   time: 0.24s   best: 76.0896
2023-09-27 19:27:58,878:INFO:  Epoch 155/500:  train Loss: 81.1025   val Loss: 80.6853   time: 0.27s   best: 76.0896
2023-09-27 19:27:59,133:INFO:  Epoch 156/500:  train Loss: 79.2224   val Loss: 77.8351   time: 0.24s   best: 76.0896
2023-09-27 19:27:59,401:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:59,470:INFO:  Epoch 157/500:  train Loss: 77.3276   val Loss: 75.9799   time: 0.26s   best: 75.9799
2023-09-27 19:27:59,721:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:27:59,805:INFO:  Epoch 158/500:  train Loss: 78.7751   val Loss: 75.6713   time: 0.25s   best: 75.6713
2023-09-27 19:28:00,071:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:00,134:INFO:  Epoch 159/500:  train Loss: 76.5056   val Loss: 75.5005   time: 0.26s   best: 75.5005
2023-09-27 19:28:00,401:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:00,476:INFO:  Epoch 160/500:  train Loss: 76.4826   val Loss: 75.4280   time: 0.26s   best: 75.4280
2023-09-27 19:28:00,727:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:00,768:INFO:  Epoch 161/500:  train Loss: 76.8632   val Loss: 75.0531   time: 0.25s   best: 75.0531
2023-09-27 19:28:01,034:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:01,117:INFO:  Epoch 162/500:  train Loss: 76.6282   val Loss: 74.5359   time: 0.26s   best: 74.5359
2023-09-27 19:28:01,387:INFO:  Epoch 163/500:  train Loss: 76.2639   val Loss: 74.9806   time: 0.24s   best: 74.5359
2023-09-27 19:28:01,657:INFO:  Epoch 164/500:  train Loss: 76.4911   val Loss: 75.4174   time: 0.26s   best: 74.5359
2023-09-27 19:28:01,932:INFO:  Epoch 165/500:  train Loss: 76.2180   val Loss: 74.8738   time: 0.26s   best: 74.5359
2023-09-27 19:28:02,183:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:02,256:INFO:  Epoch 166/500:  train Loss: 75.3926   val Loss: 73.7921   time: 0.25s   best: 73.7921
2023-09-27 19:28:02,528:INFO:  Epoch 167/500:  train Loss: 77.4619   val Loss: 76.9177   time: 0.26s   best: 73.7921
2023-09-27 19:28:02,789:INFO:  Epoch 168/500:  train Loss: 77.8128   val Loss: 76.2128   time: 0.25s   best: 73.7921
2023-09-27 19:28:03,059:INFO:  Epoch 169/500:  train Loss: 77.2442   val Loss: 75.9137   time: 0.26s   best: 73.7921
2023-09-27 19:28:03,323:INFO:  Epoch 170/500:  train Loss: 76.3276   val Loss: 77.1013   time: 0.25s   best: 73.7921
2023-09-27 19:28:03,609:INFO:  Epoch 171/500:  train Loss: 78.5800   val Loss: 77.4721   time: 0.27s   best: 73.7921
2023-09-27 19:28:03,866:INFO:  Epoch 172/500:  train Loss: 78.4961   val Loss: 77.8159   time: 0.25s   best: 73.7921
2023-09-27 19:28:04,142:INFO:  Epoch 173/500:  train Loss: 77.3661   val Loss: 75.9551   time: 0.26s   best: 73.7921
2023-09-27 19:28:04,399:INFO:  Epoch 174/500:  train Loss: 76.2821   val Loss: 74.4513   time: 0.24s   best: 73.7921
2023-09-27 19:28:04,668:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:04,718:INFO:  Epoch 175/500:  train Loss: 75.7176   val Loss: 73.6209   time: 0.26s   best: 73.6209
2023-09-27 19:28:04,986:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:05,020:INFO:  Epoch 176/500:  train Loss: 75.0956   val Loss: 73.2877   time: 0.26s   best: 73.2877
2023-09-27 19:28:05,287:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:05,390:INFO:  Epoch 177/500:  train Loss: 74.6845   val Loss: 72.9117   time: 0.26s   best: 72.9117
2023-09-27 19:28:05,849:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:05,905:INFO:  Epoch 178/500:  train Loss: 74.4798   val Loss: 72.5966   time: 0.45s   best: 72.5966
2023-09-27 19:28:06,180:INFO:  Epoch 179/500:  train Loss: 74.9125   val Loss: 73.2751   time: 0.26s   best: 72.5966
2023-09-27 19:28:06,436:INFO:  Epoch 180/500:  train Loss: 74.7821   val Loss: 73.4312   time: 0.24s   best: 72.5966
2023-09-27 19:28:06,712:INFO:  Epoch 181/500:  train Loss: 74.4507   val Loss: 73.0040   time: 0.26s   best: 72.5966
2023-09-27 19:28:06,976:INFO:  Epoch 182/500:  train Loss: 74.0219   val Loss: 72.7342   time: 0.25s   best: 72.5966
2023-09-27 19:28:07,250:INFO:  Epoch 183/500:  train Loss: 73.9344   val Loss: 72.7157   time: 0.26s   best: 72.5966
2023-09-27 19:28:07,508:INFO:  Epoch 184/500:  train Loss: 73.5856   val Loss: 72.7948   time: 0.25s   best: 72.5966
2023-09-27 19:28:07,785:INFO:  Epoch 185/500:  train Loss: 74.5141   val Loss: 73.2477   time: 0.26s   best: 72.5966
2023-09-27 19:28:08,048:INFO:  Epoch 186/500:  train Loss: 74.0034   val Loss: 73.1825   time: 0.25s   best: 72.5966
2023-09-27 19:28:08,322:INFO:  Epoch 187/500:  train Loss: 77.6968   val Loss: 76.9083   time: 0.26s   best: 72.5966
2023-09-27 19:28:08,578:INFO:  Epoch 188/500:  train Loss: 75.8887   val Loss: 78.7706   time: 0.25s   best: 72.5966
2023-09-27 19:28:08,845:INFO:  Epoch 189/500:  train Loss: 78.3615   val Loss: 75.6762   time: 0.27s   best: 72.5966
2023-09-27 19:28:09,115:INFO:  Epoch 190/500:  train Loss: 79.0605   val Loss: 76.0847   time: 0.26s   best: 72.5966
2023-09-27 19:28:09,399:INFO:  Epoch 191/500:  train Loss: 76.3638   val Loss: 75.1236   time: 0.27s   best: 72.5966
2023-09-27 19:28:09,657:INFO:  Epoch 192/500:  train Loss: 75.1478   val Loss: 73.9062   time: 0.25s   best: 72.5966
2023-09-27 19:28:09,935:INFO:  Epoch 193/500:  train Loss: 74.2446   val Loss: 72.8672   time: 0.27s   best: 72.5966
2023-09-27 19:28:10,190:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:10,263:INFO:  Epoch 194/500:  train Loss: 73.3470   val Loss: 72.2604   time: 0.25s   best: 72.2604
2023-09-27 19:28:10,537:INFO:  Epoch 195/500:  train Loss: 73.4542   val Loss: 72.4011   time: 0.26s   best: 72.2604
2023-09-27 19:28:10,799:INFO:  Epoch 196/500:  train Loss: 73.5260   val Loss: 72.2681   time: 0.26s   best: 72.2604
2023-09-27 19:28:11,100:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:11,192:INFO:  Epoch 197/500:  train Loss: 73.2610   val Loss: 71.9019   time: 0.30s   best: 71.9019
2023-09-27 19:28:11,462:INFO:  Epoch 198/500:  train Loss: 73.0560   val Loss: 72.4324   time: 0.26s   best: 71.9019
2023-09-27 19:28:11,721:INFO:  Epoch 199/500:  train Loss: 73.1916   val Loss: 72.4031   time: 0.25s   best: 71.9019
2023-09-27 19:28:12,039:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:12,116:INFO:  Epoch 200/500:  train Loss: 73.1079   val Loss: 71.7777   time: 0.31s   best: 71.7777
2023-09-27 19:28:12,393:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:12,556:INFO:  Epoch 201/500:  train Loss: 72.7079   val Loss: 71.1171   time: 0.27s   best: 71.1171
2023-09-27 19:28:12,818:INFO:  Epoch 202/500:  train Loss: 72.3334   val Loss: 71.4733   time: 0.25s   best: 71.1171
2023-09-27 19:28:13,089:INFO:  Epoch 203/500:  train Loss: 74.0604   val Loss: 71.8914   time: 0.26s   best: 71.1171
2023-09-27 19:28:13,339:INFO:  Epoch 204/500:  train Loss: 72.3518   val Loss: 71.6297   time: 0.24s   best: 71.1171
2023-09-27 19:28:13,626:INFO:  Epoch 205/500:  train Loss: 71.8439   val Loss: 71.4872   time: 0.27s   best: 71.1171
2023-09-27 19:28:13,884:INFO:  Epoch 206/500:  train Loss: 72.8099   val Loss: 72.2708   time: 0.25s   best: 71.1171
2023-09-27 19:28:14,163:INFO:  Epoch 207/500:  train Loss: 74.2179   val Loss: 71.6697   time: 0.27s   best: 71.1171
2023-09-27 19:28:14,421:INFO:  Epoch 208/500:  train Loss: 72.7212   val Loss: 71.3952   time: 0.24s   best: 71.1171
2023-09-27 19:28:14,688:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:14,717:INFO:  Epoch 209/500:  train Loss: 71.6581   val Loss: 69.8796   time: 0.26s   best: 69.8796
2023-09-27 19:28:14,997:INFO:  Epoch 210/500:  train Loss: 71.9105   val Loss: 70.0747   time: 0.27s   best: 69.8796
2023-09-27 19:28:15,254:INFO:  Epoch 211/500:  train Loss: 71.8266   val Loss: 71.4048   time: 0.24s   best: 69.8796
2023-09-27 19:28:15,530:INFO:  Epoch 212/500:  train Loss: 73.5897   val Loss: 72.0848   time: 0.26s   best: 69.8796
2023-09-27 19:28:15,787:INFO:  Epoch 213/500:  train Loss: 72.6353   val Loss: 71.4524   time: 0.24s   best: 69.8796
2023-09-27 19:28:16,068:INFO:  Epoch 214/500:  train Loss: 72.3734   val Loss: 73.6474   time: 0.27s   best: 69.8796
2023-09-27 19:28:16,323:INFO:  Epoch 215/500:  train Loss: 73.6790   val Loss: 74.5578   time: 0.24s   best: 69.8796
2023-09-27 19:28:16,599:INFO:  Epoch 216/500:  train Loss: 74.3278   val Loss: 72.2270   time: 0.26s   best: 69.8796
2023-09-27 19:28:16,860:INFO:  Epoch 217/500:  train Loss: 72.9811   val Loss: 71.1149   time: 0.25s   best: 69.8796
2023-09-27 19:28:17,136:INFO:  Epoch 218/500:  train Loss: 71.5095   val Loss: 70.3650   time: 0.26s   best: 69.8796
2023-09-27 19:28:17,391:INFO:  Epoch 219/500:  train Loss: 71.4162   val Loss: 70.8536   time: 0.24s   best: 69.8796
2023-09-27 19:28:17,667:INFO:  Epoch 220/500:  train Loss: 71.7104   val Loss: 70.7847   time: 0.26s   best: 69.8796
2023-09-27 19:28:17,918:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:18,120:INFO:  Epoch 221/500:  train Loss: 71.0461   val Loss: 69.7700   time: 0.25s   best: 69.7700
2023-09-27 19:28:18,369:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:18,482:INFO:  Epoch 222/500:  train Loss: 71.2609   val Loss: 69.3378   time: 0.24s   best: 69.3378
2023-09-27 19:28:18,752:INFO:  Epoch 223/500:  train Loss: 70.4816   val Loss: 69.8804   time: 0.26s   best: 69.3378
2023-09-27 19:28:19,016:INFO:  Epoch 224/500:  train Loss: 71.4353   val Loss: 69.7851   time: 0.25s   best: 69.3378
2023-09-27 19:28:19,283:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:19,343:INFO:  Epoch 225/500:  train Loss: 70.4373   val Loss: 68.7911   time: 0.26s   best: 68.7911
2023-09-27 19:28:19,602:INFO:  Epoch 226/500:  train Loss: 70.3377   val Loss: 69.5619   time: 0.25s   best: 68.7911
2023-09-27 19:28:19,879:INFO:  Epoch 227/500:  train Loss: 71.4471   val Loss: 69.7682   time: 0.26s   best: 68.7911
2023-09-27 19:28:20,137:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:20,173:INFO:  Epoch 228/500:  train Loss: 71.5109   val Loss: 68.4820   time: 0.25s   best: 68.4820
2023-09-27 19:28:20,434:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:20,593:INFO:  Epoch 229/500:  train Loss: 69.8755   val Loss: 68.3926   time: 0.26s   best: 68.3926
2023-09-27 19:28:20,868:INFO:  Epoch 230/500:  train Loss: 69.6527   val Loss: 69.0072   time: 0.26s   best: 68.3926
2023-09-27 19:28:21,126:INFO:  Epoch 231/500:  train Loss: 73.5353   val Loss: 79.5826   time: 0.25s   best: 68.3926
2023-09-27 19:28:21,400:INFO:  Epoch 232/500:  train Loss: 82.1911   val Loss: 78.1118   time: 0.26s   best: 68.3926
2023-09-27 19:28:21,658:INFO:  Epoch 233/500:  train Loss: 81.5272   val Loss: 83.8978   time: 0.25s   best: 68.3926
2023-09-27 19:28:21,936:INFO:  Epoch 234/500:  train Loss: 79.8382   val Loss: 77.4732   time: 0.26s   best: 68.3926
2023-09-27 19:28:22,199:INFO:  Epoch 235/500:  train Loss: 77.6057   val Loss: 74.4440   time: 0.25s   best: 68.3926
2023-09-27 19:28:22,474:INFO:  Epoch 236/500:  train Loss: 75.7759   val Loss: 73.6635   time: 0.26s   best: 68.3926
2023-09-27 19:28:22,731:INFO:  Epoch 237/500:  train Loss: 72.5041   val Loss: 71.8642   time: 0.24s   best: 68.3926
2023-09-27 19:28:23,013:INFO:  Epoch 238/500:  train Loss: 72.2035   val Loss: 70.6373   time: 0.27s   best: 68.3926
2023-09-27 19:28:23,269:INFO:  Epoch 239/500:  train Loss: 71.1632   val Loss: 70.8594   time: 0.24s   best: 68.3926
2023-09-27 19:28:23,545:INFO:  Epoch 240/500:  train Loss: 70.8378   val Loss: 69.9256   time: 0.27s   best: 68.3926
2023-09-27 19:28:23,810:INFO:  Epoch 241/500:  train Loss: 70.8132   val Loss: 69.6739   time: 0.25s   best: 68.3926
2023-09-27 19:28:24,095:INFO:  Epoch 242/500:  train Loss: 70.6344   val Loss: 69.3679   time: 0.27s   best: 68.3926
2023-09-27 19:28:24,366:INFO:  Epoch 243/500:  train Loss: 70.8384   val Loss: 68.5655   time: 0.24s   best: 68.3926
2023-09-27 19:28:24,628:INFO:  Epoch 244/500:  train Loss: 69.9621   val Loss: 68.9801   time: 0.25s   best: 68.3926
2023-09-27 19:28:24,896:INFO:  Epoch 245/500:  train Loss: 69.9258   val Loss: 68.6013   time: 0.25s   best: 68.3926
2023-09-27 19:28:25,167:INFO:  Epoch 246/500:  train Loss: 69.9116   val Loss: 68.6694   time: 0.26s   best: 68.3926
2023-09-27 19:28:25,437:INFO:  Epoch 247/500:  train Loss: 70.0114   val Loss: 68.9539   time: 0.24s   best: 68.3926
2023-09-27 19:28:25,702:INFO:  Epoch 248/500:  train Loss: 70.0069   val Loss: 69.7074   time: 0.25s   best: 68.3926
2023-09-27 19:28:25,967:INFO:  Epoch 249/500:  train Loss: 70.1452   val Loss: 69.1184   time: 0.25s   best: 68.3926
2023-09-27 19:28:26,246:INFO:  Epoch 250/500:  train Loss: 70.4277   val Loss: 69.4617   time: 0.27s   best: 68.3926
2023-09-27 19:28:26,521:INFO:  Epoch 251/500:  train Loss: 71.1018   val Loss: 69.7307   time: 0.26s   best: 68.3926
2023-09-27 19:28:26,783:INFO:  Epoch 252/500:  train Loss: 70.7392   val Loss: 69.5745   time: 0.25s   best: 68.3926
2023-09-27 19:28:27,053:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:27,084:INFO:  Epoch 253/500:  train Loss: 69.7543   val Loss: 68.1545   time: 0.26s   best: 68.1545
2023-09-27 19:28:27,333:INFO:  Epoch 254/500:  train Loss: 69.5132   val Loss: 68.6203   time: 0.24s   best: 68.1545
2023-09-27 19:28:27,616:INFO:  Epoch 255/500:  train Loss: 69.6102   val Loss: 68.5683   time: 0.27s   best: 68.1545
2023-09-27 19:28:27,875:INFO:  Epoch 256/500:  train Loss: 69.5599   val Loss: 68.4299   time: 0.25s   best: 68.1545
2023-09-27 19:28:28,159:INFO:  Epoch 257/500:  train Loss: 69.4002   val Loss: 68.9825   time: 0.27s   best: 68.1545
2023-09-27 19:28:28,415:INFO:  Epoch 258/500:  train Loss: 70.1026   val Loss: 70.6109   time: 0.24s   best: 68.1545
2023-09-27 19:28:28,684:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:28,809:INFO:  Epoch 259/500:  train Loss: 69.9799   val Loss: 67.9453   time: 0.26s   best: 67.9453
2023-09-27 19:28:29,070:INFO:  Epoch 260/500:  train Loss: 69.7372   val Loss: 68.8413   time: 0.25s   best: 67.9453
2023-09-27 19:28:29,342:INFO:  Epoch 261/500:  train Loss: 70.4804   val Loss: 71.3064   time: 0.26s   best: 67.9453
2023-09-27 19:28:29,601:INFO:  Epoch 262/500:  train Loss: 71.5100   val Loss: 70.8062   time: 0.25s   best: 67.9453
2023-09-27 19:28:29,878:INFO:  Epoch 263/500:  train Loss: 70.9604   val Loss: 70.0064   time: 0.25s   best: 67.9453
2023-09-27 19:28:30,143:INFO:  Epoch 264/500:  train Loss: 71.3415   val Loss: 68.2855   time: 0.26s   best: 67.9453
2023-09-27 19:28:30,410:INFO:  Epoch 265/500:  train Loss: 69.3869   val Loss: 68.0062   time: 0.26s   best: 67.9453
2023-09-27 19:28:30,667:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:30,771:INFO:  Epoch 266/500:  train Loss: 68.9733   val Loss: 67.7574   time: 0.25s   best: 67.7574
2023-09-27 19:28:31,033:INFO:  Epoch 267/500:  train Loss: 69.0640   val Loss: 68.3541   time: 0.25s   best: 67.7574
2023-09-27 19:28:31,297:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:31,373:INFO:  Epoch 268/500:  train Loss: 68.5517   val Loss: 67.4663   time: 0.26s   best: 67.4663
2023-09-27 19:28:31,633:INFO:  Epoch 269/500:  train Loss: 68.9438   val Loss: 68.5687   time: 0.25s   best: 67.4663
2023-09-27 19:28:31,909:INFO:  Epoch 270/500:  train Loss: 69.8463   val Loss: 71.9693   time: 0.26s   best: 67.4663
2023-09-27 19:28:32,174:INFO:  Epoch 271/500:  train Loss: 72.4275   val Loss: 71.7985   time: 0.25s   best: 67.4663
2023-09-27 19:28:32,449:INFO:  Epoch 272/500:  train Loss: 71.0972   val Loss: 71.8092   time: 0.26s   best: 67.4663
2023-09-27 19:28:32,707:INFO:  Epoch 273/500:  train Loss: 69.7652   val Loss: 67.6728   time: 0.25s   best: 67.4663
2023-09-27 19:28:32,983:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:33,071:INFO:  Epoch 274/500:  train Loss: 68.3894   val Loss: 67.4635   time: 0.27s   best: 67.4635
2023-09-27 19:28:33,335:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:33,412:INFO:  Epoch 275/500:  train Loss: 68.2951   val Loss: 67.3554   time: 0.26s   best: 67.3554
2023-09-27 19:28:33,668:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:33,728:INFO:  Epoch 276/500:  train Loss: 68.2873   val Loss: 66.9999   time: 0.25s   best: 66.9999
2023-09-27 19:28:34,000:INFO:  Epoch 277/500:  train Loss: 68.9346   val Loss: 67.6859   time: 0.26s   best: 66.9999
2023-09-27 19:28:34,260:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:34,347:INFO:  Epoch 278/500:  train Loss: 68.2511   val Loss: 66.3881   time: 0.25s   best: 66.3881
2023-09-27 19:28:34,597:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:34,656:INFO:  Epoch 279/500:  train Loss: 67.7999   val Loss: 65.8186   time: 0.24s   best: 65.8186
2023-09-27 19:28:34,938:INFO:  Epoch 280/500:  train Loss: 67.5429   val Loss: 66.4734   time: 0.27s   best: 65.8186
2023-09-27 19:28:35,195:INFO:  Epoch 281/500:  train Loss: 68.0334   val Loss: 65.9902   time: 0.25s   best: 65.8186
2023-09-27 19:28:35,471:INFO:  Epoch 282/500:  train Loss: 67.6036   val Loss: 66.4056   time: 0.26s   best: 65.8186
2023-09-27 19:28:35,729:INFO:  Epoch 283/500:  train Loss: 67.8048   val Loss: 66.7893   time: 0.25s   best: 65.8186
2023-09-27 19:28:36,006:INFO:  Epoch 284/500:  train Loss: 68.2476   val Loss: 66.6661   time: 0.26s   best: 65.8186
2023-09-27 19:28:36,449:INFO:  Epoch 285/500:  train Loss: 67.7235   val Loss: 68.6133   time: 0.44s   best: 65.8186
2023-09-27 19:28:36,761:INFO:  Epoch 286/500:  train Loss: 68.2948   val Loss: 68.0993   time: 0.30s   best: 65.8186
2023-09-27 19:28:37,042:INFO:  Epoch 287/500:  train Loss: 68.1218   val Loss: 68.0837   time: 0.27s   best: 65.8186
2023-09-27 19:28:37,299:INFO:  Epoch 288/500:  train Loss: 68.9649   val Loss: 66.9743   time: 0.24s   best: 65.8186
2023-09-27 19:28:37,576:INFO:  Epoch 289/500:  train Loss: 68.4120   val Loss: 68.7643   time: 0.26s   best: 65.8186
2023-09-27 19:28:37,833:INFO:  Epoch 290/500:  train Loss: 68.2091   val Loss: 67.3699   time: 0.24s   best: 65.8186
2023-09-27 19:28:38,109:INFO:  Epoch 291/500:  train Loss: 69.1205   val Loss: 70.3574   time: 0.26s   best: 65.8186
2023-09-27 19:28:38,366:INFO:  Epoch 292/500:  train Loss: 69.5445   val Loss: 68.1478   time: 0.24s   best: 65.8186
2023-09-27 19:28:38,647:INFO:  Epoch 293/500:  train Loss: 68.7720   val Loss: 69.5689   time: 0.27s   best: 65.8186
2023-09-27 19:28:38,916:INFO:  Epoch 294/500:  train Loss: 68.8846   val Loss: 68.5938   time: 0.26s   best: 65.8186
2023-09-27 19:28:39,192:INFO:  Epoch 295/500:  train Loss: 68.9356   val Loss: 67.7529   time: 0.26s   best: 65.8186
2023-09-27 19:28:39,441:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:39,492:INFO:  Epoch 296/500:  train Loss: 68.9417   val Loss: 65.7646   time: 0.24s   best: 65.7646
2023-09-27 19:28:39,768:INFO:  Epoch 297/500:  train Loss: 67.2544   val Loss: 66.6581   time: 0.26s   best: 65.7646
2023-09-27 19:28:40,026:INFO:  Epoch 298/500:  train Loss: 67.6132   val Loss: 66.1742   time: 0.25s   best: 65.7646
2023-09-27 19:28:40,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:40,429:INFO:  Epoch 299/500:  train Loss: 66.8420   val Loss: 65.3184   time: 0.26s   best: 65.3184
2023-09-27 19:28:40,743:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:40,836:INFO:  Epoch 300/500:  train Loss: 66.6717   val Loss: 64.7737   time: 0.31s   best: 64.7737
2023-09-27 19:28:41,114:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:28:41,229:INFO:  Epoch 301/500:  train Loss: 67.5641   val Loss: 64.5272   time: 0.27s   best: 64.5272
2023-09-27 19:28:41,486:INFO:  Epoch 302/500:  train Loss: 65.9068   val Loss: 64.6119   time: 0.24s   best: 64.5272
2023-09-27 19:28:41,758:INFO:  Epoch 303/500:  train Loss: 66.7658   val Loss: 65.8604   time: 0.26s   best: 64.5272
2023-09-27 19:28:42,015:INFO:  Epoch 304/500:  train Loss: 67.8242   val Loss: 66.4005   time: 0.24s   best: 64.5272
2023-09-27 19:28:42,291:INFO:  Epoch 305/500:  train Loss: 67.2278   val Loss: 66.3789   time: 0.26s   best: 64.5272
2023-09-27 19:28:42,548:INFO:  Epoch 306/500:  train Loss: 67.8899   val Loss: 67.3273   time: 0.24s   best: 64.5272
2023-09-27 19:28:42,835:INFO:  Epoch 307/500:  train Loss: 69.1281   val Loss: 71.6266   time: 0.27s   best: 64.5272
2023-09-27 19:28:43,094:INFO:  Epoch 308/500:  train Loss: 77.0474   val Loss: 78.9456   time: 0.25s   best: 64.5272
2023-09-27 19:28:43,367:INFO:  Epoch 309/500:  train Loss: 80.6557   val Loss: 80.1818   time: 0.26s   best: 64.5272
2023-09-27 19:28:43,628:INFO:  Epoch 310/500:  train Loss: 80.4463   val Loss: 78.9683   time: 0.25s   best: 64.5272
2023-09-27 19:28:43,905:INFO:  Epoch 311/500:  train Loss: 77.6461   val Loss: 75.2964   time: 0.27s   best: 64.5272
2023-09-27 19:28:44,163:INFO:  Epoch 312/500:  train Loss: 75.3253   val Loss: 74.3022   time: 0.24s   best: 64.5272
2023-09-27 19:28:44,438:INFO:  Epoch 313/500:  train Loss: 73.3990   val Loss: 71.3148   time: 0.26s   best: 64.5272
2023-09-27 19:28:44,702:INFO:  Epoch 314/500:  train Loss: 71.7234   val Loss: 72.5986   time: 0.25s   best: 64.5272
2023-09-27 19:28:44,983:INFO:  Epoch 315/500:  train Loss: 71.3361   val Loss: 70.6842   time: 0.27s   best: 64.5272
2023-09-27 19:28:45,240:INFO:  Epoch 316/500:  train Loss: 70.9289   val Loss: 70.7175   time: 0.24s   best: 64.5272
2023-09-27 19:28:45,515:INFO:  Epoch 317/500:  train Loss: 69.6682   val Loss: 68.4179   time: 0.26s   best: 64.5272
2023-09-27 19:28:45,773:INFO:  Epoch 318/500:  train Loss: 70.9222   val Loss: 73.8153   time: 0.25s   best: 64.5272
2023-09-27 19:28:46,049:INFO:  Epoch 319/500:  train Loss: 71.6210   val Loss: 74.8387   time: 0.26s   best: 64.5272
2023-09-27 19:28:46,306:INFO:  Epoch 320/500:  train Loss: 75.4255   val Loss: 78.7712   time: 0.24s   best: 64.5272
2023-09-27 19:28:46,580:INFO:  Epoch 321/500:  train Loss: 79.7233   val Loss: 75.7941   time: 0.26s   best: 64.5272
2023-09-27 19:28:46,862:INFO:  Epoch 322/500:  train Loss: 75.2953   val Loss: 75.1843   time: 0.25s   best: 64.5272
2023-09-27 19:28:47,126:INFO:  Epoch 323/500:  train Loss: 74.0886   val Loss: 74.3285   time: 0.25s   best: 64.5272
2023-09-27 19:28:47,396:INFO:  Epoch 324/500:  train Loss: 73.3352   val Loss: 70.2701   time: 0.24s   best: 64.5272
2023-09-27 19:28:47,659:INFO:  Epoch 325/500:  train Loss: 71.0135   val Loss: 69.7602   time: 0.25s   best: 64.5272
2023-09-27 19:28:47,924:INFO:  Epoch 326/500:  train Loss: 72.5934   val Loss: 70.3632   time: 0.25s   best: 64.5272
2023-09-27 19:28:48,194:INFO:  Epoch 327/500:  train Loss: 73.1476   val Loss: 74.8963   time: 0.26s   best: 64.5272
2023-09-27 19:28:48,464:INFO:  Epoch 328/500:  train Loss: 73.0299   val Loss: 72.9196   time: 0.24s   best: 64.5272
2023-09-27 19:28:48,732:INFO:  Epoch 329/500:  train Loss: 71.6083   val Loss: 69.9510   time: 0.26s   best: 64.5272
2023-09-27 19:28:49,012:INFO:  Epoch 330/500:  train Loss: 69.8395   val Loss: 68.7460   time: 0.27s   best: 64.5272
2023-09-27 19:28:49,270:INFO:  Epoch 331/500:  train Loss: 70.5606   val Loss: 70.3406   time: 0.24s   best: 64.5272
2023-09-27 19:28:49,544:INFO:  Epoch 332/500:  train Loss: 69.5254   val Loss: 68.9358   time: 0.26s   best: 64.5272
2023-09-27 19:28:49,803:INFO:  Epoch 333/500:  train Loss: 69.6903   val Loss: 67.6281   time: 0.25s   best: 64.5272
2023-09-27 19:28:50,080:INFO:  Epoch 334/500:  train Loss: 68.0428   val Loss: 67.9470   time: 0.26s   best: 64.5272
2023-09-27 19:28:50,336:INFO:  Epoch 335/500:  train Loss: 68.7425   val Loss: 67.5443   time: 0.24s   best: 64.5272
2023-09-27 19:28:50,611:INFO:  Epoch 336/500:  train Loss: 68.5436   val Loss: 68.4939   time: 0.26s   best: 64.5272
2023-09-27 19:28:50,879:INFO:  Epoch 337/500:  train Loss: 68.6162   val Loss: 67.6830   time: 0.25s   best: 64.5272
2023-09-27 19:28:51,155:INFO:  Epoch 338/500:  train Loss: 69.1068   val Loss: 70.4763   time: 0.26s   best: 64.5272
2023-09-27 19:28:51,411:INFO:  Epoch 339/500:  train Loss: 69.0551   val Loss: 69.5253   time: 0.24s   best: 64.5272
2023-09-27 19:28:51,687:INFO:  Epoch 340/500:  train Loss: 70.2489   val Loss: 72.8301   time: 0.26s   best: 64.5272
2023-09-27 19:28:51,946:INFO:  Epoch 341/500:  train Loss: 69.9129   val Loss: 67.5093   time: 0.25s   best: 64.5272
2023-09-27 19:28:52,222:INFO:  Epoch 342/500:  train Loss: 68.9174   val Loss: 70.2947   time: 0.26s   best: 64.5272
2023-09-27 19:28:52,478:INFO:  Epoch 343/500:  train Loss: 70.0832   val Loss: 77.2072   time: 0.24s   best: 64.5272
2023-09-27 19:28:52,762:INFO:  Epoch 344/500:  train Loss: 72.8470   val Loss: 69.9045   time: 0.27s   best: 64.5272
2023-09-27 19:28:53,026:INFO:  Epoch 345/500:  train Loss: 70.1531   val Loss: 69.6224   time: 0.25s   best: 64.5272
2023-09-27 19:28:53,301:INFO:  Epoch 346/500:  train Loss: 69.1532   val Loss: 69.4357   time: 0.26s   best: 64.5272
2023-09-27 19:28:53,556:INFO:  Epoch 347/500:  train Loss: 68.8836   val Loss: 66.4695   time: 0.25s   best: 64.5272
2023-09-27 19:28:53,832:INFO:  Epoch 348/500:  train Loss: 67.2199   val Loss: 67.6041   time: 0.26s   best: 64.5272
2023-09-27 19:28:54,090:INFO:  Epoch 349/500:  train Loss: 67.9375   val Loss: 67.4169   time: 0.25s   best: 64.5272
2023-09-27 19:28:54,369:INFO:  Epoch 350/500:  train Loss: 67.7245   val Loss: 67.0255   time: 0.27s   best: 64.5272
2023-09-27 19:28:54,633:INFO:  Epoch 351/500:  train Loss: 67.3738   val Loss: 66.7962   time: 0.25s   best: 64.5272
2023-09-27 19:28:54,923:INFO:  Epoch 352/500:  train Loss: 68.3166   val Loss: 70.3135   time: 0.28s   best: 64.5272
2023-09-27 19:28:55,181:INFO:  Epoch 353/500:  train Loss: 69.7189   val Loss: 68.2186   time: 0.25s   best: 64.5272
2023-09-27 19:28:55,455:INFO:  Epoch 354/500:  train Loss: 67.3202   val Loss: 67.7608   time: 0.26s   best: 64.5272
2023-09-27 19:28:55,714:INFO:  Epoch 355/500:  train Loss: 67.9671   val Loss: 67.1738   time: 0.25s   best: 64.5272
2023-09-27 19:28:55,993:INFO:  Epoch 356/500:  train Loss: 68.6725   val Loss: 69.1350   time: 0.27s   best: 64.5272
2023-09-27 19:28:56,249:INFO:  Epoch 357/500:  train Loss: 69.2233   val Loss: 67.6665   time: 0.24s   best: 64.5272
2023-09-27 19:28:56,524:INFO:  Epoch 358/500:  train Loss: 66.9447   val Loss: 66.6328   time: 0.26s   best: 64.5272
2023-09-27 19:28:56,790:INFO:  Epoch 359/500:  train Loss: 67.0833   val Loss: 64.5301   time: 0.25s   best: 64.5272
2023-09-27 19:28:57,071:INFO:  Epoch 360/500:  train Loss: 66.8452   val Loss: 67.1452   time: 0.27s   best: 64.5272
2023-09-27 19:28:57,328:INFO:  Epoch 361/500:  train Loss: 66.9726   val Loss: 66.1021   time: 0.24s   best: 64.5272
2023-09-27 19:28:57,604:INFO:  Epoch 362/500:  train Loss: 65.8496   val Loss: 64.7236   time: 0.26s   best: 64.5272
2023-09-27 19:28:57,861:INFO:  Epoch 363/500:  train Loss: 67.0350   val Loss: 66.3144   time: 0.24s   best: 64.5272
2023-09-27 19:28:58,138:INFO:  Epoch 364/500:  train Loss: 68.0030   val Loss: 67.2555   time: 0.26s   best: 64.5272
2023-09-27 19:28:58,402:INFO:  Epoch 365/500:  train Loss: 66.8850   val Loss: 65.1892   time: 0.26s   best: 64.5272
2023-09-27 19:28:58,679:INFO:  Epoch 366/500:  train Loss: 66.0651   val Loss: 65.0537   time: 0.26s   best: 64.5272
2023-09-27 19:28:58,958:INFO:  Epoch 367/500:  train Loss: 68.0563   val Loss: 68.1925   time: 0.27s   best: 64.5272
2023-09-27 19:28:59,217:INFO:  Epoch 368/500:  train Loss: 71.6479   val Loss: 71.4733   time: 0.25s   best: 64.5272
2023-09-27 19:28:59,490:INFO:  Epoch 369/500:  train Loss: 71.8210   val Loss: 70.1017   time: 0.26s   best: 64.5272
2023-09-27 19:28:59,749:INFO:  Epoch 370/500:  train Loss: 70.1090   val Loss: 68.5334   time: 0.25s   best: 64.5272
2023-09-27 19:29:00,026:INFO:  Epoch 371/500:  train Loss: 69.3217   val Loss: 68.1053   time: 0.26s   best: 64.5272
2023-09-27 19:29:00,283:INFO:  Epoch 372/500:  train Loss: 69.0547   val Loss: 67.6569   time: 0.24s   best: 64.5272
2023-09-27 19:29:00,558:INFO:  Epoch 373/500:  train Loss: 68.2923   val Loss: 66.4624   time: 0.26s   best: 64.5272
2023-09-27 19:29:00,825:INFO:  Epoch 374/500:  train Loss: 66.9020   val Loss: 65.6689   time: 0.25s   best: 64.5272
2023-09-27 19:29:01,105:INFO:  Epoch 375/500:  train Loss: 66.8905   val Loss: 65.2952   time: 0.27s   best: 64.5272
2023-09-27 19:29:01,362:INFO:  Epoch 376/500:  train Loss: 67.0139   val Loss: 68.0780   time: 0.24s   best: 64.5272
2023-09-27 19:29:01,637:INFO:  Epoch 377/500:  train Loss: 68.6306   val Loss: 66.1664   time: 0.26s   best: 64.5272
2023-09-27 19:29:01,894:INFO:  Epoch 378/500:  train Loss: 67.2044   val Loss: 66.8063   time: 0.24s   best: 64.5272
2023-09-27 19:29:02,171:INFO:  Epoch 379/500:  train Loss: 68.8970   val Loss: 68.0103   time: 0.26s   best: 64.5272
2023-09-27 19:29:02,427:INFO:  Epoch 380/500:  train Loss: 67.0126   val Loss: 66.8653   time: 0.24s   best: 64.5272
2023-09-27 19:29:02,703:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:29:02,733:INFO:  Epoch 381/500:  train Loss: 68.4149   val Loss: 64.5259   time: 0.27s   best: 64.5259
2023-09-27 19:29:02,996:INFO:  Epoch 382/500:  train Loss: 65.7423   val Loss: 64.8661   time: 0.25s   best: 64.5259
2023-09-27 19:29:03,271:INFO:  Epoch 383/500:  train Loss: 66.3897   val Loss: 65.3251   time: 0.26s   best: 64.5259
2023-09-27 19:29:03,528:INFO:  Epoch 384/500:  train Loss: 65.9548   val Loss: 64.6980   time: 0.24s   best: 64.5259
2023-09-27 19:29:03,808:INFO:  Epoch 385/500:  train Loss: 66.4424   val Loss: 66.5034   time: 0.27s   best: 64.5259
2023-09-27 19:29:04,066:INFO:  Epoch 386/500:  train Loss: 67.1956   val Loss: 66.3010   time: 0.25s   best: 64.5259
2023-09-27 19:29:04,342:INFO:  Epoch 387/500:  train Loss: 67.7525   val Loss: 65.3659   time: 0.26s   best: 64.5259
2023-09-27 19:29:04,600:INFO:  Epoch 388/500:  train Loss: 68.4651   val Loss: 70.3904   time: 0.25s   best: 64.5259
2023-09-27 19:29:04,892:INFO:  Epoch 389/500:  train Loss: 70.1845   val Loss: 66.9042   time: 0.28s   best: 64.5259
2023-09-27 19:29:05,151:INFO:  Epoch 390/500:  train Loss: 67.8434   val Loss: 66.9060   time: 0.25s   best: 64.5259
2023-09-27 19:29:05,427:INFO:  Epoch 391/500:  train Loss: 68.1477   val Loss: 70.6398   time: 0.26s   best: 64.5259
2023-09-27 19:29:05,685:INFO:  Epoch 392/500:  train Loss: 86.7389   val Loss: 95.9926   time: 0.25s   best: 64.5259
2023-09-27 19:29:05,962:INFO:  Epoch 393/500:  train Loss: 95.2614   val Loss: 96.1046   time: 0.26s   best: 64.5259
2023-09-27 19:29:06,219:INFO:  Epoch 394/500:  train Loss: 94.8359   val Loss: 93.3846   time: 0.24s   best: 64.5259
2023-09-27 19:29:06,494:INFO:  Epoch 395/500:  train Loss: 90.8495   val Loss: 90.2828   time: 0.26s   best: 64.5259
2023-09-27 19:29:06,824:INFO:  Epoch 396/500:  train Loss: 89.0795   val Loss: 87.0043   time: 0.32s   best: 64.5259
2023-09-27 19:29:07,254:INFO:  Epoch 397/500:  train Loss: 85.9817   val Loss: 83.9114   time: 0.42s   best: 64.5259
2023-09-27 19:29:07,523:INFO:  Epoch 398/500:  train Loss: 84.3823   val Loss: 81.5249   time: 0.26s   best: 64.5259
2023-09-27 19:29:07,781:INFO:  Epoch 399/500:  train Loss: 80.1338   val Loss: 77.7757   time: 0.25s   best: 64.5259
2023-09-27 19:29:08,104:INFO:  Epoch 400/500:  train Loss: 76.9933   val Loss: 75.1924   time: 0.31s   best: 64.5259
2023-09-27 19:29:08,388:INFO:  Epoch 401/500:  train Loss: 75.4748   val Loss: 74.5552   time: 0.27s   best: 64.5259
2023-09-27 19:29:08,642:INFO:  Epoch 402/500:  train Loss: 76.3435   val Loss: 80.4413   time: 0.25s   best: 64.5259
2023-09-27 19:29:08,929:INFO:  Epoch 403/500:  train Loss: 75.8072   val Loss: 74.4878   time: 0.28s   best: 64.5259
2023-09-27 19:29:09,187:INFO:  Epoch 404/500:  train Loss: 72.0870   val Loss: 69.8555   time: 0.25s   best: 64.5259
2023-09-27 19:29:09,467:INFO:  Epoch 405/500:  train Loss: 70.7930   val Loss: 68.5883   time: 0.27s   best: 64.5259
2023-09-27 19:29:09,726:INFO:  Epoch 406/500:  train Loss: 70.4350   val Loss: 69.9084   time: 0.25s   best: 64.5259
2023-09-27 19:29:10,002:INFO:  Epoch 407/500:  train Loss: 69.9393   val Loss: 68.3322   time: 0.26s   best: 64.5259
2023-09-27 19:29:10,259:INFO:  Epoch 408/500:  train Loss: 68.2695   val Loss: 69.2664   time: 0.24s   best: 64.5259
2023-09-27 19:29:10,534:INFO:  Epoch 409/500:  train Loss: 69.0459   val Loss: 68.3200   time: 0.26s   best: 64.5259
2023-09-27 19:29:10,791:INFO:  Epoch 410/500:  train Loss: 69.3890   val Loss: 67.6230   time: 0.24s   best: 64.5259
2023-09-27 19:29:11,073:INFO:  Epoch 411/500:  train Loss: 69.5524   val Loss: 68.3140   time: 0.27s   best: 64.5259
2023-09-27 19:29:11,338:INFO:  Epoch 412/500:  train Loss: 67.3133   val Loss: 67.1656   time: 0.25s   best: 64.5259
2023-09-27 19:29:11,605:INFO:  Epoch 413/500:  train Loss: 67.2445   val Loss: 66.4853   time: 0.26s   best: 64.5259
2023-09-27 19:29:11,870:INFO:  Epoch 414/500:  train Loss: 66.2240   val Loss: 65.7714   time: 0.25s   best: 64.5259
2023-09-27 19:29:12,140:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:29:12,169:INFO:  Epoch 415/500:  train Loss: 66.4841   val Loss: 63.9629   time: 0.26s   best: 63.9629
2023-09-27 19:29:12,427:INFO:  Epoch 416/500:  train Loss: 65.8351   val Loss: 66.1569   time: 0.24s   best: 63.9629
2023-09-27 19:29:12,701:INFO:  Epoch 417/500:  train Loss: 67.0590   val Loss: 64.5880   time: 0.26s   best: 63.9629
2023-09-27 19:29:12,964:INFO:  Epoch 418/500:  train Loss: 65.6949   val Loss: 67.3777   time: 0.25s   best: 63.9629
2023-09-27 19:29:13,248:INFO:  Epoch 419/500:  train Loss: 67.7645   val Loss: 67.5308   time: 0.26s   best: 63.9629
2023-09-27 19:29:13,505:INFO:  Epoch 420/500:  train Loss: 66.9579   val Loss: 67.9635   time: 0.24s   best: 63.9629
2023-09-27 19:29:13,787:INFO:  Epoch 421/500:  train Loss: 67.5198   val Loss: 66.3076   time: 0.27s   best: 63.9629
2023-09-27 19:29:14,044:INFO:  Epoch 422/500:  train Loss: 65.8649   val Loss: 65.4404   time: 0.24s   best: 63.9629
2023-09-27 19:29:14,320:INFO:  Epoch 423/500:  train Loss: 65.8856   val Loss: 65.0280   time: 0.26s   best: 63.9629
2023-09-27 19:29:14,570:INFO:  Epoch 424/500:  train Loss: 66.9692   val Loss: 66.4308   time: 0.24s   best: 63.9629
2023-09-27 19:29:14,856:INFO:  Epoch 425/500:  train Loss: 66.8936   val Loss: 66.9122   time: 0.27s   best: 63.9629
2023-09-27 19:29:15,115:INFO:  Epoch 426/500:  train Loss: 66.6159   val Loss: 66.6369   time: 0.25s   best: 63.9629
2023-09-27 19:29:15,399:INFO:  Epoch 427/500:  train Loss: 65.3030   val Loss: 65.4717   time: 0.27s   best: 63.9629
2023-09-27 19:29:15,672:INFO:  Epoch 428/500:  train Loss: 65.8206   val Loss: 66.3032   time: 0.24s   best: 63.9629
2023-09-27 19:29:15,935:INFO:  Epoch 429/500:  train Loss: 66.9360   val Loss: 64.6154   time: 0.25s   best: 63.9629
2023-09-27 19:29:16,199:INFO:  Epoch 430/500:  train Loss: 66.6530   val Loss: 67.6806   time: 0.24s   best: 63.9629
2023-09-27 19:29:16,468:INFO:  Epoch 431/500:  train Loss: 70.1531   val Loss: 68.9180   time: 0.26s   best: 63.9629
2023-09-27 19:29:16,738:INFO:  Epoch 432/500:  train Loss: 67.3383   val Loss: 68.0968   time: 0.24s   best: 63.9629
2023-09-27 19:29:17,006:INFO:  Epoch 433/500:  train Loss: 67.2450   val Loss: 67.1036   time: 0.25s   best: 63.9629
2023-09-27 19:29:17,291:INFO:  Epoch 434/500:  train Loss: 67.0458   val Loss: 68.9639   time: 0.27s   best: 63.9629
2023-09-27 19:29:17,541:INFO:  Epoch 435/500:  train Loss: 68.6153   val Loss: 66.3980   time: 0.24s   best: 63.9629
2023-09-27 19:29:17,816:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:29:17,846:INFO:  Epoch 436/500:  train Loss: 64.7201   val Loss: 63.5192   time: 0.27s   best: 63.5192
2023-09-27 19:29:18,104:INFO:  Epoch 437/500:  train Loss: 64.7663   val Loss: 64.9675   time: 0.25s   best: 63.5192
2023-09-27 19:29:18,379:INFO:  Epoch 438/500:  train Loss: 66.1849   val Loss: 68.8902   time: 0.26s   best: 63.5192
2023-09-27 19:29:18,635:INFO:  Epoch 439/500:  train Loss: 68.2073   val Loss: 69.1148   time: 0.24s   best: 63.5192
2023-09-27 19:29:18,915:INFO:  Epoch 440/500:  train Loss: 71.1978   val Loss: 75.5663   time: 0.27s   best: 63.5192
2023-09-27 19:29:19,174:INFO:  Epoch 441/500:  train Loss: 73.4225   val Loss: 70.2295   time: 0.25s   best: 63.5192
2023-09-27 19:29:19,454:INFO:  Epoch 442/500:  train Loss: 71.6003   val Loss: 67.2056   time: 0.27s   best: 63.5192
2023-09-27 19:29:19,712:INFO:  Epoch 443/500:  train Loss: 66.7518   val Loss: 66.4616   time: 0.25s   best: 63.5192
2023-09-27 19:29:19,989:INFO:  Epoch 444/500:  train Loss: 67.9280   val Loss: 65.3456   time: 0.26s   best: 63.5192
2023-09-27 19:29:20,246:INFO:  Epoch 445/500:  train Loss: 65.8228   val Loss: 64.5990   time: 0.24s   best: 63.5192
2023-09-27 19:29:20,515:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:29:20,663:INFO:  Epoch 446/500:  train Loss: 64.6061   val Loss: 62.8831   time: 0.26s   best: 62.8831
2023-09-27 19:29:20,935:INFO:  Epoch 447/500:  train Loss: 64.3480   val Loss: 63.9980   time: 0.26s   best: 62.8831
2023-09-27 19:29:21,198:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:29:21,228:INFO:  Epoch 448/500:  train Loss: 66.9013   val Loss: 62.6788   time: 0.26s   best: 62.6788
2023-09-27 19:29:21,508:INFO:  Epoch 449/500:  train Loss: 66.3111   val Loss: 67.4250   time: 0.27s   best: 62.6788
2023-09-27 19:29:21,766:INFO:  Epoch 450/500:  train Loss: 67.0091   val Loss: 67.8204   time: 0.25s   best: 62.6788
2023-09-27 19:29:22,044:INFO:  Epoch 451/500:  train Loss: 66.9145   val Loss: 67.6111   time: 0.26s   best: 62.6788
2023-09-27 19:29:22,302:INFO:  Epoch 452/500:  train Loss: 73.4729   val Loss: 73.9043   time: 0.25s   best: 62.6788
2023-09-27 19:29:22,577:INFO:  Epoch 453/500:  train Loss: 81.4289   val Loss: 82.9014   time: 0.26s   best: 62.6788
2023-09-27 19:29:22,835:INFO:  Epoch 454/500:  train Loss: 84.5388   val Loss: 85.9561   time: 0.24s   best: 62.6788
2023-09-27 19:29:23,116:INFO:  Epoch 455/500:  train Loss: 85.4683   val Loss: 83.9988   time: 0.27s   best: 62.6788
2023-09-27 19:29:23,378:INFO:  Epoch 456/500:  train Loss: 83.4593   val Loss: 82.4282   time: 0.25s   best: 62.6788
2023-09-27 19:29:23,676:INFO:  Epoch 457/500:  train Loss: 81.9578   val Loss: 80.9290   time: 0.29s   best: 62.6788
2023-09-27 19:29:23,945:INFO:  Epoch 458/500:  train Loss: 80.4766   val Loss: 79.0111   time: 0.26s   best: 62.6788
2023-09-27 19:29:24,226:INFO:  Epoch 459/500:  train Loss: 78.6597   val Loss: 77.0533   time: 0.27s   best: 62.6788
2023-09-27 19:29:24,484:INFO:  Epoch 460/500:  train Loss: 76.8027   val Loss: 74.4179   time: 0.25s   best: 62.6788
2023-09-27 19:29:24,759:INFO:  Epoch 461/500:  train Loss: 74.1519   val Loss: 72.3082   time: 0.26s   best: 62.6788
2023-09-27 19:29:25,021:INFO:  Epoch 462/500:  train Loss: 75.3995   val Loss: 74.6502   time: 0.25s   best: 62.6788
2023-09-27 19:29:25,306:INFO:  Epoch 463/500:  train Loss: 75.8287   val Loss: 74.7688   time: 0.26s   best: 62.6788
2023-09-27 19:29:25,562:INFO:  Epoch 464/500:  train Loss: 73.5031   val Loss: 72.2559   time: 0.24s   best: 62.6788
2023-09-27 19:29:25,839:INFO:  Epoch 465/500:  train Loss: 72.7737   val Loss: 71.3821   time: 0.26s   best: 62.6788
2023-09-27 19:29:26,097:INFO:  Epoch 466/500:  train Loss: 72.7552   val Loss: 75.1151   time: 0.25s   best: 62.6788
2023-09-27 19:29:26,374:INFO:  Epoch 467/500:  train Loss: 76.0956   val Loss: 75.1057   time: 0.26s   best: 62.6788
2023-09-27 19:29:26,631:INFO:  Epoch 468/500:  train Loss: 75.2047   val Loss: 74.2343   time: 0.24s   best: 62.6788
2023-09-27 19:29:26,909:INFO:  Epoch 469/500:  train Loss: 74.3702   val Loss: 71.9123   time: 0.27s   best: 62.6788
2023-09-27 19:29:27,168:INFO:  Epoch 470/500:  train Loss: 70.5830   val Loss: 68.2233   time: 0.25s   best: 62.6788
2023-09-27 19:29:27,451:INFO:  Epoch 471/500:  train Loss: 68.8532   val Loss: 67.2322   time: 0.27s   best: 62.6788
2023-09-27 19:29:27,708:INFO:  Epoch 472/500:  train Loss: 67.4127   val Loss: 66.2928   time: 0.24s   best: 62.6788
2023-09-27 19:29:27,985:INFO:  Epoch 473/500:  train Loss: 68.2609   val Loss: 66.3738   time: 0.26s   best: 62.6788
2023-09-27 19:29:28,242:INFO:  Epoch 474/500:  train Loss: 66.7967   val Loss: 64.7587   time: 0.24s   best: 62.6788
2023-09-27 19:29:28,517:INFO:  Epoch 475/500:  train Loss: 67.0885   val Loss: 66.9582   time: 0.26s   best: 62.6788
2023-09-27 19:29:28,774:INFO:  Epoch 476/500:  train Loss: 67.6638   val Loss: 65.6764   time: 0.24s   best: 62.6788
2023-09-27 19:29:29,056:INFO:  Epoch 477/500:  train Loss: 67.9963   val Loss: 67.0170   time: 0.27s   best: 62.6788
2023-09-27 19:29:29,318:INFO:  Epoch 478/500:  train Loss: 71.2771   val Loss: 69.6804   time: 0.24s   best: 62.6788
2023-09-27 19:29:29,592:INFO:  Epoch 479/500:  train Loss: 69.4987   val Loss: 67.8161   time: 0.26s   best: 62.6788
2023-09-27 19:29:29,851:INFO:  Epoch 480/500:  train Loss: 67.7190   val Loss: 67.2310   time: 0.25s   best: 62.6788
2023-09-27 19:29:30,129:INFO:  Epoch 481/500:  train Loss: 67.5579   val Loss: 65.5627   time: 0.27s   best: 62.6788
2023-09-27 19:29:30,387:INFO:  Epoch 482/500:  train Loss: 66.1325   val Loss: 64.1632   time: 0.25s   best: 62.6788
2023-09-27 19:29:30,663:INFO:  Epoch 483/500:  train Loss: 65.9179   val Loss: 65.6064   time: 0.26s   best: 62.6788
2023-09-27 19:29:30,925:INFO:  Epoch 484/500:  train Loss: 66.6732   val Loss: 68.6055   time: 0.25s   best: 62.6788
2023-09-27 19:29:31,203:INFO:  Epoch 485/500:  train Loss: 70.9558   val Loss: 69.1362   time: 0.27s   best: 62.6788
2023-09-27 19:29:31,468:INFO:  Epoch 486/500:  train Loss: 68.1083   val Loss: 66.6006   time: 0.25s   best: 62.6788
2023-09-27 19:29:31,745:INFO:  Epoch 487/500:  train Loss: 67.7159   val Loss: 64.5341   time: 0.26s   best: 62.6788
2023-09-27 19:29:32,021:INFO:  Epoch 488/500:  train Loss: 66.1464   val Loss: 64.7506   time: 0.25s   best: 62.6788
2023-09-27 19:29:32,280:INFO:  Epoch 489/500:  train Loss: 65.2317   val Loss: 63.3162   time: 0.25s   best: 62.6788
2023-09-27 19:29:32,550:INFO:  Epoch 490/500:  train Loss: 64.6107   val Loss: 63.3499   time: 0.24s   best: 62.6788
2023-09-27 19:29:32,806:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:29:32,836:INFO:  Epoch 491/500:  train Loss: 64.0402   val Loss: 62.4240   time: 0.25s   best: 62.4240
2023-09-27 19:29:33,108:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:29:33,235:INFO:  Epoch 492/500:  train Loss: 63.1061   val Loss: 61.6669   time: 0.27s   best: 61.6669
2023-09-27 19:29:33,498:INFO:  Epoch 493/500:  train Loss: 63.8238   val Loss: 62.1937   time: 0.25s   best: 61.6669
2023-09-27 19:29:33,773:INFO:  Epoch 494/500:  train Loss: 63.7083   val Loss: 62.7584   time: 0.26s   best: 61.6669
2023-09-27 19:29:34,025:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_ 584.pt
2023-09-27 19:29:34,054:INFO:  Epoch 495/500:  train Loss: 63.2861   val Loss: 61.3493   time: 0.25s   best: 61.3493
2023-09-27 19:29:34,324:INFO:  Epoch 496/500:  train Loss: 64.8870   val Loss: 63.2803   time: 0.26s   best: 61.3493
2023-09-27 19:29:34,588:INFO:  Epoch 497/500:  train Loss: 66.0644   val Loss: 64.3478   time: 0.25s   best: 61.3493
2023-09-27 19:29:34,868:INFO:  Epoch 498/500:  train Loss: 65.4279   val Loss: 63.4247   time: 0.27s   best: 61.3493
2023-09-27 19:29:35,132:INFO:  Epoch 499/500:  train Loss: 65.2556   val Loss: 63.0402   time: 0.25s   best: 61.3493
2023-09-27 19:29:35,458:INFO:  Epoch 500/500:  train Loss: 63.6428   val Loss: 61.7692   time: 0.31s   best: 61.3493
2023-09-27 19:29:35,459:INFO:  -----> Training complete in 2m 27s   best validation loss: 61.3493
 
2023-09-28 09:25:38,954:INFO:  Starting experiment lstm autoencoder
2023-09-28 09:25:38,959:INFO:  Defining the model
2023-09-28 09:25:39,365:INFO:  Reading the dataset
2023-09-28 09:59:57,279:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 09:59:57,626:INFO:  Epoch 1/500:  train Loss: 74.1500   val Loss: 68.0449   time: 526.05s   best: 68.0449
2023-09-28 10:08:54,069:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 10:08:54,405:INFO:  Epoch 2/500:  train Loss: 61.0844   val Loss: 56.2247   time: 536.27s   best: 56.2247
2023-09-28 10:17:33,155:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 10:17:33,588:INFO:  Epoch 3/500:  train Loss: 52.7941   val Loss: 50.3974   time: 518.56s   best: 50.3974
2023-09-28 10:26:15,082:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 10:26:15,503:INFO:  Epoch 4/500:  train Loss: 47.1199   val Loss: 45.8262   time: 521.34s   best: 45.8262
2023-09-28 10:34:46,761:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 10:34:47,119:INFO:  Epoch 5/500:  train Loss: 43.3043   val Loss: 42.4120   time: 511.11s   best: 42.4120
2023-09-28 10:43:08,699:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 10:43:09,073:INFO:  Epoch 6/500:  train Loss: 40.5160   val Loss: 42.0525   time: 501.49s   best: 42.0525
2023-09-28 10:51:42,741:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 10:51:43,054:INFO:  Epoch 7/500:  train Loss: 38.4450   val Loss: 38.5032   time: 513.48s   best: 38.5032
2023-09-28 11:00:12,557:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 11:00:13,022:INFO:  Epoch 8/500:  train Loss: 36.8963   val Loss: 37.0451   time: 509.34s   best: 37.0451
2023-09-28 11:08:47,979:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 11:08:48,339:INFO:  Epoch 9/500:  train Loss: 35.5374   val Loss: 36.4533   time: 514.76s   best: 36.4533
2023-09-28 11:17:05,596:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 11:17:08,178:INFO:  Epoch 10/500:  train Loss: 34.5159   val Loss: 34.5902   time: 497.08s   best: 34.5902
2023-09-28 11:26:02,259:INFO:  Epoch 11/500:  train Loss: 33.5767   val Loss: 36.8999   time: 534.08s   best: 34.5902
2023-09-28 11:34:48,817:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 11:34:49,926:INFO:  Epoch 12/500:  train Loss: 33.0455   val Loss: 33.8742   time: 526.31s   best: 33.8742
2023-09-28 11:43:32,401:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 11:43:32,724:INFO:  Epoch 13/500:  train Loss: 32.3636   val Loss: 33.6479   time: 522.30s   best: 33.6479
2023-09-28 11:51:58,964:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 11:51:59,367:INFO:  Epoch 14/500:  train Loss: 31.6863   val Loss: 33.5030   time: 506.07s   best: 33.5030
2023-09-28 12:00:47,240:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 12:00:47,669:INFO:  Epoch 15/500:  train Loss: 31.1952   val Loss: 32.5291   time: 527.65s   best: 32.5291
2023-09-28 12:09:51,908:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 12:09:52,626:INFO:  Epoch 16/500:  train Loss: 30.7311   val Loss: 32.4418   time: 544.10s   best: 32.4418
2023-09-28 12:19:01,986:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 12:19:02,319:INFO:  Epoch 17/500:  train Loss: 30.3113   val Loss: 32.4188   time: 549.20s   best: 32.4188
2023-09-28 12:27:56,819:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 12:27:57,365:INFO:  Epoch 18/500:  train Loss: 30.0816   val Loss: 32.1416   time: 534.19s   best: 32.1416
2023-09-28 12:36:20,439:INFO:  Epoch 19/500:  train Loss: 29.6941   val Loss: 39.0628   time: 503.07s   best: 32.1416
2023-09-28 12:45:11,070:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 12:45:11,481:INFO:  Epoch 20/500:  train Loss: 29.3809   val Loss: 32.1295   time: 530.29s   best: 32.1295
2023-09-28 12:53:50,727:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 12:53:51,047:INFO:  Epoch 21/500:  train Loss: 29.0675   val Loss: 32.0614   time: 519.08s   best: 32.0614
2023-09-28 13:02:08,642:INFO:  Epoch 22/500:  train Loss: 28.7591   val Loss: 33.3282   time: 497.59s   best: 32.0614
2023-09-28 13:09:57,989:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 13:09:58,956:INFO:  Epoch 23/500:  train Loss: 28.5622   val Loss: 31.2121   time: 469.19s   best: 31.2121
2023-09-28 13:17:55,536:INFO:  Epoch 24/500:  train Loss: 28.3478   val Loss: 31.3209   time: 476.58s   best: 31.2121
2023-09-28 13:26:12,551:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 13:26:12,836:INFO:  Epoch 25/500:  train Loss: 28.1311   val Loss: 30.7983   time: 496.87s   best: 30.7983
2023-09-28 13:33:51,855:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 13:33:52,259:INFO:  Epoch 26/500:  train Loss: 27.8534   val Loss: 30.4909   time: 458.83s   best: 30.4909
2023-09-28 13:41:54,493:INFO:  Epoch 27/500:  train Loss: 27.8312   val Loss: 31.1009   time: 482.23s   best: 30.4909
2023-09-28 13:51:55,318:INFO:  Epoch 28/500:  train Loss: 27.4802   val Loss: 30.7758   time: 600.81s   best: 30.4909
2023-09-28 14:00:26,398:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 14:00:26,816:INFO:  Epoch 29/500:  train Loss: 27.3377   val Loss: 30.1769   time: 510.97s   best: 30.1769
2023-09-28 14:08:40,601:INFO:  Epoch 30/500:  train Loss: 27.1528   val Loss: 30.3808   time: 493.78s   best: 30.1769
2023-09-28 14:16:48,299:INFO:  Epoch 31/500:  train Loss: 26.9897   val Loss: 33.3390   time: 487.64s   best: 30.1769
2023-09-28 14:25:25,920:INFO:  Epoch 32/500:  train Loss: 26.8821   val Loss: 30.4187   time: 517.61s   best: 30.1769
2023-09-28 14:35:01,163:INFO:  Epoch 33/500:  train Loss: 26.7099   val Loss: 30.2119   time: 575.15s   best: 30.1769
2023-09-28 14:44:31,998:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 14:44:32,893:INFO:  Epoch 34/500:  train Loss: 26.5588   val Loss: 29.8755   time: 569.72s   best: 29.8755
2023-09-28 14:54:07,260:INFO:  Epoch 35/500:  train Loss: 26.4352   val Loss: 30.5619   time: 574.37s   best: 29.8755
2023-09-28 15:05:10,541:INFO:  Epoch 36/500:  train Loss: 26.2864   val Loss: 30.2853   time: 663.26s   best: 29.8755
2023-09-28 15:15:07,724:INFO:  Epoch 37/500:  train Loss: 26.1521   val Loss: 32.5596   time: 597.16s   best: 29.8755
2023-09-28 15:26:21,149:INFO:  Epoch 38/500:  train Loss: 26.1140   val Loss: 30.0248   time: 673.40s   best: 29.8755
2023-09-28 15:38:25,662:INFO:  Epoch 39/500:  train Loss: 25.9379   val Loss: 30.1757   time: 724.48s   best: 29.8755
2023-09-28 15:48:45,936:INFO:  Epoch 40/500:  train Loss: 25.8834   val Loss: 30.2475   time: 620.23s   best: 29.8755
2023-09-28 16:00:16,770:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 16:00:17,932:INFO:  Epoch 41/500:  train Loss: 25.7876   val Loss: 29.6413   time: 690.39s   best: 29.6413
2023-09-28 16:11:16,928:INFO:  Epoch 42/500:  train Loss: 25.6013   val Loss: 29.7348   time: 658.99s   best: 29.6413
2023-09-28 16:21:37,947:INFO:  Epoch 43/500:  train Loss: 25.5450   val Loss: 30.1247   time: 620.95s   best: 29.6413
2023-09-28 16:29:53,507:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 16:29:57,108:INFO:  Epoch 44/500:  train Loss: 25.5475   val Loss: 29.6308   time: 495.48s   best: 29.6308
2023-09-28 16:40:57,245:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 16:41:00,600:INFO:  Epoch 45/500:  train Loss: 25.3866   val Loss: 29.6214   time: 659.93s   best: 29.6214
2023-09-28 16:54:23,925:INFO:  Epoch 46/500:  train Loss: 25.3230   val Loss: 29.6883   time: 803.32s   best: 29.6214
2023-09-28 17:05:22,285:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 17:05:23,152:INFO:  Epoch 47/500:  train Loss: 25.2095   val Loss: 29.4680   time: 657.84s   best: 29.4680
2023-09-28 17:16:07,919:INFO:  Epoch 48/500:  train Loss: 25.3468   val Loss: 30.9853   time: 644.77s   best: 29.4680
2023-09-28 17:26:17,592:INFO:  Epoch 49/500:  train Loss: 25.1106   val Loss: 29.8634   time: 609.66s   best: 29.4680
2023-09-28 17:36:49,551:INFO:  Epoch 50/500:  train Loss: 24.9937   val Loss: 30.2067   time: 631.94s   best: 29.4680
2023-09-28 17:47:32,931:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 17:47:33,617:INFO:  Epoch 51/500:  train Loss: 24.8379   val Loss: 29.3507   time: 643.05s   best: 29.3507
2023-09-28 17:58:08,823:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 17:58:09,420:INFO:  Epoch 52/500:  train Loss: 24.7992   val Loss: 29.1947   time: 634.81s   best: 29.1947
2023-09-28 18:08:42,188:INFO:  Epoch 53/500:  train Loss: 24.7869   val Loss: 29.5928   time: 632.75s   best: 29.1947
2023-09-28 18:18:42,181:INFO:  Epoch 54/500:  train Loss: 24.6674   val Loss: 29.6901   time: 599.98s   best: 29.1947
2023-09-28 18:28:31,037:INFO:  Epoch 55/500:  train Loss: 24.6468   val Loss: 29.5664   time: 588.85s   best: 29.1947
2023-09-28 18:38:20,807:INFO:  Epoch 56/500:  train Loss: 24.6785   val Loss: 30.1335   time: 589.76s   best: 29.1947
2023-09-28 18:47:50,602:INFO:  Epoch 57/500:  train Loss: 24.4976   val Loss: 29.6846   time: 569.78s   best: 29.1947
2023-09-28 19:02:11,440:INFO:  Epoch 58/500:  train Loss: 24.6277   val Loss: 30.0173   time: 860.77s   best: 29.1947
2023-09-28 19:11:12,658:INFO:  Epoch 59/500:  train Loss: 24.6107   val Loss: 29.2288   time: 541.18s   best: 29.1947
2023-09-28 19:20:17,927:INFO:  Epoch 60/500:  train Loss: 24.4068   val Loss: 29.6281   time: 545.26s   best: 29.1947
2023-09-28 19:29:16,516:INFO:  Epoch 61/500:  train Loss: 24.3052   val Loss: 29.7198   time: 538.56s   best: 29.1947
2023-09-28 19:38:21,873:INFO:  Epoch 62/500:  train Loss: 24.1568   val Loss: 29.7574   time: 545.35s   best: 29.1947
2023-09-28 19:47:52,505:INFO:  Epoch 63/500:  train Loss: 24.1450   val Loss: 30.3125   time: 570.59s   best: 29.1947
2023-09-28 19:56:50,878:INFO:  Epoch 64/500:  train Loss: 24.0255   val Loss: 29.6482   time: 538.36s   best: 29.1947
2023-09-28 20:05:37,193:INFO:  Epoch 65/500:  train Loss: 24.0807   val Loss: 29.3798   time: 526.30s   best: 29.1947
2023-09-28 20:13:55,063:INFO:  Epoch 66/500:  train Loss: 23.9921   val Loss: 29.3246   time: 497.83s   best: 29.1947
2023-09-28 20:22:50,535:INFO:  Epoch 67/500:  train Loss: 24.0064   val Loss: 30.0362   time: 535.45s   best: 29.1947
2023-09-28 20:31:39,024:INFO:  Epoch 68/500:  train Loss: 23.8216   val Loss: 29.3790   time: 528.48s   best: 29.1947
2023-09-28 20:39:48,625:INFO:  Epoch 69/500:  train Loss: 23.7969   val Loss: 29.6800   time: 489.54s   best: 29.1947
2023-09-28 20:47:45,715:INFO:  Epoch 70/500:  train Loss: 23.7302   val Loss: 29.2913   time: 477.07s   best: 29.1947
2023-09-28 20:55:44,341:INFO:  Epoch 71/500:  train Loss: 23.7289   val Loss: 29.3346   time: 478.62s   best: 29.1947
2023-09-28 21:03:43,181:INFO:  Epoch 72/500:  train Loss: 23.7404   val Loss: 29.6156   time: 478.80s   best: 29.1947
2023-09-28 21:11:55,125:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 21:11:55,428:INFO:  Epoch 73/500:  train Loss: 24.0482   val Loss: 29.1753   time: 491.82s   best: 29.1753
2023-09-28 21:19:53,482:INFO:  Epoch 74/500:  train Loss: 23.6088   val Loss: 29.2790   time: 478.05s   best: 29.1753
2023-09-28 21:27:54,291:INFO:  Epoch 75/500:  train Loss: 23.6047   val Loss: 29.5434   time: 480.79s   best: 29.1753
2023-09-28 21:36:11,598:INFO:  Epoch 76/500:  train Loss: 23.5502   val Loss: 29.2657   time: 497.29s   best: 29.1753
2023-09-28 21:44:19,763:INFO:  Epoch 77/500:  train Loss: 23.7000   val Loss: 29.4058   time: 488.14s   best: 29.1753
2023-09-28 21:52:36,622:INFO:  Epoch 78/500:  train Loss: 23.4300   val Loss: 29.6145   time: 496.85s   best: 29.1753
2023-09-28 22:00:46,560:INFO:  Epoch 79/500:  train Loss: 23.4283   val Loss: 29.3986   time: 489.93s   best: 29.1753
2023-09-28 22:08:52,572:INFO:  Epoch 80/500:  train Loss: 23.3568   val Loss: 29.7426   time: 485.99s   best: 29.1753
2023-09-28 22:17:13,048:INFO:  Epoch 81/500:  train Loss: 23.5339   val Loss: 30.0463   time: 500.45s   best: 29.1753
2023-09-28 22:25:17,820:INFO:  Epoch 82/500:  train Loss: 23.3623   val Loss: 29.8658   time: 484.74s   best: 29.1753
2023-09-28 22:33:19,461:INFO:  Epoch 83/500:  train Loss: 23.2383   val Loss: 29.5386   time: 481.63s   best: 29.1753
2023-09-28 22:41:26,126:INFO:  Epoch 84/500:  train Loss: 23.2140   val Loss: 29.5382   time: 486.65s   best: 29.1753
2023-09-28 22:49:25,688:INFO:  Epoch 85/500:  train Loss: 23.3193   val Loss: 29.2841   time: 479.54s   best: 29.1753
2023-09-28 22:57:28,570:INFO:  Epoch 86/500:  train Loss: 23.1628   val Loss: 29.4382   time: 482.87s   best: 29.1753
2023-09-28 23:05:29,365:INFO:  Epoch 87/500:  train Loss: 23.2844   val Loss: 29.2927   time: 480.78s   best: 29.1753
2023-09-28 23:13:32,079:INFO:  Epoch 88/500:  train Loss: 23.1134   val Loss: 30.0297   time: 482.69s   best: 29.1753
2023-09-28 23:21:45,877:INFO:  Epoch 89/500:  train Loss: 23.0029   val Loss: 29.6672   time: 493.79s   best: 29.1753
2023-09-28 23:29:48,002:INFO:  Epoch 90/500:  train Loss: 22.9947   val Loss: 30.0669   time: 482.10s   best: 29.1753
2023-09-28 23:38:04,402:INFO:  Epoch 91/500:  train Loss: 23.2135   val Loss: 29.5447   time: 496.38s   best: 29.1753
2023-09-28 23:46:20,415:INFO:  Epoch 92/500:  train Loss: 22.9373   val Loss: 29.8541   time: 495.99s   best: 29.1753
2023-09-28 23:54:30,922:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-28 23:54:31,344:INFO:  Epoch 93/500:  train Loss: 22.8942   val Loss: 28.9301   time: 490.41s   best: 28.9301
2023-09-29 00:02:35,112:INFO:  Epoch 94/500:  train Loss: 22.9924   val Loss: 29.2525   time: 483.76s   best: 28.9301
2023-09-29 00:10:47,991:INFO:  Epoch 95/500:  train Loss: 22.8560   val Loss: 29.2134   time: 492.86s   best: 28.9301
2023-09-29 00:19:09,787:INFO:  Epoch 96/500:  train Loss: 22.9901   val Loss: 29.1792   time: 501.78s   best: 28.9301
2023-09-29 00:27:16,381:INFO:  Epoch 97/500:  train Loss: 22.9552   val Loss: 29.6074   time: 486.57s   best: 28.9301
2023-09-29 00:35:31,147:INFO:  Epoch 98/500:  train Loss: 22.8826   val Loss: 30.5277   time: 494.75s   best: 28.9301
2023-09-29 00:43:20,162:INFO:  Epoch 99/500:  train Loss: 22.7816   val Loss: 29.8092   time: 469.01s   best: 28.9301
2023-09-29 00:51:37,128:INFO:  Epoch 100/500:  train Loss: 22.7992   val Loss: 29.5864   time: 496.94s   best: 28.9301
2023-09-29 01:00:01,146:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 01:00:01,848:INFO:  Epoch 101/500:  train Loss: 22.7571   val Loss: 28.8508   time: 503.85s   best: 28.8508
2023-09-29 01:08:05,705:INFO:  Epoch 102/500:  train Loss: 22.8587   val Loss: 31.2596   time: 483.85s   best: 28.8508
2023-09-29 01:16:23,188:INFO:  Epoch 103/500:  train Loss: 22.7909   val Loss: 29.7657   time: 497.47s   best: 28.8508
2023-09-29 01:24:19,946:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 01:24:20,465:INFO:  Epoch 104/500:  train Loss: 22.8214   val Loss: 28.7953   time: 476.65s   best: 28.7953
2023-09-29 01:32:26,647:INFO:  Epoch 105/500:  train Loss: 22.8246   val Loss: 28.9444   time: 486.18s   best: 28.7953
2023-09-29 01:40:41,277:INFO:  Epoch 106/500:  train Loss: 22.6453   val Loss: 28.9215   time: 494.61s   best: 28.7953
2023-09-29 01:48:40,832:INFO:  Epoch 107/500:  train Loss: 22.5969   val Loss: 28.8602   time: 479.47s   best: 28.7953
2023-09-29 01:56:49,417:INFO:  Epoch 108/500:  train Loss: 22.6143   val Loss: 29.8962   time: 488.56s   best: 28.7953
2023-09-29 02:05:27,929:INFO:  Epoch 109/500:  train Loss: 22.5796   val Loss: 29.2136   time: 518.50s   best: 28.7953
2023-09-29 02:13:24,280:INFO:  Epoch 110/500:  train Loss: 22.5440   val Loss: 29.8663   time: 476.34s   best: 28.7953
2023-09-29 02:20:51,558:INFO:  Epoch 111/500:  train Loss: 22.4883   val Loss: 29.3027   time: 447.24s   best: 28.7953
2023-09-29 02:28:31,950:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 02:28:34,595:INFO:  Epoch 112/500:  train Loss: 22.6994   val Loss: 28.6589   time: 460.10s   best: 28.6589
2023-09-29 02:36:36,563:INFO:  Epoch 113/500:  train Loss: 22.5412   val Loss: 28.9140   time: 481.97s   best: 28.6589
2023-09-29 02:44:11,124:INFO:  Epoch 114/500:  train Loss: 22.4690   val Loss: 28.9504   time: 454.54s   best: 28.6589
2023-09-29 02:51:55,917:INFO:  Epoch 115/500:  train Loss: 22.4295   val Loss: 29.8830   time: 464.75s   best: 28.6589
2023-09-29 02:59:40,981:INFO:  Epoch 116/500:  train Loss: 22.4413   val Loss: 29.1784   time: 465.03s   best: 28.6589
2023-09-29 03:07:12,552:INFO:  Epoch 117/500:  train Loss: 22.3824   val Loss: 28.8885   time: 451.55s   best: 28.6589
2023-09-29 03:14:44,273:INFO:  Epoch 118/500:  train Loss: 22.3649   val Loss: 29.3218   time: 451.70s   best: 28.6589
2023-09-29 03:22:17,284:INFO:  Epoch 119/500:  train Loss: 22.3062   val Loss: 28.8846   time: 452.99s   best: 28.6589
2023-09-29 03:29:42,704:INFO:  Epoch 120/500:  train Loss: 22.2861   val Loss: 29.4591   time: 445.41s   best: 28.6589
2023-09-29 03:37:29,269:INFO:  Epoch 121/500:  train Loss: 22.3194   val Loss: 29.2260   time: 466.55s   best: 28.6589
2023-09-29 03:45:04,422:INFO:  Epoch 122/500:  train Loss: 22.3786   val Loss: 31.9663   time: 455.13s   best: 28.6589
2023-09-29 03:52:33,906:INFO:  Epoch 123/500:  train Loss: 22.3443   val Loss: 29.0080   time: 449.47s   best: 28.6589
2023-09-29 04:00:05,209:INFO:  Epoch 124/500:  train Loss: 22.2392   val Loss: 29.1627   time: 451.29s   best: 28.6589
2023-09-29 04:07:42,358:INFO:  Epoch 125/500:  train Loss: 22.3067   val Loss: 29.1746   time: 457.13s   best: 28.6589
2023-09-29 04:15:18,103:INFO:  Epoch 126/500:  train Loss: 22.2415   val Loss: 28.9263   time: 455.73s   best: 28.6589
2023-09-29 04:22:45,257:INFO:  Epoch 127/500:  train Loss: 22.2517   val Loss: 28.8712   time: 447.14s   best: 28.6589
2023-09-29 04:30:10,502:INFO:  Epoch 128/500:  train Loss: 22.0888   val Loss: 29.5097   time: 445.24s   best: 28.6589
2023-09-29 04:37:41,976:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 04:37:42,487:INFO:  Epoch 129/500:  train Loss: 22.3187   val Loss: 28.6073   time: 451.34s   best: 28.6073
2023-09-29 04:45:00,588:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 04:45:01,089:INFO:  Epoch 130/500:  train Loss: 22.0784   val Loss: 28.5747   time: 437.86s   best: 28.5747
2023-09-29 04:52:35,250:INFO:  Epoch 131/500:  train Loss: 22.1556   val Loss: 29.2576   time: 454.16s   best: 28.5747
2023-09-29 04:59:58,967:INFO:  Epoch 132/500:  train Loss: 22.1836   val Loss: 29.9753   time: 443.70s   best: 28.5747
2023-09-29 05:07:51,806:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 05:07:52,185:INFO:  Epoch 133/500:  train Loss: 22.2650   val Loss: 28.5142   time: 472.71s   best: 28.5142
2023-09-29 05:15:18,958:INFO:  Epoch 134/500:  train Loss: 22.0085   val Loss: 28.9159   time: 446.77s   best: 28.5142
2023-09-29 05:22:44,261:INFO:  Epoch 135/500:  train Loss: 22.1692   val Loss: 28.8768   time: 445.29s   best: 28.5142
2023-09-29 05:30:06,225:INFO:  Epoch 136/500:  train Loss: 22.0017   val Loss: 28.6565   time: 441.95s   best: 28.5142
2023-09-29 05:37:47,490:INFO:  Epoch 137/500:  train Loss: 22.0291   val Loss: 28.9186   time: 461.25s   best: 28.5142
2023-09-29 05:45:14,027:INFO:  Epoch 138/500:  train Loss: 22.1200   val Loss: 28.6051   time: 446.52s   best: 28.5142
2023-09-29 05:52:42,602:INFO:  Epoch 139/500:  train Loss: 22.0235   val Loss: 29.8299   time: 448.57s   best: 28.5142
2023-09-29 06:00:20,679:INFO:  Epoch 140/500:  train Loss: 21.9787   val Loss: 28.9275   time: 458.07s   best: 28.5142
2023-09-29 06:07:54,057:INFO:  Epoch 141/500:  train Loss: 21.9976   val Loss: 29.4751   time: 453.37s   best: 28.5142
2023-09-29 06:15:09,717:INFO:  Epoch 142/500:  train Loss: 22.0170   val Loss: 29.1260   time: 435.64s   best: 28.5142
2023-09-29 06:22:26,898:INFO:  Epoch 143/500:  train Loss: 21.9644   val Loss: 29.5989   time: 437.16s   best: 28.5142
2023-09-29 06:29:45,099:INFO:  Epoch 144/500:  train Loss: 21.9339   val Loss: 29.2129   time: 438.19s   best: 28.5142
2023-09-29 06:37:06,287:INFO:  Epoch 145/500:  train Loss: 22.0015   val Loss: 28.8328   time: 441.18s   best: 28.5142
2023-09-29 06:44:29,773:INFO:  Epoch 146/500:  train Loss: 21.9000   val Loss: 29.1959   time: 443.46s   best: 28.5142
2023-09-29 06:51:48,527:INFO:  Epoch 147/500:  train Loss: 21.8756   val Loss: 29.0924   time: 438.75s   best: 28.5142
2023-09-29 06:59:08,923:INFO:  Epoch 148/500:  train Loss: 21.8613   val Loss: 28.6090   time: 440.39s   best: 28.5142
2023-09-29 07:06:31,282:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 07:06:31,570:INFO:  Epoch 149/500:  train Loss: 21.8270   val Loss: 28.3275   time: 442.33s   best: 28.3275
2023-09-29 07:13:54,706:INFO:  Epoch 150/500:  train Loss: 21.7800   val Loss: 29.2009   time: 443.13s   best: 28.3275
2023-09-29 07:21:16,150:INFO:  Epoch 151/500:  train Loss: 21.9412   val Loss: 28.8282   time: 441.43s   best: 28.3275
2023-09-29 07:28:32,738:INFO:  Epoch 152/500:  train Loss: 21.8270   val Loss: 29.5924   time: 436.54s   best: 28.3275
2023-09-29 07:35:59,139:INFO:  Epoch 153/500:  train Loss: 21.7689   val Loss: 28.6198   time: 446.39s   best: 28.3275
2023-09-29 07:43:22,057:INFO:  Epoch 154/500:  train Loss: 21.7671   val Loss: 29.0277   time: 442.91s   best: 28.3275
2023-09-29 07:50:42,225:INFO:  Epoch 155/500:  train Loss: 21.7449   val Loss: 28.8818   time: 440.16s   best: 28.3275
2023-09-29 07:58:01,045:INFO:  Epoch 156/500:  train Loss: 21.6996   val Loss: 29.0349   time: 438.80s   best: 28.3275
2023-09-29 08:05:19,250:INFO:  Epoch 157/500:  train Loss: 21.7735   val Loss: 29.0971   time: 438.19s   best: 28.3275
2023-09-29 08:13:16,469:INFO:  Epoch 158/500:  train Loss: 21.7218   val Loss: 29.1082   time: 477.20s   best: 28.3275
2023-09-29 08:20:38,467:INFO:  Epoch 159/500:  train Loss: 21.6637   val Loss: 28.8574   time: 441.98s   best: 28.3275
2023-09-29 08:27:56,355:INFO:  Epoch 160/500:  train Loss: 22.0047   val Loss: 28.7112   time: 437.88s   best: 28.3275
2023-09-29 08:35:12,982:INFO:  Epoch 161/500:  train Loss: 21.7459   val Loss: 28.9863   time: 436.61s   best: 28.3275
2023-09-29 08:42:34,338:INFO:  Epoch 162/500:  train Loss: 21.7105   val Loss: 29.2278   time: 441.34s   best: 28.3275
2023-09-29 08:49:55,649:INFO:  Epoch 163/500:  train Loss: 21.6173   val Loss: 28.7345   time: 441.30s   best: 28.3275
2023-09-29 08:57:13,220:INFO:  Epoch 164/500:  train Loss: 21.6398   val Loss: 29.0686   time: 437.55s   best: 28.3275
2023-09-29 09:04:29,218:INFO:  Epoch 165/500:  train Loss: 22.0306   val Loss: 29.1240   time: 435.98s   best: 28.3275
2023-09-29 09:11:50,490:INFO:  Epoch 166/500:  train Loss: 21.6395   val Loss: 29.5541   time: 441.25s   best: 28.3275
2023-09-29 09:19:07,308:INFO:  Epoch 167/500:  train Loss: 21.5884   val Loss: 28.7436   time: 436.81s   best: 28.3275
2023-09-29 09:26:25,437:INFO:  Epoch 168/500:  train Loss: 21.6719   val Loss: 28.6276   time: 438.11s   best: 28.3275
2023-09-29 09:33:46,021:INFO:  Epoch 169/500:  train Loss: 21.5567   val Loss: 29.1224   time: 440.56s   best: 28.3275
2023-09-29 09:41:13,870:INFO:  Epoch 170/500:  train Loss: 21.5142   val Loss: 28.7420   time: 447.84s   best: 28.3275
2023-09-29 09:48:36,047:INFO:  Epoch 171/500:  train Loss: 21.6452   val Loss: 28.9941   time: 442.16s   best: 28.3275
2023-09-29 09:56:00,161:INFO:  Epoch 172/500:  train Loss: 21.4977   val Loss: 29.9470   time: 444.11s   best: 28.3275
2023-09-29 10:03:17,720:INFO:  Epoch 173/500:  train Loss: 21.4464   val Loss: 31.5130   time: 437.55s   best: 28.3275
2023-09-29 10:10:42,394:INFO:  Epoch 174/500:  train Loss: 21.5704   val Loss: 28.6002   time: 444.66s   best: 28.3275
2023-09-29 10:18:12,155:INFO:  Epoch 175/500:  train Loss: 21.5202   val Loss: 28.6548   time: 449.74s   best: 28.3275
2023-09-29 10:25:35,163:INFO:  Epoch 176/500:  train Loss: 21.5260   val Loss: 28.6832   time: 442.99s   best: 28.3275
2023-09-29 10:33:21,178:INFO:  Epoch 177/500:  train Loss: 21.4816   val Loss: 28.8127   time: 465.98s   best: 28.3275
2023-09-29 10:41:02,739:INFO:  Epoch 178/500:  train Loss: 21.4652   val Loss: 28.6648   time: 461.53s   best: 28.3275
2023-09-29 10:48:37,129:INFO:  Epoch 179/500:  train Loss: 21.4685   val Loss: 28.8014   time: 454.35s   best: 28.3275
2023-09-29 10:56:13,450:INFO:  Epoch 180/500:  train Loss: 21.4736   val Loss: 29.3177   time: 456.31s   best: 28.3275
2023-09-29 11:04:09,123:INFO:  Epoch 181/500:  train Loss: 21.5710   val Loss: 29.0260   time: 475.64s   best: 28.3275
2023-09-29 11:12:00,573:INFO:  Epoch 182/500:  train Loss: 21.4473   val Loss: 29.4548   time: 471.44s   best: 28.3275
2023-09-29 11:19:38,374:INFO:  Epoch 183/500:  train Loss: 21.5669   val Loss: 29.2245   time: 457.78s   best: 28.3275
2023-09-29 11:27:28,602:INFO:  Epoch 184/500:  train Loss: 21.4593   val Loss: 28.5843   time: 470.22s   best: 28.3275
2023-09-29 11:35:13,313:INFO:  Epoch 185/500:  train Loss: 21.3540   val Loss: 28.7529   time: 464.67s   best: 28.3275
2023-09-29 11:42:44,942:INFO:  Epoch 186/500:  train Loss: 21.3977   val Loss: 28.9491   time: 451.60s   best: 28.3275
2023-09-29 11:50:30,538:INFO:  Epoch 187/500:  train Loss: 21.3087   val Loss: 29.0325   time: 465.58s   best: 28.3275
2023-09-29 11:58:19,411:INFO:  Epoch 188/500:  train Loss: 21.2998   val Loss: 29.0252   time: 468.85s   best: 28.3275
2023-09-29 12:06:05,757:INFO:  Epoch 189/500:  train Loss: 21.2864   val Loss: 30.3557   time: 466.33s   best: 28.3275
2023-09-29 12:14:03,809:INFO:  Epoch 190/500:  train Loss: 21.2424   val Loss: 28.6943   time: 478.04s   best: 28.3275
2023-09-29 12:21:50,156:INFO:  Epoch 191/500:  train Loss: 21.3693   val Loss: 30.8813   time: 466.32s   best: 28.3275
2023-09-29 12:29:36,761:INFO:  Epoch 192/500:  train Loss: 21.4558   val Loss: 29.5456   time: 466.59s   best: 28.3275
2023-09-29 12:37:24,811:INFO:  Epoch 193/500:  train Loss: 21.5063   val Loss: 29.5118   time: 468.03s   best: 28.3275
2023-09-29 12:45:08,493:INFO:  Epoch 194/500:  train Loss: 21.3716   val Loss: 28.8948   time: 463.64s   best: 28.3275
2023-09-29 12:52:50,337:INFO:  Epoch 195/500:  train Loss: 21.2485   val Loss: 29.2727   time: 461.82s   best: 28.3275
2023-09-29 13:00:34,691:INFO:  Epoch 196/500:  train Loss: 21.3458   val Loss: 29.1705   time: 464.30s   best: 28.3275
2023-09-29 13:08:19,517:INFO:  Epoch 197/500:  train Loss: 21.3219   val Loss: 29.1049   time: 464.79s   best: 28.3275
2023-09-29 13:16:09,033:INFO:  Epoch 198/500:  train Loss: 21.3439   val Loss: 28.9941   time: 469.49s   best: 28.3275
2023-09-29 13:24:22,437:INFO:  Epoch 199/500:  train Loss: 21.3136   val Loss: 29.4738   time: 493.39s   best: 28.3275
2023-09-29 13:32:03,721:INFO:  Epoch 200/500:  train Loss: 21.3031   val Loss: 30.7649   time: 461.27s   best: 28.3275
2023-09-29 13:39:53,248:INFO:  Epoch 201/500:  train Loss: 21.1937   val Loss: 28.8046   time: 469.50s   best: 28.3275
2023-09-29 13:47:40,592:INFO:  Epoch 202/500:  train Loss: 21.1828   val Loss: 28.9389   time: 467.33s   best: 28.3275
2023-09-29 13:55:22,913:INFO:  Epoch 203/500:  train Loss: 21.1821   val Loss: 29.1318   time: 462.31s   best: 28.3275
2023-09-29 14:03:23,912:INFO:  Epoch 204/500:  train Loss: 21.2433   val Loss: 29.1165   time: 480.97s   best: 28.3275
2023-09-29 14:11:11,593:INFO:  Epoch 205/500:  train Loss: 21.2902   val Loss: 29.4740   time: 467.67s   best: 28.3275
2023-09-29 14:18:59,926:INFO:  Epoch 206/500:  train Loss: 21.2560   val Loss: 29.4855   time: 468.32s   best: 28.3275
2023-09-29 14:26:59,045:INFO:  Epoch 207/500:  train Loss: 21.2802   val Loss: 28.8276   time: 479.11s   best: 28.3275
2023-09-29 14:34:52,942:INFO:  Epoch 208/500:  train Loss: 21.4469   val Loss: 29.0096   time: 473.88s   best: 28.3275
2023-09-29 14:42:45,594:INFO:  Epoch 209/500:  train Loss: 20.9948   val Loss: 28.8887   time: 472.63s   best: 28.3275
2023-09-29 14:50:22,079:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 14:50:22,437:INFO:  Epoch 210/500:  train Loss: 20.8857   val Loss: 28.2430   time: 456.42s   best: 28.2430
2023-09-29 14:57:50,756:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 14:57:51,070:INFO:  Epoch 211/500:  train Loss: 20.4711   val Loss: 27.7026   time: 448.20s   best: 27.7026
2023-09-29 15:05:18,392:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 15:05:18,723:INFO:  Epoch 212/500:  train Loss: 20.3188   val Loss: 27.1497   time: 447.19s   best: 27.1497
2023-09-29 15:12:40,712:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 15:12:41,019:INFO:  Epoch 213/500:  train Loss: 20.2916   val Loss: 26.7586   time: 441.88s   best: 26.7586
2023-09-29 15:20:07,474:INFO:  Epoch 214/500:  train Loss: 20.1073   val Loss: 26.8173   time: 446.45s   best: 26.7586
2023-09-29 15:27:31,396:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 15:27:31,819:INFO:  Epoch 215/500:  train Loss: 20.1852   val Loss: 25.9131   time: 443.72s   best: 25.9131
2023-09-29 15:34:51,384:INFO:  Epoch 216/500:  train Loss: 19.9120   val Loss: 26.0574   time: 439.56s   best: 25.9131
2023-09-29 15:42:20,647:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 15:42:21,559:INFO:  Epoch 217/500:  train Loss: 19.7643   val Loss: 25.8787   time: 448.93s   best: 25.8787
2023-09-29 15:49:50,307:INFO:  Epoch 218/500:  train Loss: 19.6839   val Loss: 26.1749   time: 448.75s   best: 25.8787
2023-09-29 15:57:17,302:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 15:57:17,633:INFO:  Epoch 219/500:  train Loss: 19.7798   val Loss: 25.6021   time: 446.94s   best: 25.6021
2023-09-29 16:04:41,412:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 16:04:41,669:INFO:  Epoch 220/500:  train Loss: 19.8880   val Loss: 25.3216   time: 443.73s   best: 25.3216
2023-09-29 16:12:10,821:INFO:  Epoch 221/500:  train Loss: 19.6460   val Loss: 28.6124   time: 449.14s   best: 25.3216
2023-09-29 16:19:38,841:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 16:19:39,158:INFO:  Epoch 222/500:  train Loss: 19.6940   val Loss: 25.3043   time: 447.85s   best: 25.3043
2023-09-29 16:27:02,140:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 16:27:02,414:INFO:  Epoch 223/500:  train Loss: 19.5849   val Loss: 25.2098   time: 442.90s   best: 25.2098
2023-09-29 16:34:16,235:INFO:  Epoch 224/500:  train Loss: 19.6498   val Loss: 25.3499   time: 433.82s   best: 25.2098
2023-09-29 16:41:34,437:INFO:  Epoch 225/500:  train Loss: 19.5063   val Loss: 25.8904   time: 438.18s   best: 25.2098
2023-09-29 16:48:55,354:INFO:  Epoch 226/500:  train Loss: 19.7095   val Loss: 26.2481   time: 440.90s   best: 25.2098
2023-09-29 16:56:10,392:INFO:  Epoch 227/500:  train Loss: 19.5466   val Loss: 29.7226   time: 435.01s   best: 25.2098
2023-09-29 17:03:33,728:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 17:03:34,324:INFO:  Epoch 228/500:  train Loss: 19.6052   val Loss: 25.1716   time: 443.08s   best: 25.1716
2023-09-29 17:11:03,796:INFO:  Epoch 229/500:  train Loss: 19.3686   val Loss: 25.6987   time: 449.47s   best: 25.1716
2023-09-29 17:18:29,130:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 17:18:29,497:INFO:  Epoch 230/500:  train Loss: 19.5759   val Loss: 24.6370   time: 445.31s   best: 24.6370
2023-09-29 17:25:47,967:INFO:  Epoch 231/500:  train Loss: 19.6851   val Loss: 25.5997   time: 438.47s   best: 24.6370
2023-09-29 17:33:05,870:INFO:  Epoch 232/500:  train Loss: 19.2694   val Loss: 25.2513   time: 437.89s   best: 24.6370
2023-09-29 17:40:35,412:INFO:  Epoch 233/500:  train Loss: 19.2741   val Loss: 24.7737   time: 449.53s   best: 24.6370
2023-09-29 17:47:57,438:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 17:47:57,824:INFO:  Epoch 234/500:  train Loss: 19.4487   val Loss: 24.6245   time: 441.98s   best: 24.6245
2023-09-29 17:55:19,486:INFO:  Epoch 235/500:  train Loss: 19.5050   val Loss: 24.9540   time: 441.66s   best: 24.6245
2023-09-29 18:02:41,530:INFO:  Epoch 236/500:  train Loss: 19.2570   val Loss: 25.2946   time: 442.03s   best: 24.6245
2023-09-29 18:10:01,905:INFO:  Epoch 237/500:  train Loss: 19.3177   val Loss: 24.8972   time: 440.37s   best: 24.6245
2023-09-29 18:17:20,023:INFO:  Epoch 238/500:  train Loss: 19.3331   val Loss: 24.7084   time: 438.09s   best: 24.6245
2023-09-29 18:24:36,056:INFO:  Epoch 239/500:  train Loss: 19.3309   val Loss: 24.8824   time: 436.02s   best: 24.6245
2023-09-29 18:31:58,324:INFO:  Epoch 240/500:  train Loss: 19.1696   val Loss: 24.7304   time: 442.26s   best: 24.6245
2023-09-29 18:39:21,361:INFO:  Epoch 241/500:  train Loss: 19.2024   val Loss: 25.0544   time: 443.02s   best: 24.6245
2023-09-29 18:46:40,476:INFO:  Epoch 242/500:  train Loss: 19.1676   val Loss: 24.6823   time: 439.10s   best: 24.6245
2023-09-29 18:54:16,979:INFO:  Epoch 243/500:  train Loss: 19.1061   val Loss: 24.6791   time: 456.49s   best: 24.6245
2023-09-29 19:01:36,995:INFO:  Epoch 244/500:  train Loss: 19.0793   val Loss: 24.9258   time: 439.99s   best: 24.6245
2023-09-29 19:08:58,248:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 19:08:58,539:INFO:  Epoch 245/500:  train Loss: 19.1058   val Loss: 24.4641   time: 441.22s   best: 24.4641
2023-09-29 19:16:24,015:INFO:  Epoch 246/500:  train Loss: 19.4456   val Loss: 24.5293   time: 445.47s   best: 24.4641
2023-09-29 19:23:53,116:INFO:  Epoch 247/500:  train Loss: 19.0269   val Loss: 24.6156   time: 449.09s   best: 24.4641
2023-09-29 19:31:18,743:INFO:  Epoch 248/500:  train Loss: 19.1026   val Loss: 24.5172   time: 445.61s   best: 24.4641
2023-09-29 19:38:45,904:INFO:  Epoch 249/500:  train Loss: 19.1631   val Loss: 24.8469   time: 447.14s   best: 24.4641
2023-09-29 19:46:07,306:INFO:  Epoch 250/500:  train Loss: 18.9832   val Loss: 24.5331   time: 441.38s   best: 24.4641
2023-09-29 19:53:39,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 19:53:39,873:INFO:  Epoch 251/500:  train Loss: 19.0634   val Loss: 24.3010   time: 452.28s   best: 24.3010
2023-09-29 20:01:05,611:INFO:  Epoch 252/500:  train Loss: 19.1309   val Loss: 24.3761   time: 445.74s   best: 24.3010
2023-09-29 20:08:31,773:INFO:  Epoch 253/500:  train Loss: 19.0719   val Loss: 24.3632   time: 446.14s   best: 24.3010
2023-09-29 20:15:58,230:INFO:  Epoch 254/500:  train Loss: 18.9369   val Loss: 24.6079   time: 446.44s   best: 24.3010
2023-09-29 20:23:22,614:INFO:  Epoch 255/500:  train Loss: 18.9963   val Loss: 26.2711   time: 444.37s   best: 24.3010
2023-09-29 20:30:47,522:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 20:30:47,725:INFO:  Epoch 256/500:  train Loss: 19.0581   val Loss: 24.1580   time: 444.85s   best: 24.1580
2023-09-29 20:38:11,048:INFO:  Epoch 257/500:  train Loss: 18.8382   val Loss: 24.4497   time: 443.31s   best: 24.1580
2023-09-29 20:45:34,012:INFO:  Epoch 258/500:  train Loss: 19.0844   val Loss: 24.2653   time: 442.92s   best: 24.1580
2023-09-29 20:52:58,781:INFO:  Epoch 259/500:  train Loss: 18.8156   val Loss: 25.5272   time: 444.76s   best: 24.1580
2023-09-29 21:00:25,441:INFO:  Epoch 260/500:  train Loss: 18.9289   val Loss: 24.3144   time: 446.64s   best: 24.1580
2023-09-29 21:07:59,641:INFO:  Epoch 261/500:  train Loss: 18.7989   val Loss: 24.8166   time: 454.17s   best: 24.1580
2023-09-29 21:15:29,888:INFO:  Epoch 262/500:  train Loss: 18.8163   val Loss: 24.4254   time: 450.23s   best: 24.1580
2023-09-29 21:23:08,280:INFO:  Epoch 263/500:  train Loss: 18.8353   val Loss: 24.4540   time: 458.38s   best: 24.1580
2023-09-29 21:31:15,504:INFO:  Epoch 264/500:  train Loss: 18.7238   val Loss: 24.7520   time: 487.21s   best: 24.1580
2023-09-29 21:38:42,395:INFO:  Epoch 265/500:  train Loss: 19.0240   val Loss: 24.6599   time: 446.88s   best: 24.1580
2023-09-29 21:46:07,569:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 21:46:08,226:INFO:  Epoch 266/500:  train Loss: 18.8847   val Loss: 24.0867   time: 444.93s   best: 24.0867
2023-09-29 21:53:36,199:INFO:  Epoch 267/500:  train Loss: 18.8372   val Loss: 24.1019   time: 447.97s   best: 24.0867
2023-09-29 22:01:03,825:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 22:01:04,199:INFO:  Epoch 268/500:  train Loss: 18.8087   val Loss: 24.0038   time: 447.54s   best: 24.0038
2023-09-29 22:08:36,103:INFO:  Epoch 269/500:  train Loss: 18.7090   val Loss: 24.1006   time: 451.88s   best: 24.0038
2023-09-29 22:16:16,275:INFO:  Epoch 270/500:  train Loss: 18.7310   val Loss: 24.5155   time: 460.16s   best: 24.0038
2023-09-29 22:23:50,301:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 22:23:50,805:INFO:  Epoch 271/500:  train Loss: 18.7067   val Loss: 23.7742   time: 453.77s   best: 23.7742
2023-09-29 22:31:23,771:INFO:  Epoch 272/500:  train Loss: 18.6961   val Loss: 24.0836   time: 452.96s   best: 23.7742
2023-09-29 22:39:09,175:INFO:  Epoch 273/500:  train Loss: 18.7893   val Loss: 24.0144   time: 465.39s   best: 23.7742
2023-09-29 22:46:40,666:INFO:  Epoch 274/500:  train Loss: 18.7870   val Loss: 24.1339   time: 451.48s   best: 23.7742
2023-09-29 22:54:22,343:INFO:  Epoch 275/500:  train Loss: 18.6862   val Loss: 24.0921   time: 461.65s   best: 23.7742
2023-09-29 23:02:00,930:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-29 23:02:01,329:INFO:  Epoch 276/500:  train Loss: 18.6782   val Loss: 23.6962   time: 458.32s   best: 23.6962
2023-09-29 23:09:42,346:INFO:  Epoch 277/500:  train Loss: 18.6714   val Loss: 24.5269   time: 461.01s   best: 23.6962
2023-09-29 23:17:15,583:INFO:  Epoch 278/500:  train Loss: 18.6687   val Loss: 31.6986   time: 453.22s   best: 23.6962
2023-09-29 23:24:58,141:INFO:  Epoch 279/500:  train Loss: 18.6292   val Loss: 24.3058   time: 462.53s   best: 23.6962
2023-09-29 23:32:46,412:INFO:  Epoch 280/500:  train Loss: 18.5849   val Loss: 23.8656   time: 468.26s   best: 23.6962
2023-09-29 23:40:23,957:INFO:  Epoch 281/500:  train Loss: 18.5830   val Loss: 27.2531   time: 457.52s   best: 23.6962
2023-09-29 23:47:51,749:INFO:  Epoch 282/500:  train Loss: 18.8336   val Loss: 25.7610   time: 447.78s   best: 23.6962
2023-09-29 23:55:23,280:INFO:  Epoch 283/500:  train Loss: 18.6854   val Loss: 24.4970   time: 451.52s   best: 23.6962
2023-09-30 00:03:04,626:INFO:  Epoch 284/500:  train Loss: 18.5304   val Loss: 24.0955   time: 461.28s   best: 23.6962
2023-09-30 00:10:35,438:INFO:  Epoch 285/500:  train Loss: 18.5151   val Loss: 24.9194   time: 450.77s   best: 23.6962
2023-09-30 00:18:09,437:INFO:  Epoch 286/500:  train Loss: 18.6814   val Loss: 24.1526   time: 453.99s   best: 23.6962
2023-09-30 00:25:40,582:INFO:  Epoch 287/500:  train Loss: 18.7979   val Loss: 24.3231   time: 451.12s   best: 23.6962
2023-09-30 00:33:18,376:INFO:  Epoch 288/500:  train Loss: 18.8395   val Loss: 23.9093   time: 457.77s   best: 23.6962
2023-09-30 00:41:16,765:INFO:  Epoch 289/500:  train Loss: 18.6803   val Loss: 24.3835   time: 478.36s   best: 23.6962
2023-09-30 00:48:42,237:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 00:48:42,521:INFO:  Epoch 290/500:  train Loss: 18.6070   val Loss: 23.4781   time: 445.36s   best: 23.4781
2023-09-30 00:56:13,633:INFO:  Epoch 291/500:  train Loss: 18.6142   val Loss: 23.8940   time: 451.11s   best: 23.4781
2023-09-30 01:03:44,376:INFO:  Epoch 292/500:  train Loss: 18.5706   val Loss: 23.5651   time: 450.73s   best: 23.4781
2023-09-30 01:11:10,539:INFO:  Epoch 293/500:  train Loss: 18.4385   val Loss: 23.7294   time: 446.15s   best: 23.4781
2023-09-30 01:18:37,452:INFO:  Epoch 294/500:  train Loss: 18.5690   val Loss: 23.7618   time: 446.89s   best: 23.4781
2023-09-30 01:26:02,435:INFO:  Epoch 295/500:  train Loss: 18.5339   val Loss: 23.9905   time: 444.97s   best: 23.4781
2023-09-30 01:33:27,562:INFO:  Epoch 296/500:  train Loss: 18.4663   val Loss: 24.1275   time: 445.08s   best: 23.4781
2023-09-30 01:40:57,887:INFO:  Epoch 297/500:  train Loss: 18.7362   val Loss: 23.7187   time: 450.31s   best: 23.4781
2023-09-30 01:48:16,122:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 01:48:16,548:INFO:  Epoch 298/500:  train Loss: 18.4602   val Loss: 23.3874   time: 438.02s   best: 23.3874
2023-09-30 01:55:39,278:INFO:  Epoch 299/500:  train Loss: 18.3869   val Loss: 23.6620   time: 442.73s   best: 23.3874
2023-09-30 02:02:58,175:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 02:02:58,614:INFO:  Epoch 300/500:  train Loss: 18.4300   val Loss: 23.3561   time: 438.80s   best: 23.3561
2023-09-30 02:10:21,531:INFO:  Epoch 301/500:  train Loss: 18.2892   val Loss: 24.4299   time: 442.92s   best: 23.3561
2023-09-30 02:17:40,402:INFO:  Epoch 302/500:  train Loss: 18.3099   val Loss: 23.9589   time: 438.86s   best: 23.3561
2023-09-30 02:25:13,067:INFO:  Epoch 303/500:  train Loss: 18.7669   val Loss: 23.4560   time: 452.65s   best: 23.3561
2023-09-30 02:32:31,585:INFO:  Epoch 304/500:  train Loss: 18.4421   val Loss: 23.9524   time: 438.50s   best: 23.3561
2023-09-30 02:39:50,095:INFO:  Epoch 305/500:  train Loss: 18.4007   val Loss: 23.6278   time: 438.50s   best: 23.3561
2023-09-30 02:47:07,177:INFO:  Epoch 306/500:  train Loss: 18.3974   val Loss: 24.9380   time: 437.07s   best: 23.3561
2023-09-30 02:54:24,857:INFO:  Epoch 307/500:  train Loss: 18.3803   val Loss: 24.1139   time: 437.67s   best: 23.3561
2023-09-30 03:01:43,431:INFO:  Epoch 308/500:  train Loss: 18.3789   val Loss: 23.5584   time: 438.57s   best: 23.3561
2023-09-30 03:09:02,219:INFO:  Epoch 309/500:  train Loss: 18.4752   val Loss: 23.6359   time: 438.77s   best: 23.3561
2023-09-30 03:16:20,753:INFO:  Epoch 310/500:  train Loss: 18.4276   val Loss: 23.7628   time: 438.53s   best: 23.3561
2023-09-30 03:23:38,985:INFO:  Epoch 311/500:  train Loss: 18.3661   val Loss: 23.7443   time: 438.22s   best: 23.3561
2023-09-30 03:30:59,887:INFO:  Epoch 312/500:  train Loss: 18.4371   val Loss: 23.6551   time: 440.88s   best: 23.3561
2023-09-30 03:38:20,612:INFO:  Epoch 313/500:  train Loss: 18.5215   val Loss: 24.1834   time: 440.71s   best: 23.3561
2023-09-30 03:45:43,818:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 03:45:44,126:INFO:  Epoch 314/500:  train Loss: 18.4433   val Loss: 23.1667   time: 443.08s   best: 23.1667
2023-09-30 03:53:02,004:INFO:  Epoch 315/500:  train Loss: 18.3498   val Loss: 24.0178   time: 437.87s   best: 23.1667
2023-09-30 04:00:20,347:INFO:  Epoch 316/500:  train Loss: 18.2962   val Loss: 23.5618   time: 438.33s   best: 23.1667
2023-09-30 04:07:38,783:INFO:  Epoch 317/500:  train Loss: 18.3299   val Loss: 23.6198   time: 438.43s   best: 23.1667
2023-09-30 04:14:54,155:INFO:  Epoch 318/500:  train Loss: 18.3734   val Loss: 23.5921   time: 435.35s   best: 23.1667
2023-09-30 04:22:11,674:INFO:  Epoch 319/500:  train Loss: 18.3186   val Loss: 25.2130   time: 437.49s   best: 23.1667
2023-09-30 04:29:33,063:INFO:  Epoch 320/500:  train Loss: 18.4824   val Loss: 23.5786   time: 441.37s   best: 23.1667
2023-09-30 04:36:51,263:INFO:  Epoch 321/500:  train Loss: 18.3456   val Loss: 26.6435   time: 438.17s   best: 23.1667
2023-09-30 04:44:09,339:INFO:  Epoch 322/500:  train Loss: 18.2131   val Loss: 23.6660   time: 438.06s   best: 23.1667
2023-09-30 04:51:30,878:INFO:  Epoch 323/500:  train Loss: 18.1954   val Loss: 23.8171   time: 441.51s   best: 23.1667
2023-09-30 04:58:53,614:INFO:  Epoch 324/500:  train Loss: 18.2470   val Loss: 24.0947   time: 442.72s   best: 23.1667
2023-09-30 05:06:08,668:INFO:  Epoch 325/500:  train Loss: 18.5131   val Loss: 23.7009   time: 435.04s   best: 23.1667
2023-09-30 05:13:28,962:INFO:  Epoch 326/500:  train Loss: 18.1631   val Loss: 23.5655   time: 440.27s   best: 23.1667
2023-09-30 05:20:46,909:INFO:  Epoch 327/500:  train Loss: 18.4367   val Loss: 23.9432   time: 437.92s   best: 23.1667
2023-09-30 05:28:12,337:INFO:  Epoch 328/500:  train Loss: 18.4084   val Loss: 23.5161   time: 445.40s   best: 23.1667
2023-09-30 05:35:32,992:INFO:  Epoch 329/500:  train Loss: 18.3310   val Loss: 23.3759   time: 440.64s   best: 23.1667
2023-09-30 05:42:53,892:INFO:  Epoch 330/500:  train Loss: 18.2663   val Loss: 23.2823   time: 440.87s   best: 23.1667
2023-09-30 05:50:12,789:INFO:  Epoch 331/500:  train Loss: 18.3686   val Loss: 24.5552   time: 438.87s   best: 23.1667
2023-09-30 05:57:31,115:INFO:  Epoch 332/500:  train Loss: 18.1357   val Loss: 23.5146   time: 438.30s   best: 23.1667
2023-09-30 06:04:48,709:INFO:  Epoch 333/500:  train Loss: 18.1572   val Loss: 23.5579   time: 437.57s   best: 23.1667
2023-09-30 06:12:04,483:INFO:  Epoch 334/500:  train Loss: 18.1163   val Loss: 23.1935   time: 435.76s   best: 23.1667
2023-09-30 06:19:21,986:INFO:  Epoch 335/500:  train Loss: 18.3619   val Loss: 23.5085   time: 437.47s   best: 23.1667
2023-09-30 06:26:44,243:INFO:  Epoch 336/500:  train Loss: 18.1142   val Loss: 23.3791   time: 442.25s   best: 23.1667
2023-09-30 06:34:01,210:INFO:  Epoch 337/500:  train Loss: 18.1667   val Loss: 24.5300   time: 436.94s   best: 23.1667
2023-09-30 06:41:20,749:INFO:  Epoch 338/500:  train Loss: 18.2907   val Loss: 24.0243   time: 439.53s   best: 23.1667
2023-09-30 06:48:40,364:INFO:  Epoch 339/500:  train Loss: 18.2290   val Loss: 23.5477   time: 439.59s   best: 23.1667
2023-09-30 06:56:00,346:INFO:  Epoch 340/500:  train Loss: 18.0979   val Loss: 23.5591   time: 439.96s   best: 23.1667
2023-09-30 07:03:18,800:INFO:  Epoch 341/500:  train Loss: 18.1572   val Loss: 23.4087   time: 438.45s   best: 23.1667
2023-09-30 07:10:35,117:INFO:  Epoch 342/500:  train Loss: 18.1650   val Loss: 23.2684   time: 436.30s   best: 23.1667
2023-09-30 07:17:52,905:INFO:  Epoch 343/500:  train Loss: 18.3788   val Loss: 23.4008   time: 437.77s   best: 23.1667
2023-09-30 07:25:12,420:INFO:  Epoch 344/500:  train Loss: 18.0548   val Loss: 23.3751   time: 439.49s   best: 23.1667
2023-09-30 07:32:39,081:INFO:  Epoch 345/500:  train Loss: 18.0352   val Loss: 23.5936   time: 446.64s   best: 23.1667
2023-09-30 07:40:01,362:INFO:  Epoch 346/500:  train Loss: 18.2363   val Loss: 23.7015   time: 442.27s   best: 23.1667
2023-09-30 07:47:25,385:INFO:  Epoch 347/500:  train Loss: 18.3151   val Loss: 23.3921   time: 444.01s   best: 23.1667
2023-09-30 07:54:52,671:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 07:54:52,953:INFO:  Epoch 348/500:  train Loss: 18.1955   val Loss: 23.1197   time: 447.23s   best: 23.1197
2023-09-30 08:02:14,434:INFO:  Epoch 349/500:  train Loss: 18.1935   val Loss: 23.2787   time: 441.47s   best: 23.1197
2023-09-30 08:09:37,913:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 08:09:38,119:INFO:  Epoch 350/500:  train Loss: 18.0803   val Loss: 23.0080   time: 443.43s   best: 23.0080
2023-09-30 08:16:59,727:INFO:  Epoch 351/500:  train Loss: 18.1797   val Loss: 24.5035   time: 441.60s   best: 23.0080
2023-09-30 08:24:21,044:INFO:  Epoch 352/500:  train Loss: 18.0421   val Loss: 23.4769   time: 441.30s   best: 23.0080
2023-09-30 08:31:45,713:INFO:  Epoch 353/500:  train Loss: 18.0225   val Loss: 23.8990   time: 444.63s   best: 23.0080
2023-09-30 08:39:11,117:INFO:  Epoch 354/500:  train Loss: 18.2609   val Loss: 23.4790   time: 445.38s   best: 23.0080
2023-09-30 08:46:36,094:INFO:  Epoch 355/500:  train Loss: 17.9927   val Loss: 23.2506   time: 444.96s   best: 23.0080
2023-09-30 08:54:00,383:INFO:  Epoch 356/500:  train Loss: 17.9499   val Loss: 24.5419   time: 444.27s   best: 23.0080
2023-09-30 09:01:28,310:INFO:  Epoch 357/500:  train Loss: 18.0196   val Loss: 23.4161   time: 447.90s   best: 23.0080
2023-09-30 09:08:53,021:INFO:  Epoch 358/500:  train Loss: 17.9279   val Loss: 23.4597   time: 444.70s   best: 23.0080
2023-09-30 09:16:18,678:INFO:  Epoch 359/500:  train Loss: 17.9326   val Loss: 23.6253   time: 445.64s   best: 23.0080
2023-09-30 09:23:46,250:INFO:  Epoch 360/500:  train Loss: 18.0181   val Loss: 23.4458   time: 447.54s   best: 23.0080
2023-09-30 09:31:12,235:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 09:31:12,558:INFO:  Epoch 361/500:  train Loss: 17.9625   val Loss: 23.0062   time: 445.87s   best: 23.0062
2023-09-30 09:38:36,092:INFO:  Epoch 362/500:  train Loss: 18.1404   val Loss: 23.1512   time: 443.48s   best: 23.0062
2023-09-30 09:45:58,998:INFO:  Epoch 363/500:  train Loss: 18.0103   val Loss: 24.4774   time: 442.85s   best: 23.0062
2023-09-30 09:53:26,608:INFO:  Epoch 364/500:  train Loss: 17.9500   val Loss: 23.2122   time: 447.59s   best: 23.0062
2023-09-30 10:00:50,933:INFO:  Epoch 365/500:  train Loss: 18.1832   val Loss: 23.3186   time: 444.30s   best: 23.0062
2023-09-30 10:08:16,902:INFO:  Epoch 366/500:  train Loss: 17.9812   val Loss: 26.4251   time: 445.92s   best: 23.0062
2023-09-30 10:15:42,091:INFO:  Epoch 367/500:  train Loss: 17.9629   val Loss: 23.2016   time: 445.16s   best: 23.0062
2023-09-30 10:23:02,389:INFO:  Epoch 368/500:  train Loss: 18.0009   val Loss: 23.2988   time: 440.26s   best: 23.0062
2023-09-30 10:30:30,960:INFO:  Epoch 369/500:  train Loss: 17.8573   val Loss: 23.7929   time: 448.54s   best: 23.0062
2023-09-30 10:37:47,974:INFO:  Epoch 370/500:  train Loss: 17.9145   val Loss: 23.1567   time: 436.99s   best: 23.0062
2023-09-30 10:45:16,678:INFO:  Epoch 371/500:  train Loss: 17.9342   val Loss: 23.5368   time: 448.69s   best: 23.0062
2023-09-30 10:52:41,590:INFO:  Epoch 372/500:  train Loss: 17.8494   val Loss: 23.2427   time: 444.90s   best: 23.0062
2023-09-30 10:59:02,497:INFO:  Starting experiment lstm autoencoder debug
2023-09-30 10:59:02,529:INFO:  Defining the model
2023-09-30 10:59:02,572:INFO:  Reading the dataset
2023-09-30 10:59:43,589:INFO:  Starting experiment lstm autoencoder debug
2023-09-30 10:59:43,590:INFO:  Defining the model
2023-09-30 10:59:43,633:INFO:  Reading the dataset
2023-09-30 10:59:50,678:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 10:59:50,702:INFO:  Epoch 1/500:  train Loss: 99.4806   val Loss: 97.1457   time: 1.75s   best: 97.1457
2023-09-30 10:59:50,979:INFO:  Epoch 2/500:  train Loss: 98.7218   val Loss: 97.7046   time: 0.27s   best: 97.1457
2023-09-30 10:59:51,245:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 10:59:51,266:INFO:  Epoch 3/500:  train Loss: 98.5567   val Loss: 96.3332   time: 0.26s   best: 96.3332
2023-09-30 10:59:51,539:INFO:  Epoch 4/500:  train Loss: 98.3302   val Loss: 99.5990   time: 0.27s   best: 96.3332
2023-09-30 10:59:51,791:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 10:59:51,812:INFO:  Epoch 5/500:  train Loss: 98.4769   val Loss: 95.0584   time: 0.25s   best: 95.0584
2023-09-30 10:59:52,084:INFO:  Epoch 6/500:  train Loss: 98.3401   val Loss: 95.0820   time: 0.27s   best: 95.0584
2023-09-30 10:59:52,336:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 10:59:52,357:INFO:  Epoch 7/500:  train Loss: 97.6709   val Loss: 94.5565   time: 0.25s   best: 94.5565
2023-09-30 10:59:52,628:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 10:59:52,649:INFO:  Epoch 8/500:  train Loss: 97.2218   val Loss: 93.9349   time: 0.27s   best: 93.9349
2023-09-30 10:59:52,903:INFO:  Epoch 9/500:  train Loss: 96.9537   val Loss: 93.9831   time: 0.25s   best: 93.9349
2023-09-30 10:59:53,182:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 10:59:53,203:INFO:  Epoch 10/500:  train Loss: 96.0444   val Loss: 92.8775   time: 0.27s   best: 92.8775
2023-09-30 10:59:53,457:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 10:59:53,477:INFO:  Epoch 11/500:  train Loss: 95.5021   val Loss: 91.9705   time: 0.25s   best: 91.9705
2023-09-30 10:59:53,747:INFO:  Epoch 12/500:  train Loss: 94.4738   val Loss: 92.3263   time: 0.27s   best: 91.9705
2023-09-30 10:59:54,002:INFO:  Epoch 13/500:  train Loss: 94.1718   val Loss: 95.1718   time: 0.25s   best: 91.9705
2023-09-30 10:59:54,273:INFO:  Epoch 14/500:  train Loss: 94.7988   val Loss: 95.0556   time: 0.27s   best: 91.9705
2023-09-30 10:59:54,523:INFO:  Epoch 15/500:  train Loss: 94.8900   val Loss: 94.8821   time: 0.25s   best: 91.9705
2023-09-30 10:59:54,792:INFO:  Epoch 16/500:  train Loss: 94.6718   val Loss: 94.6740   time: 0.26s   best: 91.9705
2023-09-30 10:59:55,045:INFO:  Epoch 17/500:  train Loss: 93.8917   val Loss: 94.5729   time: 0.25s   best: 91.9705
2023-09-30 10:59:55,326:INFO:  Epoch 18/500:  train Loss: 94.5984   val Loss: 94.4539   time: 0.28s   best: 91.9705
2023-09-30 10:59:55,576:INFO:  Epoch 19/500:  train Loss: 93.8175   val Loss: 94.3265   time: 0.25s   best: 91.9705
2023-09-30 10:59:55,845:INFO:  Epoch 20/500:  train Loss: 93.3550   val Loss: 94.2471   time: 0.27s   best: 91.9705
2023-09-30 10:59:56,100:INFO:  Epoch 21/500:  train Loss: 93.6107   val Loss: 94.3357   time: 0.25s   best: 91.9705
2023-09-30 10:59:56,369:INFO:  Epoch 22/500:  train Loss: 93.7626   val Loss: 94.3141   time: 0.27s   best: 91.9705
2023-09-30 10:59:56,621:INFO:  Epoch 23/500:  train Loss: 93.9058   val Loss: 94.3676   time: 0.25s   best: 91.9705
2023-09-30 10:59:56,891:INFO:  Epoch 24/500:  train Loss: 94.3725   val Loss: 94.2459   time: 0.27s   best: 91.9705
2023-09-30 10:59:57,149:INFO:  Epoch 25/500:  train Loss: 93.9979   val Loss: 94.0635   time: 0.25s   best: 91.9705
2023-09-30 10:59:57,426:INFO:  Epoch 26/500:  train Loss: 93.4589   val Loss: 93.6193   time: 0.27s   best: 91.9705
2023-09-30 10:59:57,677:INFO:  Epoch 27/500:  train Loss: 93.5831   val Loss: 93.0017   time: 0.25s   best: 91.9705
2023-09-30 10:59:57,952:INFO:  Epoch 28/500:  train Loss: 93.0382   val Loss: 92.6610   time: 0.27s   best: 91.9705
2023-09-30 10:59:58,204:INFO:  Epoch 29/500:  train Loss: 93.3602   val Loss: 92.7120   time: 0.25s   best: 91.9705
2023-09-30 10:59:58,473:INFO:  Epoch 30/500:  train Loss: 93.5783   val Loss: 93.7503   time: 0.27s   best: 91.9705
2023-09-30 10:59:58,723:INFO:  Epoch 31/500:  train Loss: 93.2085   val Loss: 94.0183   time: 0.25s   best: 91.9705
2023-09-30 10:59:58,993:INFO:  Epoch 32/500:  train Loss: 93.3563   val Loss: 94.1145   time: 0.27s   best: 91.9705
2023-09-30 10:59:59,256:INFO:  Epoch 33/500:  train Loss: 93.8787   val Loss: 94.1044   time: 0.26s   best: 91.9705
2023-09-30 10:59:59,531:INFO:  Epoch 34/500:  train Loss: 93.7163   val Loss: 94.0424   time: 0.27s   best: 91.9705
2023-09-30 10:59:59,783:INFO:  Epoch 35/500:  train Loss: 93.3639   val Loss: 93.7118   time: 0.25s   best: 91.9705
2023-09-30 11:00:00,057:INFO:  Epoch 36/500:  train Loss: 93.6222   val Loss: 93.5035   time: 0.27s   best: 91.9705
2023-09-30 11:00:00,307:INFO:  Epoch 37/500:  train Loss: 93.3193   val Loss: 93.2579   time: 0.25s   best: 91.9705
2023-09-30 11:00:00,576:INFO:  Epoch 38/500:  train Loss: 92.9002   val Loss: 92.4986   time: 0.27s   best: 91.9705
2023-09-30 11:00:00,826:INFO:  Epoch 39/500:  train Loss: 92.3620   val Loss: 92.0759   time: 0.25s   best: 91.9705
2023-09-30 11:00:01,097:INFO:  Epoch 40/500:  train Loss: 92.5183   val Loss: 92.1226   time: 0.27s   best: 91.9705
2023-09-30 11:00:01,364:INFO:  Epoch 41/500:  train Loss: 91.8203   val Loss: 92.2390   time: 0.26s   best: 91.9705
2023-09-30 11:00:01,634:INFO:  Epoch 42/500:  train Loss: 92.3936   val Loss: 92.0845   time: 0.27s   best: 91.9705
2023-09-30 11:00:01,886:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:01,927:INFO:  Epoch 43/500:  train Loss: 91.9749   val Loss: 91.7803   time: 0.25s   best: 91.7803
2023-09-30 11:00:02,180:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:02,202:INFO:  Epoch 44/500:  train Loss: 91.2557   val Loss: 91.1886   time: 0.25s   best: 91.1886
2023-09-30 11:00:02,472:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:02,494:INFO:  Epoch 45/500:  train Loss: 91.0577   val Loss: 90.7943   time: 0.27s   best: 90.7943
2023-09-30 11:00:02,745:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:02,767:INFO:  Epoch 46/500:  train Loss: 90.5390   val Loss: 90.1684   time: 0.25s   best: 90.1684
2023-09-30 11:00:03,035:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:03,057:INFO:  Epoch 47/500:  train Loss: 90.1515   val Loss: 89.5911   time: 0.26s   best: 89.5911
2023-09-30 11:00:03,318:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:03,343:INFO:  Epoch 48/500:  train Loss: 89.6257   val Loss: 88.6252   time: 0.26s   best: 88.6252
2023-09-30 11:00:03,612:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:03,635:INFO:  Epoch 49/500:  train Loss: 88.7762   val Loss: 88.0039   time: 0.26s   best: 88.0039
2023-09-30 11:00:03,888:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:03,910:INFO:  Epoch 50/500:  train Loss: 88.2328   val Loss: 87.4833   time: 0.25s   best: 87.4833
2023-09-30 11:00:04,183:INFO:  Epoch 51/500:  train Loss: 88.4907   val Loss: 87.5429   time: 0.27s   best: 87.4833
2023-09-30 11:00:04,438:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:04,459:INFO:  Epoch 52/500:  train Loss: 87.9108   val Loss: 86.9886   time: 0.25s   best: 86.9886
2023-09-30 11:00:04,492:INFO:  Epoch 373/500:  train Loss: 17.9871   val Loss: 23.4497   time: 442.89s   best: 23.0062
2023-09-30 11:00:04,731:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:04,771:INFO:  Epoch 53/500:  train Loss: 87.4312   val Loss: 86.5818   time: 0.27s   best: 86.5818
2023-09-30 11:00:05,023:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:05,044:INFO:  Epoch 54/500:  train Loss: 87.2343   val Loss: 86.1142   time: 0.25s   best: 86.1142
2023-09-30 11:00:05,325:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:05,350:INFO:  Epoch 55/500:  train Loss: 86.8111   val Loss: 85.9072   time: 0.28s   best: 85.9072
2023-09-30 11:00:05,622:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:05,643:INFO:  Epoch 56/500:  train Loss: 86.1879   val Loss: 85.7128   time: 0.27s   best: 85.7128
2023-09-30 11:00:05,896:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:05,917:INFO:  Epoch 57/500:  train Loss: 86.3138   val Loss: 85.3193   time: 0.25s   best: 85.3193
2023-09-30 11:00:06,184:INFO:  Epoch 58/500:  train Loss: 85.8998   val Loss: 86.1488   time: 0.26s   best: 85.3193
2023-09-30 11:00:06,434:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:06,455:INFO:  Epoch 59/500:  train Loss: 86.3248   val Loss: 85.1002   time: 0.24s   best: 85.1002
2023-09-30 11:00:06,721:INFO:  Epoch 60/500:  train Loss: 85.6401   val Loss: 85.9468   time: 0.26s   best: 85.1002
2023-09-30 11:00:06,972:INFO:  Epoch 61/500:  train Loss: 85.9832   val Loss: 86.2617   time: 0.25s   best: 85.1002
2023-09-30 11:00:07,251:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:07,272:INFO:  Epoch 62/500:  train Loss: 85.9589   val Loss: 84.6761   time: 0.27s   best: 84.6761
2023-09-30 11:00:07,527:INFO:  Epoch 63/500:  train Loss: 88.6016   val Loss: 84.7819   time: 0.25s   best: 84.6761
2023-09-30 11:00:07,797:INFO:  Epoch 64/500:  train Loss: 85.7178   val Loss: 85.1323   time: 0.27s   best: 84.6761
2023-09-30 11:00:08,054:INFO:  Epoch 65/500:  train Loss: 85.5785   val Loss: 84.9565   time: 0.25s   best: 84.6761
2023-09-30 11:00:08,322:INFO:  Epoch 66/500:  train Loss: 85.1379   val Loss: 85.3891   time: 0.26s   best: 84.6761
2023-09-30 11:00:08,573:INFO:  Epoch 67/500:  train Loss: 85.8932   val Loss: 85.5095   time: 0.25s   best: 84.6761
2023-09-30 11:00:08,842:INFO:  Epoch 68/500:  train Loss: 86.4798   val Loss: 85.6845   time: 0.26s   best: 84.6761
2023-09-30 11:00:09,094:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:09,116:INFO:  Epoch 69/500:  train Loss: 85.3830   val Loss: 84.5297   time: 0.25s   best: 84.5297
2023-09-30 11:00:09,397:INFO:  Epoch 70/500:  train Loss: 85.4312   val Loss: 84.8169   time: 0.28s   best: 84.5297
2023-09-30 11:00:09,651:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:09,672:INFO:  Epoch 71/500:  train Loss: 84.6422   val Loss: 84.1932   time: 0.25s   best: 84.1932
2023-09-30 11:00:09,942:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:09,988:INFO:  Epoch 72/500:  train Loss: 84.3938   val Loss: 83.6912   time: 0.27s   best: 83.6912
2023-09-30 11:00:10,242:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:10,263:INFO:  Epoch 73/500:  train Loss: 84.8301   val Loss: 83.4238   time: 0.25s   best: 83.4238
2023-09-30 11:00:10,532:INFO:  Epoch 74/500:  train Loss: 83.9918   val Loss: 83.4452   time: 0.27s   best: 83.4238
2023-09-30 11:00:10,835:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:10,856:INFO:  Epoch 75/500:  train Loss: 83.5705   val Loss: 83.2520   time: 0.30s   best: 83.2520
2023-09-30 11:00:11,110:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:11,131:INFO:  Epoch 76/500:  train Loss: 83.8344   val Loss: 83.0550   time: 0.25s   best: 83.0550
2023-09-30 11:00:11,411:INFO:  Epoch 77/500:  train Loss: 83.5855   val Loss: 84.3459   time: 0.28s   best: 83.0550
2023-09-30 11:00:11,665:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:11,686:INFO:  Epoch 78/500:  train Loss: 83.8782   val Loss: 82.5093   time: 0.25s   best: 82.5093
2023-09-30 11:00:11,960:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:11,981:INFO:  Epoch 79/500:  train Loss: 83.5067   val Loss: 82.4111   time: 0.27s   best: 82.4111
2023-09-30 11:00:12,234:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:12,256:INFO:  Epoch 80/500:  train Loss: 83.0153   val Loss: 82.3347   time: 0.25s   best: 82.3347
2023-09-30 11:00:12,526:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:12,547:INFO:  Epoch 81/500:  train Loss: 82.7951   val Loss: 82.2592   time: 0.26s   best: 82.2592
2023-09-30 11:00:12,799:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:12,820:INFO:  Epoch 82/500:  train Loss: 82.7481   val Loss: 81.7972   time: 0.25s   best: 81.7972
2023-09-30 11:00:13,090:INFO:  Epoch 83/500:  train Loss: 83.0141   val Loss: 82.6961   time: 0.27s   best: 81.7972
2023-09-30 11:00:13,357:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:13,381:INFO:  Epoch 84/500:  train Loss: 82.7224   val Loss: 81.5110   time: 0.26s   best: 81.5110
2023-09-30 11:00:13,685:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:13,706:INFO:  Epoch 85/500:  train Loss: 84.0772   val Loss: 81.3730   time: 0.30s   best: 81.3730
2023-09-30 11:00:13,962:INFO:  Epoch 86/500:  train Loss: 84.3774   val Loss: 86.1312   time: 0.25s   best: 81.3730
2023-09-30 11:00:14,230:INFO:  Epoch 87/500:  train Loss: 86.0350   val Loss: 83.6065   time: 0.26s   best: 81.3730
2023-09-30 11:00:14,481:INFO:  Epoch 88/500:  train Loss: 83.2554   val Loss: 83.3622   time: 0.25s   best: 81.3730
2023-09-30 11:00:14,749:INFO:  Epoch 89/500:  train Loss: 83.6984   val Loss: 82.1859   time: 0.26s   best: 81.3730
2023-09-30 11:00:15,001:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:15,022:INFO:  Epoch 90/500:  train Loss: 82.1479   val Loss: 81.2698   time: 0.25s   best: 81.2698
2023-09-30 11:00:15,305:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:15,326:INFO:  Epoch 91/500:  train Loss: 81.3795   val Loss: 80.8432   time: 0.28s   best: 80.8432
2023-09-30 11:00:15,582:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:15,621:INFO:  Epoch 92/500:  train Loss: 81.5010   val Loss: 80.4071   time: 0.25s   best: 80.4071
2023-09-30 11:00:15,874:INFO:  Epoch 93/500:  train Loss: 81.8172   val Loss: 81.3731   time: 0.25s   best: 80.4071
2023-09-30 11:00:16,146:INFO:  Epoch 94/500:  train Loss: 81.6983   val Loss: 81.2134   time: 0.27s   best: 80.4071
2023-09-30 11:00:16,401:INFO:  Epoch 95/500:  train Loss: 84.0568   val Loss: 80.5306   time: 0.25s   best: 80.4071
2023-09-30 11:00:16,674:INFO:  Epoch 96/500:  train Loss: 81.5679   val Loss: 81.3132   time: 0.27s   best: 80.4071
2023-09-30 11:00:16,925:INFO:  Epoch 97/500:  train Loss: 81.3292   val Loss: 81.0026   time: 0.25s   best: 80.4071
2023-09-30 11:00:17,200:INFO:  Epoch 98/500:  train Loss: 81.5791   val Loss: 80.5898   time: 0.27s   best: 80.4071
2023-09-30 11:00:17,663:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:17,708:INFO:  Epoch 99/500:  train Loss: 80.9002   val Loss: 80.1308   time: 0.45s   best: 80.1308
2023-09-30 11:00:18,021:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:18,052:INFO:  Epoch 100/500:  train Loss: 81.1339   val Loss: 79.9471   time: 0.31s   best: 79.9471
2023-09-30 11:00:18,330:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:18,359:INFO:  Epoch 101/500:  train Loss: 79.6746   val Loss: 79.6291   time: 0.27s   best: 79.6291
2023-09-30 11:00:18,611:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:18,640:INFO:  Epoch 102/500:  train Loss: 80.4286   val Loss: 79.2992   time: 0.25s   best: 79.2992
2023-09-30 11:00:18,909:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:18,943:INFO:  Epoch 103/500:  train Loss: 80.5561   val Loss: 79.2613   time: 0.26s   best: 79.2613
2023-09-30 11:00:19,201:INFO:  Epoch 104/500:  train Loss: 80.3447   val Loss: 79.9873   time: 0.26s   best: 79.2613
2023-09-30 11:00:19,491:INFO:  Epoch 105/500:  train Loss: 81.0980   val Loss: 80.1180   time: 0.28s   best: 79.2613
2023-09-30 11:00:19,758:INFO:  Epoch 106/500:  train Loss: 80.7595   val Loss: 80.3082   time: 0.25s   best: 79.2613
2023-09-30 11:00:20,033:INFO:  Epoch 107/500:  train Loss: 80.8583   val Loss: 80.1461   time: 0.27s   best: 79.2613
2023-09-30 11:00:20,383:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:20,413:INFO:  Epoch 108/500:  train Loss: 79.5842   val Loss: 79.0965   time: 0.25s   best: 79.0965
2023-09-30 11:00:20,665:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:20,694:INFO:  Epoch 109/500:  train Loss: 79.8107   val Loss: 78.6979   time: 0.25s   best: 78.6979
2023-09-30 11:00:20,967:INFO:  Epoch 110/500:  train Loss: 79.7766   val Loss: 78.9842   time: 0.26s   best: 78.6979
2023-09-30 11:00:21,230:INFO:  Epoch 111/500:  train Loss: 79.7611   val Loss: 78.8336   time: 0.25s   best: 78.6979
2023-09-30 11:00:21,512:INFO:  Epoch 112/500:  train Loss: 80.3184   val Loss: 79.5565   time: 0.27s   best: 78.6979
2023-09-30 11:00:21,776:INFO:  Epoch 113/500:  train Loss: 79.9422   val Loss: 78.8501   time: 0.25s   best: 78.6979
2023-09-30 11:00:22,050:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:22,084:INFO:  Epoch 114/500:  train Loss: 79.5834   val Loss: 78.4339   time: 0.27s   best: 78.4339
2023-09-30 11:00:22,336:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:22,365:INFO:  Epoch 115/500:  train Loss: 79.5485   val Loss: 78.3313   time: 0.25s   best: 78.3313
2023-09-30 11:00:22,634:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:22,664:INFO:  Epoch 116/500:  train Loss: 79.1431   val Loss: 78.1338   time: 0.26s   best: 78.1338
2023-09-30 11:00:22,922:INFO:  Epoch 117/500:  train Loss: 79.0226   val Loss: 78.2300   time: 0.25s   best: 78.1338
2023-09-30 11:00:23,197:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:23,220:INFO:  Epoch 118/500:  train Loss: 79.4018   val Loss: 77.9257   time: 0.27s   best: 77.9257
2023-09-30 11:00:23,500:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:23,530:INFO:  Epoch 119/500:  train Loss: 78.8115   val Loss: 77.8786   time: 0.27s   best: 77.8786
2023-09-30 11:00:23,787:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:23,817:INFO:  Epoch 120/500:  train Loss: 78.7807   val Loss: 77.8260   time: 0.25s   best: 77.8260
2023-09-30 11:00:24,089:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:24,119:INFO:  Epoch 121/500:  train Loss: 78.8334   val Loss: 77.2305   time: 0.27s   best: 77.2305
2023-09-30 11:00:24,379:INFO:  Epoch 122/500:  train Loss: 78.1548   val Loss: 77.5268   time: 0.25s   best: 77.2305
2023-09-30 11:00:24,648:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:24,697:INFO:  Epoch 123/500:  train Loss: 78.5632   val Loss: 77.1262   time: 0.26s   best: 77.1262
2023-09-30 11:00:24,956:INFO:  Epoch 124/500:  train Loss: 80.1565   val Loss: 80.8876   time: 0.25s   best: 77.1262
2023-09-30 11:00:25,236:INFO:  Epoch 125/500:  train Loss: 80.9247   val Loss: 80.2295   time: 0.27s   best: 77.1262
2023-09-30 11:00:25,499:INFO:  Epoch 126/500:  train Loss: 81.0759   val Loss: 79.2993   time: 0.25s   best: 77.1262
2023-09-30 11:00:25,782:INFO:  Epoch 127/500:  train Loss: 79.1224   val Loss: 77.3505   time: 0.27s   best: 77.1262
2023-09-30 11:00:26,035:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:26,066:INFO:  Epoch 128/500:  train Loss: 77.8420   val Loss: 76.6250   time: 0.25s   best: 76.6250
2023-09-30 11:00:26,336:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:26,369:INFO:  Epoch 129/500:  train Loss: 78.6091   val Loss: 76.5915   time: 0.27s   best: 76.5915
2023-09-30 11:00:26,646:INFO:  Epoch 130/500:  train Loss: 77.8262   val Loss: 76.9777   time: 0.26s   best: 76.5915
2023-09-30 11:00:26,904:INFO:  Epoch 131/500:  train Loss: 78.5228   val Loss: 77.6736   time: 0.25s   best: 76.5915
2023-09-30 11:00:27,187:INFO:  Epoch 132/500:  train Loss: 79.3770   val Loss: 78.5547   time: 0.27s   best: 76.5915
2023-09-30 11:00:27,450:INFO:  Epoch 133/500:  train Loss: 78.5107   val Loss: 76.7450   time: 0.25s   best: 76.5915
2023-09-30 11:00:27,720:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:27,756:INFO:  Epoch 134/500:  train Loss: 77.5983   val Loss: 76.4276   time: 0.26s   best: 76.4276
2023-09-30 11:00:28,015:INFO:  Epoch 135/500:  train Loss: 79.0705   val Loss: 76.8973   time: 0.25s   best: 76.4276
2023-09-30 11:00:28,294:INFO:  Epoch 136/500:  train Loss: 77.9667   val Loss: 77.2115   time: 0.27s   best: 76.4276
2023-09-30 11:00:28,555:INFO:  Epoch 137/500:  train Loss: 78.8302   val Loss: 78.0412   time: 0.25s   best: 76.4276
2023-09-30 11:00:28,832:INFO:  Epoch 138/500:  train Loss: 78.0428   val Loss: 76.9333   time: 0.26s   best: 76.4276
2023-09-30 11:00:29,091:INFO:  Epoch 139/500:  train Loss: 78.8142   val Loss: 76.4935   time: 0.25s   best: 76.4276
2023-09-30 11:00:29,366:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:29,400:INFO:  Epoch 140/500:  train Loss: 78.3538   val Loss: 75.0316   time: 0.27s   best: 75.0316
2023-09-30 11:00:29,651:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:29,681:INFO:  Epoch 141/500:  train Loss: 76.7118   val Loss: 74.9068   time: 0.25s   best: 74.9068
2023-09-30 11:00:29,956:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:29,988:INFO:  Epoch 142/500:  train Loss: 76.0754   val Loss: 74.4036   time: 0.27s   best: 74.4036
2023-09-30 11:00:30,264:INFO:  Epoch 143/500:  train Loss: 76.5952   val Loss: 75.2148   time: 0.25s   best: 74.4036
2023-09-30 11:00:30,527:INFO:  Epoch 144/500:  train Loss: 76.5526   val Loss: 74.6664   time: 0.25s   best: 74.4036
2023-09-30 11:00:30,788:INFO:  Epoch 145/500:  train Loss: 76.1619   val Loss: 75.4097   time: 0.25s   best: 74.4036
2023-09-30 11:00:31,066:INFO:  Epoch 146/500:  train Loss: 76.4216   val Loss: 74.8813   time: 0.26s   best: 74.4036
2023-09-30 11:00:31,322:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:31,371:INFO:  Epoch 147/500:  train Loss: 75.4881   val Loss: 74.1691   time: 0.25s   best: 74.1691
2023-09-30 11:00:31,627:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:31,657:INFO:  Epoch 148/500:  train Loss: 75.2639   val Loss: 73.4452   time: 0.25s   best: 73.4452
2023-09-30 11:00:31,936:INFO:  Epoch 149/500:  train Loss: 79.1358   val Loss: 75.8954   time: 0.27s   best: 73.4452
2023-09-30 11:00:32,201:INFO:  Epoch 150/500:  train Loss: 79.9632   val Loss: 78.2414   time: 0.25s   best: 73.4452
2023-09-30 11:00:32,478:INFO:  Epoch 151/500:  train Loss: 79.4612   val Loss: 78.9302   time: 0.26s   best: 73.4452
2023-09-30 11:00:32,737:INFO:  Epoch 152/500:  train Loss: 78.4139   val Loss: 75.6925   time: 0.25s   best: 73.4452
2023-09-30 11:00:33,014:INFO:  Epoch 153/500:  train Loss: 79.0085   val Loss: 77.5327   time: 0.26s   best: 73.4452
2023-09-30 11:00:33,278:INFO:  Epoch 154/500:  train Loss: 77.2991   val Loss: 75.7572   time: 0.25s   best: 73.4452
2023-09-30 11:00:33,559:INFO:  Epoch 155/500:  train Loss: 77.5285   val Loss: 76.6796   time: 0.27s   best: 73.4452
2023-09-30 11:00:33,822:INFO:  Epoch 156/500:  train Loss: 77.2601   val Loss: 75.3436   time: 0.25s   best: 73.4452
2023-09-30 11:00:34,101:INFO:  Epoch 157/500:  train Loss: 77.4661   val Loss: 74.7796   time: 0.27s   best: 73.4452
2023-09-30 11:00:34,368:INFO:  Epoch 158/500:  train Loss: 80.5046   val Loss: 80.7643   time: 0.26s   best: 73.4452
2023-09-30 11:00:34,645:INFO:  Epoch 159/500:  train Loss: 79.5070   val Loss: 78.8199   time: 0.26s   best: 73.4452
2023-09-30 11:00:34,904:INFO:  Epoch 160/500:  train Loss: 80.3194   val Loss: 78.8890   time: 0.25s   best: 73.4452
2023-09-30 11:00:35,186:INFO:  Epoch 161/500:  train Loss: 79.0641   val Loss: 77.2731   time: 0.27s   best: 73.4452
2023-09-30 11:00:35,442:INFO:  Epoch 162/500:  train Loss: 77.8348   val Loss: 75.3447   time: 0.25s   best: 73.4452
2023-09-30 11:00:35,726:INFO:  Epoch 163/500:  train Loss: 76.5771   val Loss: 74.3394   time: 0.27s   best: 73.4452
2023-09-30 11:00:35,985:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:36,033:INFO:  Epoch 164/500:  train Loss: 75.0634   val Loss: 73.3017   time: 0.25s   best: 73.3017
2023-09-30 11:00:36,297:INFO:  Epoch 165/500:  train Loss: 75.3463   val Loss: 73.8031   time: 0.25s   best: 73.3017
2023-09-30 11:00:36,575:INFO:  Epoch 166/500:  train Loss: 76.4618   val Loss: 75.8782   time: 0.27s   best: 73.3017
2023-09-30 11:00:36,834:INFO:  Epoch 167/500:  train Loss: 75.6511   val Loss: 73.4307   time: 0.25s   best: 73.3017
2023-09-30 11:00:37,111:INFO:  Epoch 168/500:  train Loss: 75.8871   val Loss: 74.4083   time: 0.26s   best: 73.3017
2023-09-30 11:00:37,375:INFO:  Epoch 169/500:  train Loss: 75.1364   val Loss: 73.5302   time: 0.25s   best: 73.3017
2023-09-30 11:00:37,648:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:37,678:INFO:  Epoch 170/500:  train Loss: 74.3830   val Loss: 72.4873   time: 0.27s   best: 72.4873
2023-09-30 11:00:37,943:INFO:  Epoch 171/500:  train Loss: 75.4821   val Loss: 73.0663   time: 0.25s   best: 72.4873
2023-09-30 11:00:38,222:INFO:  Epoch 172/500:  train Loss: 74.3737   val Loss: 73.1381   time: 0.27s   best: 72.4873
2023-09-30 11:00:38,483:INFO:  Epoch 173/500:  train Loss: 74.0409   val Loss: 72.5913   time: 0.25s   best: 72.4873
2023-09-30 11:00:38,755:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:38,784:INFO:  Epoch 174/500:  train Loss: 73.1052   val Loss: 72.2800   time: 0.27s   best: 72.2800
2023-09-30 11:00:39,037:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:39,066:INFO:  Epoch 175/500:  train Loss: 73.6529   val Loss: 72.0888   time: 0.25s   best: 72.0888
2023-09-30 11:00:39,339:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:39,368:INFO:  Epoch 176/500:  train Loss: 73.1602   val Loss: 71.9194   time: 0.27s   best: 71.9194
2023-09-30 11:00:39,633:INFO:  Epoch 177/500:  train Loss: 73.5815   val Loss: 72.5521   time: 0.25s   best: 71.9194
2023-09-30 11:00:39,913:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:39,942:INFO:  Epoch 178/500:  train Loss: 72.9541   val Loss: 71.6369   time: 0.27s   best: 71.6369
2023-09-30 11:00:40,224:INFO:  Epoch 179/500:  train Loss: 74.1495   val Loss: 72.3052   time: 0.27s   best: 71.6369
2023-09-30 11:00:40,484:INFO:  Epoch 180/500:  train Loss: 74.2386   val Loss: 72.9586   time: 0.25s   best: 71.6369
2023-09-30 11:00:40,762:INFO:  Epoch 181/500:  train Loss: 73.8681   val Loss: 72.0141   time: 0.27s   best: 71.6369
2023-09-30 11:00:41,020:INFO:  Epoch 182/500:  train Loss: 73.0169   val Loss: 71.8761   time: 0.25s   best: 71.6369
2023-09-30 11:00:41,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:41,325:INFO:  Epoch 183/500:  train Loss: 73.2791   val Loss: 71.4561   time: 0.27s   best: 71.4561
2023-09-30 11:00:41,588:INFO:  Epoch 184/500:  train Loss: 73.6545   val Loss: 71.8399   time: 0.25s   best: 71.4561
2023-09-30 11:00:41,872:INFO:  Epoch 185/500:  train Loss: 73.8935   val Loss: 71.7197   time: 0.27s   best: 71.4561
2023-09-30 11:00:42,128:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:42,159:INFO:  Epoch 186/500:  train Loss: 72.9709   val Loss: 70.6889   time: 0.25s   best: 70.6889
2023-09-30 11:00:42,428:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:42,464:INFO:  Epoch 187/500:  train Loss: 72.4608   val Loss: 70.6141   time: 0.26s   best: 70.6141
2023-09-30 11:00:42,723:INFO:  Epoch 188/500:  train Loss: 72.3496   val Loss: 71.3581   time: 0.25s   best: 70.6141
2023-09-30 11:00:42,993:INFO:  Epoch 189/500:  train Loss: 72.8929   val Loss: 71.6767   time: 0.26s   best: 70.6141
2023-09-30 11:00:43,266:INFO:  Epoch 190/500:  train Loss: 72.8271   val Loss: 71.4894   time: 0.26s   best: 70.6141
2023-09-30 11:00:43,548:INFO:  Epoch 191/500:  train Loss: 71.2184   val Loss: 71.1122   time: 0.27s   best: 70.6141
2023-09-30 11:00:43,800:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:43,838:INFO:  Epoch 192/500:  train Loss: 73.0942   val Loss: 70.1973   time: 0.25s   best: 70.1973
2023-09-30 11:00:44,119:INFO:  Epoch 193/500:  train Loss: 72.8825   val Loss: 70.8419   time: 0.27s   best: 70.1973
2023-09-30 11:00:44,388:INFO:  Epoch 194/500:  train Loss: 71.9609   val Loss: 71.3963   time: 0.25s   best: 70.1973
2023-09-30 11:00:44,659:INFO:  Epoch 195/500:  train Loss: 72.0330   val Loss: 70.6115   time: 0.26s   best: 70.1973
2023-09-30 11:00:44,925:INFO:  Epoch 196/500:  train Loss: 72.3696   val Loss: 70.3106   time: 0.25s   best: 70.1973
2023-09-30 11:00:45,201:INFO:  Epoch 197/500:  train Loss: 72.0746   val Loss: 70.6517   time: 0.26s   best: 70.1973
2023-09-30 11:00:45,481:INFO:  Epoch 198/500:  train Loss: 71.9484   val Loss: 71.2427   time: 0.27s   best: 70.1973
2023-09-30 11:00:45,740:INFO:  Epoch 199/500:  train Loss: 72.8025   val Loss: 70.3584   time: 0.25s   best: 70.1973
2023-09-30 11:00:46,066:INFO:  Epoch 200/500:  train Loss: 71.9390   val Loss: 70.6125   time: 0.31s   best: 70.1973
2023-09-30 11:00:46,342:INFO:  Epoch 201/500:  train Loss: 71.0403   val Loss: 70.2976   time: 0.26s   best: 70.1973
2023-09-30 11:00:46,619:INFO:  Epoch 202/500:  train Loss: 71.3121   val Loss: 70.4635   time: 0.26s   best: 70.1973
2023-09-30 11:00:46,871:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:46,899:INFO:  Epoch 203/500:  train Loss: 71.5984   val Loss: 70.1335   time: 0.25s   best: 70.1335
2023-09-30 11:00:47,185:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:47,215:INFO:  Epoch 204/500:  train Loss: 71.9895   val Loss: 69.4216   time: 0.28s   best: 69.4216
2023-09-30 11:00:47,476:INFO:  Epoch 205/500:  train Loss: 71.4236   val Loss: 70.1739   time: 0.25s   best: 69.4216
2023-09-30 11:00:47,751:INFO:  Epoch 206/500:  train Loss: 71.5085   val Loss: 69.7921   time: 0.26s   best: 69.4216
2023-09-30 11:00:48,174:INFO:  Epoch 207/500:  train Loss: 72.3741   val Loss: 69.8259   time: 0.42s   best: 69.4216
2023-09-30 11:00:48,505:INFO:  Epoch 208/500:  train Loss: 71.4669   val Loss: 70.5853   time: 0.32s   best: 69.4216
2023-09-30 11:00:48,778:INFO:  Epoch 209/500:  train Loss: 72.0692   val Loss: 70.7589   time: 0.26s   best: 69.4216
2023-09-30 11:00:49,033:INFO:  Epoch 210/500:  train Loss: 71.2837   val Loss: 70.2841   time: 0.25s   best: 69.4216
2023-09-30 11:00:49,305:INFO:  Epoch 211/500:  train Loss: 71.0309   val Loss: 69.7345   time: 0.27s   best: 69.4216
2023-09-30 11:00:49,569:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:49,600:INFO:  Epoch 212/500:  train Loss: 71.3064   val Loss: 69.4015   time: 0.26s   best: 69.4015
2023-09-30 11:00:49,876:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:00:49,900:INFO:  Epoch 213/500:  train Loss: 70.9878   val Loss: 68.6968   time: 0.27s   best: 68.6968
2023-09-30 11:00:50,184:INFO:  Epoch 214/500:  train Loss: 70.3470   val Loss: 70.0787   time: 0.27s   best: 68.6968
2023-09-30 11:00:50,450:INFO:  Epoch 215/500:  train Loss: 71.4931   val Loss: 70.1240   time: 0.25s   best: 68.6968
2023-09-30 11:00:50,728:INFO:  Epoch 216/500:  train Loss: 71.1811   val Loss: 69.7883   time: 0.27s   best: 68.6968
2023-09-30 11:00:50,986:INFO:  Epoch 217/500:  train Loss: 71.0208   val Loss: 70.0202   time: 0.25s   best: 68.6968
2023-09-30 11:00:51,269:INFO:  Epoch 218/500:  train Loss: 72.6374   val Loss: 70.3431   time: 0.27s   best: 68.6968
2023-09-30 11:00:51,530:INFO:  Epoch 219/500:  train Loss: 72.7770   val Loss: 70.4788   time: 0.25s   best: 68.6968
2023-09-30 11:00:51,807:INFO:  Epoch 220/500:  train Loss: 71.9735   val Loss: 70.8741   time: 0.26s   best: 68.6968
2023-09-30 11:00:52,067:INFO:  Epoch 221/500:  train Loss: 70.9828   val Loss: 70.4958   time: 0.25s   best: 68.6968
2023-09-30 11:00:52,350:INFO:  Epoch 222/500:  train Loss: 71.4690   val Loss: 69.9968   time: 0.26s   best: 68.6968
2023-09-30 11:00:52,609:INFO:  Epoch 223/500:  train Loss: 72.6530   val Loss: 70.1455   time: 0.25s   best: 68.6968
2023-09-30 11:00:52,880:INFO:  Epoch 224/500:  train Loss: 74.2769   val Loss: 69.3776   time: 0.26s   best: 68.6968
2023-09-30 11:00:53,146:INFO:  Epoch 225/500:  train Loss: 71.4141   val Loss: 69.2109   time: 0.25s   best: 68.6968
2023-09-30 11:00:53,430:INFO:  Epoch 226/500:  train Loss: 71.8130   val Loss: 69.0342   time: 0.27s   best: 68.6968
2023-09-30 11:00:53,689:INFO:  Epoch 227/500:  train Loss: 74.4524   val Loss: 72.4471   time: 0.25s   best: 68.6968
2023-09-30 11:00:53,967:INFO:  Epoch 228/500:  train Loss: 71.6723   val Loss: 72.9590   time: 0.26s   best: 68.6968
2023-09-30 11:00:54,227:INFO:  Epoch 229/500:  train Loss: 72.8511   val Loss: 70.0871   time: 0.25s   best: 68.6968
2023-09-30 11:00:54,510:INFO:  Epoch 230/500:  train Loss: 70.8094   val Loss: 69.1621   time: 0.27s   best: 68.6968
2023-09-30 11:00:54,768:INFO:  Epoch 231/500:  train Loss: 70.9914   val Loss: 69.4501   time: 0.25s   best: 68.6968
2023-09-30 11:00:55,045:INFO:  Epoch 232/500:  train Loss: 70.4810   val Loss: 69.5348   time: 0.26s   best: 68.6968
2023-09-30 11:00:55,310:INFO:  Epoch 233/500:  train Loss: 71.0110   val Loss: 69.5672   time: 0.25s   best: 68.6968
2023-09-30 11:00:55,592:INFO:  Epoch 234/500:  train Loss: 72.1141   val Loss: 72.0245   time: 0.27s   best: 68.6968
2023-09-30 11:00:55,845:INFO:  Epoch 235/500:  train Loss: 73.0836   val Loss: 71.6700   time: 0.25s   best: 68.6968
2023-09-30 11:00:56,130:INFO:  Epoch 236/500:  train Loss: 72.7512   val Loss: 70.4419   time: 0.27s   best: 68.6968
2023-09-30 11:00:56,402:INFO:  Epoch 237/500:  train Loss: 74.4219   val Loss: 70.7593   time: 0.26s   best: 68.6968
2023-09-30 11:00:56,682:INFO:  Epoch 238/500:  train Loss: 72.4277   val Loss: 71.4913   time: 0.27s   best: 68.6968
2023-09-30 11:00:56,953:INFO:  Epoch 239/500:  train Loss: 72.1169   val Loss: 71.0609   time: 0.25s   best: 68.6968
2023-09-30 11:00:57,223:INFO:  Epoch 240/500:  train Loss: 72.1448   val Loss: 71.0019   time: 0.26s   best: 68.6968
2023-09-30 11:00:57,501:INFO:  Epoch 241/500:  train Loss: 71.2393   val Loss: 69.8615   time: 0.27s   best: 68.6968
2023-09-30 11:00:57,761:INFO:  Epoch 242/500:  train Loss: 78.1702   val Loss: 74.9182   time: 0.25s   best: 68.6968
2023-09-30 11:00:58,037:INFO:  Epoch 243/500:  train Loss: 77.1278   val Loss: 76.8579   time: 0.26s   best: 68.6968
2023-09-30 11:00:58,300:INFO:  Epoch 244/500:  train Loss: 78.1183   val Loss: 75.9535   time: 0.25s   best: 68.6968
2023-09-30 11:00:58,584:INFO:  Epoch 245/500:  train Loss: 76.3929   val Loss: 74.3255   time: 0.27s   best: 68.6968
2023-09-30 11:00:58,837:INFO:  Epoch 246/500:  train Loss: 75.5369   val Loss: 75.3902   time: 0.25s   best: 68.6968
2023-09-30 11:00:59,120:INFO:  Epoch 247/500:  train Loss: 75.8983   val Loss: 75.1571   time: 0.27s   best: 68.6968
2023-09-30 11:00:59,384:INFO:  Epoch 248/500:  train Loss: 75.1184   val Loss: 73.3431   time: 0.25s   best: 68.6968
2023-09-30 11:00:59,664:INFO:  Epoch 249/500:  train Loss: 74.3453   val Loss: 72.7225   time: 0.27s   best: 68.6968
2023-09-30 11:00:59,923:INFO:  Epoch 250/500:  train Loss: 74.5343   val Loss: 75.0758   time: 0.25s   best: 68.6968
2023-09-30 11:01:00,203:INFO:  Epoch 251/500:  train Loss: 76.1891   val Loss: 74.2500   time: 0.27s   best: 68.6968
2023-09-30 11:01:00,471:INFO:  Epoch 252/500:  train Loss: 75.3126   val Loss: 73.0139   time: 0.26s   best: 68.6968
2023-09-30 11:01:00,750:INFO:  Epoch 253/500:  train Loss: 73.8025   val Loss: 72.1450   time: 0.27s   best: 68.6968
2023-09-30 11:01:01,009:INFO:  Epoch 254/500:  train Loss: 73.6182   val Loss: 71.3377   time: 0.25s   best: 68.6968
2023-09-30 11:01:01,293:INFO:  Epoch 255/500:  train Loss: 73.8579   val Loss: 71.5462   time: 0.27s   best: 68.6968
2023-09-30 11:01:01,553:INFO:  Epoch 256/500:  train Loss: 75.7915   val Loss: 74.5895   time: 0.25s   best: 68.6968
2023-09-30 11:01:01,824:INFO:  Epoch 257/500:  train Loss: 87.2830   val Loss: 90.6537   time: 0.27s   best: 68.6968
2023-09-30 11:01:02,090:INFO:  Epoch 258/500:  train Loss: 89.0833   val Loss: 85.1897   time: 0.25s   best: 68.6968
2023-09-30 11:01:02,368:INFO:  Epoch 259/500:  train Loss: 85.7084   val Loss: 85.6434   time: 0.27s   best: 68.6968
2023-09-30 11:01:02,635:INFO:  Epoch 260/500:  train Loss: 85.6268   val Loss: 84.7489   time: 0.25s   best: 68.6968
2023-09-30 11:01:02,913:INFO:  Epoch 261/500:  train Loss: 84.0274   val Loss: 83.4352   time: 0.27s   best: 68.6968
2023-09-30 11:01:03,175:INFO:  Epoch 262/500:  train Loss: 83.4611   val Loss: 81.7634   time: 0.25s   best: 68.6968
2023-09-30 11:01:03,457:INFO:  Epoch 263/500:  train Loss: 81.6907   val Loss: 79.6142   time: 0.27s   best: 68.6968
2023-09-30 11:01:03,717:INFO:  Epoch 264/500:  train Loss: 78.9810   val Loss: 76.4021   time: 0.25s   best: 68.6968
2023-09-30 11:01:03,988:INFO:  Epoch 265/500:  train Loss: 76.7035   val Loss: 74.6358   time: 0.27s   best: 68.6968
2023-09-30 11:01:04,242:INFO:  Epoch 266/500:  train Loss: 75.7096   val Loss: 74.8157   time: 0.25s   best: 68.6968
2023-09-30 11:01:04,532:INFO:  Epoch 267/500:  train Loss: 74.8671   val Loss: 73.4706   time: 0.28s   best: 68.6968
2023-09-30 11:01:04,804:INFO:  Epoch 268/500:  train Loss: 74.1023   val Loss: 72.8746   time: 0.27s   best: 68.6968
2023-09-30 11:01:05,080:INFO:  Epoch 269/500:  train Loss: 73.3569   val Loss: 71.7249   time: 0.26s   best: 68.6968
2023-09-30 11:01:05,361:INFO:  Epoch 270/500:  train Loss: 73.0305   val Loss: 72.3538   time: 0.25s   best: 68.6968
2023-09-30 11:01:05,625:INFO:  Epoch 271/500:  train Loss: 73.4620   val Loss: 71.8323   time: 0.25s   best: 68.6968
2023-09-30 11:01:05,903:INFO:  Epoch 272/500:  train Loss: 72.2282   val Loss: 70.8676   time: 0.27s   best: 68.6968
2023-09-30 11:01:06,166:INFO:  Epoch 273/500:  train Loss: 72.2523   val Loss: 71.1751   time: 0.25s   best: 68.6968
2023-09-30 11:01:06,453:INFO:  Epoch 274/500:  train Loss: 72.1641   val Loss: 71.7967   time: 0.27s   best: 68.6968
2023-09-30 11:01:06,713:INFO:  Epoch 275/500:  train Loss: 72.0719   val Loss: 70.2651   time: 0.25s   best: 68.6968
2023-09-30 11:01:06,990:INFO:  Epoch 276/500:  train Loss: 71.7220   val Loss: 70.0226   time: 0.26s   best: 68.6968
2023-09-30 11:01:07,254:INFO:  Epoch 277/500:  train Loss: 70.8183   val Loss: 69.7612   time: 0.25s   best: 68.6968
2023-09-30 11:01:07,532:INFO:  Epoch 278/500:  train Loss: 71.0447   val Loss: 70.1905   time: 0.26s   best: 68.6968
2023-09-30 11:01:07,792:INFO:  Epoch 279/500:  train Loss: 70.9028   val Loss: 69.6660   time: 0.25s   best: 68.6968
2023-09-30 11:01:08,070:INFO:  Epoch 280/500:  train Loss: 70.4839   val Loss: 69.1913   time: 0.26s   best: 68.6968
2023-09-30 11:01:08,324:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:08,354:INFO:  Epoch 281/500:  train Loss: 70.1273   val Loss: 68.4079   time: 0.25s   best: 68.4079
2023-09-30 11:01:08,630:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:08,661:INFO:  Epoch 282/500:  train Loss: 70.0469   val Loss: 67.6586   time: 0.27s   best: 67.6586
2023-09-30 11:01:08,920:INFO:  Epoch 283/500:  train Loss: 70.6096   val Loss: 68.1207   time: 0.25s   best: 67.6586
2023-09-30 11:01:09,198:INFO:  Epoch 284/500:  train Loss: 69.9457   val Loss: 68.1063   time: 0.27s   best: 67.6586
2023-09-30 11:01:09,464:INFO:  Epoch 285/500:  train Loss: 69.9528   val Loss: 68.8464   time: 0.25s   best: 67.6586
2023-09-30 11:01:09,741:INFO:  Epoch 286/500:  train Loss: 69.6700   val Loss: 67.8765   time: 0.26s   best: 67.6586
2023-09-30 11:01:10,001:INFO:  Epoch 287/500:  train Loss: 70.4974   val Loss: 67.9833   time: 0.25s   best: 67.6586
2023-09-30 11:01:10,274:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:10,303:INFO:  Epoch 288/500:  train Loss: 68.9347   val Loss: 67.2767   time: 0.27s   best: 67.2767
2023-09-30 11:01:10,583:INFO:  Epoch 289/500:  train Loss: 70.7738   val Loss: 68.1048   time: 0.25s   best: 67.2767
2023-09-30 11:01:10,846:INFO:  Epoch 290/500:  train Loss: 69.8593   val Loss: 67.3207   time: 0.25s   best: 67.2767
2023-09-30 11:01:11,112:INFO:  Epoch 291/500:  train Loss: 70.6060   val Loss: 67.8312   time: 0.25s   best: 67.2767
2023-09-30 11:01:11,379:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:11,410:INFO:  Epoch 292/500:  train Loss: 69.6357   val Loss: 67.2629   time: 0.26s   best: 67.2629
2023-09-30 11:01:11,681:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:11,711:INFO:  Epoch 293/500:  train Loss: 71.2526   val Loss: 66.8042   time: 0.27s   best: 66.8042
2023-09-30 11:01:11,963:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:11,993:INFO:  Epoch 294/500:  train Loss: 69.1667   val Loss: 66.0354   time: 0.25s   best: 66.0354
2023-09-30 11:01:12,269:INFO:  Epoch 295/500:  train Loss: 70.5967   val Loss: 69.1779   time: 0.26s   best: 66.0354
2023-09-30 11:01:12,533:INFO:  Epoch 296/500:  train Loss: 70.7757   val Loss: 68.3588   time: 0.25s   best: 66.0354
2023-09-30 11:01:12,811:INFO:  Epoch 297/500:  train Loss: 70.9883   val Loss: 67.2728   time: 0.26s   best: 66.0354
2023-09-30 11:01:13,069:INFO:  Epoch 298/500:  train Loss: 68.0948   val Loss: 66.6049   time: 0.25s   best: 66.0354
2023-09-30 11:01:13,353:INFO:  Epoch 299/500:  train Loss: 69.3261   val Loss: 66.6511   time: 0.27s   best: 66.0354
2023-09-30 11:01:13,659:INFO:  Epoch 300/500:  train Loss: 68.8267   val Loss: 67.2010   time: 0.29s   best: 66.0354
2023-09-30 11:01:13,945:INFO:  Epoch 301/500:  train Loss: 71.3565   val Loss: 67.9852   time: 0.27s   best: 66.0354
2023-09-30 11:01:14,207:INFO:  Epoch 302/500:  train Loss: 71.4976   val Loss: 68.2164   time: 0.25s   best: 66.0354
2023-09-30 11:01:14,494:INFO:  Epoch 303/500:  train Loss: 71.4382   val Loss: 67.2456   time: 0.27s   best: 66.0354
2023-09-30 11:01:14,753:INFO:  Epoch 304/500:  train Loss: 69.4755   val Loss: 66.8091   time: 0.25s   best: 66.0354
2023-09-30 11:01:15,029:INFO:  Epoch 305/500:  train Loss: 70.2123   val Loss: 67.5829   time: 0.26s   best: 66.0354
2023-09-30 11:01:15,313:INFO:  Epoch 306/500:  train Loss: 69.7268   val Loss: 67.0348   time: 0.25s   best: 66.0354
2023-09-30 11:01:15,577:INFO:  Epoch 307/500:  train Loss: 70.3761   val Loss: 67.7401   time: 0.25s   best: 66.0354
2023-09-30 11:01:15,854:INFO:  Epoch 308/500:  train Loss: 69.6013   val Loss: 66.8560   time: 0.26s   best: 66.0354
2023-09-30 11:01:16,108:INFO:  Epoch 309/500:  train Loss: 68.8921   val Loss: 66.7310   time: 0.25s   best: 66.0354
2023-09-30 11:01:16,394:INFO:  Epoch 310/500:  train Loss: 69.6988   val Loss: 68.1085   time: 0.27s   best: 66.0354
2023-09-30 11:01:16,660:INFO:  Epoch 311/500:  train Loss: 71.6281   val Loss: 66.7850   time: 0.25s   best: 66.0354
2023-09-30 11:01:16,930:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:16,961:INFO:  Epoch 312/500:  train Loss: 69.3130   val Loss: 65.8280   time: 0.27s   best: 65.8280
2023-09-30 11:01:17,228:INFO:  Epoch 313/500:  train Loss: 69.5118   val Loss: 67.8300   time: 0.25s   best: 65.8280
2023-09-30 11:01:17,506:INFO:  Epoch 314/500:  train Loss: 69.1797   val Loss: 66.8519   time: 0.26s   best: 65.8280
2023-09-30 11:01:17,764:INFO:  Epoch 315/500:  train Loss: 70.1196   val Loss: 65.9003   time: 0.25s   best: 65.8280
2023-09-30 11:01:18,034:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:18,064:INFO:  Epoch 316/500:  train Loss: 68.3627   val Loss: 65.4515   time: 0.26s   best: 65.4515
2023-09-30 11:01:18,327:INFO:  Epoch 317/500:  train Loss: 69.0820   val Loss: 65.7144   time: 0.25s   best: 65.4515
2023-09-30 11:01:18,738:INFO:  Epoch 318/500:  train Loss: 66.6987   val Loss: 65.7316   time: 0.41s   best: 65.4515
2023-09-30 11:01:19,089:INFO:  Epoch 319/500:  train Loss: 68.4309   val Loss: 67.2255   time: 0.34s   best: 65.4515
2023-09-30 11:01:19,353:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:19,383:INFO:  Epoch 320/500:  train Loss: 67.3682   val Loss: 65.4455   time: 0.26s   best: 65.4455
2023-09-30 11:01:19,653:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:19,683:INFO:  Epoch 321/500:  train Loss: 66.5415   val Loss: 65.0343   time: 0.26s   best: 65.0343
2023-09-30 11:01:19,944:INFO:  Epoch 322/500:  train Loss: 67.1345   val Loss: 65.8819   time: 0.25s   best: 65.0343
2023-09-30 11:01:20,215:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:20,246:INFO:  Epoch 323/500:  train Loss: 67.7566   val Loss: 64.4792   time: 0.27s   best: 64.4792
2023-09-30 11:01:20,505:INFO:  Epoch 324/500:  train Loss: 70.1393   val Loss: 65.1766   time: 0.25s   best: 64.4792
2023-09-30 11:01:20,782:INFO:  Epoch 325/500:  train Loss: 69.0851   val Loss: 65.4343   time: 0.26s   best: 64.4792
2023-09-30 11:01:21,059:INFO:  Epoch 326/500:  train Loss: 71.6734   val Loss: 67.5320   time: 0.25s   best: 64.4792
2023-09-30 11:01:21,329:INFO:  Epoch 327/500:  train Loss: 70.1670   val Loss: 67.6439   time: 0.26s   best: 64.4792
2023-09-30 11:01:21,606:INFO:  Epoch 328/500:  train Loss: 71.7634   val Loss: 68.4644   time: 0.26s   best: 64.4792
2023-09-30 11:01:21,865:INFO:  Epoch 329/500:  train Loss: 74.3996   val Loss: 66.8918   time: 0.25s   best: 64.4792
2023-09-30 11:01:22,143:INFO:  Epoch 330/500:  train Loss: 68.1231   val Loss: 68.2287   time: 0.26s   best: 64.4792
2023-09-30 11:01:22,404:INFO:  Epoch 331/500:  train Loss: 71.3607   val Loss: 66.9750   time: 0.25s   best: 64.4792
2023-09-30 11:01:22,681:INFO:  Epoch 332/500:  train Loss: 68.0532   val Loss: 66.7422   time: 0.26s   best: 64.4792
2023-09-30 11:01:22,941:INFO:  Epoch 333/500:  train Loss: 69.2234   val Loss: 66.0449   time: 0.25s   best: 64.4792
2023-09-30 11:01:23,225:INFO:  Epoch 334/500:  train Loss: 67.9608   val Loss: 66.3839   time: 0.27s   best: 64.4792
2023-09-30 11:01:23,488:INFO:  Epoch 335/500:  train Loss: 69.0889   val Loss: 67.5581   time: 0.25s   best: 64.4792
2023-09-30 11:01:23,766:INFO:  Epoch 336/500:  train Loss: 69.1744   val Loss: 66.3117   time: 0.27s   best: 64.4792
2023-09-30 11:01:24,024:INFO:  Epoch 337/500:  train Loss: 68.0907   val Loss: 66.0357   time: 0.25s   best: 64.4792
2023-09-30 11:01:24,304:INFO:  Epoch 338/500:  train Loss: 67.1861   val Loss: 65.2483   time: 0.27s   best: 64.4792
2023-09-30 11:01:24,563:INFO:  Epoch 339/500:  train Loss: 67.0409   val Loss: 65.5461   time: 0.25s   best: 64.4792
2023-09-30 11:01:24,841:INFO:  Epoch 340/500:  train Loss: 68.0454   val Loss: 68.0108   time: 0.26s   best: 64.4792
2023-09-30 11:01:25,098:INFO:  Epoch 341/500:  train Loss: 71.0147   val Loss: 65.8562   time: 0.25s   best: 64.4792
2023-09-30 11:01:25,388:INFO:  Epoch 342/500:  train Loss: 67.0109   val Loss: 65.4152   time: 0.28s   best: 64.4792
2023-09-30 11:01:25,651:INFO:  Epoch 343/500:  train Loss: 66.8488   val Loss: 67.4794   time: 0.25s   best: 64.4792
2023-09-30 11:01:25,927:INFO:  Epoch 344/500:  train Loss: 69.3637   val Loss: 65.7461   time: 0.26s   best: 64.4792
2023-09-30 11:01:26,189:INFO:  Epoch 345/500:  train Loss: 68.4642   val Loss: 67.8079   time: 0.25s   best: 64.4792
2023-09-30 11:01:26,469:INFO:  Epoch 346/500:  train Loss: 68.5961   val Loss: 66.3277   time: 0.27s   best: 64.4792
2023-09-30 11:01:26,728:INFO:  Epoch 347/500:  train Loss: 70.7663   val Loss: 67.0985   time: 0.25s   best: 64.4792
2023-09-30 11:01:27,011:INFO:  Epoch 348/500:  train Loss: 71.7329   val Loss: 69.9629   time: 0.27s   best: 64.4792
2023-09-30 11:01:27,275:INFO:  Epoch 349/500:  train Loss: 74.7641   val Loss: 69.6294   time: 0.25s   best: 64.4792
2023-09-30 11:01:27,555:INFO:  Epoch 350/500:  train Loss: 76.3910   val Loss: 68.7946   time: 0.27s   best: 64.4792
2023-09-30 11:01:27,814:INFO:  Epoch 351/500:  train Loss: 75.0613   val Loss: 68.3269   time: 0.25s   best: 64.4792
2023-09-30 11:01:28,083:INFO:  Epoch 352/500:  train Loss: 77.5941   val Loss: 74.7735   time: 0.26s   best: 64.4792
2023-09-30 11:01:28,354:INFO:  Epoch 353/500:  train Loss: 72.2849   val Loss: 73.7916   time: 0.26s   best: 64.4792
2023-09-30 11:01:28,631:INFO:  Epoch 354/500:  train Loss: 73.6897   val Loss: 69.5353   time: 0.26s   best: 64.4792
2023-09-30 11:01:28,903:INFO:  Epoch 355/500:  train Loss: 70.5696   val Loss: 67.6683   time: 0.25s   best: 64.4792
2023-09-30 11:01:29,174:INFO:  Epoch 356/500:  train Loss: 81.6237   val Loss: 74.4909   time: 0.26s   best: 64.4792
2023-09-30 11:01:29,456:INFO:  Epoch 357/500:  train Loss: 77.7547   val Loss: 75.1478   time: 0.27s   best: 64.4792
2023-09-30 11:01:29,717:INFO:  Epoch 358/500:  train Loss: 78.2868   val Loss: 78.1231   time: 0.25s   best: 64.4792
2023-09-30 11:01:29,997:INFO:  Epoch 359/500:  train Loss: 77.1129   val Loss: 74.6927   time: 0.27s   best: 64.4792
2023-09-30 11:01:30,258:INFO:  Epoch 360/500:  train Loss: 75.0251   val Loss: 73.3838   time: 0.25s   best: 64.4792
2023-09-30 11:01:30,535:INFO:  Epoch 361/500:  train Loss: 74.5990   val Loss: 73.2076   time: 0.26s   best: 64.4792
2023-09-30 11:01:30,795:INFO:  Epoch 362/500:  train Loss: 81.1281   val Loss: 75.8066   time: 0.25s   best: 64.4792
2023-09-30 11:01:31,072:INFO:  Epoch 363/500:  train Loss: 76.9901   val Loss: 74.6071   time: 0.27s   best: 64.4792
2023-09-30 11:01:31,344:INFO:  Epoch 364/500:  train Loss: 78.5593   val Loss: 74.2123   time: 0.26s   best: 64.4792
2023-09-30 11:01:31,622:INFO:  Epoch 365/500:  train Loss: 75.7379   val Loss: 74.3413   time: 0.27s   best: 64.4792
2023-09-30 11:01:31,881:INFO:  Epoch 366/500:  train Loss: 73.2333   val Loss: 76.0240   time: 0.25s   best: 64.4792
2023-09-30 11:01:32,160:INFO:  Epoch 367/500:  train Loss: 75.0062   val Loss: 72.1696   time: 0.27s   best: 64.4792
2023-09-30 11:01:32,422:INFO:  Epoch 368/500:  train Loss: 73.7462   val Loss: 71.0670   time: 0.25s   best: 64.4792
2023-09-30 11:01:32,699:INFO:  Epoch 369/500:  train Loss: 71.9866   val Loss: 70.9380   time: 0.26s   best: 64.4792
2023-09-30 11:01:32,960:INFO:  Epoch 370/500:  train Loss: 75.5034   val Loss: 71.4999   time: 0.25s   best: 64.4792
2023-09-30 11:01:33,247:INFO:  Epoch 371/500:  train Loss: 72.8671   val Loss: 70.6674   time: 0.27s   best: 64.4792
2023-09-30 11:01:33,507:INFO:  Epoch 372/500:  train Loss: 73.0988   val Loss: 70.2831   time: 0.25s   best: 64.4792
2023-09-30 11:01:33,786:INFO:  Epoch 373/500:  train Loss: 73.5056   val Loss: 70.9620   time: 0.27s   best: 64.4792
2023-09-30 11:01:34,041:INFO:  Epoch 374/500:  train Loss: 73.3823   val Loss: 71.1416   time: 0.25s   best: 64.4792
2023-09-30 11:01:34,319:INFO:  Epoch 375/500:  train Loss: 71.3786   val Loss: 71.5298   time: 0.27s   best: 64.4792
2023-09-30 11:01:34,578:INFO:  Epoch 376/500:  train Loss: 71.8598   val Loss: 70.0613   time: 0.25s   best: 64.4792
2023-09-30 11:01:34,863:INFO:  Epoch 377/500:  train Loss: 70.4154   val Loss: 68.3636   time: 0.27s   best: 64.4792
2023-09-30 11:01:35,131:INFO:  Epoch 378/500:  train Loss: 68.4653   val Loss: 67.8492   time: 0.26s   best: 64.4792
2023-09-30 11:01:35,414:INFO:  Epoch 379/500:  train Loss: 69.0712   val Loss: 66.2631   time: 0.27s   best: 64.4792
2023-09-30 11:01:35,677:INFO:  Epoch 380/500:  train Loss: 67.3620   val Loss: 65.5524   time: 0.25s   best: 64.4792
2023-09-30 11:01:35,955:INFO:  Epoch 381/500:  train Loss: 67.8511   val Loss: 65.4749   time: 0.26s   best: 64.4792
2023-09-30 11:01:36,216:INFO:  Epoch 382/500:  train Loss: 66.0938   val Loss: 65.1160   time: 0.25s   best: 64.4792
2023-09-30 11:01:36,499:INFO:  Epoch 383/500:  train Loss: 68.5787   val Loss: 67.2951   time: 0.27s   best: 64.4792
2023-09-30 11:01:36,751:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:36,801:INFO:  Epoch 384/500:  train Loss: 67.2221   val Loss: 64.0316   time: 0.25s   best: 64.0316
2023-09-30 11:01:37,061:INFO:  Epoch 385/500:  train Loss: 66.5861   val Loss: 64.3825   time: 0.25s   best: 64.0316
2023-09-30 11:01:37,343:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:37,374:INFO:  Epoch 386/500:  train Loss: 65.9070   val Loss: 63.5622   time: 0.28s   best: 63.5622
2023-09-30 11:01:37,635:INFO:  Epoch 387/500:  train Loss: 67.8437   val Loss: 65.9023   time: 0.25s   best: 63.5622
2023-09-30 11:01:37,911:INFO:  Epoch 388/500:  train Loss: 67.1911   val Loss: 66.1559   time: 0.26s   best: 63.5622
2023-09-30 11:01:38,170:INFO:  Epoch 389/500:  train Loss: 68.4049   val Loss: 65.4074   time: 0.25s   best: 63.5622
2023-09-30 11:01:38,450:INFO:  Epoch 390/500:  train Loss: 69.5774   val Loss: 69.8622   time: 0.27s   best: 63.5622
2023-09-30 11:01:38,709:INFO:  Epoch 391/500:  train Loss: 71.4282   val Loss: 69.2501   time: 0.25s   best: 63.5622
2023-09-30 11:01:38,985:INFO:  Epoch 392/500:  train Loss: 70.3596   val Loss: 66.5948   time: 0.26s   best: 63.5622
2023-09-30 11:01:39,254:INFO:  Epoch 393/500:  train Loss: 68.9407   val Loss: 65.5209   time: 0.25s   best: 63.5622
2023-09-30 11:01:39,535:INFO:  Epoch 394/500:  train Loss: 68.9445   val Loss: 66.4175   time: 0.27s   best: 63.5622
2023-09-30 11:01:39,795:INFO:  Epoch 395/500:  train Loss: 70.1301   val Loss: 64.9170   time: 0.25s   best: 63.5622
2023-09-30 11:01:40,064:INFO:  Epoch 396/500:  train Loss: 71.5824   val Loss: 63.7591   time: 0.26s   best: 63.5622
2023-09-30 11:01:40,334:INFO:  Epoch 397/500:  train Loss: 70.6618   val Loss: 66.7994   time: 0.26s   best: 63.5622
2023-09-30 11:01:40,611:INFO:  Epoch 398/500:  train Loss: 70.2166   val Loss: 64.6841   time: 0.26s   best: 63.5622
2023-09-30 11:01:40,871:INFO:  Epoch 399/500:  train Loss: 66.9123   val Loss: 65.8593   time: 0.25s   best: 63.5622
2023-09-30 11:01:41,202:INFO:  Epoch 400/500:  train Loss: 68.8602   val Loss: 66.3280   time: 0.32s   best: 63.5622
2023-09-30 11:01:41,469:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:41,518:INFO:  Epoch 401/500:  train Loss: 66.1677   val Loss: 63.1092   time: 0.26s   best: 63.1092
2023-09-30 11:01:41,771:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:01:41,803:INFO:  Epoch 402/500:  train Loss: 65.7398   val Loss: 62.9271   time: 0.25s   best: 62.9271
2023-09-30 11:01:42,076:INFO:  Epoch 403/500:  train Loss: 68.0757   val Loss: 65.8094   time: 0.26s   best: 62.9271
2023-09-30 11:01:42,332:INFO:  Epoch 404/500:  train Loss: 70.3554   val Loss: 66.7122   time: 0.25s   best: 62.9271
2023-09-30 11:01:42,615:INFO:  Epoch 405/500:  train Loss: 68.7008   val Loss: 64.8295   time: 0.27s   best: 62.9271
2023-09-30 11:01:42,874:INFO:  Epoch 406/500:  train Loss: 68.7486   val Loss: 65.4263   time: 0.25s   best: 62.9271
2023-09-30 11:01:43,158:INFO:  Epoch 407/500:  train Loss: 70.4919   val Loss: 65.9227   time: 0.27s   best: 62.9271
2023-09-30 11:01:43,423:INFO:  Epoch 408/500:  train Loss: 68.0768   val Loss: 67.6307   time: 0.25s   best: 62.9271
2023-09-30 11:01:43,700:INFO:  Epoch 409/500:  train Loss: 68.4617   val Loss: 66.3559   time: 0.26s   best: 62.9271
2023-09-30 11:01:43,961:INFO:  Epoch 410/500:  train Loss: 69.0531   val Loss: 65.9366   time: 0.25s   best: 62.9271
2023-09-30 11:01:44,239:INFO:  Epoch 411/500:  train Loss: 69.8307   val Loss: 67.1118   time: 0.27s   best: 62.9271
2023-09-30 11:01:44,501:INFO:  Epoch 412/500:  train Loss: 70.7570   val Loss: 66.9052   time: 0.25s   best: 62.9271
2023-09-30 11:01:44,778:INFO:  Epoch 413/500:  train Loss: 71.1442   val Loss: 68.6385   time: 0.26s   best: 62.9271
2023-09-30 11:01:45,038:INFO:  Epoch 414/500:  train Loss: 80.1544   val Loss: 83.1423   time: 0.25s   best: 62.9271
2023-09-30 11:01:45,321:INFO:  Epoch 415/500:  train Loss: 81.6337   val Loss: 78.7049   time: 0.28s   best: 62.9271
2023-09-30 11:01:45,590:INFO:  Epoch 416/500:  train Loss: 77.8336   val Loss: 75.3569   time: 0.26s   best: 62.9271
2023-09-30 11:01:45,866:INFO:  Epoch 417/500:  train Loss: 73.9231   val Loss: 70.2206   time: 0.26s   best: 62.9271
2023-09-30 11:01:46,124:INFO:  Epoch 418/500:  train Loss: 70.2409   val Loss: 69.1549   time: 0.25s   best: 62.9271
2023-09-30 11:01:46,407:INFO:  Epoch 419/500:  train Loss: 75.3497   val Loss: 71.1173   time: 0.27s   best: 62.9271
2023-09-30 11:01:46,666:INFO:  Epoch 420/500:  train Loss: 71.8604   val Loss: 69.0501   time: 0.25s   best: 62.9271
2023-09-30 11:01:46,944:INFO:  Epoch 421/500:  train Loss: 69.8697   val Loss: 67.7550   time: 0.26s   best: 62.9271
2023-09-30 11:01:47,214:INFO:  Epoch 422/500:  train Loss: 67.6275   val Loss: 67.4627   time: 0.26s   best: 62.9271
2023-09-30 11:01:47,497:INFO:  Epoch 423/500:  train Loss: 69.5200   val Loss: 68.7904   time: 0.27s   best: 62.9271
2023-09-30 11:01:47,774:INFO:  Epoch 424/500:  train Loss: 70.3152   val Loss: 67.2316   time: 0.25s   best: 62.9271
2023-09-30 11:01:48,036:INFO:  Epoch 425/500:  train Loss: 68.3616   val Loss: 67.4209   time: 0.25s   best: 62.9271
2023-09-30 11:01:48,316:INFO:  Epoch 426/500:  train Loss: 67.6355   val Loss: 66.1367   time: 0.27s   best: 62.9271
2023-09-30 11:01:48,575:INFO:  Epoch 427/500:  train Loss: 67.4433   val Loss: 66.7266   time: 0.25s   best: 62.9271
2023-09-30 11:01:48,853:INFO:  Epoch 428/500:  train Loss: 68.6933   val Loss: 65.0679   time: 0.26s   best: 62.9271
2023-09-30 11:01:49,146:INFO:  Epoch 429/500:  train Loss: 67.6558   val Loss: 65.4663   time: 0.29s   best: 62.9271
2023-09-30 11:01:49,617:INFO:  Epoch 430/500:  train Loss: 68.2377   val Loss: 66.1746   time: 0.46s   best: 62.9271
2023-09-30 11:01:49,890:INFO:  Epoch 431/500:  train Loss: 70.4194   val Loss: 64.3243   time: 0.26s   best: 62.9271
2023-09-30 11:01:50,149:INFO:  Epoch 432/500:  train Loss: 73.7183   val Loss: 79.0010   time: 0.24s   best: 62.9271
2023-09-30 11:01:50,427:INFO:  Epoch 433/500:  train Loss: 81.7366   val Loss: 81.9735   time: 0.27s   best: 62.9271
2023-09-30 11:01:50,686:INFO:  Epoch 434/500:  train Loss: 82.2284   val Loss: 80.9113   time: 0.25s   best: 62.9271
2023-09-30 11:01:50,964:INFO:  Epoch 435/500:  train Loss: 81.6595   val Loss: 79.0873   time: 0.27s   best: 62.9271
2023-09-30 11:01:51,217:INFO:  Epoch 436/500:  train Loss: 78.9348   val Loss: 77.4124   time: 0.25s   best: 62.9271
2023-09-30 11:01:51,507:INFO:  Epoch 437/500:  train Loss: 77.4284   val Loss: 75.8122   time: 0.28s   best: 62.9271
2023-09-30 11:01:51,773:INFO:  Epoch 438/500:  train Loss: 75.1840   val Loss: 73.4219   time: 0.25s   best: 62.9271
2023-09-30 11:01:52,050:INFO:  Epoch 439/500:  train Loss: 73.4713   val Loss: 72.2106   time: 0.26s   best: 62.9271
2023-09-30 11:01:52,311:INFO:  Epoch 440/500:  train Loss: 72.1175   val Loss: 69.9034   time: 0.25s   best: 62.9271
2023-09-30 11:01:52,589:INFO:  Epoch 441/500:  train Loss: 72.7858   val Loss: 69.3584   time: 0.26s   best: 62.9271
2023-09-30 11:01:52,847:INFO:  Epoch 442/500:  train Loss: 72.8179   val Loss: 70.1201   time: 0.25s   best: 62.9271
2023-09-30 11:01:53,125:INFO:  Epoch 443/500:  train Loss: 72.4753   val Loss: 69.3368   time: 0.26s   best: 62.9271
2023-09-30 11:01:53,389:INFO:  Epoch 444/500:  train Loss: 72.7961   val Loss: 69.8504   time: 0.25s   best: 62.9271
2023-09-30 11:01:53,674:INFO:  Epoch 445/500:  train Loss: 72.7384   val Loss: 71.6357   time: 0.27s   best: 62.9271
2023-09-30 11:01:53,933:INFO:  Epoch 446/500:  train Loss: 74.1041   val Loss: 70.6057   time: 0.25s   best: 62.9271
2023-09-30 11:01:54,205:INFO:  Epoch 447/500:  train Loss: 72.9860   val Loss: 70.8331   time: 0.27s   best: 62.9271
2023-09-30 11:01:54,472:INFO:  Epoch 448/500:  train Loss: 73.0848   val Loss: 69.2077   time: 0.25s   best: 62.9271
2023-09-30 11:01:54,747:INFO:  Epoch 449/500:  train Loss: 73.1185   val Loss: 68.2807   time: 0.26s   best: 62.9271
2023-09-30 11:01:55,007:INFO:  Epoch 450/500:  train Loss: 69.3825   val Loss: 68.7550   time: 0.25s   best: 62.9271
2023-09-30 11:01:55,288:INFO:  Epoch 451/500:  train Loss: 69.1151   val Loss: 66.3575   time: 0.27s   best: 62.9271
2023-09-30 11:01:55,553:INFO:  Epoch 452/500:  train Loss: 69.0068   val Loss: 67.7771   time: 0.25s   best: 62.9271
2023-09-30 11:01:55,835:INFO:  Epoch 453/500:  train Loss: 69.0217   val Loss: 65.2907   time: 0.27s   best: 62.9271
2023-09-30 11:01:56,095:INFO:  Epoch 454/500:  train Loss: 68.2231   val Loss: 68.1591   time: 0.25s   best: 62.9271
2023-09-30 11:01:56,377:INFO:  Epoch 455/500:  train Loss: 68.2468   val Loss: 65.9895   time: 0.27s   best: 62.9271
2023-09-30 11:01:56,650:INFO:  Epoch 456/500:  train Loss: 69.4311   val Loss: 69.1042   time: 0.25s   best: 62.9271
2023-09-30 11:01:56,914:INFO:  Epoch 457/500:  train Loss: 69.0943   val Loss: 68.1307   time: 0.25s   best: 62.9271
2023-09-30 11:01:57,176:INFO:  Epoch 458/500:  train Loss: 69.9722   val Loss: 66.7605   time: 0.25s   best: 62.9271
2023-09-30 11:01:57,456:INFO:  Epoch 459/500:  train Loss: 70.6483   val Loss: 70.7097   time: 0.27s   best: 62.9271
2023-09-30 11:01:57,739:INFO:  Epoch 460/500:  train Loss: 70.9171   val Loss: 68.5715   time: 0.27s   best: 62.9271
2023-09-30 11:01:57,998:INFO:  Epoch 461/500:  train Loss: 69.0942   val Loss: 68.9444   time: 0.25s   best: 62.9271
2023-09-30 11:01:58,276:INFO:  Epoch 462/500:  train Loss: 68.8288   val Loss: 66.8324   time: 0.26s   best: 62.9271
2023-09-30 11:01:58,536:INFO:  Epoch 463/500:  train Loss: 67.8418   val Loss: 67.4971   time: 0.25s   best: 62.9271
2023-09-30 11:01:58,813:INFO:  Epoch 464/500:  train Loss: 68.1614   val Loss: 66.6886   time: 0.26s   best: 62.9271
2023-09-30 11:01:59,072:INFO:  Epoch 465/500:  train Loss: 68.5548   val Loss: 65.1882   time: 0.25s   best: 62.9271
2023-09-30 11:01:59,353:INFO:  Epoch 466/500:  train Loss: 66.8363   val Loss: 66.3123   time: 0.27s   best: 62.9271
2023-09-30 11:01:59,614:INFO:  Epoch 467/500:  train Loss: 66.0506   val Loss: 66.5689   time: 0.25s   best: 62.9271
2023-09-30 11:01:59,896:INFO:  Epoch 468/500:  train Loss: 66.1525   val Loss: 64.5804   time: 0.27s   best: 62.9271
2023-09-30 11:02:00,148:INFO:  Epoch 469/500:  train Loss: 65.0121   val Loss: 63.5278   time: 0.25s   best: 62.9271
2023-09-30 11:02:00,427:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:02:00,458:INFO:  Epoch 470/500:  train Loss: 66.4451   val Loss: 62.7458   time: 0.27s   best: 62.7458
2023-09-30 11:02:00,717:INFO:  Epoch 471/500:  train Loss: 64.8042   val Loss: 64.1618   time: 0.25s   best: 62.7458
2023-09-30 11:02:00,987:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:02:01,018:INFO:  Epoch 472/500:  train Loss: 64.2574   val Loss: 62.1817   time: 0.26s   best: 62.1817
2023-09-30 11:02:01,279:INFO:  Epoch 473/500:  train Loss: 63.4800   val Loss: 62.7635   time: 0.25s   best: 62.1817
2023-09-30 11:02:01,552:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:02:01,584:INFO:  Epoch 474/500:  train Loss: 63.2752   val Loss: 61.9882   time: 0.27s   best: 61.9882
2023-09-30 11:02:01,852:INFO:  Epoch 475/500:  train Loss: 63.6187   val Loss: 62.9877   time: 0.25s   best: 61.9882
2023-09-30 11:02:02,129:INFO:  Epoch 476/500:  train Loss: 64.8317   val Loss: 62.9078   time: 0.26s   best: 61.9882
2023-09-30 11:02:02,391:INFO:  Epoch 477/500:  train Loss: 64.4698   val Loss: 62.4865   time: 0.25s   best: 61.9882
2023-09-30 11:02:02,669:INFO:  Epoch 478/500:  train Loss: 63.8936   val Loss: 63.7989   time: 0.27s   best: 61.9882
2023-09-30 11:02:02,928:INFO:  Epoch 479/500:  train Loss: 65.9595   val Loss: 64.8295   time: 0.25s   best: 61.9882
2023-09-30 11:02:03,205:INFO:  Epoch 480/500:  train Loss: 67.6237   val Loss: 65.0087   time: 0.26s   best: 61.9882
2023-09-30 11:02:03,476:INFO:  Epoch 481/500:  train Loss: 65.5931   val Loss: 64.7374   time: 0.25s   best: 61.9882
2023-09-30 11:02:03,788:INFO:  Epoch 482/500:  train Loss: 65.3921   val Loss: 62.2532   time: 0.30s   best: 61.9882
2023-09-30 11:02:04,046:INFO:  Epoch 483/500:  train Loss: 65.9806   val Loss: 66.4490   time: 0.25s   best: 61.9882
2023-09-30 11:02:04,326:INFO:  Epoch 484/500:  train Loss: 68.0242   val Loss: 63.4042   time: 0.27s   best: 61.9882
2023-09-30 11:02:04,600:INFO:  Epoch 485/500:  train Loss: 71.2824   val Loss: 66.3232   time: 0.25s   best: 61.9882
2023-09-30 11:02:04,868:INFO:  Epoch 486/500:  train Loss: 67.1820   val Loss: 67.6203   time: 0.26s   best: 61.9882
2023-09-30 11:02:05,141:INFO:  Epoch 487/500:  train Loss: 67.0616   val Loss: 64.2248   time: 0.25s   best: 61.9882
2023-09-30 11:02:05,411:INFO:  Epoch 488/500:  train Loss: 65.2987   val Loss: 63.9472   time: 0.26s   best: 61.9882
2023-09-30 11:02:05,700:INFO:  Epoch 489/500:  train Loss: 66.8617   val Loss: 63.5172   time: 0.28s   best: 61.9882
2023-09-30 11:02:05,963:INFO:  Epoch 490/500:  train Loss: 65.9413   val Loss: 62.8969   time: 0.25s   best: 61.9882
2023-09-30 11:02:06,239:INFO:  Epoch 491/500:  train Loss: 63.2829   val Loss: 63.3880   time: 0.26s   best: 61.9882
2023-09-30 11:02:06,502:INFO:  Epoch 492/500:  train Loss: 64.4982   val Loss: 64.7099   time: 0.25s   best: 61.9882
2023-09-30 11:02:06,781:INFO:  Epoch 493/500:  train Loss: 67.3981   val Loss: 64.7323   time: 0.27s   best: 61.9882
2023-09-30 11:02:07,041:INFO:  Epoch 494/500:  train Loss: 68.4364   val Loss: 64.8587   time: 0.25s   best: 61.9882
2023-09-30 11:02:07,322:INFO:  Epoch 495/500:  train Loss: 67.1150   val Loss: 66.3846   time: 0.27s   best: 61.9882
2023-09-30 11:02:07,585:INFO:  Epoch 496/500:  train Loss: 66.0097   val Loss: 63.1919   time: 0.25s   best: 61.9882
2023-09-30 11:02:07,867:INFO:  Epoch 497/500:  train Loss: 63.6105   val Loss: 63.3568   time: 0.27s   best: 61.9882
2023-09-30 11:02:08,119:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_f1e2.pt
2023-09-30 11:02:08,150:INFO:  Epoch 498/500:  train Loss: 63.2870   val Loss: 61.1231   time: 0.25s   best: 61.1231
2023-09-30 11:02:08,430:INFO:  Epoch 499/500:  train Loss: 62.9121   val Loss: 62.5200   time: 0.27s   best: 61.1231
2023-09-30 11:02:08,731:INFO:  Epoch 500/500:  train Loss: 65.8982   val Loss: 62.9148   time: 0.29s   best: 61.1231
2023-09-30 11:02:08,731:INFO:  -----> Training complete in 2m 20s   best validation loss: 61.1231
 
2023-09-30 11:07:28,459:INFO:  Epoch 374/500:  train Loss: 17.8503   val Loss: 23.5666   time: 443.94s   best: 23.0062
2023-09-30 11:14:52,012:INFO:  Epoch 375/500:  train Loss: 17.8751   val Loss: 23.4887   time: 443.55s   best: 23.0062
2023-09-30 11:22:16,070:INFO:  Epoch 376/500:  train Loss: 17.7951   val Loss: 23.4144   time: 444.02s   best: 23.0062
2023-09-30 11:29:43,628:INFO:  Epoch 377/500:  train Loss: 17.8307   val Loss: 25.7356   time: 447.54s   best: 23.0062
2023-09-30 11:37:07,059:INFO:  Epoch 378/500:  train Loss: 17.8225   val Loss: 23.0151   time: 443.42s   best: 23.0062
2023-09-30 11:44:29,176:INFO:  Epoch 379/500:  train Loss: 17.8280   val Loss: 23.0718   time: 442.09s   best: 23.0062
2023-09-30 11:51:52,241:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 11:51:52,392:INFO:  Epoch 380/500:  train Loss: 18.0590   val Loss: 22.9445   time: 442.97s   best: 22.9445
2023-09-30 11:52:47,027:INFO:  Starting experiment lstm autoencoder bidirectional debug
2023-09-30 11:52:47,038:INFO:  Defining the model
2023-09-30 11:52:47,081:INFO:  Reading the dataset
2023-09-30 11:52:56,384:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:52:56,407:INFO:  Epoch 1/500:  train Loss: 100.5681   val Loss: 95.6656   time: 3.92s   best: 95.6656
2023-09-30 11:52:56,682:INFO:  Epoch 2/500:  train Loss: 98.8999   val Loss: 100.1096   time: 0.27s   best: 95.6656
2023-09-30 11:52:56,954:INFO:  Epoch 3/500:  train Loss: 99.8161   val Loss: 100.1815   time: 0.27s   best: 95.6656
2023-09-30 11:52:57,205:INFO:  Epoch 4/500:  train Loss: 99.9877   val Loss: 100.1978   time: 0.25s   best: 95.6656
2023-09-30 11:52:57,478:INFO:  Epoch 5/500:  train Loss: 100.0324   val Loss: 100.1985   time: 0.27s   best: 95.6656
2023-09-30 11:52:57,728:INFO:  Epoch 6/500:  train Loss: 99.9509   val Loss: 100.0972   time: 0.25s   best: 95.6656
2023-09-30 11:52:57,999:INFO:  Epoch 7/500:  train Loss: 99.4332   val Loss: 98.8922   time: 0.27s   best: 95.6656
2023-09-30 11:52:58,250:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:52:58,271:INFO:  Epoch 8/500:  train Loss: 97.0594   val Loss: 94.5484   time: 0.25s   best: 94.5484
2023-09-30 11:52:58,556:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:52:58,577:INFO:  Epoch 9/500:  train Loss: 94.9295   val Loss: 91.4659   time: 0.28s   best: 91.4659
2023-09-30 11:52:58,832:INFO:  Epoch 10/500:  train Loss: 94.7885   val Loss: 92.5688   time: 0.25s   best: 91.4659
2023-09-30 11:52:59,099:INFO:  Epoch 11/500:  train Loss: 94.6813   val Loss: 94.7310   time: 0.26s   best: 91.4659
2023-09-30 11:52:59,352:INFO:  Epoch 12/500:  train Loss: 94.3943   val Loss: 94.6079   time: 0.25s   best: 91.4659
2023-09-30 11:52:59,626:INFO:  Epoch 13/500:  train Loss: 95.0948   val Loss: 94.4722   time: 0.27s   best: 91.4659
2023-09-30 11:52:59,878:INFO:  Epoch 14/500:  train Loss: 93.9930   val Loss: 94.3119   time: 0.25s   best: 91.4659
2023-09-30 11:53:00,147:INFO:  Epoch 15/500:  train Loss: 93.8429   val Loss: 94.2230   time: 0.26s   best: 91.4659
2023-09-30 11:53:00,398:INFO:  Epoch 16/500:  train Loss: 93.6915   val Loss: 94.1934   time: 0.25s   best: 91.4659
2023-09-30 11:53:00,680:INFO:  Epoch 17/500:  train Loss: 94.1779   val Loss: 94.1837   time: 0.28s   best: 91.4659
2023-09-30 11:53:00,934:INFO:  Epoch 18/500:  train Loss: 93.5317   val Loss: 94.1915   time: 0.25s   best: 91.4659
2023-09-30 11:53:01,207:INFO:  Epoch 19/500:  train Loss: 94.0144   val Loss: 94.0437   time: 0.27s   best: 91.4659
2023-09-30 11:53:01,464:INFO:  Epoch 20/500:  train Loss: 93.8495   val Loss: 92.3940   time: 0.25s   best: 91.4659
2023-09-30 11:53:01,734:INFO:  Epoch 21/500:  train Loss: 93.8785   val Loss: 93.9159   time: 0.27s   best: 91.4659
2023-09-30 11:53:01,986:INFO:  Epoch 22/500:  train Loss: 93.3901   val Loss: 94.1762   time: 0.25s   best: 91.4659
2023-09-30 11:53:02,255:INFO:  Epoch 23/500:  train Loss: 94.0113   val Loss: 94.2338   time: 0.26s   best: 91.4659
2023-09-30 11:53:02,510:INFO:  Epoch 24/500:  train Loss: 94.2973   val Loss: 94.2505   time: 0.25s   best: 91.4659
2023-09-30 11:53:02,788:INFO:  Epoch 25/500:  train Loss: 94.0681   val Loss: 94.1292   time: 0.27s   best: 91.4659
2023-09-30 11:53:03,043:INFO:  Epoch 26/500:  train Loss: 94.0443   val Loss: 94.0633   time: 0.25s   best: 91.4659
2023-09-30 11:53:03,315:INFO:  Epoch 27/500:  train Loss: 94.2877   val Loss: 94.0180   time: 0.27s   best: 91.4659
2023-09-30 11:53:03,572:INFO:  Epoch 28/500:  train Loss: 93.7143   val Loss: 93.9875   time: 0.25s   best: 91.4659
2023-09-30 11:53:03,842:INFO:  Epoch 29/500:  train Loss: 93.9618   val Loss: 93.8349   time: 0.27s   best: 91.4659
2023-09-30 11:53:04,093:INFO:  Epoch 30/500:  train Loss: 93.1287   val Loss: 93.5799   time: 0.25s   best: 91.4659
2023-09-30 11:53:04,362:INFO:  Epoch 31/500:  train Loss: 92.5748   val Loss: 92.8161   time: 0.27s   best: 91.4659
2023-09-30 11:53:04,626:INFO:  Epoch 32/500:  train Loss: 92.7030   val Loss: 92.0039   time: 0.26s   best: 91.4659
2023-09-30 11:53:04,899:INFO:  Epoch 33/500:  train Loss: 92.9823   val Loss: 93.2613   time: 0.27s   best: 91.4659
2023-09-30 11:53:05,153:INFO:  Epoch 34/500:  train Loss: 92.6540   val Loss: 93.3369   time: 0.25s   best: 91.4659
2023-09-30 11:53:05,425:INFO:  Epoch 35/500:  train Loss: 93.2861   val Loss: 93.3093   time: 0.27s   best: 91.4659
2023-09-30 11:53:05,681:INFO:  Epoch 36/500:  train Loss: 93.0847   val Loss: 93.2675   time: 0.25s   best: 91.4659
2023-09-30 11:53:05,950:INFO:  Epoch 37/500:  train Loss: 92.3784   val Loss: 93.0223   time: 0.26s   best: 91.4659
2023-09-30 11:53:06,200:INFO:  Epoch 38/500:  train Loss: 92.7928   val Loss: 92.4359   time: 0.25s   best: 91.4659
2023-09-30 11:53:06,471:INFO:  Epoch 39/500:  train Loss: 92.4840   val Loss: 92.3551   time: 0.27s   best: 91.4659
2023-09-30 11:53:06,734:INFO:  Epoch 40/500:  train Loss: 92.2488   val Loss: 92.4318   time: 0.26s   best: 91.4659
2023-09-30 11:53:07,007:INFO:  Epoch 41/500:  train Loss: 92.5668   val Loss: 92.1730   time: 0.27s   best: 91.4659
2023-09-30 11:53:07,258:INFO:  Epoch 42/500:  train Loss: 92.1765   val Loss: 91.9012   time: 0.25s   best: 91.4659
2023-09-30 11:53:07,535:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:07,555:INFO:  Epoch 43/500:  train Loss: 91.4363   val Loss: 91.4262   time: 0.27s   best: 91.4262
2023-09-30 11:53:07,809:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:07,830:INFO:  Epoch 44/500:  train Loss: 91.3115   val Loss: 90.4738   time: 0.25s   best: 90.4738
2023-09-30 11:53:08,100:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:08,121:INFO:  Epoch 45/500:  train Loss: 90.8499   val Loss: 89.8009   time: 0.26s   best: 89.8009
2023-09-30 11:53:08,373:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:08,394:INFO:  Epoch 46/500:  train Loss: 90.0471   val Loss: 89.4024   time: 0.25s   best: 89.4024
2023-09-30 11:53:08,675:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:08,696:INFO:  Epoch 47/500:  train Loss: 89.3183   val Loss: 88.5372   time: 0.28s   best: 88.5372
2023-09-30 11:53:08,950:INFO:  Epoch 48/500:  train Loss: 89.5411   val Loss: 88.6846   time: 0.25s   best: 88.5372
2023-09-30 11:53:09,220:INFO:  Epoch 49/500:  train Loss: 89.6296   val Loss: 89.1865   time: 0.26s   best: 88.5372
2023-09-30 11:53:09,475:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:09,514:INFO:  Epoch 50/500:  train Loss: 88.7710   val Loss: 88.4182   time: 0.25s   best: 88.4182
2023-09-30 11:53:09,768:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:09,789:INFO:  Epoch 51/500:  train Loss: 87.9226   val Loss: 87.9521   time: 0.25s   best: 87.9521
2023-09-30 11:53:10,058:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:10,079:INFO:  Epoch 52/500:  train Loss: 88.3766   val Loss: 87.8650   time: 0.26s   best: 87.8650
2023-09-30 11:53:10,331:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:10,352:INFO:  Epoch 53/500:  train Loss: 88.2475   val Loss: 87.5611   time: 0.25s   best: 87.5611
2023-09-30 11:53:10,630:INFO:  Epoch 54/500:  train Loss: 88.3752   val Loss: 88.0647   time: 0.27s   best: 87.5611
2023-09-30 11:53:10,885:INFO:  Epoch 55/500:  train Loss: 88.9857   val Loss: 88.5040   time: 0.25s   best: 87.5611
2023-09-30 11:53:11,158:INFO:  Epoch 56/500:  train Loss: 88.8519   val Loss: 87.8800   time: 0.27s   best: 87.5611
2023-09-30 11:53:11,411:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:11,432:INFO:  Epoch 57/500:  train Loss: 87.8287   val Loss: 87.2712   time: 0.25s   best: 87.2712
2023-09-30 11:53:11,707:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:11,728:INFO:  Epoch 58/500:  train Loss: 88.0586   val Loss: 87.1363   time: 0.27s   best: 87.1363
2023-09-30 11:53:11,980:INFO:  Epoch 59/500:  train Loss: 88.2240   val Loss: 87.4077   time: 0.25s   best: 87.1363
2023-09-30 11:53:12,247:INFO:  Epoch 60/500:  train Loss: 87.9377   val Loss: 87.1918   time: 0.26s   best: 87.1363
2023-09-30 11:53:12,501:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:12,524:INFO:  Epoch 61/500:  train Loss: 87.5473   val Loss: 87.0592   time: 0.25s   best: 87.0592
2023-09-30 11:53:12,801:INFO:  Epoch 62/500:  train Loss: 89.1826   val Loss: 87.1330   time: 0.27s   best: 87.0592
2023-09-30 11:53:13,056:INFO:  Epoch 63/500:  train Loss: 89.0449   val Loss: 90.1324   time: 0.25s   best: 87.0592
2023-09-30 11:53:13,327:INFO:  Epoch 64/500:  train Loss: 90.3019   val Loss: 89.4289   time: 0.27s   best: 87.0592
2023-09-30 11:53:13,586:INFO:  Epoch 65/500:  train Loss: 88.5309   val Loss: 87.8336   time: 0.25s   best: 87.0592
2023-09-30 11:53:13,854:INFO:  Epoch 66/500:  train Loss: 88.0177   val Loss: 88.0237   time: 0.26s   best: 87.0592
2023-09-30 11:53:14,105:INFO:  Epoch 67/500:  train Loss: 87.9465   val Loss: 87.4405   time: 0.25s   best: 87.0592
2023-09-30 11:53:14,373:INFO:  Epoch 68/500:  train Loss: 88.1161   val Loss: 87.3905   time: 0.26s   best: 87.0592
2023-09-30 11:53:14,635:INFO:  Epoch 69/500:  train Loss: 87.1663   val Loss: 87.2339   time: 0.26s   best: 87.0592
2023-09-30 11:53:14,910:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:14,930:INFO:  Epoch 70/500:  train Loss: 88.7420   val Loss: 86.6414   time: 0.27s   best: 86.6414
2023-09-30 11:53:15,184:INFO:  Epoch 71/500:  train Loss: 88.5715   val Loss: 89.0748   time: 0.25s   best: 86.6414
2023-09-30 11:53:15,454:INFO:  Epoch 72/500:  train Loss: 88.9142   val Loss: 87.4687   time: 0.27s   best: 86.6414
2023-09-30 11:53:15,711:INFO:  Epoch 73/500:  train Loss: 87.3399   val Loss: 86.6689   time: 0.25s   best: 86.6414
2023-09-30 11:53:15,982:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:16,003:INFO:  Epoch 74/500:  train Loss: 87.1397   val Loss: 86.6207   time: 0.27s   best: 86.6207
2023-09-30 11:53:16,255:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:16,276:INFO:  Epoch 75/500:  train Loss: 86.8385   val Loss: 86.1398   time: 0.25s   best: 86.1398
2023-09-30 11:53:16,552:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:16,574:INFO:  Epoch 76/500:  train Loss: 86.7502   val Loss: 86.0529   time: 0.27s   best: 86.0529
2023-09-30 11:53:16,852:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:16,874:INFO:  Epoch 77/500:  train Loss: 86.8873   val Loss: 85.5915   time: 0.27s   best: 85.5915
2023-09-30 11:53:17,127:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:17,150:INFO:  Epoch 78/500:  train Loss: 86.3793   val Loss: 85.5909   time: 0.25s   best: 85.5909
2023-09-30 11:53:17,418:INFO:  Epoch 79/500:  train Loss: 86.6352   val Loss: 85.7596   time: 0.26s   best: 85.5909
2023-09-30 11:53:17,676:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:17,697:INFO:  Epoch 80/500:  train Loss: 85.9413   val Loss: 85.4522   time: 0.25s   best: 85.4522
2023-09-30 11:53:17,969:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:17,991:INFO:  Epoch 81/500:  train Loss: 86.0655   val Loss: 85.1085   time: 0.27s   best: 85.1085
2023-09-30 11:53:18,243:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:18,264:INFO:  Epoch 82/500:  train Loss: 85.6023   val Loss: 84.6467   time: 0.25s   best: 84.6467
2023-09-30 11:53:18,536:INFO:  Epoch 83/500:  train Loss: 85.1906   val Loss: 84.6778   time: 0.27s   best: 84.6467
2023-09-30 11:53:18,796:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:18,817:INFO:  Epoch 84/500:  train Loss: 85.1267   val Loss: 84.6086   time: 0.25s   best: 84.6086
2023-09-30 11:53:19,092:INFO:  Epoch 85/500:  train Loss: 85.1778   val Loss: 84.6994   time: 0.27s   best: 84.6086
2023-09-30 11:53:19,345:INFO:  Epoch 86/500:  train Loss: 84.5411   val Loss: 85.1677   time: 0.25s   best: 84.6086
2023-09-30 11:53:19,654:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:19,676:INFO:  Epoch 87/500:  train Loss: 85.4083   val Loss: 83.8263   time: 0.30s   best: 83.8263
2023-09-30 11:53:19,928:INFO:  Epoch 88/500:  train Loss: 85.2567   val Loss: 84.3571   time: 0.25s   best: 83.8263
2023-09-30 11:53:20,199:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:20,220:INFO:  Epoch 89/500:  train Loss: 84.0788   val Loss: 83.6190   time: 0.27s   best: 83.6190
2023-09-30 11:53:20,476:INFO:  Epoch 90/500:  train Loss: 84.2003   val Loss: 83.6990   time: 0.25s   best: 83.6190
2023-09-30 11:53:20,896:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:20,936:INFO:  Epoch 91/500:  train Loss: 84.3279   val Loss: 83.5509   time: 0.41s   best: 83.5509
2023-09-30 11:53:21,268:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:21,289:INFO:  Epoch 92/500:  train Loss: 84.0678   val Loss: 83.1142   time: 0.33s   best: 83.1142
2023-09-30 11:53:21,549:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:21,568:INFO:  Epoch 93/500:  train Loss: 83.8180   val Loss: 83.0409   time: 0.25s   best: 83.0409
2023-09-30 11:53:21,838:INFO:  Epoch 94/500:  train Loss: 84.2735   val Loss: 83.3251   time: 0.27s   best: 83.0409
2023-09-30 11:53:22,091:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:22,112:INFO:  Epoch 95/500:  train Loss: 83.6725   val Loss: 82.7666   time: 0.25s   best: 82.7666
2023-09-30 11:53:22,383:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:22,405:INFO:  Epoch 96/500:  train Loss: 83.9547   val Loss: 82.6865   time: 0.27s   best: 82.6865
2023-09-30 11:53:22,673:INFO:  Epoch 97/500:  train Loss: 83.7957   val Loss: 83.5961   time: 0.27s   best: 82.6865
2023-09-30 11:53:22,931:INFO:  Epoch 98/500:  train Loss: 84.1860   val Loss: 83.5445   time: 0.25s   best: 82.6865
2023-09-30 11:53:23,217:INFO:  Epoch 99/500:  train Loss: 83.8530   val Loss: 83.2839   time: 0.28s   best: 82.6865
2023-09-30 11:53:23,524:INFO:  Epoch 100/500:  train Loss: 85.2045   val Loss: 86.5715   time: 0.29s   best: 82.6865
2023-09-30 11:53:23,813:INFO:  Epoch 101/500:  train Loss: 86.4527   val Loss: 85.6356   time: 0.28s   best: 82.6865
2023-09-30 11:53:24,073:INFO:  Epoch 102/500:  train Loss: 85.4666   val Loss: 85.2637   time: 0.25s   best: 82.6865
2023-09-30 11:53:24,350:INFO:  Epoch 103/500:  train Loss: 85.6154   val Loss: 84.6987   time: 0.26s   best: 82.6865
2023-09-30 11:53:24,615:INFO:  Epoch 104/500:  train Loss: 84.6854   val Loss: 82.8781   time: 0.25s   best: 82.6865
2023-09-30 11:53:24,889:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:24,919:INFO:  Epoch 105/500:  train Loss: 83.3400   val Loss: 82.2223   time: 0.27s   best: 82.2223
2023-09-30 11:53:25,178:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:25,207:INFO:  Epoch 106/500:  train Loss: 83.2276   val Loss: 81.8722   time: 0.25s   best: 81.8722
2023-09-30 11:53:25,477:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:25,507:INFO:  Epoch 107/500:  train Loss: 82.8201   val Loss: 81.6253   time: 0.27s   best: 81.6253
2023-09-30 11:53:25,774:INFO:  Epoch 108/500:  train Loss: 82.3882   val Loss: 81.6598   time: 0.25s   best: 81.6253
2023-09-30 11:53:26,052:INFO:  Epoch 109/500:  train Loss: 82.1831   val Loss: 82.5953   time: 0.27s   best: 81.6253
2023-09-30 11:53:26,325:INFO:  Epoch 110/500:  train Loss: 83.1868   val Loss: 82.2231   time: 0.25s   best: 81.6253
2023-09-30 11:53:26,659:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:26,689:INFO:  Epoch 111/500:  train Loss: 82.5831   val Loss: 81.2854   time: 0.26s   best: 81.2854
2023-09-30 11:53:26,959:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:26,982:INFO:  Epoch 112/500:  train Loss: 82.5353   val Loss: 80.7552   time: 0.26s   best: 80.7552
2023-09-30 11:53:27,255:INFO:  Epoch 113/500:  train Loss: 83.0234   val Loss: 80.8792   time: 0.26s   best: 80.7552
2023-09-30 11:53:27,527:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:27,558:INFO:  Epoch 114/500:  train Loss: 81.9113   val Loss: 80.6651   time: 0.27s   best: 80.6651
2023-09-30 11:53:27,813:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:27,842:INFO:  Epoch 115/500:  train Loss: 81.5297   val Loss: 80.5573   time: 0.25s   best: 80.5573
2023-09-30 11:53:28,120:INFO:  Epoch 116/500:  train Loss: 81.8246   val Loss: 81.0549   time: 0.27s   best: 80.5573
2023-09-30 11:53:28,379:INFO:  Epoch 117/500:  train Loss: 82.0737   val Loss: 80.8273   time: 0.25s   best: 80.5573
2023-09-30 11:53:28,663:INFO:  Epoch 118/500:  train Loss: 81.7406   val Loss: 80.9553   time: 0.27s   best: 80.5573
2023-09-30 11:53:28,920:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:28,967:INFO:  Epoch 119/500:  train Loss: 80.4302   val Loss: 80.2302   time: 0.25s   best: 80.2302
2023-09-30 11:53:29,224:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:29,254:INFO:  Epoch 120/500:  train Loss: 81.5839   val Loss: 80.0984   time: 0.25s   best: 80.0984
2023-09-30 11:53:29,524:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:29,553:INFO:  Epoch 121/500:  train Loss: 80.6624   val Loss: 79.6871   time: 0.26s   best: 79.6871
2023-09-30 11:53:29,817:INFO:  Epoch 122/500:  train Loss: 80.6771   val Loss: 79.8554   time: 0.25s   best: 79.6871
2023-09-30 11:53:30,094:INFO:  Epoch 123/500:  train Loss: 82.3109   val Loss: 79.8255   time: 0.26s   best: 79.6871
2023-09-30 11:53:30,353:INFO:  Epoch 124/500:  train Loss: 81.6799   val Loss: 81.6296   time: 0.25s   best: 79.6871
2023-09-30 11:53:30,637:INFO:  Epoch 125/500:  train Loss: 81.1916   val Loss: 81.0009   time: 0.27s   best: 79.6871
2023-09-30 11:53:30,902:INFO:  Epoch 126/500:  train Loss: 82.2401   val Loss: 81.0285   time: 0.25s   best: 79.6871
2023-09-30 11:53:31,186:INFO:  Epoch 127/500:  train Loss: 80.5769   val Loss: 79.9566   time: 0.27s   best: 79.6871
2023-09-30 11:53:31,439:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:31,469:INFO:  Epoch 128/500:  train Loss: 80.4725   val Loss: 79.6113   time: 0.25s   best: 79.6113
2023-09-30 11:53:31,750:INFO:  Epoch 129/500:  train Loss: 83.3852   val Loss: 85.8547   time: 0.27s   best: 79.6113
2023-09-30 11:53:32,011:INFO:  Epoch 130/500:  train Loss: 84.9164   val Loss: 82.3600   time: 0.25s   best: 79.6113
2023-09-30 11:53:32,290:INFO:  Epoch 131/500:  train Loss: 82.8928   val Loss: 84.2330   time: 0.27s   best: 79.6113
2023-09-30 11:53:32,551:INFO:  Epoch 132/500:  train Loss: 83.9803   val Loss: 80.6276   time: 0.25s   best: 79.6113
2023-09-30 11:53:32,831:INFO:  Epoch 133/500:  train Loss: 81.1862   val Loss: 80.0868   time: 0.27s   best: 79.6113
2023-09-30 11:53:33,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:33,118:INFO:  Epoch 134/500:  train Loss: 80.0592   val Loss: 78.9108   time: 0.25s   best: 78.9108
2023-09-30 11:53:33,393:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:33,423:INFO:  Epoch 135/500:  train Loss: 79.8564   val Loss: 78.6006   time: 0.27s   best: 78.6006
2023-09-30 11:53:33,704:INFO:  Epoch 136/500:  train Loss: 80.7363   val Loss: 80.1083   time: 0.27s   best: 78.6006
2023-09-30 11:53:33,964:INFO:  Epoch 137/500:  train Loss: 79.5927   val Loss: 79.5304   time: 0.25s   best: 78.6006
2023-09-30 11:53:34,241:INFO:  Epoch 138/500:  train Loss: 80.4238   val Loss: 79.0908   time: 0.26s   best: 78.6006
2023-09-30 11:53:34,501:INFO:  Epoch 139/500:  train Loss: 80.1884   val Loss: 79.9503   time: 0.25s   best: 78.6006
2023-09-30 11:53:34,776:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:34,806:INFO:  Epoch 140/500:  train Loss: 79.3931   val Loss: 78.3470   time: 0.27s   best: 78.3470
2023-09-30 11:53:35,070:INFO:  Epoch 141/500:  train Loss: 78.9537   val Loss: 78.4581   time: 0.25s   best: 78.3470
2023-09-30 11:53:35,351:INFO:  Epoch 142/500:  train Loss: 80.7124   val Loss: 80.3719   time: 0.27s   best: 78.3470
2023-09-30 11:53:35,616:INFO:  Epoch 143/500:  train Loss: 80.7864   val Loss: 78.7517   time: 0.25s   best: 78.3470
2023-09-30 11:53:35,893:INFO:  Epoch 144/500:  train Loss: 79.2729   val Loss: 79.1119   time: 0.26s   best: 78.3470
2023-09-30 11:53:36,146:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:36,175:INFO:  Epoch 145/500:  train Loss: 79.4249   val Loss: 77.6305   time: 0.25s   best: 77.6305
2023-09-30 11:53:36,446:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:36,476:INFO:  Epoch 146/500:  train Loss: 78.2438   val Loss: 77.4377   time: 0.26s   best: 77.4377
2023-09-30 11:53:36,734:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:36,764:INFO:  Epoch 147/500:  train Loss: 78.3569   val Loss: 77.1941   time: 0.25s   best: 77.1941
2023-09-30 11:53:37,046:INFO:  Epoch 148/500:  train Loss: 79.7856   val Loss: 79.6749   time: 0.27s   best: 77.1941
2023-09-30 11:53:37,316:INFO:  Epoch 149/500:  train Loss: 80.0659   val Loss: 80.6947   time: 0.25s   best: 77.1941
2023-09-30 11:53:37,588:INFO:  Epoch 150/500:  train Loss: 80.8966   val Loss: 78.2125   time: 0.26s   best: 77.1941
2023-09-30 11:53:37,874:INFO:  Epoch 151/500:  train Loss: 78.8560   val Loss: 78.7945   time: 0.27s   best: 77.1941
2023-09-30 11:53:38,127:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:38,156:INFO:  Epoch 152/500:  train Loss: 78.9630   val Loss: 76.4414   time: 0.25s   best: 76.4414
2023-09-30 11:53:38,431:INFO:  Epoch 153/500:  train Loss: 79.1465   val Loss: 76.9834   time: 0.26s   best: 76.4414
2023-09-30 11:53:38,695:INFO:  Epoch 154/500:  train Loss: 80.4259   val Loss: 81.7037   time: 0.25s   best: 76.4414
2023-09-30 11:53:38,976:INFO:  Epoch 155/500:  train Loss: 80.2541   val Loss: 80.2122   time: 0.27s   best: 76.4414
2023-09-30 11:53:39,241:INFO:  Epoch 156/500:  train Loss: 81.4364   val Loss: 80.2936   time: 0.25s   best: 76.4414
2023-09-30 11:53:39,518:INFO:  Epoch 157/500:  train Loss: 80.4279   val Loss: 79.1910   time: 0.26s   best: 76.4414
2023-09-30 11:53:39,783:INFO:  Epoch 158/500:  train Loss: 78.6768   val Loss: 76.8352   time: 0.25s   best: 76.4414
2023-09-30 11:53:40,062:INFO:  Epoch 159/500:  train Loss: 77.6152   val Loss: 76.8695   time: 0.27s   best: 76.4414
2023-09-30 11:53:40,321:INFO:  Epoch 160/500:  train Loss: 78.7149   val Loss: 78.3445   time: 0.25s   best: 76.4414
2023-09-30 11:53:40,603:INFO:  Epoch 161/500:  train Loss: 78.1277   val Loss: 78.0094   time: 0.27s   best: 76.4414
2023-09-30 11:53:40,863:INFO:  Epoch 162/500:  train Loss: 78.6595   val Loss: 76.5408   time: 0.25s   best: 76.4414
2023-09-30 11:53:41,137:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:41,167:INFO:  Epoch 163/500:  train Loss: 77.1278   val Loss: 76.3182   time: 0.27s   best: 76.3182
2023-09-30 11:53:41,428:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:41,459:INFO:  Epoch 164/500:  train Loss: 78.1046   val Loss: 75.7198   time: 0.26s   best: 75.7198
2023-09-30 11:53:41,740:INFO:  Epoch 165/500:  train Loss: 78.4112   val Loss: 78.1790   time: 0.27s   best: 75.7198
2023-09-30 11:53:42,001:INFO:  Epoch 166/500:  train Loss: 78.1741   val Loss: 78.7670   time: 0.25s   best: 75.7198
2023-09-30 11:53:42,279:INFO:  Epoch 167/500:  train Loss: 78.9656   val Loss: 78.1107   time: 0.27s   best: 75.7198
2023-09-30 11:53:42,556:INFO:  Epoch 168/500:  train Loss: 78.0462   val Loss: 76.7356   time: 0.25s   best: 75.7198
2023-09-30 11:53:42,824:INFO:  Epoch 169/500:  train Loss: 77.4569   val Loss: 76.2395   time: 0.25s   best: 75.7198
2023-09-30 11:53:43,106:INFO:  Epoch 170/500:  train Loss: 81.3425   val Loss: 84.5107   time: 0.27s   best: 75.7198
2023-09-30 11:53:43,372:INFO:  Epoch 171/500:  train Loss: 84.2050   val Loss: 82.2334   time: 0.25s   best: 75.7198
2023-09-30 11:53:43,652:INFO:  Epoch 172/500:  train Loss: 83.0163   val Loss: 84.6024   time: 0.27s   best: 75.7198
2023-09-30 11:53:43,914:INFO:  Epoch 173/500:  train Loss: 84.0339   val Loss: 79.9751   time: 0.25s   best: 75.7198
2023-09-30 11:53:44,192:INFO:  Epoch 174/500:  train Loss: 80.4388   val Loss: 78.3893   time: 0.27s   best: 75.7198
2023-09-30 11:53:44,452:INFO:  Epoch 175/500:  train Loss: 78.7831   val Loss: 77.2956   time: 0.25s   best: 75.7198
2023-09-30 11:53:44,735:INFO:  Epoch 176/500:  train Loss: 77.4897   val Loss: 76.9588   time: 0.27s   best: 75.7198
2023-09-30 11:53:45,000:INFO:  Epoch 177/500:  train Loss: 77.8117   val Loss: 76.7271   time: 0.25s   best: 75.7198
2023-09-30 11:53:45,285:INFO:  Epoch 178/500:  train Loss: 77.1765   val Loss: 77.2425   time: 0.27s   best: 75.7198
2023-09-30 11:53:45,547:INFO:  Epoch 179/500:  train Loss: 78.2720   val Loss: 76.3336   time: 0.25s   best: 75.7198
2023-09-30 11:53:45,827:INFO:  Epoch 180/500:  train Loss: 77.1360   val Loss: 77.0962   time: 0.27s   best: 75.7198
2023-09-30 11:53:46,081:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:46,111:INFO:  Epoch 181/500:  train Loss: 77.0311   val Loss: 75.5781   time: 0.25s   best: 75.5781
2023-09-30 11:53:46,381:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:46,411:INFO:  Epoch 182/500:  train Loss: 76.4430   val Loss: 75.5190   time: 0.26s   best: 75.5190
2023-09-30 11:53:46,668:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:46,698:INFO:  Epoch 183/500:  train Loss: 76.1579   val Loss: 75.2532   time: 0.25s   best: 75.2532
2023-09-30 11:53:46,981:INFO:  Epoch 184/500:  train Loss: 76.6890   val Loss: 75.3578   time: 0.27s   best: 75.2532
2023-09-30 11:53:47,262:INFO:  Epoch 185/500:  train Loss: 77.3638   val Loss: 77.0416   time: 0.26s   best: 75.2532
2023-09-30 11:53:47,529:INFO:  Epoch 186/500:  train Loss: 76.9878   val Loss: 77.1859   time: 0.25s   best: 75.2532
2023-09-30 11:53:47,812:INFO:  Epoch 187/500:  train Loss: 77.9697   val Loss: 75.4363   time: 0.27s   best: 75.2532
2023-09-30 11:53:48,073:INFO:  Epoch 188/500:  train Loss: 76.3230   val Loss: 75.2919   time: 0.25s   best: 75.2532
2023-09-30 11:53:48,343:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:48,373:INFO:  Epoch 189/500:  train Loss: 76.5155   val Loss: 75.1930   time: 0.26s   best: 75.1930
2023-09-30 11:53:48,638:INFO:  Epoch 190/500:  train Loss: 77.5146   val Loss: 76.3344   time: 0.25s   best: 75.1930
2023-09-30 11:53:48,916:INFO:  Epoch 191/500:  train Loss: 76.1259   val Loss: 75.9602   time: 0.27s   best: 75.1930
2023-09-30 11:53:49,173:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:49,202:INFO:  Epoch 192/500:  train Loss: 76.9460   val Loss: 75.0390   time: 0.25s   best: 75.0390
2023-09-30 11:53:49,480:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:49,510:INFO:  Epoch 193/500:  train Loss: 75.6026   val Loss: 74.4718   time: 0.27s   best: 74.4718
2023-09-30 11:53:49,769:INFO:  Epoch 194/500:  train Loss: 76.3923   val Loss: 74.5872   time: 0.25s   best: 74.4718
2023-09-30 11:53:50,054:INFO:  Epoch 195/500:  train Loss: 75.7597   val Loss: 75.3007   time: 0.27s   best: 74.4718
2023-09-30 11:53:50,313:INFO:  Epoch 196/500:  train Loss: 76.0221   val Loss: 75.0373   time: 0.25s   best: 74.4718
2023-09-30 11:53:50,599:INFO:  Epoch 197/500:  train Loss: 75.7479   val Loss: 74.7405   time: 0.27s   best: 74.4718
2023-09-30 11:53:50,853:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:50,883:INFO:  Epoch 198/500:  train Loss: 75.5671   val Loss: 74.4179   time: 0.25s   best: 74.4179
2023-09-30 11:53:51,163:INFO:  Epoch 199/500:  train Loss: 75.2257   val Loss: 74.6594   time: 0.27s   best: 74.4179
2023-09-30 11:53:51,650:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:51,699:INFO:  Epoch 200/500:  train Loss: 75.6366   val Loss: 74.1211   time: 0.48s   best: 74.1211
2023-09-30 11:53:52,010:INFO:  Epoch 201/500:  train Loss: 75.5742   val Loss: 74.5063   time: 0.30s   best: 74.1211
2023-09-30 11:53:52,269:INFO:  Epoch 202/500:  train Loss: 75.4031   val Loss: 74.5163   time: 0.25s   best: 74.1211
2023-09-30 11:53:52,543:INFO:  Epoch 203/500:  train Loss: 75.7713   val Loss: 75.2953   time: 0.27s   best: 74.1211
2023-09-30 11:53:52,814:INFO:  Epoch 204/500:  train Loss: 75.1298   val Loss: 76.6409   time: 0.26s   best: 74.1211
2023-09-30 11:53:53,091:INFO:  Epoch 205/500:  train Loss: 79.5524   val Loss: 78.5617   time: 0.26s   best: 74.1211
2023-09-30 11:53:53,350:INFO:  Epoch 206/500:  train Loss: 80.5420   val Loss: 77.1724   time: 0.25s   best: 74.1211
2023-09-30 11:53:53,623:INFO:  Epoch 207/500:  train Loss: 77.3013   val Loss: 79.9435   time: 0.27s   best: 74.1211
2023-09-30 11:53:53,896:INFO:  Epoch 208/500:  train Loss: 79.8212   val Loss: 76.7936   time: 0.26s   best: 74.1211
2023-09-30 11:53:54,173:INFO:  Epoch 209/500:  train Loss: 76.6789   val Loss: 75.0449   time: 0.26s   best: 74.1211
2023-09-30 11:53:54,424:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:53:54,454:INFO:  Epoch 210/500:  train Loss: 75.5830   val Loss: 74.1012   time: 0.25s   best: 74.1012
2023-09-30 11:53:54,735:INFO:  Epoch 211/500:  train Loss: 75.8645   val Loss: 75.9047   time: 0.27s   best: 74.1012
2023-09-30 11:53:54,997:INFO:  Epoch 212/500:  train Loss: 89.0009   val Loss: 75.1364   time: 0.25s   best: 74.1012
2023-09-30 11:53:55,275:INFO:  Epoch 213/500:  train Loss: 86.7104   val Loss: 97.2020   time: 0.27s   best: 74.1012
2023-09-30 11:53:55,535:INFO:  Epoch 214/500:  train Loss: 97.9316   val Loss: 97.8888   time: 0.25s   best: 74.1012
2023-09-30 11:53:55,821:INFO:  Epoch 215/500:  train Loss: 97.2460   val Loss: 95.9725   time: 0.27s   best: 74.1012
2023-09-30 11:53:56,080:INFO:  Epoch 216/500:  train Loss: 94.7717   val Loss: 92.9943   time: 0.25s   best: 74.1012
2023-09-30 11:53:56,357:INFO:  Epoch 217/500:  train Loss: 92.2211   val Loss: 89.3741   time: 0.26s   best: 74.1012
2023-09-30 11:53:56,613:INFO:  Epoch 218/500:  train Loss: 91.6100   val Loss: 88.7825   time: 0.25s   best: 74.1012
2023-09-30 11:53:56,897:INFO:  Epoch 219/500:  train Loss: 91.8302   val Loss: 92.2785   time: 0.27s   best: 74.1012
2023-09-30 11:53:57,158:INFO:  Epoch 220/500:  train Loss: 92.1186   val Loss: 92.1222   time: 0.25s   best: 74.1012
2023-09-30 11:53:57,433:INFO:  Epoch 221/500:  train Loss: 92.1536   val Loss: 91.7350   time: 0.26s   best: 74.1012
2023-09-30 11:53:57,710:INFO:  Epoch 222/500:  train Loss: 91.1451   val Loss: 88.1438   time: 0.25s   best: 74.1012
2023-09-30 11:53:57,979:INFO:  Epoch 223/500:  train Loss: 91.1318   val Loss: 89.0438   time: 0.26s   best: 74.1012
2023-09-30 11:53:58,256:INFO:  Epoch 224/500:  train Loss: 91.8505   val Loss: 91.4164   time: 0.27s   best: 74.1012
2023-09-30 11:53:58,515:INFO:  Epoch 225/500:  train Loss: 91.7331   val Loss: 91.7352   time: 0.25s   best: 74.1012
2023-09-30 11:53:58,798:INFO:  Epoch 226/500:  train Loss: 92.2709   val Loss: 91.9330   time: 0.27s   best: 74.1012
2023-09-30 11:53:59,063:INFO:  Epoch 227/500:  train Loss: 92.4717   val Loss: 92.1511   time: 0.25s   best: 74.1012
2023-09-30 11:53:59,339:INFO:  Epoch 228/500:  train Loss: 92.3369   val Loss: 91.7721   time: 0.26s   best: 74.1012
2023-09-30 11:53:59,594:INFO:  Epoch 229/500:  train Loss: 91.8524   val Loss: 91.3791   time: 0.25s   best: 74.1012
2023-09-30 11:53:59,886:INFO:  Epoch 230/500:  train Loss: 91.3038   val Loss: 91.1719   time: 0.28s   best: 74.1012
2023-09-30 11:54:00,147:INFO:  Epoch 231/500:  train Loss: 91.1907   val Loss: 90.9691   time: 0.25s   best: 74.1012
2023-09-30 11:54:00,423:INFO:  Epoch 232/500:  train Loss: 91.2277   val Loss: 90.9469   time: 0.26s   best: 74.1012
2023-09-30 11:54:00,688:INFO:  Epoch 233/500:  train Loss: 90.9262   val Loss: 90.7920   time: 0.25s   best: 74.1012
2023-09-30 11:54:00,968:INFO:  Epoch 234/500:  train Loss: 91.0156   val Loss: 90.7655   time: 0.27s   best: 74.1012
2023-09-30 11:54:01,227:INFO:  Epoch 235/500:  train Loss: 90.7476   val Loss: 90.5000   time: 0.25s   best: 74.1012
2023-09-30 11:54:01,504:INFO:  Epoch 236/500:  train Loss: 90.8417   val Loss: 90.3195   time: 0.26s   best: 74.1012
2023-09-30 11:54:01,766:INFO:  Epoch 237/500:  train Loss: 90.4933   val Loss: 90.0879   time: 0.25s   best: 74.1012
2023-09-30 11:54:02,048:INFO:  Epoch 238/500:  train Loss: 90.1209   val Loss: 89.8359   time: 0.27s   best: 74.1012
2023-09-30 11:54:02,307:INFO:  Epoch 239/500:  train Loss: 89.9871   val Loss: 89.6284   time: 0.25s   best: 74.1012
2023-09-30 11:54:02,591:INFO:  Epoch 240/500:  train Loss: 90.0084   val Loss: 89.4773   time: 0.27s   best: 74.1012
2023-09-30 11:54:02,850:INFO:  Epoch 241/500:  train Loss: 89.7503   val Loss: 89.2004   time: 0.25s   best: 74.1012
2023-09-30 11:54:03,129:INFO:  Epoch 242/500:  train Loss: 89.4173   val Loss: 89.0658   time: 0.27s   best: 74.1012
2023-09-30 11:54:03,388:INFO:  Epoch 243/500:  train Loss: 89.3803   val Loss: 88.7083   time: 0.25s   best: 74.1012
2023-09-30 11:54:03,669:INFO:  Epoch 244/500:  train Loss: 88.7955   val Loss: 88.2026   time: 0.27s   best: 74.1012
2023-09-30 11:54:03,935:INFO:  Epoch 245/500:  train Loss: 88.2886   val Loss: 87.5932   time: 0.25s   best: 74.1012
2023-09-30 11:54:04,214:INFO:  Epoch 246/500:  train Loss: 87.7903   val Loss: 86.9547   time: 0.27s   best: 74.1012
2023-09-30 11:54:04,473:INFO:  Epoch 247/500:  train Loss: 86.8627   val Loss: 86.2674   time: 0.25s   best: 74.1012
2023-09-30 11:54:04,757:INFO:  Epoch 248/500:  train Loss: 86.5772   val Loss: 85.4741   time: 0.27s   best: 74.1012
2023-09-30 11:54:05,025:INFO:  Epoch 249/500:  train Loss: 85.8185   val Loss: 84.2908   time: 0.25s   best: 74.1012
2023-09-30 11:54:05,295:INFO:  Epoch 250/500:  train Loss: 84.3405   val Loss: 82.6649   time: 0.26s   best: 74.1012
2023-09-30 11:54:05,559:INFO:  Epoch 251/500:  train Loss: 83.9872   val Loss: 82.1140   time: 0.25s   best: 74.1012
2023-09-30 11:54:05,839:INFO:  Epoch 252/500:  train Loss: 81.9705   val Loss: 80.3907   time: 0.26s   best: 74.1012
2023-09-30 11:54:06,117:INFO:  Epoch 253/500:  train Loss: 80.6911   val Loss: 79.1224   time: 0.27s   best: 74.1012
2023-09-30 11:54:06,376:INFO:  Epoch 254/500:  train Loss: 79.2361   val Loss: 77.5798   time: 0.25s   best: 74.1012
2023-09-30 11:54:06,660:INFO:  Epoch 255/500:  train Loss: 78.6663   val Loss: 77.8230   time: 0.27s   best: 74.1012
2023-09-30 11:54:06,919:INFO:  Epoch 256/500:  train Loss: 77.6098   val Loss: 77.8657   time: 0.25s   best: 74.1012
2023-09-30 11:54:07,199:INFO:  Epoch 257/500:  train Loss: 77.7759   val Loss: 75.9772   time: 0.27s   best: 74.1012
2023-09-30 11:54:07,462:INFO:  Epoch 258/500:  train Loss: 77.4182   val Loss: 75.7848   time: 0.25s   best: 74.1012
2023-09-30 11:54:07,729:INFO:  Epoch 259/500:  train Loss: 76.8894   val Loss: 76.2753   time: 0.26s   best: 74.1012
2023-09-30 11:54:08,003:INFO:  Epoch 260/500:  train Loss: 77.4817   val Loss: 76.3957   time: 0.26s   best: 74.1012
2023-09-30 11:54:08,288:INFO:  Epoch 261/500:  train Loss: 76.6120   val Loss: 75.4611   time: 0.27s   best: 74.1012
2023-09-30 11:54:08,547:INFO:  Epoch 262/500:  train Loss: 76.6060   val Loss: 74.7396   time: 0.25s   best: 74.1012
2023-09-30 11:54:08,830:INFO:  Epoch 263/500:  train Loss: 75.7282   val Loss: 76.0052   time: 0.27s   best: 74.1012
2023-09-30 11:54:09,094:INFO:  Epoch 264/500:  train Loss: 77.1793   val Loss: 75.1325   time: 0.25s   best: 74.1012
2023-09-30 11:54:09,371:INFO:  Epoch 265/500:  train Loss: 75.8393   val Loss: 75.7593   time: 0.26s   best: 74.1012
2023-09-30 11:54:09,632:INFO:  Epoch 266/500:  train Loss: 76.4016   val Loss: 76.0053   time: 0.25s   best: 74.1012
2023-09-30 11:54:09,921:INFO:  Epoch 267/500:  train Loss: 75.6901   val Loss: 74.8753   time: 0.28s   best: 74.1012
2023-09-30 11:54:10,176:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:10,206:INFO:  Epoch 268/500:  train Loss: 75.8780   val Loss: 73.7997   time: 0.25s   best: 73.7997
2023-09-30 11:54:10,484:INFO:  Epoch 269/500:  train Loss: 74.8729   val Loss: 73.8708   time: 0.27s   best: 73.7997
2023-09-30 11:54:10,813:INFO:  Epoch 270/500:  train Loss: 75.3776   val Loss: 74.1128   time: 0.32s   best: 73.7997
2023-09-30 11:54:11,075:INFO:  Epoch 271/500:  train Loss: 74.7221   val Loss: 73.9857   time: 0.25s   best: 73.7997
2023-09-30 11:54:11,352:INFO:  Epoch 272/500:  train Loss: 74.2795   val Loss: 74.4688   time: 0.27s   best: 73.7997
2023-09-30 11:54:11,613:INFO:  Epoch 273/500:  train Loss: 75.3805   val Loss: 74.3754   time: 0.25s   best: 73.7997
2023-09-30 11:54:11,900:INFO:  Epoch 274/500:  train Loss: 75.9334   val Loss: 73.8822   time: 0.27s   best: 73.7997
2023-09-30 11:54:12,160:INFO:  Epoch 275/500:  train Loss: 74.9910   val Loss: 74.4653   time: 0.25s   best: 73.7997
2023-09-30 11:54:12,437:INFO:  Epoch 276/500:  train Loss: 75.0526   val Loss: 74.0229   time: 0.26s   best: 73.7997
2023-09-30 11:54:12,701:INFO:  Epoch 277/500:  train Loss: 74.6767   val Loss: 73.8547   time: 0.25s   best: 73.7997
2023-09-30 11:54:12,979:INFO:  Epoch 278/500:  train Loss: 74.4567   val Loss: 74.2804   time: 0.26s   best: 73.7997
2023-09-30 11:54:13,232:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:13,262:INFO:  Epoch 279/500:  train Loss: 74.9730   val Loss: 73.5582   time: 0.25s   best: 73.5582
2023-09-30 11:54:13,531:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:13,561:INFO:  Epoch 280/500:  train Loss: 73.8597   val Loss: 72.5965   time: 0.26s   best: 72.5965
2023-09-30 11:54:13,825:INFO:  Epoch 281/500:  train Loss: 73.9805   val Loss: 72.9059   time: 0.25s   best: 72.5965
2023-09-30 11:54:14,111:INFO:  Epoch 282/500:  train Loss: 73.8630   val Loss: 73.0843   time: 0.27s   best: 72.5965
2023-09-30 11:54:14,370:INFO:  Epoch 283/500:  train Loss: 73.6054   val Loss: 72.8198   time: 0.25s   best: 72.5965
2023-09-30 11:54:14,651:INFO:  Epoch 284/500:  train Loss: 74.0485   val Loss: 72.9195   time: 0.27s   best: 72.5965
2023-09-30 11:54:14,911:INFO:  Epoch 285/500:  train Loss: 73.2110   val Loss: 72.6102   time: 0.25s   best: 72.5965
2023-09-30 11:54:15,191:INFO:  Epoch 286/500:  train Loss: 74.2037   val Loss: 73.0089   time: 0.27s   best: 72.5965
2023-09-30 11:54:15,449:INFO:  Epoch 287/500:  train Loss: 73.2272   val Loss: 72.6168   time: 0.25s   best: 72.5965
2023-09-30 11:54:15,728:INFO:  Epoch 288/500:  train Loss: 75.0554   val Loss: 73.4286   time: 0.27s   best: 72.5965
2023-09-30 11:54:16,014:INFO:  Epoch 289/500:  train Loss: 75.7099   val Loss: 75.0922   time: 0.27s   best: 72.5965
2023-09-30 11:54:16,275:INFO:  Epoch 290/500:  train Loss: 76.0549   val Loss: 75.0643   time: 0.25s   best: 72.5965
2023-09-30 11:54:16,552:INFO:  Epoch 291/500:  train Loss: 75.2643   val Loss: 73.5248   time: 0.26s   best: 72.5965
2023-09-30 11:54:16,810:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:16,839:INFO:  Epoch 292/500:  train Loss: 73.9956   val Loss: 72.5264   time: 0.25s   best: 72.5264
2023-09-30 11:54:17,110:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:17,141:INFO:  Epoch 293/500:  train Loss: 73.4817   val Loss: 72.4926   time: 0.27s   best: 72.4926
2023-09-30 11:54:17,392:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:17,422:INFO:  Epoch 294/500:  train Loss: 73.0595   val Loss: 71.8495   time: 0.25s   best: 71.8495
2023-09-30 11:54:17,694:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:17,724:INFO:  Epoch 295/500:  train Loss: 72.8143   val Loss: 71.3026   time: 0.27s   best: 71.3026
2023-09-30 11:54:17,989:INFO:  Epoch 296/500:  train Loss: 72.5136   val Loss: 71.7493   time: 0.25s   best: 71.3026
2023-09-30 11:54:18,267:INFO:  Epoch 297/500:  train Loss: 72.8901   val Loss: 71.3940   time: 0.26s   best: 71.3026
2023-09-30 11:54:18,526:INFO:  Epoch 298/500:  train Loss: 73.3614   val Loss: 72.3211   time: 0.25s   best: 71.3026
2023-09-30 11:54:18,809:INFO:  Epoch 299/500:  train Loss: 73.6138   val Loss: 73.5787   time: 0.27s   best: 71.3026
2023-09-30 11:54:19,106:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:19,155:INFO:  Epoch 300/500:  train Loss: 73.8200   val Loss: 71.2841   time: 0.29s   best: 71.2841
2023-09-30 11:54:19,426:INFO:  Epoch 301/500:  train Loss: 72.8116   val Loss: 71.6414   time: 0.26s   best: 71.2841
2023-09-30 11:54:19,696:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:19,725:INFO:  Epoch 302/500:  train Loss: 72.7431   val Loss: 71.0678   time: 0.26s   best: 71.0678
2023-09-30 11:54:19,987:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:20,016:INFO:  Epoch 303/500:  train Loss: 72.7114   val Loss: 70.4897   time: 0.26s   best: 70.4897
2023-09-30 11:54:20,293:INFO:  Epoch 304/500:  train Loss: 71.9171   val Loss: 72.2869   time: 0.26s   best: 70.4897
2023-09-30 11:54:20,555:INFO:  Epoch 305/500:  train Loss: 72.7622   val Loss: 70.9603   time: 0.25s   best: 70.4897
2023-09-30 11:54:20,831:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:20,862:INFO:  Epoch 306/500:  train Loss: 71.8393   val Loss: 70.3004   time: 0.27s   best: 70.3004
2023-09-30 11:54:21,122:INFO:  Epoch 307/500:  train Loss: 71.3834   val Loss: 71.8361   time: 0.25s   best: 70.3004
2023-09-30 11:54:21,398:INFO:  Epoch 308/500:  train Loss: 72.9315   val Loss: 71.8072   time: 0.26s   best: 70.3004
2023-09-30 11:54:21,658:INFO:  Epoch 309/500:  train Loss: 72.1347   val Loss: 71.1191   time: 0.25s   best: 70.3004
2023-09-30 11:54:22,003:INFO:  Epoch 310/500:  train Loss: 74.2264   val Loss: 72.0711   time: 0.33s   best: 70.3004
2023-09-30 11:54:22,433:INFO:  Epoch 311/500:  train Loss: 75.1011   val Loss: 75.6830   time: 0.42s   best: 70.3004
2023-09-30 11:54:22,694:INFO:  Epoch 312/500:  train Loss: 75.6681   val Loss: 73.7233   time: 0.25s   best: 70.3004
2023-09-30 11:54:22,969:INFO:  Epoch 313/500:  train Loss: 74.4297   val Loss: 72.9617   time: 0.26s   best: 70.3004
2023-09-30 11:54:23,231:INFO:  Epoch 314/500:  train Loss: 74.1379   val Loss: 73.2941   time: 0.25s   best: 70.3004
2023-09-30 11:54:23,518:INFO:  Epoch 315/500:  train Loss: 74.9881   val Loss: 74.1306   time: 0.28s   best: 70.3004
2023-09-30 11:54:23,781:INFO:  Epoch 316/500:  train Loss: 74.0640   val Loss: 73.8198   time: 0.25s   best: 70.3004
2023-09-30 11:54:24,059:INFO:  Epoch 317/500:  train Loss: 73.3162   val Loss: 72.9709   time: 0.26s   best: 70.3004
2023-09-30 11:54:24,319:INFO:  Epoch 318/500:  train Loss: 73.8728   val Loss: 70.8651   time: 0.25s   best: 70.3004
2023-09-30 11:54:24,604:INFO:  Epoch 319/500:  train Loss: 74.3058   val Loss: 74.7071   time: 0.27s   best: 70.3004
2023-09-30 11:54:24,866:INFO:  Epoch 320/500:  train Loss: 73.5953   val Loss: 76.3348   time: 0.25s   best: 70.3004
2023-09-30 11:54:25,145:INFO:  Epoch 321/500:  train Loss: 74.5643   val Loss: 71.3501   time: 0.27s   best: 70.3004
2023-09-30 11:54:25,415:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:25,445:INFO:  Epoch 322/500:  train Loss: 71.8022   val Loss: 70.2261   time: 0.25s   best: 70.2261
2023-09-30 11:54:25,706:INFO:  Epoch 323/500:  train Loss: 71.4836   val Loss: 70.4080   time: 0.25s   best: 70.2261
2023-09-30 11:54:25,976:INFO:  Epoch 324/500:  train Loss: 71.5728   val Loss: 72.9271   time: 0.26s   best: 70.2261
2023-09-30 11:54:26,243:INFO:  Epoch 325/500:  train Loss: 71.4106   val Loss: 74.3549   time: 0.25s   best: 70.2261
2023-09-30 11:54:26,525:INFO:  Epoch 326/500:  train Loss: 72.6997   val Loss: 71.8183   time: 0.27s   best: 70.2261
2023-09-30 11:54:26,785:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:26,815:INFO:  Epoch 327/500:  train Loss: 71.1165   val Loss: 69.9164   time: 0.25s   best: 69.9164
2023-09-30 11:54:27,093:INFO:  Epoch 328/500:  train Loss: 71.7855   val Loss: 71.6132   time: 0.27s   best: 69.9164
2023-09-30 11:54:27,353:INFO:  Epoch 329/500:  train Loss: 71.3792   val Loss: 72.6367   time: 0.25s   best: 69.9164
2023-09-30 11:54:27,630:INFO:  Epoch 330/500:  train Loss: 71.7712   val Loss: 72.7481   time: 0.27s   best: 69.9164
2023-09-30 11:54:27,884:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:27,914:INFO:  Epoch 331/500:  train Loss: 71.2758   val Loss: 68.3713   time: 0.25s   best: 68.3713
2023-09-30 11:54:28,192:INFO:  Epoch 332/500:  train Loss: 70.4821   val Loss: 70.1334   time: 0.27s   best: 68.3713
2023-09-30 11:54:28,459:INFO:  Epoch 333/500:  train Loss: 70.1354   val Loss: 69.7718   time: 0.25s   best: 68.3713
2023-09-30 11:54:28,743:INFO:  Epoch 334/500:  train Loss: 69.4138   val Loss: 70.5435   time: 0.27s   best: 68.3713
2023-09-30 11:54:28,991:INFO:  Epoch 335/500:  train Loss: 72.0925   val Loss: 69.9935   time: 0.25s   best: 68.3713
2023-09-30 11:54:29,290:INFO:  Epoch 336/500:  train Loss: 70.0021   val Loss: 69.4945   time: 0.26s   best: 68.3713
2023-09-30 11:54:29,549:INFO:  Epoch 337/500:  train Loss: 69.6763   val Loss: 68.9930   time: 0.25s   best: 68.3713
2023-09-30 11:54:29,833:INFO:  Epoch 338/500:  train Loss: 70.2257   val Loss: 70.3207   time: 0.27s   best: 68.3713
2023-09-30 11:54:30,101:INFO:  Epoch 339/500:  train Loss: 69.7171   val Loss: 71.0143   time: 0.25s   best: 68.3713
2023-09-30 11:54:30,372:INFO:  Epoch 340/500:  train Loss: 69.6357   val Loss: 69.7747   time: 0.26s   best: 68.3713
2023-09-30 11:54:30,662:INFO:  Epoch 341/500:  train Loss: 72.4141   val Loss: 69.1803   time: 0.28s   best: 68.3713
2023-09-30 11:54:30,924:INFO:  Epoch 342/500:  train Loss: 73.5258   val Loss: 73.9678   time: 0.25s   best: 68.3713
2023-09-30 11:54:31,204:INFO:  Epoch 343/500:  train Loss: 74.2956   val Loss: 73.6607   time: 0.27s   best: 68.3713
2023-09-30 11:54:31,463:INFO:  Epoch 344/500:  train Loss: 73.8221   val Loss: 73.7092   time: 0.25s   best: 68.3713
2023-09-30 11:54:31,741:INFO:  Epoch 345/500:  train Loss: 72.1849   val Loss: 72.7954   time: 0.27s   best: 68.3713
2023-09-30 11:54:32,003:INFO:  Epoch 346/500:  train Loss: 72.7867   val Loss: 73.6116   time: 0.25s   best: 68.3713
2023-09-30 11:54:32,280:INFO:  Epoch 347/500:  train Loss: 72.8194   val Loss: 74.1042   time: 0.26s   best: 68.3713
2023-09-30 11:54:32,546:INFO:  Epoch 348/500:  train Loss: 75.7653   val Loss: 78.4157   time: 0.25s   best: 68.3713
2023-09-30 11:54:32,829:INFO:  Epoch 349/500:  train Loss: 75.7842   val Loss: 76.6451   time: 0.27s   best: 68.3713
2023-09-30 11:54:33,089:INFO:  Epoch 350/500:  train Loss: 77.6256   val Loss: 73.4449   time: 0.25s   best: 68.3713
2023-09-30 11:54:33,367:INFO:  Epoch 351/500:  train Loss: 73.7712   val Loss: 73.9965   time: 0.27s   best: 68.3713
2023-09-30 11:54:33,629:INFO:  Epoch 352/500:  train Loss: 73.3524   val Loss: 71.7760   time: 0.25s   best: 68.3713
2023-09-30 11:54:33,907:INFO:  Epoch 353/500:  train Loss: 72.6082   val Loss: 68.9709   time: 0.27s   best: 68.3713
2023-09-30 11:54:34,167:INFO:  Epoch 354/500:  train Loss: 70.0151   val Loss: 70.7831   time: 0.25s   best: 68.3713
2023-09-30 11:54:34,453:INFO:  Epoch 355/500:  train Loss: 70.2136   val Loss: 71.8838   time: 0.26s   best: 68.3713
2023-09-30 11:54:34,715:INFO:  Epoch 356/500:  train Loss: 70.4066   val Loss: 72.6128   time: 0.25s   best: 68.3713
2023-09-30 11:54:34,994:INFO:  Epoch 357/500:  train Loss: 70.7974   val Loss: 69.3688   time: 0.27s   best: 68.3713
2023-09-30 11:54:35,256:INFO:  Epoch 358/500:  train Loss: 69.8823   val Loss: 68.7168   time: 0.25s   best: 68.3713
2023-09-30 11:54:35,525:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:35,556:INFO:  Epoch 359/500:  train Loss: 68.4384   val Loss: 67.9825   time: 0.26s   best: 67.9825
2023-09-30 11:54:35,818:INFO:  Epoch 360/500:  train Loss: 69.4411   val Loss: 68.8142   time: 0.25s   best: 67.9825
2023-09-30 11:54:36,096:INFO:  Epoch 361/500:  train Loss: 70.0936   val Loss: 69.2177   time: 0.26s   best: 67.9825
2023-09-30 11:54:36,356:INFO:  Epoch 362/500:  train Loss: 71.8973   val Loss: 71.4732   time: 0.25s   best: 67.9825
2023-09-30 11:54:36,643:INFO:  Epoch 363/500:  train Loss: 71.3105   val Loss: 72.2169   time: 0.27s   best: 67.9825
2023-09-30 11:54:36,924:INFO:  Epoch 364/500:  train Loss: 69.9983   val Loss: 69.2325   time: 0.27s   best: 67.9825
2023-09-30 11:54:37,179:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:37,209:INFO:  Epoch 365/500:  train Loss: 74.2738   val Loss: 67.5789   time: 0.25s   best: 67.5789
2023-09-30 11:54:37,477:INFO:  Epoch 366/500:  train Loss: 72.9184   val Loss: 74.0556   time: 0.27s   best: 67.5789
2023-09-30 11:54:37,737:INFO:  Epoch 367/500:  train Loss: 74.6294   val Loss: 73.0288   time: 0.25s   best: 67.5789
2023-09-30 11:54:38,029:INFO:  Epoch 368/500:  train Loss: 73.2015   val Loss: 72.1380   time: 0.28s   best: 67.5789
2023-09-30 11:54:38,290:INFO:  Epoch 369/500:  train Loss: 71.8504   val Loss: 70.6191   time: 0.25s   best: 67.5789
2023-09-30 11:54:38,575:INFO:  Epoch 370/500:  train Loss: 70.3353   val Loss: 69.3891   time: 0.27s   best: 67.5789
2023-09-30 11:54:38,841:INFO:  Epoch 371/500:  train Loss: 70.8157   val Loss: 68.0451   time: 0.25s   best: 67.5789
2023-09-30 11:54:39,122:INFO:  Epoch 372/500:  train Loss: 69.4010   val Loss: 67.6933   time: 0.27s   best: 67.5789
2023-09-30 11:54:39,381:INFO:  Epoch 373/500:  train Loss: 68.0055   val Loss: 68.0489   time: 0.25s   best: 67.5789
2023-09-30 11:54:39,660:INFO:  Epoch 374/500:  train Loss: 70.2576   val Loss: 71.8872   time: 0.27s   best: 67.5789
2023-09-30 11:54:39,925:INFO:  Epoch 375/500:  train Loss: 72.2129   val Loss: 72.6635   time: 0.25s   best: 67.5789
2023-09-30 11:54:40,203:INFO:  Epoch 376/500:  train Loss: 71.3308   val Loss: 73.0900   time: 0.27s   best: 67.5789
2023-09-30 11:54:40,472:INFO:  Epoch 377/500:  train Loss: 70.9109   val Loss: 70.2344   time: 0.25s   best: 67.5789
2023-09-30 11:54:40,755:INFO:  Epoch 378/500:  train Loss: 69.1895   val Loss: 70.0620   time: 0.27s   best: 67.5789
2023-09-30 11:54:41,015:INFO:  Epoch 379/500:  train Loss: 68.9314   val Loss: 67.5835   time: 0.25s   best: 67.5789
2023-09-30 11:54:41,295:INFO:  Epoch 380/500:  train Loss: 68.1967   val Loss: 69.0139   time: 0.27s   best: 67.5789
2023-09-30 11:54:41,547:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:41,577:INFO:  Epoch 381/500:  train Loss: 69.6888   val Loss: 67.4473   time: 0.25s   best: 67.4473
2023-09-30 11:54:41,858:INFO:  Epoch 382/500:  train Loss: 69.3513   val Loss: 68.4342   time: 0.27s   best: 67.4473
2023-09-30 11:54:42,111:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:42,159:INFO:  Epoch 383/500:  train Loss: 68.4494   val Loss: 66.9106   time: 0.25s   best: 66.9106
2023-09-30 11:54:42,420:INFO:  Epoch 384/500:  train Loss: 67.7266   val Loss: 68.8694   time: 0.25s   best: 66.9106
2023-09-30 11:54:42,709:INFO:  Epoch 385/500:  train Loss: 68.6726   val Loss: 68.2647   time: 0.28s   best: 66.9106
2023-09-30 11:54:42,970:INFO:  Epoch 386/500:  train Loss: 68.1425   val Loss: 68.1318   time: 0.25s   best: 66.9106
2023-09-30 11:54:43,250:INFO:  Epoch 387/500:  train Loss: 68.4991   val Loss: 67.1912   time: 0.27s   best: 66.9106
2023-09-30 11:54:43,509:INFO:  Epoch 388/500:  train Loss: 68.4816   val Loss: 67.8783   time: 0.25s   best: 66.9106
2023-09-30 11:54:43,784:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:43,814:INFO:  Epoch 389/500:  train Loss: 67.5377   val Loss: 65.6703   time: 0.27s   best: 65.6703
2023-09-30 11:54:44,073:INFO:  Epoch 390/500:  train Loss: 66.7486   val Loss: 66.8656   time: 0.25s   best: 65.6703
2023-09-30 11:54:44,350:INFO:  Epoch 391/500:  train Loss: 67.8174   val Loss: 68.2385   time: 0.26s   best: 65.6703
2023-09-30 11:54:44,619:INFO:  Epoch 392/500:  train Loss: 68.5561   val Loss: 67.6037   time: 0.26s   best: 65.6703
2023-09-30 11:54:44,899:INFO:  Epoch 393/500:  train Loss: 67.2280   val Loss: 66.4702   time: 0.27s   best: 65.6703
2023-09-30 11:54:45,160:INFO:  Epoch 394/500:  train Loss: 66.8227   val Loss: 67.5921   time: 0.25s   best: 65.6703
2023-09-30 11:54:45,438:INFO:  Epoch 395/500:  train Loss: 67.3211   val Loss: 67.4106   time: 0.26s   best: 65.6703
2023-09-30 11:54:45,699:INFO:  Epoch 396/500:  train Loss: 69.6456   val Loss: 66.7692   time: 0.25s   best: 65.6703
2023-09-30 11:54:45,979:INFO:  Epoch 397/500:  train Loss: 68.2662   val Loss: 67.7290   time: 0.27s   best: 65.6703
2023-09-30 11:54:46,238:INFO:  Epoch 398/500:  train Loss: 68.4300   val Loss: 70.9247   time: 0.25s   best: 65.6703
2023-09-30 11:54:46,524:INFO:  Epoch 399/500:  train Loss: 70.2433   val Loss: 68.9780   time: 0.27s   best: 65.6703
2023-09-30 11:54:46,834:INFO:  Epoch 400/500:  train Loss: 68.4672   val Loss: 69.9815   time: 0.29s   best: 65.6703
2023-09-30 11:54:47,122:INFO:  Epoch 401/500:  train Loss: 73.7452   val Loss: 68.9964   time: 0.28s   best: 65.6703
2023-09-30 11:54:47,401:INFO:  Epoch 402/500:  train Loss: 71.9857   val Loss: 72.4760   time: 0.27s   best: 65.6703
2023-09-30 11:54:47,662:INFO:  Epoch 403/500:  train Loss: 72.6241   val Loss: 70.0488   time: 0.25s   best: 65.6703
2023-09-30 11:54:47,940:INFO:  Epoch 404/500:  train Loss: 69.7412   val Loss: 68.5582   time: 0.27s   best: 65.6703
2023-09-30 11:54:48,199:INFO:  Epoch 405/500:  train Loss: 70.0346   val Loss: 66.7295   time: 0.25s   best: 65.6703
2023-09-30 11:54:48,476:INFO:  Epoch 406/500:  train Loss: 67.0759   val Loss: 66.2040   time: 0.26s   best: 65.6703
2023-09-30 11:54:48,740:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:54:48,770:INFO:  Epoch 407/500:  train Loss: 66.9290   val Loss: 65.2408   time: 0.26s   best: 65.2408
2023-09-30 11:54:49,051:INFO:  Epoch 408/500:  train Loss: 68.5898   val Loss: 65.4342   time: 0.27s   best: 65.2408
2023-09-30 11:54:49,303:INFO:  Epoch 409/500:  train Loss: 67.8851   val Loss: 67.6170   time: 0.25s   best: 65.2408
2023-09-30 11:54:49,588:INFO:  Epoch 410/500:  train Loss: 67.9636   val Loss: 66.3052   time: 0.27s   best: 65.2408
2023-09-30 11:54:49,854:INFO:  Epoch 411/500:  train Loss: 68.2280   val Loss: 67.5378   time: 0.25s   best: 65.2408
2023-09-30 11:54:50,131:INFO:  Epoch 412/500:  train Loss: 68.1044   val Loss: 69.4027   time: 0.26s   best: 65.2408
2023-09-30 11:54:50,389:INFO:  Epoch 413/500:  train Loss: 69.2060   val Loss: 68.2286   time: 0.25s   best: 65.2408
2023-09-30 11:54:50,680:INFO:  Epoch 414/500:  train Loss: 69.4979   val Loss: 66.7595   time: 0.28s   best: 65.2408
2023-09-30 11:54:50,942:INFO:  Epoch 415/500:  train Loss: 69.8595   val Loss: 69.8561   time: 0.25s   best: 65.2408
2023-09-30 11:54:51,222:INFO:  Epoch 416/500:  train Loss: 68.9609   val Loss: 69.5124   time: 0.27s   best: 65.2408
2023-09-30 11:54:51,481:INFO:  Epoch 417/500:  train Loss: 68.5511   val Loss: 68.4954   time: 0.25s   best: 65.2408
2023-09-30 11:54:51,761:INFO:  Epoch 418/500:  train Loss: 67.4197   val Loss: 65.9307   time: 0.27s   best: 65.2408
2023-09-30 11:54:52,021:INFO:  Epoch 419/500:  train Loss: 66.7727   val Loss: 66.7451   time: 0.25s   best: 65.2408
2023-09-30 11:54:52,299:INFO:  Epoch 420/500:  train Loss: 66.3748   val Loss: 66.0777   time: 0.27s   best: 65.2408
2023-09-30 11:54:52,626:INFO:  Epoch 421/500:  train Loss: 68.5008   val Loss: 69.1118   time: 0.32s   best: 65.2408
2023-09-30 11:54:53,066:INFO:  Epoch 422/500:  train Loss: 68.3101   val Loss: 73.2203   time: 0.43s   best: 65.2408
2023-09-30 11:54:53,344:INFO:  Epoch 423/500:  train Loss: 71.0746   val Loss: 72.1202   time: 0.26s   best: 65.2408
2023-09-30 11:54:53,603:INFO:  Epoch 424/500:  train Loss: 68.3503   val Loss: 69.7047   time: 0.25s   best: 65.2408
2023-09-30 11:54:53,884:INFO:  Epoch 425/500:  train Loss: 67.8310   val Loss: 69.9035   time: 0.27s   best: 65.2408
2023-09-30 11:54:54,142:INFO:  Epoch 426/500:  train Loss: 67.4887   val Loss: 67.5086   time: 0.25s   best: 65.2408
2023-09-30 11:54:54,420:INFO:  Epoch 427/500:  train Loss: 72.5790   val Loss: 78.6668   time: 0.27s   best: 65.2408
2023-09-30 11:54:54,697:INFO:  Epoch 428/500:  train Loss: 77.3576   val Loss: 76.5759   time: 0.25s   best: 65.2408
2023-09-30 11:54:54,962:INFO:  Epoch 429/500:  train Loss: 76.7907   val Loss: 73.1707   time: 0.25s   best: 65.2408
2023-09-30 11:54:55,239:INFO:  Epoch 430/500:  train Loss: 72.9243   val Loss: 71.4800   time: 0.27s   best: 65.2408
2023-09-30 11:54:55,506:INFO:  Epoch 431/500:  train Loss: 71.1641   val Loss: 70.1644   time: 0.25s   best: 65.2408
2023-09-30 11:54:55,785:INFO:  Epoch 432/500:  train Loss: 70.7306   val Loss: 68.8247   time: 0.27s   best: 65.2408
2023-09-30 11:54:56,047:INFO:  Epoch 433/500:  train Loss: 68.3421   val Loss: 67.4755   time: 0.25s   best: 65.2408
2023-09-30 11:54:56,323:INFO:  Epoch 434/500:  train Loss: 68.2233   val Loss: 66.7032   time: 0.26s   best: 65.2408
2023-09-30 11:54:56,582:INFO:  Epoch 435/500:  train Loss: 67.8099   val Loss: 66.4158   time: 0.25s   best: 65.2408
2023-09-30 11:54:56,867:INFO:  Epoch 436/500:  train Loss: 67.6026   val Loss: 66.6464   time: 0.27s   best: 65.2408
2023-09-30 11:54:57,133:INFO:  Epoch 437/500:  train Loss: 66.5755   val Loss: 66.6506   time: 0.25s   best: 65.2408
2023-09-30 11:54:57,409:INFO:  Epoch 438/500:  train Loss: 69.0102   val Loss: 67.6446   time: 0.26s   best: 65.2408
2023-09-30 11:54:57,669:INFO:  Epoch 439/500:  train Loss: 69.3328   val Loss: 70.6049   time: 0.25s   best: 65.2408
2023-09-30 11:54:57,949:INFO:  Epoch 440/500:  train Loss: 70.7965   val Loss: 71.1560   time: 0.27s   best: 65.2408
2023-09-30 11:54:58,201:INFO:  Epoch 441/500:  train Loss: 69.9534   val Loss: 70.6292   time: 0.25s   best: 65.2408
2023-09-30 11:54:58,488:INFO:  Epoch 442/500:  train Loss: 71.1945   val Loss: 75.9492   time: 0.27s   best: 65.2408
2023-09-30 11:54:58,750:INFO:  Epoch 443/500:  train Loss: 71.5900   val Loss: 73.0931   time: 0.25s   best: 65.2408
2023-09-30 11:54:59,031:INFO:  Epoch 444/500:  train Loss: 70.0446   val Loss: 71.4194   time: 0.27s   best: 65.2408
2023-09-30 11:54:59,300:INFO:  Epoch 445/500:  train Loss: 71.1217   val Loss: 66.7400   time: 0.26s   best: 65.2408
2023-09-30 11:54:59,580:INFO:  Epoch 446/500:  train Loss: 69.4149   val Loss: 72.5159   time: 0.27s   best: 65.2408
2023-09-30 11:54:59,846:INFO:  Epoch 447/500:  train Loss: 70.2018   val Loss: 70.6639   time: 0.25s   best: 65.2408
2023-09-30 11:55:00,124:INFO:  Epoch 448/500:  train Loss: 70.1263   val Loss: 70.9568   time: 0.26s   best: 65.2408
2023-09-30 11:55:00,382:INFO:  Epoch 449/500:  train Loss: 69.5122   val Loss: 68.0348   time: 0.25s   best: 65.2408
2023-09-30 11:55:00,661:INFO:  Epoch 450/500:  train Loss: 67.7102   val Loss: 67.9769   time: 0.27s   best: 65.2408
2023-09-30 11:55:00,926:INFO:  Epoch 451/500:  train Loss: 67.3101   val Loss: 67.2382   time: 0.25s   best: 65.2408
2023-09-30 11:55:01,206:INFO:  Epoch 452/500:  train Loss: 67.7305   val Loss: 68.8289   time: 0.27s   best: 65.2408
2023-09-30 11:55:01,474:INFO:  Epoch 453/500:  train Loss: 66.6398   val Loss: 70.6761   time: 0.25s   best: 65.2408
2023-09-30 11:55:01,752:INFO:  Epoch 454/500:  train Loss: 68.3085   val Loss: 71.5018   time: 0.25s   best: 65.2408
2023-09-30 11:55:02,031:INFO:  Epoch 455/500:  train Loss: 68.1261   val Loss: 70.8414   time: 0.27s   best: 65.2408
2023-09-30 11:55:02,291:INFO:  Epoch 456/500:  train Loss: 67.9383   val Loss: 70.5661   time: 0.25s   best: 65.2408
2023-09-30 11:55:02,567:INFO:  Epoch 457/500:  train Loss: 67.6935   val Loss: 69.4637   time: 0.26s   best: 65.2408
2023-09-30 11:55:02,829:INFO:  Epoch 458/500:  train Loss: 67.9713   val Loss: 67.4410   time: 0.25s   best: 65.2408
2023-09-30 11:55:03,117:INFO:  Epoch 459/500:  train Loss: 66.1855   val Loss: 66.5519   time: 0.27s   best: 65.2408
2023-09-30 11:55:03,376:INFO:  Epoch 460/500:  train Loss: 66.9443   val Loss: 69.9920   time: 0.25s   best: 65.2408
2023-09-30 11:55:03,652:INFO:  Epoch 461/500:  train Loss: 67.4823   val Loss: 67.7778   time: 0.26s   best: 65.2408
2023-09-30 11:55:03,916:INFO:  Epoch 462/500:  train Loss: 66.8485   val Loss: 67.8966   time: 0.25s   best: 65.2408
2023-09-30 11:55:04,193:INFO:  Epoch 463/500:  train Loss: 65.3792   val Loss: 65.9390   time: 0.27s   best: 65.2408
2023-09-30 11:55:04,453:INFO:  Epoch 464/500:  train Loss: 65.5773   val Loss: 67.7382   time: 0.25s   best: 65.2408
2023-09-30 11:55:04,733:INFO:  Epoch 465/500:  train Loss: 65.9458   val Loss: 66.3548   time: 0.27s   best: 65.2408
2023-09-30 11:55:04,994:INFO:  Epoch 466/500:  train Loss: 65.3784   val Loss: 69.0367   time: 0.25s   best: 65.2408
2023-09-30 11:55:05,278:INFO:  Epoch 467/500:  train Loss: 67.6000   val Loss: 66.5160   time: 0.27s   best: 65.2408
2023-09-30 11:55:05,539:INFO:  Epoch 468/500:  train Loss: 66.9166   val Loss: 68.7387   time: 0.25s   best: 65.2408
2023-09-30 11:55:05,818:INFO:  Epoch 469/500:  train Loss: 66.3066   val Loss: 68.6887   time: 0.27s   best: 65.2408
2023-09-30 11:55:06,080:INFO:  Epoch 470/500:  train Loss: 66.3854   val Loss: 66.4382   time: 0.25s   best: 65.2408
2023-09-30 11:55:06,357:INFO:  Epoch 471/500:  train Loss: 65.8233   val Loss: 66.3969   time: 0.26s   best: 65.2408
2023-09-30 11:55:06,617:INFO:  Epoch 472/500:  train Loss: 69.1819   val Loss: 73.2738   time: 0.25s   best: 65.2408
2023-09-30 11:55:06,901:INFO:  Epoch 473/500:  train Loss: 72.5152   val Loss: 69.2897   time: 0.27s   best: 65.2408
2023-09-30 11:55:07,168:INFO:  Epoch 474/500:  train Loss: 67.9530   val Loss: 68.9519   time: 0.25s   best: 65.2408
2023-09-30 11:55:07,446:INFO:  Epoch 475/500:  train Loss: 67.5045   val Loss: 65.5471   time: 0.27s   best: 65.2408
2023-09-30 11:55:07,704:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional debug_d528.pt
2023-09-30 11:55:07,736:INFO:  Epoch 476/500:  train Loss: 66.3016   val Loss: 64.8988   time: 0.25s   best: 64.8988
2023-09-30 11:55:08,016:INFO:  Epoch 477/500:  train Loss: 65.3484   val Loss: 65.6571   time: 0.27s   best: 64.8988
2023-09-30 11:55:08,276:INFO:  Epoch 478/500:  train Loss: 67.4671   val Loss: 67.6740   time: 0.25s   best: 64.8988
2023-09-30 11:55:08,583:INFO:  Epoch 479/500:  train Loss: 67.3165   val Loss: 68.4297   time: 0.29s   best: 64.8988
2023-09-30 11:55:08,847:INFO:  Epoch 480/500:  train Loss: 67.3356   val Loss: 67.0420   time: 0.25s   best: 64.8988
2023-09-30 11:55:09,135:INFO:  Epoch 481/500:  train Loss: 65.3759   val Loss: 66.5234   time: 0.28s   best: 64.8988
2023-09-30 11:55:09,395:INFO:  Epoch 482/500:  train Loss: 65.4097   val Loss: 66.1945   time: 0.25s   best: 64.8988
2023-09-30 11:55:09,672:INFO:  Epoch 483/500:  train Loss: 65.3595   val Loss: 65.6565   time: 0.26s   best: 64.8988
2023-09-30 11:55:09,955:INFO:  Epoch 484/500:  train Loss: 65.9749   val Loss: 65.5217   time: 0.27s   best: 64.8988
2023-09-30 11:55:10,215:INFO:  Epoch 485/500:  train Loss: 66.4758   val Loss: 67.0889   time: 0.25s   best: 64.8988
2023-09-30 11:55:10,492:INFO:  Epoch 486/500:  train Loss: 66.7656   val Loss: 69.9352   time: 0.26s   best: 64.8988
2023-09-30 11:55:10,756:INFO:  Epoch 487/500:  train Loss: 67.7303   val Loss: 67.9190   time: 0.25s   best: 64.8988
2023-09-30 11:55:11,036:INFO:  Epoch 488/500:  train Loss: 70.3472   val Loss: 71.6245   time: 0.27s   best: 64.8988
2023-09-30 11:55:11,304:INFO:  Epoch 489/500:  train Loss: 70.4580   val Loss: 69.3921   time: 0.26s   best: 64.8988
2023-09-30 11:55:11,581:INFO:  Epoch 490/500:  train Loss: 68.2846   val Loss: 66.7015   time: 0.27s   best: 64.8988
2023-09-30 11:55:11,843:INFO:  Epoch 491/500:  train Loss: 65.0600   val Loss: 65.0021   time: 0.25s   best: 64.8988
2023-09-30 11:55:12,122:INFO:  Epoch 492/500:  train Loss: 64.5259   val Loss: 67.0249   time: 0.27s   best: 64.8988
2023-09-30 11:55:12,381:INFO:  Epoch 493/500:  train Loss: 64.9410   val Loss: 67.0317   time: 0.25s   best: 64.8988
2023-09-30 11:55:12,662:INFO:  Epoch 494/500:  train Loss: 65.2768   val Loss: 67.6621   time: 0.27s   best: 64.8988
2023-09-30 11:55:12,926:INFO:  Epoch 495/500:  train Loss: 64.7630   val Loss: 66.2837   time: 0.25s   best: 64.8988
2023-09-30 11:55:13,212:INFO:  Epoch 496/500:  train Loss: 64.9001   val Loss: 66.7841   time: 0.27s   best: 64.8988
2023-09-30 11:55:13,472:INFO:  Epoch 497/500:  train Loss: 65.1510   val Loss: 65.6566   time: 0.25s   best: 64.8988
2023-09-30 11:55:13,751:INFO:  Epoch 498/500:  train Loss: 65.6360   val Loss: 68.6554   time: 0.27s   best: 64.8988
2023-09-30 11:55:14,012:INFO:  Epoch 499/500:  train Loss: 66.3332   val Loss: 66.9682   time: 0.25s   best: 64.8988
2023-09-30 11:55:14,334:INFO:  Epoch 500/500:  train Loss: 65.5865   val Loss: 67.0560   time: 0.31s   best: 64.8988
2023-09-30 11:55:14,334:INFO:  -----> Training complete in 2m 22s   best validation loss: 64.8988
 
2023-09-30 11:59:16,256:INFO:  Epoch 381/500:  train Loss: 17.9802   val Loss: 23.3912   time: 443.86s   best: 22.9445
2023-09-30 12:06:38,834:INFO:  Epoch 382/500:  train Loss: 17.8872   val Loss: 23.0902   time: 442.57s   best: 22.9445
2023-09-30 12:13:58,667:INFO:  Epoch 383/500:  train Loss: 17.8535   val Loss: 23.2502   time: 439.81s   best: 22.9445
2023-09-30 12:21:16,329:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 12:21:16,693:INFO:  Epoch 384/500:  train Loss: 17.9385   val Loss: 22.8945   time: 437.60s   best: 22.8945
2023-09-30 12:28:39,206:INFO:  Epoch 385/500:  train Loss: 17.7943   val Loss: 24.2442   time: 442.51s   best: 22.8945
2023-09-30 12:36:02,397:INFO:  Epoch 386/500:  train Loss: 17.8515   val Loss: 23.2293   time: 443.17s   best: 22.8945
2023-09-30 12:43:19,712:INFO:  Epoch 387/500:  train Loss: 17.9544   val Loss: 23.1462   time: 437.30s   best: 22.8945
2023-09-30 12:50:38,851:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 12:50:39,262:INFO:  Epoch 388/500:  train Loss: 17.9053   val Loss: 22.7650   time: 439.08s   best: 22.7650
2023-09-30 12:57:59,630:INFO:  Epoch 389/500:  train Loss: 18.1976   val Loss: 24.2061   time: 440.37s   best: 22.7650
2023-09-30 13:05:21,978:INFO:  Epoch 390/500:  train Loss: 17.7590   val Loss: 22.8751   time: 442.31s   best: 22.7650
2023-09-30 13:12:40,402:INFO:  Epoch 391/500:  train Loss: 18.2952   val Loss: 23.6103   time: 438.42s   best: 22.7650
2023-09-30 13:20:00,968:INFO:  Epoch 392/500:  train Loss: 17.7813   val Loss: 23.5340   time: 440.55s   best: 22.7650
2023-09-30 13:27:22,993:INFO:  Epoch 393/500:  train Loss: 17.7778   val Loss: 23.2085   time: 442.00s   best: 22.7650
2023-09-30 13:34:04,325:INFO:  Starting experiment lstm autoencoder bidirectional
2023-09-30 13:34:04,344:INFO:  Defining the model
2023-09-30 13:34:04,389:INFO:  Reading the dataset
2023-09-30 13:34:17,231:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:17,253:INFO:  Epoch 1/500:  train Loss: 135.6213   val Loss: 121.1055   time: 4.18s   best: 121.1055
2023-09-30 13:34:17,515:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:17,535:INFO:  Epoch 2/500:  train Loss: 115.8398   val Loss: 102.7419   time: 0.26s   best: 102.7419
2023-09-30 13:34:17,781:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:17,802:INFO:  Epoch 3/500:  train Loss: 101.6986   val Loss: 100.0539   time: 0.24s   best: 100.0539
2023-09-30 13:34:18,065:INFO:  Epoch 4/500:  train Loss: 99.9445   val Loss: 100.2567   time: 0.26s   best: 100.0539
2023-09-30 13:34:18,309:INFO:  Epoch 5/500:  train Loss: 100.2684   val Loss: 100.3727   time: 0.24s   best: 100.0539
2023-09-30 13:34:18,572:INFO:  Epoch 6/500:  train Loss: 100.4002   val Loss: 100.4418   time: 0.26s   best: 100.0539
2023-09-30 13:34:18,815:INFO:  Epoch 7/500:  train Loss: 100.4653   val Loss: 100.4796   time: 0.24s   best: 100.0539
2023-09-30 13:34:19,078:INFO:  Epoch 8/500:  train Loss: 100.4902   val Loss: 100.4970   time: 0.26s   best: 100.0539
2023-09-30 13:34:19,333:INFO:  Epoch 9/500:  train Loss: 100.4964   val Loss: 100.5012   time: 0.25s   best: 100.0539
2023-09-30 13:34:19,598:INFO:  Epoch 10/500:  train Loss: 100.4981   val Loss: 100.4968   time: 0.26s   best: 100.0539
2023-09-30 13:34:19,842:INFO:  Epoch 11/500:  train Loss: 100.5021   val Loss: 100.4867   time: 0.24s   best: 100.0539
2023-09-30 13:34:20,109:INFO:  Epoch 12/500:  train Loss: 100.4938   val Loss: 100.4728   time: 0.26s   best: 100.0539
2023-09-30 13:34:20,352:INFO:  Epoch 13/500:  train Loss: 100.4704   val Loss: 100.4563   time: 0.24s   best: 100.0539
2023-09-30 13:34:20,614:INFO:  Epoch 14/500:  train Loss: 100.4502   val Loss: 100.4379   time: 0.26s   best: 100.0539
2023-09-30 13:34:20,857:INFO:  Epoch 15/500:  train Loss: 100.4168   val Loss: 100.4183   time: 0.24s   best: 100.0539
2023-09-30 13:34:21,121:INFO:  Epoch 16/500:  train Loss: 100.3910   val Loss: 100.3922   time: 0.26s   best: 100.0539
2023-09-30 13:34:21,376:INFO:  Epoch 17/500:  train Loss: 100.3560   val Loss: 100.3631   time: 0.25s   best: 100.0539
2023-09-30 13:34:21,642:INFO:  Epoch 18/500:  train Loss: 100.3288   val Loss: 100.3291   time: 0.26s   best: 100.0539
2023-09-30 13:34:21,892:INFO:  Epoch 19/500:  train Loss: 100.2883   val Loss: 100.2964   time: 0.25s   best: 100.0539
2023-09-30 13:34:22,156:INFO:  Epoch 20/500:  train Loss: 100.2123   val Loss: 100.2618   time: 0.26s   best: 100.0539
2023-09-30 13:34:22,399:INFO:  Epoch 21/500:  train Loss: 100.0449   val Loss: 100.2188   time: 0.24s   best: 100.0539
2023-09-30 13:34:22,663:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:22,684:INFO:  Epoch 22/500:  train Loss: 99.7655   val Loss: 99.7386   time: 0.26s   best: 99.7386
2023-09-30 13:34:22,929:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:22,950:INFO:  Epoch 23/500:  train Loss: 98.9383   val Loss: 98.2486   time: 0.24s   best: 98.2486
2023-09-30 13:34:23,226:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:23,247:INFO:  Epoch 24/500:  train Loss: 96.8114   val Loss: 96.0251   time: 0.27s   best: 96.0251
2023-09-30 13:34:23,495:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:23,515:INFO:  Epoch 25/500:  train Loss: 95.4163   val Loss: 92.2030   time: 0.24s   best: 92.2030
2023-09-30 13:34:23,778:INFO:  Epoch 26/500:  train Loss: 94.9290   val Loss: 93.7534   time: 0.26s   best: 92.2030
2023-09-30 13:34:24,026:INFO:  Epoch 27/500:  train Loss: 93.8704   val Loss: 94.4343   time: 0.24s   best: 92.2030
2023-09-30 13:34:24,290:INFO:  Epoch 28/500:  train Loss: 94.6957   val Loss: 94.6054   time: 0.26s   best: 92.2030
2023-09-30 13:34:24,534:INFO:  Epoch 29/500:  train Loss: 95.2128   val Loss: 94.7112   time: 0.24s   best: 92.2030
2023-09-30 13:34:24,795:INFO:  Epoch 30/500:  train Loss: 95.1858   val Loss: 94.7227   time: 0.26s   best: 92.2030
2023-09-30 13:34:25,038:INFO:  Epoch 31/500:  train Loss: 94.7587   val Loss: 94.6793   time: 0.24s   best: 92.2030
2023-09-30 13:34:25,313:INFO:  Epoch 32/500:  train Loss: 94.5963   val Loss: 94.5732   time: 0.27s   best: 92.2030
2023-09-30 13:34:25,561:INFO:  Epoch 33/500:  train Loss: 94.6988   val Loss: 94.5362   time: 0.24s   best: 92.2030
2023-09-30 13:34:25,825:INFO:  Epoch 34/500:  train Loss: 94.4535   val Loss: 94.4381   time: 0.26s   best: 92.2030
2023-09-30 13:34:26,074:INFO:  Epoch 35/500:  train Loss: 94.6785   val Loss: 94.4040   time: 0.24s   best: 92.2030
2023-09-30 13:34:26,337:INFO:  Epoch 36/500:  train Loss: 94.4582   val Loss: 94.3580   time: 0.26s   best: 92.2030
2023-09-30 13:34:26,581:INFO:  Epoch 37/500:  train Loss: 94.8195   val Loss: 94.3348   time: 0.24s   best: 92.2030
2023-09-30 13:34:26,843:INFO:  Epoch 38/500:  train Loss: 94.5015   val Loss: 94.3985   time: 0.26s   best: 92.2030
2023-09-30 13:34:27,084:INFO:  Epoch 39/500:  train Loss: 94.5320   val Loss: 94.4907   time: 0.24s   best: 92.2030
2023-09-30 13:34:27,353:INFO:  Epoch 40/500:  train Loss: 94.4376   val Loss: 94.3380   time: 0.27s   best: 92.2030
2023-09-30 13:34:27,605:INFO:  Epoch 41/500:  train Loss: 94.1020   val Loss: 93.9781   time: 0.25s   best: 92.2030
2023-09-30 13:34:27,851:INFO:  Epoch 42/500:  train Loss: 94.0327   val Loss: 93.7218   time: 0.24s   best: 92.2030
2023-09-30 13:34:28,120:INFO:  Epoch 43/500:  train Loss: 94.1811   val Loss: 93.5121   time: 0.27s   best: 92.2030
2023-09-30 13:34:28,361:INFO:  Epoch 44/500:  train Loss: 93.6800   val Loss: 93.3016   time: 0.24s   best: 92.2030
2023-09-30 13:34:28,626:INFO:  Epoch 45/500:  train Loss: 93.5545   val Loss: 93.2085   time: 0.26s   best: 92.2030
2023-09-30 13:34:28,868:INFO:  Epoch 46/500:  train Loss: 93.1551   val Loss: 93.0597   time: 0.24s   best: 92.2030
2023-09-30 13:34:29,132:INFO:  Epoch 47/500:  train Loss: 93.1653   val Loss: 92.6460   time: 0.26s   best: 92.2030
2023-09-30 13:34:29,387:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:29,407:INFO:  Epoch 48/500:  train Loss: 92.3938   val Loss: 91.7562   time: 0.25s   best: 91.7562
2023-09-30 13:34:29,672:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:29,693:INFO:  Epoch 49/500:  train Loss: 91.5098   val Loss: 90.8598   time: 0.26s   best: 90.8598
2023-09-30 13:34:29,942:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:29,981:INFO:  Epoch 50/500:  train Loss: 90.4687   val Loss: 89.7137   time: 0.24s   best: 89.7137
2023-09-30 13:34:30,227:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:30,248:INFO:  Epoch 51/500:  train Loss: 90.0330   val Loss: 88.9426   time: 0.24s   best: 88.9426
2023-09-30 13:34:30,505:INFO:  Epoch 52/500:  train Loss: 89.2028   val Loss: 89.0694   time: 0.24s   best: 88.9426
2023-09-30 13:34:30,751:INFO:  Epoch 53/500:  train Loss: 89.1010   val Loss: 89.2539   time: 0.24s   best: 88.9426
2023-09-30 13:34:30,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:31,016:INFO:  Epoch 54/500:  train Loss: 89.5185   val Loss: 88.8781   time: 0.24s   best: 88.8781
2023-09-30 13:34:31,292:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:31,314:INFO:  Epoch 55/500:  train Loss: 88.8021   val Loss: 87.6887   time: 0.27s   best: 87.6887
2023-09-30 13:34:31,578:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:31,599:INFO:  Epoch 56/500:  train Loss: 87.5780   val Loss: 87.4680   time: 0.26s   best: 87.4680
2023-09-30 13:34:31,846:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:31,866:INFO:  Epoch 57/500:  train Loss: 87.5387   val Loss: 86.7309   time: 0.24s   best: 86.7309
2023-09-30 13:34:32,132:INFO:  Epoch 58/500:  train Loss: 87.1507   val Loss: 87.7593   time: 0.26s   best: 86.7309
2023-09-30 13:34:32,375:INFO:  Epoch 59/500:  train Loss: 87.8681   val Loss: 87.1543   time: 0.24s   best: 86.7309
2023-09-30 13:34:32,637:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:32,658:INFO:  Epoch 60/500:  train Loss: 87.5643   val Loss: 86.5282   time: 0.26s   best: 86.5282
2023-09-30 13:34:32,899:INFO:  Epoch 61/500:  train Loss: 87.6505   val Loss: 87.0822   time: 0.24s   best: 86.5282
2023-09-30 13:34:33,163:INFO:  Epoch 62/500:  train Loss: 87.2682   val Loss: 87.4163   time: 0.26s   best: 86.5282
2023-09-30 13:34:33,417:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:33,442:INFO:  Epoch 63/500:  train Loss: 87.6372   val Loss: 86.0965   time: 0.25s   best: 86.0965
2023-09-30 13:34:33,703:INFO:  Epoch 64/500:  train Loss: 87.3164   val Loss: 86.3063   time: 0.26s   best: 86.0965
2023-09-30 13:34:33,951:INFO:  Epoch 65/500:  train Loss: 86.9454   val Loss: 87.0498   time: 0.24s   best: 86.0965
2023-09-30 13:34:34,214:INFO:  Epoch 66/500:  train Loss: 87.1433   val Loss: 86.2682   time: 0.26s   best: 86.0965
2023-09-30 13:34:34,458:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:34,479:INFO:  Epoch 67/500:  train Loss: 87.0360   val Loss: 85.6794   time: 0.24s   best: 85.6794
2023-09-30 13:34:34,740:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:34,762:INFO:  Epoch 68/500:  train Loss: 86.0656   val Loss: 85.5914   time: 0.26s   best: 85.5914
2023-09-30 13:34:35,006:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:35,027:INFO:  Epoch 69/500:  train Loss: 86.2374   val Loss: 85.4482   time: 0.24s   best: 85.4482
2023-09-30 13:34:35,298:INFO:  Epoch 70/500:  train Loss: 86.0916   val Loss: 85.4700   time: 0.27s   best: 85.4482
2023-09-30 13:34:35,545:INFO:  Epoch 71/500:  train Loss: 86.8005   val Loss: 87.5556   time: 0.24s   best: 85.4482
2023-09-30 13:34:35,808:INFO:  Epoch 72/500:  train Loss: 88.0718   val Loss: 87.4523   time: 0.26s   best: 85.4482
2023-09-30 13:34:36,056:INFO:  Epoch 73/500:  train Loss: 87.3940   val Loss: 85.9907   time: 0.24s   best: 85.4482
2023-09-30 13:34:36,319:INFO:  Epoch 74/500:  train Loss: 86.7162   val Loss: 86.1166   time: 0.26s   best: 85.4482
2023-09-30 13:34:36,562:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:36,583:INFO:  Epoch 75/500:  train Loss: 85.8756   val Loss: 85.3861   time: 0.24s   best: 85.3861
2023-09-30 13:34:36,842:INFO:  Epoch 76/500:  train Loss: 85.7657   val Loss: 86.0886   time: 0.26s   best: 85.3861
2023-09-30 13:34:37,086:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:37,107:INFO:  Epoch 77/500:  train Loss: 85.8251   val Loss: 84.5176   time: 0.24s   best: 84.5176
2023-09-30 13:34:37,376:INFO:  Epoch 78/500:  train Loss: 85.5131   val Loss: 87.7282   time: 0.27s   best: 84.5176
2023-09-30 13:34:37,625:INFO:  Epoch 79/500:  train Loss: 86.0072   val Loss: 85.5764   time: 0.25s   best: 84.5176
2023-09-30 13:34:37,889:INFO:  Epoch 80/500:  train Loss: 85.9355   val Loss: 85.6849   time: 0.26s   best: 84.5176
2023-09-30 13:34:38,138:INFO:  Epoch 81/500:  train Loss: 85.6402   val Loss: 84.9864   time: 0.24s   best: 84.5176
2023-09-30 13:34:38,399:INFO:  Epoch 82/500:  train Loss: 85.6531   val Loss: 85.3988   time: 0.26s   best: 84.5176
2023-09-30 13:34:38,646:INFO:  Epoch 83/500:  train Loss: 85.7517   val Loss: 85.3986   time: 0.24s   best: 84.5176
2023-09-30 13:34:38,911:INFO:  Epoch 84/500:  train Loss: 86.0791   val Loss: 85.1246   time: 0.26s   best: 84.5176
2023-09-30 13:34:39,157:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:39,183:INFO:  Epoch 85/500:  train Loss: 85.1091   val Loss: 84.4917   time: 0.24s   best: 84.4917
2023-09-30 13:34:39,596:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:39,631:INFO:  Epoch 86/500:  train Loss: 84.9469   val Loss: 84.4536   time: 0.40s   best: 84.4536
2023-09-30 13:34:39,970:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:39,992:INFO:  Epoch 87/500:  train Loss: 84.8962   val Loss: 84.0617   time: 0.33s   best: 84.0617
2023-09-30 13:34:40,237:INFO:  Epoch 88/500:  train Loss: 84.5234   val Loss: 84.8726   time: 0.24s   best: 84.0617
2023-09-30 13:34:40,499:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:40,521:INFO:  Epoch 89/500:  train Loss: 84.6050   val Loss: 83.7844   time: 0.26s   best: 83.7844
2023-09-30 13:34:40,765:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:40,786:INFO:  Epoch 90/500:  train Loss: 85.2006   val Loss: 83.7116   time: 0.24s   best: 83.7116
2023-09-30 13:34:41,045:INFO:  Epoch 91/500:  train Loss: 84.3812   val Loss: 83.9061   time: 0.25s   best: 83.7116
2023-09-30 13:34:41,293:INFO:  Epoch 92/500:  train Loss: 84.1597   val Loss: 83.8573   time: 0.24s   best: 83.7116
2023-09-30 13:34:41,558:INFO:  Epoch 93/500:  train Loss: 83.8775   val Loss: 83.8092   time: 0.26s   best: 83.7116
2023-09-30 13:34:41,810:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:41,831:INFO:  Epoch 94/500:  train Loss: 83.8032   val Loss: 83.6345   time: 0.25s   best: 83.6345
2023-09-30 13:34:42,095:INFO:  Epoch 95/500:  train Loss: 84.4463   val Loss: 85.0968   time: 0.26s   best: 83.6345
2023-09-30 13:34:42,340:INFO:  Epoch 96/500:  train Loss: 85.8617   val Loss: 85.8843   time: 0.24s   best: 83.6345
2023-09-30 13:34:42,601:INFO:  Epoch 97/500:  train Loss: 87.3103   val Loss: 87.6204   time: 0.26s   best: 83.6345
2023-09-30 13:34:42,844:INFO:  Epoch 98/500:  train Loss: 87.3628   val Loss: 85.3750   time: 0.24s   best: 83.6345
2023-09-30 13:34:43,107:INFO:  Epoch 99/500:  train Loss: 85.7692   val Loss: 85.1273   time: 0.26s   best: 83.6345
2023-09-30 13:34:43,450:INFO:  Epoch 100/500:  train Loss: 85.9393   val Loss: 85.2243   time: 0.33s   best: 83.6345
2023-09-30 13:34:43,730:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:43,766:INFO:  Epoch 101/500:  train Loss: 86.4231   val Loss: 83.6196   time: 0.28s   best: 83.6196
2023-09-30 13:34:44,021:INFO:  Epoch 102/500:  train Loss: 83.2997   val Loss: 84.1236   time: 0.24s   best: 83.6196
2023-09-30 13:34:44,283:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:44,313:INFO:  Epoch 103/500:  train Loss: 84.3672   val Loss: 83.4844   time: 0.26s   best: 83.4844
2023-09-30 13:34:44,557:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:44,587:INFO:  Epoch 104/500:  train Loss: 83.9785   val Loss: 82.7415   time: 0.24s   best: 82.7415
2023-09-30 13:34:44,855:INFO:  Epoch 105/500:  train Loss: 83.7903   val Loss: 82.8731   time: 0.26s   best: 82.7415
2023-09-30 13:34:45,106:INFO:  Epoch 106/500:  train Loss: 83.3159   val Loss: 82.8751   time: 0.24s   best: 82.7415
2023-09-30 13:34:45,381:INFO:  Epoch 107/500:  train Loss: 83.6988   val Loss: 83.6809   time: 0.26s   best: 82.7415
2023-09-30 13:34:45,631:INFO:  Epoch 108/500:  train Loss: 84.3344   val Loss: 83.2559   time: 0.24s   best: 82.7415
2023-09-30 13:34:45,911:INFO:  Epoch 109/500:  train Loss: 83.7056   val Loss: 83.0169   time: 0.27s   best: 82.7415
2023-09-30 13:34:46,162:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:46,191:INFO:  Epoch 110/500:  train Loss: 83.3324   val Loss: 82.6339   time: 0.25s   best: 82.6339
2023-09-30 13:34:46,454:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:46,484:INFO:  Epoch 111/500:  train Loss: 82.8618   val Loss: 82.2547   time: 0.26s   best: 82.2547
2023-09-30 13:34:46,728:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:46,776:INFO:  Epoch 112/500:  train Loss: 83.3356   val Loss: 82.0282   time: 0.24s   best: 82.0282
2023-09-30 13:34:47,027:INFO:  Epoch 113/500:  train Loss: 82.4912   val Loss: 82.1992   time: 0.24s   best: 82.0282
2023-09-30 13:34:47,293:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:47,323:INFO:  Epoch 114/500:  train Loss: 83.3871   val Loss: 81.9237   time: 0.26s   best: 81.9237
2023-09-30 13:34:47,571:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:47,600:INFO:  Epoch 115/500:  train Loss: 82.9383   val Loss: 81.6332   time: 0.24s   best: 81.6332
2023-09-30 13:34:47,872:INFO:  Epoch 116/500:  train Loss: 82.6530   val Loss: 82.3545   time: 0.26s   best: 81.6332
2023-09-30 13:34:48,126:INFO:  Epoch 117/500:  train Loss: 83.6181   val Loss: 83.1582   time: 0.24s   best: 81.6332
2023-09-30 13:34:48,401:INFO:  Epoch 118/500:  train Loss: 83.8144   val Loss: 82.5132   time: 0.26s   best: 81.6332
2023-09-30 13:34:48,654:INFO:  Epoch 119/500:  train Loss: 84.2608   val Loss: 85.8189   time: 0.24s   best: 81.6332
2023-09-30 13:34:48,923:INFO:  Epoch 120/500:  train Loss: 85.6175   val Loss: 86.4544   time: 0.26s   best: 81.6332
2023-09-30 13:34:49,175:INFO:  Epoch 121/500:  train Loss: 87.2618   val Loss: 86.5871   time: 0.24s   best: 81.6332
2023-09-30 13:34:49,448:INFO:  Epoch 122/500:  train Loss: 86.7077   val Loss: 85.7190   time: 0.26s   best: 81.6332
2023-09-30 13:34:49,703:INFO:  Epoch 123/500:  train Loss: 86.0161   val Loss: 85.5668   time: 0.24s   best: 81.6332
2023-09-30 13:34:49,980:INFO:  Epoch 124/500:  train Loss: 86.0702   val Loss: 84.5816   time: 0.26s   best: 81.6332
2023-09-30 13:34:50,238:INFO:  Epoch 125/500:  train Loss: 84.1416   val Loss: 82.9780   time: 0.25s   best: 81.6332
2023-09-30 13:34:50,508:INFO:  Epoch 126/500:  train Loss: 82.9536   val Loss: 82.4496   time: 0.26s   best: 81.6332
2023-09-30 13:34:50,760:INFO:  Epoch 127/500:  train Loss: 82.8105   val Loss: 82.3528   time: 0.24s   best: 81.6332
2023-09-30 13:34:51,101:INFO:  Epoch 128/500:  train Loss: 82.8284   val Loss: 81.7834   time: 0.33s   best: 81.6332
2023-09-30 13:34:51,358:INFO:  Epoch 129/500:  train Loss: 82.7070   val Loss: 81.9407   time: 0.24s   best: 81.6332
2023-09-30 13:34:51,631:INFO:  Epoch 130/500:  train Loss: 82.2687   val Loss: 81.9235   time: 0.26s   best: 81.6332
2023-09-30 13:34:51,889:INFO:  Epoch 131/500:  train Loss: 81.9405   val Loss: 81.6413   time: 0.25s   best: 81.6332
2023-09-30 13:34:52,155:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:52,185:INFO:  Epoch 132/500:  train Loss: 81.8937   val Loss: 81.4493   time: 0.26s   best: 81.4493
2023-09-30 13:34:52,477:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:52,510:INFO:  Epoch 133/500:  train Loss: 83.1927   val Loss: 80.9200   time: 0.27s   best: 80.9200
2023-09-30 13:34:52,754:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:52,784:INFO:  Epoch 134/500:  train Loss: 81.6813   val Loss: 80.9138   time: 0.24s   best: 80.9138
2023-09-30 13:34:53,049:INFO:  Epoch 135/500:  train Loss: 81.7627   val Loss: 82.8716   time: 0.25s   best: 80.9138
2023-09-30 13:34:53,306:INFO:  Epoch 136/500:  train Loss: 82.6324   val Loss: 80.9596   time: 0.24s   best: 80.9138
2023-09-30 13:34:53,580:INFO:  Epoch 137/500:  train Loss: 81.4063   val Loss: 81.5146   time: 0.26s   best: 80.9138
2023-09-30 13:34:53,839:INFO:  Epoch 138/500:  train Loss: 82.0356   val Loss: 81.4228   time: 0.25s   best: 80.9138
2023-09-30 13:34:54,109:INFO:  Epoch 139/500:  train Loss: 82.7410   val Loss: 81.0556   time: 0.26s   best: 80.9138
2023-09-30 13:34:54,357:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:54,387:INFO:  Epoch 140/500:  train Loss: 81.3870   val Loss: 80.5816   time: 0.24s   best: 80.5816
2023-09-30 13:34:54,649:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:54,679:INFO:  Epoch 141/500:  train Loss: 81.5057   val Loss: 80.3531   time: 0.26s   best: 80.3531
2023-09-30 13:34:54,930:INFO:  Epoch 142/500:  train Loss: 82.0156   val Loss: 83.2741   time: 0.24s   best: 80.3531
2023-09-30 13:34:55,204:INFO:  Epoch 143/500:  train Loss: 83.4220   val Loss: 82.7972   time: 0.26s   best: 80.3531
2023-09-30 13:34:55,456:INFO:  Epoch 144/500:  train Loss: 83.8367   val Loss: 83.1599   time: 0.24s   best: 80.3531
2023-09-30 13:34:55,728:INFO:  Epoch 145/500:  train Loss: 82.9549   val Loss: 81.6194   time: 0.26s   best: 80.3531
2023-09-30 13:34:55,986:INFO:  Epoch 146/500:  train Loss: 81.6731   val Loss: 80.7112   time: 0.25s   best: 80.3531
2023-09-30 13:34:56,255:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:56,285:INFO:  Epoch 147/500:  train Loss: 81.1490   val Loss: 80.1135   time: 0.26s   best: 80.1135
2023-09-30 13:34:56,529:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:56,559:INFO:  Epoch 148/500:  train Loss: 80.4817   val Loss: 80.0494   time: 0.24s   best: 80.0494
2023-09-30 13:34:56,819:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:56,848:INFO:  Epoch 149/500:  train Loss: 80.9736   val Loss: 79.7972   time: 0.25s   best: 79.7972
2023-09-30 13:34:57,099:INFO:  Epoch 150/500:  train Loss: 80.3381   val Loss: 79.8151   time: 0.24s   best: 79.7972
2023-09-30 13:34:57,314:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 13:34:57,367:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:57,400:INFO:  Epoch 151/500:  train Loss: 80.4748   val Loss: 79.3718   time: 0.26s   best: 79.3718
2023-09-30 13:34:57,529:INFO:  Epoch 394/500:  train Loss: 17.9233   val Loss: 22.6744   time: 454.27s   best: 22.6744
2023-09-30 13:34:57,656:INFO:  Epoch 152/500:  train Loss: 80.5282   val Loss: 80.9114   time: 0.24s   best: 79.3718
2023-09-30 13:34:57,925:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:57,955:INFO:  Epoch 153/500:  train Loss: 80.4822   val Loss: 79.3213   time: 0.26s   best: 79.3213
2023-09-30 13:34:58,209:INFO:  Epoch 154/500:  train Loss: 80.2650   val Loss: 79.6946   time: 0.25s   best: 79.3213
2023-09-30 13:34:58,478:INFO:  Epoch 155/500:  train Loss: 80.1349   val Loss: 79.3332   time: 0.26s   best: 79.3213
2023-09-30 13:34:58,723:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:34:58,766:INFO:  Epoch 156/500:  train Loss: 80.1726   val Loss: 78.2281   time: 0.24s   best: 78.2281
2023-09-30 13:34:59,031:INFO:  Epoch 157/500:  train Loss: 79.6769   val Loss: 78.5632   time: 0.25s   best: 78.2281
2023-09-30 13:34:59,303:INFO:  Epoch 158/500:  train Loss: 79.4864   val Loss: 78.2957   time: 0.25s   best: 78.2281
2023-09-30 13:34:59,562:INFO:  Epoch 159/500:  train Loss: 80.5837   val Loss: 79.1526   time: 0.25s   best: 78.2281
2023-09-30 13:34:59,812:INFO:  Epoch 160/500:  train Loss: 80.0251   val Loss: 79.0559   time: 0.24s   best: 78.2281
2023-09-30 13:35:00,089:INFO:  Epoch 161/500:  train Loss: 80.1813   val Loss: 79.8501   time: 0.26s   best: 78.2281
2023-09-30 13:35:00,358:INFO:  Epoch 162/500:  train Loss: 80.4549   val Loss: 79.3197   time: 0.24s   best: 78.2281
2023-09-30 13:35:00,614:INFO:  Epoch 163/500:  train Loss: 79.5555   val Loss: 78.2288   time: 0.24s   best: 78.2281
2023-09-30 13:35:00,865:INFO:  Epoch 164/500:  train Loss: 78.8006   val Loss: 79.1411   time: 0.24s   best: 78.2281
2023-09-30 13:35:01,135:INFO:  Epoch 165/500:  train Loss: 80.0331   val Loss: 79.0914   time: 0.26s   best: 78.2281
2023-09-30 13:35:01,391:INFO:  Epoch 166/500:  train Loss: 80.0283   val Loss: 78.6525   time: 0.24s   best: 78.2281
2023-09-30 13:35:01,657:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:01,687:INFO:  Epoch 167/500:  train Loss: 79.4891   val Loss: 78.0156   time: 0.26s   best: 78.0156
2023-09-30 13:35:01,952:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:01,981:INFO:  Epoch 168/500:  train Loss: 79.0028   val Loss: 77.3921   time: 0.26s   best: 77.3921
2023-09-30 13:35:02,232:INFO:  Epoch 169/500:  train Loss: 79.0542   val Loss: 77.7732   time: 0.24s   best: 77.3921
2023-09-30 13:35:02,500:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:02,531:INFO:  Epoch 170/500:  train Loss: 79.3978   val Loss: 77.3350   time: 0.26s   best: 77.3350
2023-09-30 13:35:02,782:INFO:  Epoch 171/500:  train Loss: 78.9144   val Loss: 78.0068   time: 0.24s   best: 77.3350
2023-09-30 13:35:03,049:INFO:  Epoch 172/500:  train Loss: 79.1871   val Loss: 78.6770   time: 0.26s   best: 77.3350
2023-09-30 13:35:03,306:INFO:  Epoch 173/500:  train Loss: 78.9921   val Loss: 77.5641   time: 0.24s   best: 77.3350
2023-09-30 13:35:03,579:INFO:  Epoch 174/500:  train Loss: 78.6407   val Loss: 79.1238   time: 0.26s   best: 77.3350
2023-09-30 13:35:03,830:INFO:  Epoch 175/500:  train Loss: 79.7657   val Loss: 77.9578   time: 0.24s   best: 77.3350
2023-09-30 13:35:04,105:INFO:  Epoch 176/500:  train Loss: 78.6270   val Loss: 78.4164   time: 0.26s   best: 77.3350
2023-09-30 13:35:04,359:INFO:  Epoch 177/500:  train Loss: 79.1308   val Loss: 78.1349   time: 0.24s   best: 77.3350
2023-09-30 13:35:04,621:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:04,651:INFO:  Epoch 178/500:  train Loss: 78.9134   val Loss: 77.2296   time: 0.26s   best: 77.2296
2023-09-30 13:35:04,895:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:04,925:INFO:  Epoch 179/500:  train Loss: 77.9572   val Loss: 76.6757   time: 0.24s   best: 76.6757
2023-09-30 13:35:05,194:INFO:  Epoch 180/500:  train Loss: 81.9521   val Loss: 78.6608   time: 0.26s   best: 76.6757
2023-09-30 13:35:05,451:INFO:  Epoch 181/500:  train Loss: 82.6718   val Loss: 85.6322   time: 0.24s   best: 76.6757
2023-09-30 13:35:05,724:INFO:  Epoch 182/500:  train Loss: 85.5787   val Loss: 84.8160   time: 0.26s   best: 76.6757
2023-09-30 13:35:05,981:INFO:  Epoch 183/500:  train Loss: 86.0450   val Loss: 86.6208   time: 0.24s   best: 76.6757
2023-09-30 13:35:06,256:INFO:  Epoch 184/500:  train Loss: 86.4663   val Loss: 84.6809   time: 0.26s   best: 76.6757
2023-09-30 13:35:06,508:INFO:  Epoch 185/500:  train Loss: 84.0445   val Loss: 83.1878   time: 0.24s   best: 76.6757
2023-09-30 13:35:06,778:INFO:  Epoch 186/500:  train Loss: 83.2333   val Loss: 81.9397   time: 0.26s   best: 76.6757
2023-09-30 13:35:07,029:INFO:  Epoch 187/500:  train Loss: 81.8706   val Loss: 80.3093   time: 0.24s   best: 76.6757
2023-09-30 13:35:07,303:INFO:  Epoch 188/500:  train Loss: 81.0096   val Loss: 79.2910   time: 0.26s   best: 76.6757
2023-09-30 13:35:07,560:INFO:  Epoch 189/500:  train Loss: 79.5000   val Loss: 77.7111   time: 0.24s   best: 76.6757
2023-09-30 13:35:07,830:INFO:  Epoch 190/500:  train Loss: 78.2295   val Loss: 77.3739   time: 0.26s   best: 76.6757
2023-09-30 13:35:08,087:INFO:  Epoch 191/500:  train Loss: 77.9697   val Loss: 76.9289   time: 0.24s   best: 76.6757
2023-09-30 13:35:08,356:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:08,386:INFO:  Epoch 192/500:  train Loss: 78.0161   val Loss: 76.3838   time: 0.26s   best: 76.3838
2023-09-30 13:35:08,638:INFO:  Epoch 193/500:  train Loss: 77.5431   val Loss: 77.3695   time: 0.24s   best: 76.3838
2023-09-30 13:35:08,901:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:08,935:INFO:  Epoch 194/500:  train Loss: 77.8897   val Loss: 76.0658   time: 0.26s   best: 76.0658
2023-09-30 13:35:09,183:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:09,213:INFO:  Epoch 195/500:  train Loss: 77.4539   val Loss: 75.9859   time: 0.24s   best: 75.9859
2023-09-30 13:35:09,480:INFO:  Epoch 196/500:  train Loss: 77.7458   val Loss: 77.5541   time: 0.26s   best: 75.9859
2023-09-30 13:35:09,741:INFO:  Epoch 197/500:  train Loss: 77.4301   val Loss: 77.7467   time: 0.25s   best: 75.9859
2023-09-30 13:35:10,128:INFO:  Epoch 198/500:  train Loss: 79.8410   val Loss: 76.7561   time: 0.39s   best: 75.9859
2023-09-30 13:35:10,484:INFO:  Epoch 199/500:  train Loss: 79.0712   val Loss: 78.3642   time: 0.34s   best: 75.9859
2023-09-30 13:35:10,777:INFO:  Epoch 200/500:  train Loss: 79.3366   val Loss: 79.3609   time: 0.28s   best: 75.9859
2023-09-30 13:35:11,056:INFO:  Epoch 201/500:  train Loss: 80.1772   val Loss: 77.8345   time: 0.27s   best: 75.9859
2023-09-30 13:35:11,312:INFO:  Epoch 202/500:  train Loss: 77.6774   val Loss: 77.6766   time: 0.24s   best: 75.9859
2023-09-30 13:35:11,583:INFO:  Epoch 203/500:  train Loss: 77.7138   val Loss: 76.2585   time: 0.26s   best: 75.9859
2023-09-30 13:35:11,827:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:11,864:INFO:  Epoch 204/500:  train Loss: 77.3745   val Loss: 74.6058   time: 0.24s   best: 74.6058
2023-09-30 13:35:12,128:INFO:  Epoch 205/500:  train Loss: 77.1007   val Loss: 76.8534   time: 0.25s   best: 74.6058
2023-09-30 13:35:12,404:INFO:  Epoch 206/500:  train Loss: 76.4815   val Loss: 74.9543   time: 0.25s   best: 74.6058
2023-09-30 13:35:12,657:INFO:  Epoch 207/500:  train Loss: 76.2227   val Loss: 74.9396   time: 0.24s   best: 74.6058
2023-09-30 13:35:12,908:INFO:  Epoch 208/500:  train Loss: 75.5368   val Loss: 75.6768   time: 0.24s   best: 74.6058
2023-09-30 13:35:13,175:INFO:  Epoch 209/500:  train Loss: 77.2582   val Loss: 75.6102   time: 0.26s   best: 74.6058
2023-09-30 13:35:13,426:INFO:  Epoch 210/500:  train Loss: 75.9710   val Loss: 75.0775   time: 0.25s   best: 74.6058
2023-09-30 13:35:13,709:INFO:  Epoch 211/500:  train Loss: 77.0640   val Loss: 75.3377   time: 0.27s   best: 74.6058
2023-09-30 13:35:13,959:INFO:  Epoch 212/500:  train Loss: 76.0442   val Loss: 74.7873   time: 0.24s   best: 74.6058
2023-09-30 13:35:14,231:INFO:  Epoch 213/500:  train Loss: 75.8997   val Loss: 74.6548   time: 0.26s   best: 74.6058
2023-09-30 13:35:14,493:INFO:  Epoch 214/500:  train Loss: 75.7636   val Loss: 75.6370   time: 0.25s   best: 74.6058
2023-09-30 13:35:14,761:INFO:  Epoch 215/500:  train Loss: 76.1814   val Loss: 75.3725   time: 0.26s   best: 74.6058
2023-09-30 13:35:15,004:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:15,041:INFO:  Epoch 216/500:  train Loss: 76.7865   val Loss: 74.5922   time: 0.24s   best: 74.5922
2023-09-30 13:35:15,307:INFO:  Epoch 217/500:  train Loss: 75.8808   val Loss: 75.9028   time: 0.25s   best: 74.5922
2023-09-30 13:35:15,560:INFO:  Epoch 218/500:  train Loss: 75.8600   val Loss: 75.1995   time: 0.24s   best: 74.5922
2023-09-30 13:35:15,830:INFO:  Epoch 219/500:  train Loss: 76.1349   val Loss: 76.0976   time: 0.24s   best: 74.5922
2023-09-30 13:35:16,083:INFO:  Epoch 220/500:  train Loss: 76.3052   val Loss: 75.5428   time: 0.24s   best: 74.5922
2023-09-30 13:35:16,345:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:16,382:INFO:  Epoch 221/500:  train Loss: 76.2983   val Loss: 73.9228   time: 0.26s   best: 73.9228
2023-09-30 13:35:16,639:INFO:  Epoch 222/500:  train Loss: 75.0715   val Loss: 75.6014   time: 0.24s   best: 73.9228
2023-09-30 13:35:16,903:INFO:  Epoch 223/500:  train Loss: 76.8139   val Loss: 75.1212   time: 0.25s   best: 73.9228
2023-09-30 13:35:17,153:INFO:  Epoch 224/500:  train Loss: 76.8139   val Loss: 74.2243   time: 0.24s   best: 73.9228
2023-09-30 13:35:17,429:INFO:  Epoch 225/500:  train Loss: 75.6885   val Loss: 74.1497   time: 0.26s   best: 73.9228
2023-09-30 13:35:17,684:INFO:  Epoch 226/500:  train Loss: 75.8229   val Loss: 75.5223   time: 0.24s   best: 73.9228
2023-09-30 13:35:17,953:INFO:  Epoch 227/500:  train Loss: 76.0360   val Loss: 74.3236   time: 0.26s   best: 73.9228
2023-09-30 13:35:18,205:INFO:  Epoch 228/500:  train Loss: 75.0283   val Loss: 74.1567   time: 0.24s   best: 73.9228
2023-09-30 13:35:18,483:INFO:  Epoch 229/500:  train Loss: 77.5029   val Loss: 77.5085   time: 0.27s   best: 73.9228
2023-09-30 13:35:18,750:INFO:  Epoch 230/500:  train Loss: 78.3992   val Loss: 78.4046   time: 0.24s   best: 73.9228
2023-09-30 13:35:19,005:INFO:  Epoch 231/500:  train Loss: 78.1958   val Loss: 77.2687   time: 0.24s   best: 73.9228
2023-09-30 13:35:19,255:INFO:  Epoch 232/500:  train Loss: 77.5808   val Loss: 76.3219   time: 0.24s   best: 73.9228
2023-09-30 13:35:19,533:INFO:  Epoch 233/500:  train Loss: 77.7494   val Loss: 76.4408   time: 0.26s   best: 73.9228
2023-09-30 13:35:19,784:INFO:  Epoch 234/500:  train Loss: 77.6014   val Loss: 75.3234   time: 0.24s   best: 73.9228
2023-09-30 13:35:20,056:INFO:  Epoch 235/500:  train Loss: 76.4227   val Loss: 75.6905   time: 0.26s   best: 73.9228
2023-09-30 13:35:20,309:INFO:  Epoch 236/500:  train Loss: 76.3872   val Loss: 74.7313   time: 0.24s   best: 73.9228
2023-09-30 13:35:20,579:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:20,608:INFO:  Epoch 237/500:  train Loss: 75.0203   val Loss: 73.3623   time: 0.27s   best: 73.3623
2023-09-30 13:35:20,874:INFO:  Epoch 238/500:  train Loss: 75.0278   val Loss: 73.5632   time: 0.24s   best: 73.3623
2023-09-30 13:35:21,129:INFO:  Epoch 239/500:  train Loss: 74.6618   val Loss: 74.3582   time: 0.24s   best: 73.3623
2023-09-30 13:35:21,386:INFO:  Epoch 240/500:  train Loss: 74.1713   val Loss: 73.9936   time: 0.25s   best: 73.3623
2023-09-30 13:35:21,658:INFO:  Epoch 241/500:  train Loss: 76.1721   val Loss: 76.8554   time: 0.26s   best: 73.3623
2023-09-30 13:35:21,909:INFO:  Epoch 242/500:  train Loss: 81.0701   val Loss: 81.2134   time: 0.24s   best: 73.3623
2023-09-30 13:35:22,173:INFO:  Epoch 243/500:  train Loss: 81.6831   val Loss: 82.4833   time: 0.26s   best: 73.3623
2023-09-30 13:35:22,453:INFO:  Epoch 244/500:  train Loss: 83.6401   val Loss: 81.2119   time: 0.26s   best: 73.3623
2023-09-30 13:35:22,710:INFO:  Epoch 245/500:  train Loss: 80.8108   val Loss: 79.1121   time: 0.24s   best: 73.3623
2023-09-30 13:35:22,962:INFO:  Epoch 246/500:  train Loss: 78.9296   val Loss: 76.9889   time: 0.24s   best: 73.3623
2023-09-30 13:35:23,232:INFO:  Epoch 247/500:  train Loss: 77.1301   val Loss: 75.2211   time: 0.26s   best: 73.3623
2023-09-30 13:35:23,502:INFO:  Epoch 248/500:  train Loss: 75.3293   val Loss: 74.1530   time: 0.24s   best: 73.3623
2023-09-30 13:35:23,751:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:23,781:INFO:  Epoch 249/500:  train Loss: 74.6744   val Loss: 73.3159   time: 0.24s   best: 73.3159
2023-09-30 13:35:24,042:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:24,073:INFO:  Epoch 250/500:  train Loss: 74.0557   val Loss: 72.3746   time: 0.26s   best: 72.3746
2023-09-30 13:35:24,327:INFO:  Epoch 251/500:  train Loss: 74.2626   val Loss: 73.1787   time: 0.24s   best: 72.3746
2023-09-30 13:35:24,603:INFO:  Epoch 252/500:  train Loss: 73.7009   val Loss: 72.7398   time: 0.26s   best: 72.3746
2023-09-30 13:35:24,855:INFO:  Epoch 253/500:  train Loss: 73.7552   val Loss: 72.8729   time: 0.24s   best: 72.3746
2023-09-30 13:35:25,117:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:25,147:INFO:  Epoch 254/500:  train Loss: 73.6268   val Loss: 71.9231   time: 0.26s   best: 71.9231
2023-09-30 13:35:25,405:INFO:  Epoch 255/500:  train Loss: 73.8314   val Loss: 73.4288   time: 0.25s   best: 71.9231
2023-09-30 13:35:25,675:INFO:  Epoch 256/500:  train Loss: 74.6226   val Loss: 73.1346   time: 0.26s   best: 71.9231
2023-09-30 13:35:25,927:INFO:  Epoch 257/500:  train Loss: 73.6579   val Loss: 71.9514   time: 0.24s   best: 71.9231
2023-09-30 13:35:26,190:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:26,220:INFO:  Epoch 258/500:  train Loss: 73.6421   val Loss: 71.8060   time: 0.26s   best: 71.8060
2023-09-30 13:35:26,481:INFO:  Epoch 259/500:  train Loss: 73.6587   val Loss: 73.8146   time: 0.25s   best: 71.8060
2023-09-30 13:35:26,750:INFO:  Epoch 260/500:  train Loss: 74.4813   val Loss: 72.4982   time: 0.26s   best: 71.8060
2023-09-30 13:35:26,994:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:27,024:INFO:  Epoch 261/500:  train Loss: 73.0413   val Loss: 71.2646   time: 0.24s   best: 71.2646
2023-09-30 13:35:27,299:INFO:  Epoch 262/500:  train Loss: 73.4329   val Loss: 72.8829   time: 0.26s   best: 71.2646
2023-09-30 13:35:27,555:INFO:  Epoch 263/500:  train Loss: 73.7390   val Loss: 72.7303   time: 0.24s   best: 71.2646
2023-09-30 13:35:27,825:INFO:  Epoch 264/500:  train Loss: 73.1935   val Loss: 71.3881   time: 0.26s   best: 71.2646
2023-09-30 13:35:28,075:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:28,099:INFO:  Epoch 265/500:  train Loss: 71.9381   val Loss: 70.8658   time: 0.25s   best: 70.8658
2023-09-30 13:35:28,359:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:28,383:INFO:  Epoch 266/500:  train Loss: 71.2880   val Loss: 70.5261   time: 0.26s   best: 70.5261
2023-09-30 13:35:28,657:INFO:  Epoch 267/500:  train Loss: 73.9481   val Loss: 74.9738   time: 0.26s   best: 70.5261
2023-09-30 13:35:28,931:INFO:  Epoch 268/500:  train Loss: 75.5679   val Loss: 75.4867   time: 0.26s   best: 70.5261
2023-09-30 13:35:29,182:INFO:  Epoch 269/500:  train Loss: 75.7917   val Loss: 75.5562   time: 0.24s   best: 70.5261
2023-09-30 13:35:29,458:INFO:  Epoch 270/500:  train Loss: 75.1088   val Loss: 73.6878   time: 0.26s   best: 70.5261
2023-09-30 13:35:29,703:INFO:  Epoch 271/500:  train Loss: 74.6134   val Loss: 72.9338   time: 0.24s   best: 70.5261
2023-09-30 13:35:29,979:INFO:  Epoch 272/500:  train Loss: 73.5907   val Loss: 71.5407   time: 0.26s   best: 70.5261
2023-09-30 13:35:30,224:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:30,255:INFO:  Epoch 273/500:  train Loss: 73.4117   val Loss: 69.4739   time: 0.24s   best: 69.4739
2023-09-30 13:35:30,530:INFO:  Epoch 274/500:  train Loss: 71.8583   val Loss: 71.3073   time: 0.26s   best: 69.4739
2023-09-30 13:35:30,782:INFO:  Epoch 275/500:  train Loss: 71.7733   val Loss: 69.8075   time: 0.24s   best: 69.4739
2023-09-30 13:35:31,052:INFO:  Epoch 276/500:  train Loss: 71.1878   val Loss: 71.0898   time: 0.26s   best: 69.4739
2023-09-30 13:35:31,308:INFO:  Epoch 277/500:  train Loss: 71.2706   val Loss: 71.0613   time: 0.24s   best: 69.4739
2023-09-30 13:35:31,579:INFO:  Epoch 278/500:  train Loss: 72.4652   val Loss: 73.3848   time: 0.26s   best: 69.4739
2023-09-30 13:35:31,830:INFO:  Epoch 279/500:  train Loss: 77.9115   val Loss: 75.6726   time: 0.24s   best: 69.4739
2023-09-30 13:35:32,101:INFO:  Epoch 280/500:  train Loss: 75.8675   val Loss: 77.2705   time: 0.26s   best: 69.4739
2023-09-30 13:35:32,355:INFO:  Epoch 281/500:  train Loss: 77.1761   val Loss: 72.2746   time: 0.24s   best: 69.4739
2023-09-30 13:35:32,623:INFO:  Epoch 282/500:  train Loss: 73.3555   val Loss: 70.7291   time: 0.26s   best: 69.4739
2023-09-30 13:35:32,882:INFO:  Epoch 283/500:  train Loss: 73.5639   val Loss: 73.3880   time: 0.25s   best: 69.4739
2023-09-30 13:35:33,151:INFO:  Epoch 284/500:  train Loss: 73.3347   val Loss: 72.2241   time: 0.26s   best: 69.4739
2023-09-30 13:35:33,409:INFO:  Epoch 285/500:  train Loss: 72.9643   val Loss: 71.8215   time: 0.25s   best: 69.4739
2023-09-30 13:35:33,680:INFO:  Epoch 286/500:  train Loss: 72.5271   val Loss: 71.1298   time: 0.26s   best: 69.4739
2023-09-30 13:35:33,932:INFO:  Epoch 287/500:  train Loss: 71.6352   val Loss: 70.3873   time: 0.24s   best: 69.4739
2023-09-30 13:35:34,195:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:34,225:INFO:  Epoch 288/500:  train Loss: 71.0055   val Loss: 68.7082   time: 0.26s   best: 68.7082
2023-09-30 13:35:34,484:INFO:  Epoch 289/500:  train Loss: 71.6065   val Loss: 70.5155   time: 0.25s   best: 68.7082
2023-09-30 13:35:34,753:INFO:  Epoch 290/500:  train Loss: 71.4738   val Loss: 69.7528   time: 0.24s   best: 68.7082
2023-09-30 13:35:35,004:INFO:  Epoch 291/500:  train Loss: 71.0753   val Loss: 69.1269   time: 0.24s   best: 68.7082
2023-09-30 13:35:35,277:INFO:  Epoch 292/500:  train Loss: 69.8980   val Loss: 69.1084   time: 0.26s   best: 68.7082
2023-09-30 13:35:35,522:INFO:  Epoch 293/500:  train Loss: 69.5868   val Loss: 71.5461   time: 0.24s   best: 68.7082
2023-09-30 13:35:35,799:INFO:  Epoch 294/500:  train Loss: 73.5492   val Loss: 72.5346   time: 0.26s   best: 68.7082
2023-09-30 13:35:36,050:INFO:  Epoch 295/500:  train Loss: 73.3360   val Loss: 72.0382   time: 0.24s   best: 68.7082
2023-09-30 13:35:36,324:INFO:  Epoch 296/500:  train Loss: 73.7446   val Loss: 75.8133   time: 0.26s   best: 68.7082
2023-09-30 13:35:36,581:INFO:  Epoch 297/500:  train Loss: 76.2693   val Loss: 94.2109   time: 0.24s   best: 68.7082
2023-09-30 13:35:36,851:INFO:  Epoch 298/500:  train Loss: 89.4437   val Loss: 80.4216   time: 0.26s   best: 68.7082
2023-09-30 13:35:37,101:INFO:  Epoch 299/500:  train Loss: 80.6546   val Loss: 80.4839   time: 0.24s   best: 68.7082
2023-09-30 13:35:37,420:INFO:  Epoch 300/500:  train Loss: 80.2230   val Loss: 76.8952   time: 0.30s   best: 68.7082
2023-09-30 13:35:37,702:INFO:  Epoch 301/500:  train Loss: 76.3542   val Loss: 75.3334   time: 0.27s   best: 68.7082
2023-09-30 13:35:37,954:INFO:  Epoch 302/500:  train Loss: 75.5980   val Loss: 74.1138   time: 0.24s   best: 68.7082
2023-09-30 13:35:38,224:INFO:  Epoch 303/500:  train Loss: 75.7061   val Loss: 74.3899   time: 0.26s   best: 68.7082
2023-09-30 13:35:38,486:INFO:  Epoch 304/500:  train Loss: 76.4053   val Loss: 78.5555   time: 0.25s   best: 68.7082
2023-09-30 13:35:38,756:INFO:  Epoch 305/500:  train Loss: 86.0356   val Loss: 88.3581   time: 0.26s   best: 68.7082
2023-09-30 13:35:39,008:INFO:  Epoch 306/500:  train Loss: 88.7852   val Loss: 88.1847   time: 0.24s   best: 68.7082
2023-09-30 13:35:39,284:INFO:  Epoch 307/500:  train Loss: 88.4661   val Loss: 87.8992   time: 0.26s   best: 68.7082
2023-09-30 13:35:39,537:INFO:  Epoch 308/500:  train Loss: 88.0905   val Loss: 87.4349   time: 0.24s   best: 68.7082
2023-09-30 13:35:39,808:INFO:  Epoch 309/500:  train Loss: 87.3304   val Loss: 86.3689   time: 0.26s   best: 68.7082
2023-09-30 13:35:40,058:INFO:  Epoch 310/500:  train Loss: 86.6482   val Loss: 84.8679   time: 0.24s   best: 68.7082
2023-09-30 13:35:40,329:INFO:  Epoch 311/500:  train Loss: 84.6782   val Loss: 83.3860   time: 0.26s   best: 68.7082
2023-09-30 13:35:40,689:INFO:  Epoch 312/500:  train Loss: 82.8942   val Loss: 80.6767   time: 0.35s   best: 68.7082
2023-09-30 13:35:41,053:INFO:  Epoch 313/500:  train Loss: 80.2037   val Loss: 78.4121   time: 0.35s   best: 68.7082
2023-09-30 13:35:41,325:INFO:  Epoch 314/500:  train Loss: 78.5222   val Loss: 76.7175   time: 0.26s   best: 68.7082
2023-09-30 13:35:41,577:INFO:  Epoch 315/500:  train Loss: 76.9189   val Loss: 75.0303   time: 0.24s   best: 68.7082
2023-09-30 13:35:41,847:INFO:  Epoch 316/500:  train Loss: 75.1127   val Loss: 73.7796   time: 0.26s   best: 68.7082
2023-09-30 13:35:42,092:INFO:  Epoch 317/500:  train Loss: 74.9201   val Loss: 73.5727   time: 0.24s   best: 68.7082
2023-09-30 13:35:42,369:INFO:  Epoch 318/500:  train Loss: 73.6395   val Loss: 72.0384   time: 0.27s   best: 68.7082
2023-09-30 13:35:42,620:INFO:  Epoch 319/500:  train Loss: 73.1664   val Loss: 72.4641   time: 0.24s   best: 68.7082
2023-09-30 13:35:42,889:INFO:  Epoch 320/500:  train Loss: 72.9101   val Loss: 71.8036   time: 0.26s   best: 68.7082
2023-09-30 13:35:43,142:INFO:  Epoch 321/500:  train Loss: 72.3541   val Loss: 71.1039   time: 0.25s   best: 68.7082
2023-09-30 13:35:43,423:INFO:  Epoch 322/500:  train Loss: 72.0277   val Loss: 70.3986   time: 0.27s   best: 68.7082
2023-09-30 13:35:43,668:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:43,698:INFO:  Epoch 323/500:  train Loss: 70.6478   val Loss: 67.5067   time: 0.24s   best: 67.5067
2023-09-30 13:35:43,967:INFO:  Epoch 324/500:  train Loss: 71.5126   val Loss: 71.4310   time: 0.26s   best: 67.5067
2023-09-30 13:35:44,214:INFO:  Epoch 325/500:  train Loss: 71.9573   val Loss: 70.6961   time: 0.24s   best: 67.5067
2023-09-30 13:35:44,516:INFO:  Epoch 326/500:  train Loss: 73.7020   val Loss: 75.6149   time: 0.26s   best: 67.5067
2023-09-30 13:35:44,767:INFO:  Epoch 327/500:  train Loss: 72.9688   val Loss: 69.2546   time: 0.24s   best: 67.5067
2023-09-30 13:35:45,045:INFO:  Epoch 328/500:  train Loss: 74.0907   val Loss: 74.5355   time: 0.27s   best: 67.5067
2023-09-30 13:35:45,300:INFO:  Epoch 329/500:  train Loss: 74.3049   val Loss: 72.8519   time: 0.24s   best: 67.5067
2023-09-30 13:35:45,572:INFO:  Epoch 330/500:  train Loss: 73.4731   val Loss: 71.9594   time: 0.26s   best: 67.5067
2023-09-30 13:35:45,825:INFO:  Epoch 331/500:  train Loss: 72.7430   val Loss: 71.4042   time: 0.24s   best: 67.5067
2023-09-30 13:35:46,094:INFO:  Epoch 332/500:  train Loss: 72.0589   val Loss: 70.3671   time: 0.26s   best: 67.5067
2023-09-30 13:35:46,349:INFO:  Epoch 333/500:  train Loss: 71.4857   val Loss: 70.5823   time: 0.24s   best: 67.5067
2023-09-30 13:35:46,609:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:46,639:INFO:  Epoch 334/500:  train Loss: 71.0073   val Loss: 67.4418   time: 0.25s   best: 67.4418
2023-09-30 13:35:46,891:INFO:  Epoch 335/500:  train Loss: 70.3796   val Loss: 70.6665   time: 0.24s   best: 67.4418
2023-09-30 13:35:47,166:INFO:  Epoch 336/500:  train Loss: 71.0454   val Loss: 70.0401   time: 0.26s   best: 67.4418
2023-09-30 13:35:47,423:INFO:  Epoch 337/500:  train Loss: 70.7886   val Loss: 70.0819   time: 0.24s   best: 67.4418
2023-09-30 13:35:47,696:INFO:  Epoch 338/500:  train Loss: 71.9350   val Loss: 72.1462   time: 0.26s   best: 67.4418
2023-09-30 13:35:47,947:INFO:  Epoch 339/500:  train Loss: 71.8825   val Loss: 70.6362   time: 0.24s   best: 67.4418
2023-09-30 13:35:48,218:INFO:  Epoch 340/500:  train Loss: 72.0176   val Loss: 68.8341   time: 0.26s   best: 67.4418
2023-09-30 13:35:48,475:INFO:  Epoch 341/500:  train Loss: 69.7107   val Loss: 69.0061   time: 0.24s   best: 67.4418
2023-09-30 13:35:48,745:INFO:  Epoch 342/500:  train Loss: 71.2480   val Loss: 68.2760   time: 0.26s   best: 67.4418
2023-09-30 13:35:49,006:INFO:  Epoch 343/500:  train Loss: 69.1377   val Loss: 68.7622   time: 0.25s   best: 67.4418
2023-09-30 13:35:49,275:INFO:  Epoch 344/500:  train Loss: 70.1169   val Loss: 68.6389   time: 0.26s   best: 67.4418
2023-09-30 13:35:49,532:INFO:  Epoch 345/500:  train Loss: 69.1245   val Loss: 68.9764   time: 0.24s   best: 67.4418
2023-09-30 13:35:49,803:INFO:  Epoch 346/500:  train Loss: 70.2609   val Loss: 69.7373   time: 0.26s   best: 67.4418
2023-09-30 13:35:50,056:INFO:  Epoch 347/500:  train Loss: 69.4230   val Loss: 70.4817   time: 0.24s   best: 67.4418
2023-09-30 13:35:50,327:INFO:  Epoch 348/500:  train Loss: 70.9278   val Loss: 70.0124   time: 0.26s   best: 67.4418
2023-09-30 13:35:50,578:INFO:  Epoch 349/500:  train Loss: 70.6530   val Loss: 69.9245   time: 0.24s   best: 67.4418
2023-09-30 13:35:50,840:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:50,870:INFO:  Epoch 350/500:  train Loss: 71.0289   val Loss: 67.3730   time: 0.26s   best: 67.3730
2023-09-30 13:35:51,130:INFO:  Epoch 351/500:  train Loss: 69.8229   val Loss: 67.8251   time: 0.25s   best: 67.3730
2023-09-30 13:35:51,405:INFO:  Epoch 352/500:  train Loss: 69.6208   val Loss: 67.7228   time: 0.26s   best: 67.3730
2023-09-30 13:35:51,651:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:51,681:INFO:  Epoch 353/500:  train Loss: 67.9218   val Loss: 66.5940   time: 0.24s   best: 66.5940
2023-09-30 13:35:51,943:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:51,973:INFO:  Epoch 354/500:  train Loss: 67.1951   val Loss: 66.2220   time: 0.26s   best: 66.2220
2023-09-30 13:35:52,227:INFO:  Epoch 355/500:  train Loss: 69.0194   val Loss: 69.9430   time: 0.24s   best: 66.2220
2023-09-30 13:35:52,490:INFO:  Epoch 356/500:  train Loss: 71.2070   val Loss: 70.3350   time: 0.26s   best: 66.2220
2023-09-30 13:35:52,749:INFO:  Epoch 357/500:  train Loss: 71.1827   val Loss: 69.9228   time: 0.25s   best: 66.2220
2023-09-30 13:35:53,029:INFO:  Epoch 358/500:  train Loss: 70.9538   val Loss: 69.9546   time: 0.27s   best: 66.2220
2023-09-30 13:35:53,280:INFO:  Epoch 359/500:  train Loss: 69.6368   val Loss: 70.1297   time: 0.24s   best: 66.2220
2023-09-30 13:35:53,554:INFO:  Epoch 360/500:  train Loss: 70.2045   val Loss: 69.2852   time: 0.26s   best: 66.2220
2023-09-30 13:35:53,807:INFO:  Epoch 361/500:  train Loss: 71.7264   val Loss: 67.9860   time: 0.24s   best: 66.2220
2023-09-30 13:35:54,078:INFO:  Epoch 362/500:  train Loss: 68.6345   val Loss: 68.0609   time: 0.26s   best: 66.2220
2023-09-30 13:35:54,330:INFO:  Epoch 363/500:  train Loss: 69.5744   val Loss: 67.0813   time: 0.24s   best: 66.2220
2023-09-30 13:35:54,595:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:35:54,625:INFO:  Epoch 364/500:  train Loss: 67.5814   val Loss: 65.9730   time: 0.26s   best: 65.9730
2023-09-30 13:35:54,877:INFO:  Epoch 365/500:  train Loss: 68.2411   val Loss: 68.5279   time: 0.24s   best: 65.9730
2023-09-30 13:35:55,154:INFO:  Epoch 366/500:  train Loss: 67.9878   val Loss: 66.9701   time: 0.26s   best: 65.9730
2023-09-30 13:35:55,403:INFO:  Epoch 367/500:  train Loss: 67.4719   val Loss: 68.8102   time: 0.24s   best: 65.9730
2023-09-30 13:35:55,680:INFO:  Epoch 368/500:  train Loss: 67.7545   val Loss: 67.1571   time: 0.26s   best: 65.9730
2023-09-30 13:35:55,931:INFO:  Epoch 369/500:  train Loss: 68.0135   val Loss: 68.4756   time: 0.24s   best: 65.9730
2023-09-30 13:35:56,200:INFO:  Epoch 370/500:  train Loss: 68.5629   val Loss: 67.3002   time: 0.26s   best: 65.9730
2023-09-30 13:35:56,453:INFO:  Epoch 371/500:  train Loss: 67.4002   val Loss: 67.3853   time: 0.24s   best: 65.9730
2023-09-30 13:35:56,721:INFO:  Epoch 372/500:  train Loss: 67.4726   val Loss: 67.9697   time: 0.26s   best: 65.9730
2023-09-30 13:35:56,974:INFO:  Epoch 373/500:  train Loss: 68.9029   val Loss: 69.6945   time: 0.24s   best: 65.9730
2023-09-30 13:35:57,251:INFO:  Epoch 374/500:  train Loss: 69.0381   val Loss: 70.1563   time: 0.26s   best: 65.9730
2023-09-30 13:35:57,510:INFO:  Epoch 375/500:  train Loss: 70.7030   val Loss: 68.8565   time: 0.25s   best: 65.9730
2023-09-30 13:35:57,783:INFO:  Epoch 376/500:  train Loss: 69.5141   val Loss: 69.3321   time: 0.26s   best: 65.9730
2023-09-30 13:35:58,051:INFO:  Epoch 377/500:  train Loss: 69.7185   val Loss: 69.8991   time: 0.24s   best: 65.9730
2023-09-30 13:35:58,304:INFO:  Epoch 378/500:  train Loss: 70.7337   val Loss: 70.2056   time: 0.24s   best: 65.9730
2023-09-30 13:35:58,579:INFO:  Epoch 379/500:  train Loss: 69.3733   val Loss: 67.7821   time: 0.25s   best: 65.9730
2023-09-30 13:35:58,835:INFO:  Epoch 380/500:  train Loss: 67.8448   val Loss: 66.8354   time: 0.24s   best: 65.9730
2023-09-30 13:35:59,108:INFO:  Epoch 381/500:  train Loss: 68.4524   val Loss: 68.9113   time: 0.25s   best: 65.9730
2023-09-30 13:35:59,364:INFO:  Epoch 382/500:  train Loss: 69.1454   val Loss: 70.3875   time: 0.24s   best: 65.9730
2023-09-30 13:35:59,618:INFO:  Epoch 383/500:  train Loss: 69.6296   val Loss: 69.8325   time: 0.24s   best: 65.9730
2023-09-30 13:35:59,888:INFO:  Epoch 384/500:  train Loss: 70.6695   val Loss: 67.1260   time: 0.26s   best: 65.9730
2023-09-30 13:36:00,140:INFO:  Epoch 385/500:  train Loss: 67.5244   val Loss: 66.9195   time: 0.24s   best: 65.9730
2023-09-30 13:36:00,404:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_b5fa.pt
2023-09-30 13:36:00,433:INFO:  Epoch 386/500:  train Loss: 66.5112   val Loss: 65.5364   time: 0.26s   best: 65.5364
2023-09-30 13:36:00,693:INFO:  Epoch 387/500:  train Loss: 76.9100   val Loss: 84.7577   time: 0.24s   best: 65.5364
2023-09-30 13:36:00,955:INFO:  Epoch 388/500:  train Loss: 92.2589   val Loss: 93.5347   time: 0.25s   best: 65.5364
2023-09-30 13:36:01,228:INFO:  Epoch 389/500:  train Loss: 95.5210   val Loss: 93.9209   time: 0.24s   best: 65.5364
2023-09-30 13:36:01,489:INFO:  Epoch 390/500:  train Loss: 96.9751   val Loss: 97.4185   time: 0.25s   best: 65.5364
2023-09-30 13:36:01,756:INFO:  Epoch 391/500:  train Loss: 95.2690   val Loss: 93.5579   time: 0.24s   best: 65.5364
2023-09-30 13:36:02,012:INFO:  Epoch 392/500:  train Loss: 96.7356   val Loss: 97.2512   time: 0.24s   best: 65.5364
2023-09-30 13:36:02,264:INFO:  Epoch 393/500:  train Loss: 97.0392   val Loss: 96.2419   time: 0.24s   best: 65.5364
2023-09-30 13:36:02,536:INFO:  Epoch 394/500:  train Loss: 96.0771   val Loss: 95.3341   time: 0.26s   best: 65.5364
2023-09-30 13:36:02,788:INFO:  Epoch 395/500:  train Loss: 95.1081   val Loss: 94.7460   time: 0.24s   best: 65.5364
2023-09-30 13:36:03,063:INFO:  Epoch 396/500:  train Loss: 94.8100   val Loss: 94.6290   time: 0.26s   best: 65.5364
2023-09-30 13:36:03,335:INFO:  Epoch 397/500:  train Loss: 94.9422   val Loss: 94.7515   time: 0.24s   best: 65.5364
2023-09-30 13:36:03,590:INFO:  Epoch 398/500:  train Loss: 95.2709   val Loss: 95.1617   time: 0.24s   best: 65.5364
2023-09-30 13:36:03,843:INFO:  Epoch 399/500:  train Loss: 95.1747   val Loss: 94.7983   time: 0.24s   best: 65.5364
2023-09-30 13:36:04,157:INFO:  Epoch 400/500:  train Loss: 94.8756   val Loss: 94.3356   time: 0.30s   best: 65.5364
2023-09-30 13:36:04,436:INFO:  Epoch 401/500:  train Loss: 94.7336   val Loss: 94.4215   time: 0.27s   best: 65.5364
2023-09-30 13:36:04,687:INFO:  Epoch 402/500:  train Loss: 95.1137   val Loss: 95.3994   time: 0.24s   best: 65.5364
2023-09-30 13:36:04,955:INFO:  Epoch 403/500:  train Loss: 95.3697   val Loss: 94.7550   time: 0.26s   best: 65.5364
2023-09-30 13:36:05,213:INFO:  Epoch 404/500:  train Loss: 94.7027   val Loss: 94.1965   time: 0.24s   best: 65.5364
2023-09-30 13:36:05,489:INFO:  Epoch 405/500:  train Loss: 94.7846   val Loss: 94.2173   time: 0.26s   best: 65.5364
2023-09-30 13:36:05,743:INFO:  Epoch 406/500:  train Loss: 94.3883   val Loss: 94.2823   time: 0.24s   best: 65.5364
2023-09-30 13:36:06,013:INFO:  Epoch 407/500:  train Loss: 94.7108   val Loss: 94.9765   time: 0.26s   best: 65.5364
2023-09-30 13:36:06,265:INFO:  Epoch 408/500:  train Loss: 94.7805   val Loss: 94.6811   time: 0.24s   best: 65.5364
2023-09-30 13:36:06,536:INFO:  Epoch 409/500:  train Loss: 94.6679   val Loss: 94.1426   time: 0.26s   best: 65.5364
2023-09-30 13:36:06,788:INFO:  Epoch 410/500:  train Loss: 94.3419   val Loss: 93.9866   time: 0.24s   best: 65.5364
2023-09-30 13:36:07,063:INFO:  Epoch 411/500:  train Loss: 94.5347   val Loss: 94.6376   time: 0.26s   best: 65.5364
2023-09-30 13:36:07,319:INFO:  Epoch 412/500:  train Loss: 94.6928   val Loss: 94.6056   time: 0.24s   best: 65.5364
2023-09-30 13:36:07,592:INFO:  Epoch 413/500:  train Loss: 94.6408   val Loss: 94.1162   time: 0.26s   best: 65.5364
2023-09-30 13:36:07,848:INFO:  Epoch 414/500:  train Loss: 94.3970   val Loss: 94.3987   time: 0.24s   best: 65.5364
2023-09-30 13:36:08,118:INFO:  Epoch 415/500:  train Loss: 94.7514   val Loss: 95.3050   time: 0.26s   best: 65.5364
2023-09-30 13:36:08,371:INFO:  Epoch 416/500:  train Loss: 95.3091   val Loss: 95.3971   time: 0.24s   best: 65.5364
2023-09-30 13:36:08,642:INFO:  Epoch 417/500:  train Loss: 95.2688   val Loss: 94.8322   time: 0.26s   best: 65.5364
2023-09-30 13:36:08,893:INFO:  Epoch 418/500:  train Loss: 94.4228   val Loss: 93.9715   time: 0.24s   best: 65.5364
2023-09-30 13:36:09,167:INFO:  Epoch 419/500:  train Loss: 93.6640   val Loss: 93.2355   time: 0.27s   best: 65.5364
2023-09-30 13:36:09,434:INFO:  Epoch 420/500:  train Loss: 93.9848   val Loss: 93.2302   time: 0.25s   best: 65.5364
2023-09-30 13:36:09,703:INFO:  Epoch 421/500:  train Loss: 93.6673   val Loss: 94.1367   time: 0.26s   best: 65.5364
2023-09-30 13:36:09,954:INFO:  Epoch 422/500:  train Loss: 94.3255   val Loss: 94.1945   time: 0.24s   best: 65.5364
2023-09-30 13:36:10,225:INFO:  Epoch 423/500:  train Loss: 94.0622   val Loss: 93.6484   time: 0.26s   best: 65.5364
2023-09-30 13:36:10,471:INFO:  Epoch 424/500:  train Loss: 93.5658   val Loss: 93.0394   time: 0.24s   best: 65.5364
2023-09-30 13:36:10,748:INFO:  Epoch 425/500:  train Loss: 93.5792   val Loss: 93.2141   time: 0.26s   best: 65.5364
2023-09-30 13:36:10,999:INFO:  Epoch 426/500:  train Loss: 93.2845   val Loss: 93.1605   time: 0.24s   best: 65.5364
2023-09-30 13:36:11,412:INFO:  Epoch 427/500:  train Loss: 93.1285   val Loss: 92.9900   time: 0.41s   best: 65.5364
2023-09-30 13:36:11,748:INFO:  Epoch 428/500:  train Loss: 93.1395   val Loss: 92.8976   time: 0.32s   best: 65.5364
2023-09-30 13:36:11,999:INFO:  Epoch 429/500:  train Loss: 92.7722   val Loss: 92.3170   time: 0.24s   best: 65.5364
2023-09-30 13:36:12,269:INFO:  Epoch 430/500:  train Loss: 92.3465   val Loss: 91.8375   time: 0.26s   best: 65.5364
2023-09-30 13:36:12,523:INFO:  Epoch 431/500:  train Loss: 91.9643   val Loss: 91.4462   time: 0.24s   best: 65.5364
2023-09-30 13:36:12,793:INFO:  Epoch 432/500:  train Loss: 91.1887   val Loss: 90.6889   time: 0.26s   best: 65.5364
2023-09-30 13:36:13,050:INFO:  Epoch 433/500:  train Loss: 90.8214   val Loss: 90.4226   time: 0.24s   best: 65.5364
2023-09-30 13:36:13,306:INFO:  Epoch 434/500:  train Loss: 90.4541   val Loss: 89.5457   time: 0.25s   best: 65.5364
2023-09-30 13:36:13,573:INFO:  Epoch 435/500:  train Loss: 89.3439   val Loss: 88.4861   time: 0.26s   best: 65.5364
2023-09-30 13:36:13,858:INFO:  Epoch 436/500:  train Loss: 88.4879   val Loss: 88.1816   time: 0.27s   best: 65.5364
2023-09-30 13:36:14,110:INFO:  Epoch 437/500:  train Loss: 88.2511   val Loss: 86.6564   time: 0.24s   best: 65.5364
2023-09-30 13:36:14,380:INFO:  Epoch 438/500:  train Loss: 85.9307   val Loss: 84.9137   time: 0.26s   best: 65.5364
2023-09-30 13:36:14,634:INFO:  Epoch 439/500:  train Loss: 85.4118   val Loss: 82.1738   time: 0.24s   best: 65.5364
2023-09-30 13:36:14,903:INFO:  Epoch 440/500:  train Loss: 84.7671   val Loss: 83.3254   time: 0.26s   best: 65.5364
2023-09-30 13:36:15,154:INFO:  Epoch 441/500:  train Loss: 85.8140   val Loss: 85.8289   time: 0.24s   best: 65.5364
2023-09-30 13:36:15,427:INFO:  Epoch 442/500:  train Loss: 86.3851   val Loss: 85.5517   time: 0.26s   best: 65.5364
2023-09-30 13:36:15,687:INFO:  Epoch 443/500:  train Loss: 85.7617   val Loss: 84.2960   time: 0.25s   best: 65.5364
2023-09-30 13:36:15,955:INFO:  Epoch 444/500:  train Loss: 83.8683   val Loss: 82.7839   time: 0.25s   best: 65.5364
2023-09-30 13:36:16,208:INFO:  Epoch 445/500:  train Loss: 82.5662   val Loss: 82.2178   time: 0.24s   best: 65.5364
2023-09-30 13:36:16,481:INFO:  Epoch 446/500:  train Loss: 83.0854   val Loss: 82.0897   time: 0.26s   best: 65.5364
2023-09-30 13:36:16,734:INFO:  Epoch 447/500:  train Loss: 83.3599   val Loss: 83.2966   time: 0.24s   best: 65.5364
2023-09-30 13:36:17,004:INFO:  Epoch 448/500:  train Loss: 83.6377   val Loss: 82.2618   time: 0.26s   best: 65.5364
2023-09-30 13:36:17,256:INFO:  Epoch 449/500:  train Loss: 81.9473   val Loss: 80.0371   time: 0.24s   best: 65.5364
2023-09-30 13:36:17,532:INFO:  Epoch 450/500:  train Loss: 80.5201   val Loss: 79.3894   time: 0.26s   best: 65.5364
2023-09-30 13:36:17,793:INFO:  Epoch 451/500:  train Loss: 80.5409   val Loss: 80.1816   time: 0.25s   best: 65.5364
2023-09-30 13:36:18,062:INFO:  Epoch 452/500:  train Loss: 80.4300   val Loss: 90.8528   time: 0.26s   best: 65.5364
2023-09-30 13:36:18,316:INFO:  Epoch 453/500:  train Loss: 92.2879   val Loss: 86.2953   time: 0.24s   best: 65.5364
2023-09-30 13:36:18,592:INFO:  Epoch 454/500:  train Loss: 86.6135   val Loss: 85.6431   time: 0.26s   best: 65.5364
2023-09-30 13:36:18,844:INFO:  Epoch 455/500:  train Loss: 85.4093   val Loss: 83.5472   time: 0.24s   best: 65.5364
2023-09-30 13:36:19,115:INFO:  Epoch 456/500:  train Loss: 83.4402   val Loss: 82.7865   time: 0.26s   best: 65.5364
2023-09-30 13:36:19,370:INFO:  Epoch 457/500:  train Loss: 83.1262   val Loss: 82.4957   time: 0.24s   best: 65.5364
2023-09-30 13:36:19,651:INFO:  Epoch 458/500:  train Loss: 83.0690   val Loss: 81.8186   time: 0.27s   best: 65.5364
2023-09-30 13:36:19,904:INFO:  Epoch 459/500:  train Loss: 81.6334   val Loss: 80.6665   time: 0.24s   best: 65.5364
2023-09-30 13:36:20,176:INFO:  Epoch 460/500:  train Loss: 80.7340   val Loss: 79.0011   time: 0.26s   best: 65.5364
2023-09-30 13:36:20,427:INFO:  Epoch 461/500:  train Loss: 79.3600   val Loss: 77.2161   time: 0.24s   best: 65.5364
2023-09-30 13:36:20,700:INFO:  Epoch 462/500:  train Loss: 77.3218   val Loss: 76.6060   time: 0.26s   best: 65.5364
2023-09-30 13:36:20,952:INFO:  Epoch 463/500:  train Loss: 78.8138   val Loss: 77.3887   time: 0.24s   best: 65.5364
2023-09-30 13:36:21,223:INFO:  Epoch 464/500:  train Loss: 77.6683   val Loss: 76.8843   time: 0.26s   best: 65.5364
2023-09-30 13:36:21,480:INFO:  Epoch 465/500:  train Loss: 76.1415   val Loss: 73.1502   time: 0.24s   best: 65.5364
2023-09-30 13:36:21,757:INFO:  Epoch 466/500:  train Loss: 73.7979   val Loss: 74.2953   time: 0.26s   best: 65.5364
2023-09-30 13:36:22,010:INFO:  Epoch 467/500:  train Loss: 75.8219   val Loss: 77.0959   time: 0.24s   best: 65.5364
2023-09-30 13:36:22,282:INFO:  Epoch 468/500:  train Loss: 82.5374   val Loss: 82.0289   time: 0.26s   best: 65.5364
2023-09-30 13:36:22,537:INFO:  Epoch 469/500:  train Loss: 81.1916   val Loss: 83.2857   time: 0.24s   best: 65.5364
2023-09-30 13:36:22,807:INFO:  Epoch 470/500:  train Loss: 83.2060   val Loss: 82.1338   time: 0.26s   best: 65.5364
2023-09-30 13:36:23,059:INFO:  Epoch 471/500:  train Loss: 84.5293   val Loss: 81.4033   time: 0.24s   best: 65.5364
2023-09-30 13:36:23,332:INFO:  Epoch 472/500:  train Loss: 80.5794   val Loss: 80.2516   time: 0.26s   best: 65.5364
2023-09-30 13:36:23,593:INFO:  Epoch 473/500:  train Loss: 78.4854   val Loss: 76.4309   time: 0.24s   best: 65.5364
2023-09-30 13:36:23,857:INFO:  Epoch 474/500:  train Loss: 75.6337   val Loss: 75.3961   time: 0.26s   best: 65.5364
2023-09-30 13:36:24,117:INFO:  Epoch 475/500:  train Loss: 75.1410   val Loss: 72.6943   time: 0.25s   best: 65.5364
2023-09-30 13:36:24,387:INFO:  Epoch 476/500:  train Loss: 73.5325   val Loss: 71.0454   time: 0.26s   best: 65.5364
2023-09-30 13:36:24,641:INFO:  Epoch 477/500:  train Loss: 71.8870   val Loss: 72.2910   time: 0.24s   best: 65.5364
2023-09-30 13:36:24,912:INFO:  Epoch 478/500:  train Loss: 71.7217   val Loss: 70.3520   time: 0.26s   best: 65.5364
2023-09-30 13:36:25,165:INFO:  Epoch 479/500:  train Loss: 70.7779   val Loss: 70.3809   time: 0.24s   best: 65.5364
2023-09-30 13:36:25,439:INFO:  Epoch 480/500:  train Loss: 71.5034   val Loss: 73.2910   time: 0.26s   best: 65.5364
2023-09-30 13:36:25,699:INFO:  Epoch 481/500:  train Loss: 71.9477   val Loss: 72.1607   time: 0.25s   best: 65.5364
2023-09-30 13:36:25,969:INFO:  Epoch 482/500:  train Loss: 72.5884   val Loss: 73.1131   time: 0.26s   best: 65.5364
2023-09-30 13:36:26,222:INFO:  Epoch 483/500:  train Loss: 72.8029   val Loss: 74.0767   time: 0.24s   best: 65.5364
2023-09-30 13:36:26,494:INFO:  Epoch 484/500:  train Loss: 73.7917   val Loss: 75.0660   time: 0.26s   best: 65.5364
2023-09-30 13:36:26,739:INFO:  Epoch 485/500:  train Loss: 73.2637   val Loss: 73.9475   time: 0.24s   best: 65.5364
2023-09-30 13:36:27,018:INFO:  Epoch 486/500:  train Loss: 73.5210   val Loss: 71.5409   time: 0.27s   best: 65.5364
2023-09-30 13:36:27,269:INFO:  Epoch 487/500:  train Loss: 73.2525   val Loss: 69.7218   time: 0.24s   best: 65.5364
2023-09-30 13:36:27,546:INFO:  Epoch 488/500:  train Loss: 70.8864   val Loss: 70.0035   time: 0.26s   best: 65.5364
2023-09-30 13:36:27,808:INFO:  Epoch 489/500:  train Loss: 70.0624   val Loss: 70.2275   time: 0.25s   best: 65.5364
2023-09-30 13:36:28,071:INFO:  Epoch 490/500:  train Loss: 70.1280   val Loss: 70.0767   time: 0.26s   best: 65.5364
2023-09-30 13:36:28,325:INFO:  Epoch 491/500:  train Loss: 69.7440   val Loss: 66.8924   time: 0.25s   best: 65.5364
2023-09-30 13:36:28,612:INFO:  Epoch 492/500:  train Loss: 69.6212   val Loss: 71.3418   time: 0.27s   best: 65.5364
2023-09-30 13:36:28,864:INFO:  Epoch 493/500:  train Loss: 73.4816   val Loss: 74.8112   time: 0.24s   best: 65.5364
2023-09-30 13:36:29,168:INFO:  Epoch 494/500:  train Loss: 74.2114   val Loss: 69.3598   time: 0.29s   best: 65.5364
2023-09-30 13:36:29,424:INFO:  Epoch 495/500:  train Loss: 72.1215   val Loss: 69.0029   time: 0.24s   best: 65.5364
2023-09-30 13:36:29,697:INFO:  Epoch 496/500:  train Loss: 69.2402   val Loss: 68.6110   time: 0.27s   best: 65.5364
2023-09-30 13:36:29,956:INFO:  Epoch 497/500:  train Loss: 70.2435   val Loss: 69.3199   time: 0.25s   best: 65.5364
2023-09-30 13:36:30,228:INFO:  Epoch 498/500:  train Loss: 70.3238   val Loss: 69.7082   time: 0.26s   best: 65.5364
2023-09-30 13:36:30,480:INFO:  Epoch 499/500:  train Loss: 69.3735   val Loss: 69.4617   time: 0.24s   best: 65.5364
2023-09-30 13:36:30,793:INFO:  Epoch 500/500:  train Loss: 71.0617   val Loss: 70.2880   time: 0.30s   best: 65.5364
2023-09-30 13:36:30,793:INFO:  -----> Training complete in 2m 18s   best validation loss: 65.5364
 
2023-09-30 13:42:16,349:INFO:  Epoch 395/500:  train Loss: 17.9141   val Loss: 24.3876   time: 438.82s   best: 22.6744
2023-09-30 13:49:31,658:INFO:  Epoch 396/500:  train Loss: 17.7054   val Loss: 22.8468   time: 435.26s   best: 22.6744
2023-09-30 13:56:51,914:INFO:  Epoch 397/500:  train Loss: 17.6396   val Loss: 22.9074   time: 440.24s   best: 22.6744
2023-09-30 14:04:11,445:INFO:  Epoch 398/500:  train Loss: 17.9332   val Loss: 23.3168   time: 439.51s   best: 22.6744
2023-09-30 14:11:29,409:INFO:  Epoch 399/500:  train Loss: 17.7246   val Loss: 22.7545   time: 437.94s   best: 22.6744
2023-09-30 14:18:50,319:INFO:  Epoch 400/500:  train Loss: 17.8696   val Loss: 23.1885   time: 440.86s   best: 22.6744
2023-09-30 14:26:08,186:INFO:  Epoch 401/500:  train Loss: 17.7076   val Loss: 22.9321   time: 437.85s   best: 22.6744
2023-09-30 14:33:27,758:INFO:  Epoch 402/500:  train Loss: 17.6800   val Loss: 22.9657   time: 439.55s   best: 22.6744
2023-09-30 14:40:59,242:INFO:  Epoch 403/500:  train Loss: 17.7648   val Loss: 23.0357   time: 451.47s   best: 22.6744
2023-09-30 14:48:17,547:INFO:  Epoch 404/500:  train Loss: 17.6807   val Loss: 23.1260   time: 438.30s   best: 22.6744
2023-09-30 14:55:34,886:INFO:  Epoch 405/500:  train Loss: 17.9807   val Loss: 23.0998   time: 437.33s   best: 22.6744
2023-09-30 15:02:52,563:INFO:  Epoch 406/500:  train Loss: 17.9032   val Loss: 23.1691   time: 437.64s   best: 22.6744
2023-09-30 15:10:11,573:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 15:10:11,798:INFO:  Epoch 407/500:  train Loss: 17.7812   val Loss: 22.6722   time: 438.96s   best: 22.6722
2023-09-30 15:17:30,635:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 15:17:30,967:INFO:  Epoch 408/500:  train Loss: 17.7107   val Loss: 22.6084   time: 438.77s   best: 22.6084
2023-09-30 15:24:47,535:INFO:  Epoch 409/500:  train Loss: 17.7700   val Loss: 22.9431   time: 436.56s   best: 22.6084
2023-09-30 15:32:10,523:INFO:  Epoch 410/500:  train Loss: 17.6669   val Loss: 22.9786   time: 442.96s   best: 22.6084
2023-09-30 15:39:33,950:INFO:  Epoch 411/500:  train Loss: 17.7761   val Loss: 22.9091   time: 443.40s   best: 22.6084
2023-09-30 15:47:04,933:INFO:  Epoch 412/500:  train Loss: 17.6979   val Loss: 22.9612   time: 450.95s   best: 22.6084
2023-09-30 15:54:35,244:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 15:54:35,714:INFO:  Epoch 413/500:  train Loss: 17.6902   val Loss: 22.5887   time: 450.17s   best: 22.5887
2023-09-30 16:02:02,004:INFO:  Epoch 414/500:  train Loss: 17.8445   val Loss: 22.6766   time: 446.27s   best: 22.5887
2023-09-30 16:09:27,798:INFO:  Epoch 415/500:  train Loss: 17.5736   val Loss: 23.2038   time: 445.77s   best: 22.5887
2023-09-30 16:16:52,164:INFO:  Epoch 416/500:  train Loss: 17.5504   val Loss: 23.1188   time: 444.34s   best: 22.5887
2023-09-30 16:24:14,546:INFO:  Epoch 417/500:  train Loss: 17.5730   val Loss: 22.8369   time: 442.37s   best: 22.5887
2023-09-30 16:31:39,426:INFO:  Epoch 418/500:  train Loss: 17.6444   val Loss: 22.9318   time: 444.87s   best: 22.5887
2023-09-30 16:39:05,477:INFO:  Epoch 419/500:  train Loss: 18.0222   val Loss: 23.0349   time: 446.04s   best: 22.5887
2023-09-30 16:46:25,057:INFO:  Epoch 420/500:  train Loss: 17.6329   val Loss: 22.9740   time: 439.55s   best: 22.5887
2023-09-30 16:53:42,019:INFO:  Epoch 421/500:  train Loss: 17.5782   val Loss: 22.9156   time: 436.92s   best: 22.5887
2023-09-30 17:01:02,167:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 17:01:02,252:INFO:  Epoch 422/500:  train Loss: 17.7055   val Loss: 22.2826   time: 440.04s   best: 22.2826
2023-09-30 17:08:20,712:INFO:  Epoch 423/500:  train Loss: 17.6045   val Loss: 26.3070   time: 438.46s   best: 22.2826
2023-09-30 17:15:37,541:INFO:  Epoch 424/500:  train Loss: 17.7023   val Loss: 22.4423   time: 436.79s   best: 22.2826
2023-09-30 17:22:59,393:INFO:  Epoch 425/500:  train Loss: 17.6248   val Loss: 23.0992   time: 441.84s   best: 22.2826
2023-09-30 17:30:21,527:INFO:  Epoch 426/500:  train Loss: 17.5626   val Loss: 22.7726   time: 442.12s   best: 22.2826
2023-09-30 17:37:44,654:INFO:  Epoch 427/500:  train Loss: 17.6478   val Loss: 23.4328   time: 443.12s   best: 22.2826
2023-09-30 17:45:05,587:INFO:  Epoch 428/500:  train Loss: 17.9994   val Loss: 23.2334   time: 440.91s   best: 22.2826
2023-09-30 17:52:25,964:INFO:  Epoch 429/500:  train Loss: 17.8494   val Loss: 23.1035   time: 440.36s   best: 22.2826
2023-09-30 17:59:46,729:INFO:  Epoch 430/500:  train Loss: 17.8157   val Loss: 23.2339   time: 440.76s   best: 22.2826
2023-09-30 18:07:09,976:INFO:  Epoch 431/500:  train Loss: 17.7347   val Loss: 22.5250   time: 443.24s   best: 22.2826
2023-09-30 18:14:29,950:INFO:  Epoch 432/500:  train Loss: 17.6365   val Loss: 23.2776   time: 439.96s   best: 22.2826
2023-09-30 18:21:47,489:INFO:  Epoch 433/500:  train Loss: 17.6984   val Loss: 23.8612   time: 437.53s   best: 22.2826
2023-09-30 18:29:07,584:INFO:  Epoch 434/500:  train Loss: 17.6757   val Loss: 22.9999   time: 440.08s   best: 22.2826
2023-09-30 18:36:44,997:INFO:  Epoch 435/500:  train Loss: 17.7620   val Loss: 23.0471   time: 457.38s   best: 22.2826
2023-09-30 18:44:05,099:INFO:  Epoch 436/500:  train Loss: 17.7691   val Loss: 22.6892   time: 440.09s   best: 22.2826
2023-09-30 18:51:21,959:INFO:  Epoch 437/500:  train Loss: 17.5544   val Loss: 22.8962   time: 436.83s   best: 22.2826
2023-09-30 18:58:39,281:INFO:  Epoch 438/500:  train Loss: 17.5442   val Loss: 22.9522   time: 437.31s   best: 22.2826
2023-09-30 19:06:04,377:INFO:  Epoch 439/500:  train Loss: 17.5609   val Loss: 22.7103   time: 445.07s   best: 22.2826
2023-09-30 19:13:30,584:INFO:  Epoch 440/500:  train Loss: 17.4837   val Loss: 23.6409   time: 446.20s   best: 22.2826
2023-09-30 19:20:51,819:INFO:  Epoch 441/500:  train Loss: 17.4506   val Loss: 22.8122   time: 441.21s   best: 22.2826
2023-09-30 19:28:10,435:INFO:  Epoch 442/500:  train Loss: 17.4953   val Loss: 22.9316   time: 438.60s   best: 22.2826
2023-09-30 19:35:32,302:INFO:  Epoch 443/500:  train Loss: 17.5534   val Loss: 22.5973   time: 441.85s   best: 22.2826
2023-09-30 19:42:49,701:INFO:  Epoch 444/500:  train Loss: 17.4452   val Loss: 22.6217   time: 437.39s   best: 22.2826
2023-09-30 19:50:06,594:INFO:  Epoch 445/500:  train Loss: 17.5368   val Loss: 22.9874   time: 436.88s   best: 22.2826
2023-09-30 19:57:25,708:INFO:  Epoch 446/500:  train Loss: 17.5939   val Loss: 22.6846   time: 439.10s   best: 22.2826
2023-09-30 20:04:45,962:INFO:  Epoch 447/500:  train Loss: 17.5608   val Loss: 24.0494   time: 440.23s   best: 22.2826
2023-09-30 20:12:01,055:INFO:  Epoch 448/500:  train Loss: 17.6574   val Loss: 23.0976   time: 435.07s   best: 22.2826
2023-09-30 20:19:21,089:INFO:  Epoch 449/500:  train Loss: 17.4537   val Loss: 22.8348   time: 440.02s   best: 22.2826
2023-09-30 20:26:38,121:INFO:  Epoch 450/500:  train Loss: 17.5234   val Loss: 23.0374   time: 437.02s   best: 22.2826
2023-09-30 20:33:57,454:INFO:  Epoch 451/500:  train Loss: 17.6852   val Loss: 24.7199   time: 439.33s   best: 22.2826
2023-09-30 20:41:44,580:INFO:  Epoch 452/500:  train Loss: 17.4705   val Loss: 22.8681   time: 467.10s   best: 22.2826
2023-09-30 20:49:01,587:INFO:  Epoch 453/500:  train Loss: 17.5578   val Loss: 23.0496   time: 437.00s   best: 22.2826
2023-09-30 20:56:21,278:INFO:  Epoch 454/500:  train Loss: 17.5091   val Loss: 22.8249   time: 439.66s   best: 22.2826
2023-09-30 21:03:41,797:INFO:  Epoch 455/500:  train Loss: 17.5274   val Loss: 22.8157   time: 440.50s   best: 22.2826
2023-09-30 21:10:56,587:INFO:  Epoch 456/500:  train Loss: 17.4622   val Loss: 22.5069   time: 434.76s   best: 22.2826
2023-09-30 21:18:17,134:INFO:  Epoch 457/500:  train Loss: 17.4657   val Loss: 22.7682   time: 440.54s   best: 22.2826
2023-09-30 21:25:45,242:INFO:  Epoch 458/500:  train Loss: 17.5691   val Loss: 22.6077   time: 448.09s   best: 22.2826
2023-09-30 21:33:13,927:INFO:  Epoch 459/500:  train Loss: 17.3975   val Loss: 22.6475   time: 448.66s   best: 22.2826
2023-09-30 21:40:41,734:INFO:  Epoch 460/500:  train Loss: 17.6763   val Loss: 22.8578   time: 447.79s   best: 22.2826
2023-09-30 21:48:10,061:INFO:  Epoch 461/500:  train Loss: 17.6192   val Loss: 22.6125   time: 448.30s   best: 22.2826
2023-09-30 21:55:43,481:INFO:  Epoch 462/500:  train Loss: 17.4825   val Loss: 22.9012   time: 453.41s   best: 22.2826
2023-09-30 22:03:10,336:INFO:  Epoch 463/500:  train Loss: 17.3680   val Loss: 23.4923   time: 446.80s   best: 22.2826
2023-09-30 22:10:39,673:INFO:  Epoch 464/500:  train Loss: 17.5314   val Loss: 22.6387   time: 449.31s   best: 22.2826
2023-09-30 22:18:07,722:INFO:  Epoch 465/500:  train Loss: 17.5396   val Loss: 22.9341   time: 448.04s   best: 22.2826
2023-09-30 22:25:33,509:INFO:  Epoch 466/500:  train Loss: 17.3738   val Loss: 22.6798   time: 445.77s   best: 22.2826
2023-09-30 22:32:55,266:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder_82f3.pt
2023-09-30 22:32:55,704:INFO:  Epoch 467/500:  train Loss: 17.3150   val Loss: 22.2212   time: 441.47s   best: 22.2212
2023-09-30 22:40:20,698:INFO:  Epoch 468/500:  train Loss: 17.3308   val Loss: 22.4512   time: 444.99s   best: 22.2212
2023-09-30 22:47:44,197:INFO:  Epoch 469/500:  train Loss: 17.3570   val Loss: 23.0592   time: 443.46s   best: 22.2212
2023-09-30 22:55:24,998:INFO:  Epoch 470/500:  train Loss: 17.3439   val Loss: 22.9923   time: 460.78s   best: 22.2212
2023-09-30 23:02:42,544:INFO:  Epoch 471/500:  train Loss: 17.3721   val Loss: 22.4939   time: 437.54s   best: 22.2212
2023-09-30 23:10:03,331:INFO:  Epoch 472/500:  train Loss: 17.4626   val Loss: 22.8839   time: 440.77s   best: 22.2212
2023-09-30 23:17:21,850:INFO:  Epoch 473/500:  train Loss: 17.3826   val Loss: 24.1394   time: 438.50s   best: 22.2212
2023-09-30 23:24:39,500:INFO:  Epoch 474/500:  train Loss: 18.0065   val Loss: 23.3289   time: 437.62s   best: 22.2212
2023-09-30 23:31:58,069:INFO:  Epoch 475/500:  train Loss: 17.5376   val Loss: 22.5096   time: 438.55s   best: 22.2212
2023-09-30 23:39:43,341:INFO:  Epoch 476/500:  train Loss: 17.4783   val Loss: 23.5791   time: 465.27s   best: 22.2212
2023-09-30 23:47:03,120:INFO:  Epoch 477/500:  train Loss: 17.7213   val Loss: 22.9361   time: 439.76s   best: 22.2212
2023-09-30 23:54:27,259:INFO:  Epoch 478/500:  train Loss: 17.5331   val Loss: 22.9523   time: 444.13s   best: 22.2212
2023-10-01 00:01:48,232:INFO:  Epoch 479/500:  train Loss: 17.3765   val Loss: 24.3316   time: 440.95s   best: 22.2212
2023-10-01 00:09:03,962:INFO:  Epoch 480/500:  train Loss: 17.4642   val Loss: 22.8508   time: 435.72s   best: 22.2212
2023-10-01 00:16:24,413:INFO:  Epoch 481/500:  train Loss: 17.3343   val Loss: 27.0714   time: 440.44s   best: 22.2212
2023-10-01 00:23:42,022:INFO:  Epoch 482/500:  train Loss: 17.6621   val Loss: 29.2188   time: 437.58s   best: 22.2212
2023-10-01 00:31:01,278:INFO:  Epoch 483/500:  train Loss: 17.4038   val Loss: 22.3944   time: 439.23s   best: 22.2212
2023-10-01 00:38:15,701:INFO:  Epoch 484/500:  train Loss: 17.3889   val Loss: 23.5856   time: 434.41s   best: 22.2212
2023-10-01 00:45:33,236:INFO:  Epoch 485/500:  train Loss: 17.2706   val Loss: 22.6688   time: 437.52s   best: 22.2212
2023-10-01 00:52:53,589:INFO:  Epoch 486/500:  train Loss: 17.3479   val Loss: 22.9593   time: 440.34s   best: 22.2212
2023-10-01 01:00:12,131:INFO:  Epoch 487/500:  train Loss: 17.3866   val Loss: 22.6739   time: 438.53s   best: 22.2212
2023-10-01 01:08:12,976:INFO:  Epoch 488/500:  train Loss: 17.2778   val Loss: 23.2713   time: 480.81s   best: 22.2212
2023-10-01 01:15:49,278:INFO:  Epoch 489/500:  train Loss: 17.3410   val Loss: 23.6062   time: 456.28s   best: 22.2212
2023-10-01 01:23:21,199:INFO:  Epoch 490/500:  train Loss: 17.3192   val Loss: 22.7341   time: 451.90s   best: 22.2212
2023-10-01 01:30:56,460:INFO:  Epoch 491/500:  train Loss: 17.3109   val Loss: 22.8036   time: 455.25s   best: 22.2212
2023-10-01 01:38:30,674:INFO:  Epoch 492/500:  train Loss: 17.2135   val Loss: 22.5974   time: 454.19s   best: 22.2212
2023-10-01 01:46:07,358:INFO:  Epoch 493/500:  train Loss: 17.3233   val Loss: 22.6481   time: 456.67s   best: 22.2212
2023-10-01 01:53:43,779:INFO:  Epoch 494/500:  train Loss: 17.3239   val Loss: 22.5046   time: 455.32s   best: 22.2212
2023-10-01 02:01:37,041:INFO:  Epoch 495/500:  train Loss: 17.3572   val Loss: 22.5761   time: 473.24s   best: 22.2212
2023-10-01 02:09:18,472:INFO:  Epoch 496/500:  train Loss: 17.3614   val Loss: 23.6248   time: 461.40s   best: 22.2212
2023-10-01 02:17:02,578:INFO:  Epoch 497/500:  train Loss: 17.3392   val Loss: 22.6801   time: 464.10s   best: 22.2212
2023-10-01 02:24:36,062:INFO:  Epoch 498/500:  train Loss: 17.4462   val Loss: 23.4813   time: 453.48s   best: 22.2212
2023-10-01 02:32:15,047:INFO:  Epoch 499/500:  train Loss: 17.7008   val Loss: 22.4745   time: 458.96s   best: 22.2212
2023-10-01 02:40:11,962:INFO:  Epoch 500/500:  train Loss: 17.4041   val Loss: 22.8828   time: 476.90s   best: 22.2212
2023-10-01 02:40:11,993:INFO:  -----> Training complete in 3889m 1s   best validation loss: 22.2212
 
2023-10-01 10:57:20,285:INFO:  Starting experiment lstm autoencoder bidirectional
2023-10-01 10:57:20,353:INFO:  Defining the model
2023-10-01 10:57:20,687:INFO:  Reading the dataset
2023-10-01 11:32:51,887:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 11:32:54,247:INFO:  Epoch 1/500:  train Loss: 74.3438   val Loss: 67.1123   time: 451.30s   best: 67.1123
2023-10-01 11:40:32,288:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 11:40:32,965:INFO:  Epoch 2/500:  train Loss: 63.2245   val Loss: 60.0088   time: 457.98s   best: 60.0088
2023-10-01 11:48:05,389:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 11:48:07,177:INFO:  Epoch 3/500:  train Loss: 57.1122   val Loss: 53.6380   time: 452.16s   best: 53.6380
2023-10-01 11:56:33,049:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 11:56:33,704:INFO:  Epoch 4/500:  train Loss: 51.7169   val Loss: 49.2046   time: 505.62s   best: 49.2046
2023-10-01 12:04:54,835:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 12:04:55,404:INFO:  Epoch 5/500:  train Loss: 47.3767   val Loss: 46.8508   time: 500.94s   best: 46.8508
2023-10-01 12:13:25,312:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 12:13:26,045:INFO:  Epoch 6/500:  train Loss: 43.9965   val Loss: 42.7390   time: 509.53s   best: 42.7390
2023-10-01 12:22:13,894:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 12:22:14,746:INFO:  Epoch 7/500:  train Loss: 41.4555   val Loss: 40.7855   time: 527.49s   best: 40.7855
2023-10-01 12:30:48,195:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 12:30:49,013:INFO:  Epoch 8/500:  train Loss: 39.3794   val Loss: 39.2354   time: 513.29s   best: 39.2354
2023-10-01 12:39:15,472:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 12:39:15,836:INFO:  Epoch 9/500:  train Loss: 37.6726   val Loss: 38.1563   time: 506.37s   best: 38.1563
2023-10-01 12:47:50,023:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 12:47:50,843:INFO:  Epoch 10/500:  train Loss: 36.2686   val Loss: 37.6968   time: 513.64s   best: 37.6968
2023-10-01 12:56:26,582:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 12:56:27,443:INFO:  Epoch 11/500:  train Loss: 35.1337   val Loss: 35.1780   time: 515.27s   best: 35.1780
2023-10-01 13:05:02,816:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 13:05:04,100:INFO:  Epoch 12/500:  train Loss: 34.1373   val Loss: 34.7946   time: 515.08s   best: 34.7946
2023-10-01 13:13:32,363:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 13:13:32,777:INFO:  Epoch 13/500:  train Loss: 33.2186   val Loss: 33.1403   time: 508.12s   best: 33.1403
2023-10-01 13:22:03,303:INFO:  Epoch 14/500:  train Loss: 32.4959   val Loss: 33.3076   time: 510.51s   best: 33.1403
2023-10-01 13:30:46,794:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 13:30:47,438:INFO:  Epoch 15/500:  train Loss: 31.8607   val Loss: 32.6645   time: 523.15s   best: 32.6645
2023-10-01 13:39:15,897:INFO:  Epoch 16/500:  train Loss: 31.2453   val Loss: 34.6906   time: 508.46s   best: 32.6645
2023-10-01 13:47:45,597:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 13:47:46,089:INFO:  Epoch 17/500:  train Loss: 30.8715   val Loss: 31.2868   time: 509.17s   best: 31.2868
2023-10-01 13:56:26,119:INFO:  Epoch 18/500:  train Loss: 30.8095   val Loss: 36.9331   time: 520.02s   best: 31.2868
2023-10-01 14:04:49,693:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 14:04:50,301:INFO:  Epoch 19/500:  train Loss: 29.8925   val Loss: 30.4726   time: 503.35s   best: 30.4726
2023-10-01 14:13:23,904:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 14:13:24,752:INFO:  Epoch 20/500:  train Loss: 29.6020   val Loss: 30.0620   time: 513.39s   best: 30.0620
2023-10-01 14:21:54,412:INFO:  Epoch 21/500:  train Loss: 29.2091   val Loss: 31.6230   time: 509.66s   best: 30.0620
2023-10-01 14:30:27,128:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 14:30:27,518:INFO:  Epoch 22/500:  train Loss: 28.7798   val Loss: 29.7881   time: 512.32s   best: 29.7881
2023-10-01 14:38:50,506:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 14:38:51,570:INFO:  Epoch 23/500:  train Loss: 28.4676   val Loss: 29.6523   time: 502.83s   best: 29.6523
2023-10-01 14:47:44,611:INFO:  Epoch 24/500:  train Loss: 27.9877   val Loss: 29.7116   time: 533.02s   best: 29.6523
2023-10-01 14:56:19,813:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 14:56:20,136:INFO:  Epoch 25/500:  train Loss: 27.9080   val Loss: 29.1127   time: 514.88s   best: 29.1127
2023-10-01 15:05:06,946:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 15:05:08,269:INFO:  Epoch 26/500:  train Loss: 27.6676   val Loss: 28.6963   time: 526.35s   best: 28.6963
2023-10-01 15:13:53,395:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 15:13:54,170:INFO:  Epoch 27/500:  train Loss: 27.1385   val Loss: 28.5823   time: 524.82s   best: 28.5823
2023-10-01 15:22:24,993:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 15:22:25,362:INFO:  Epoch 28/500:  train Loss: 26.8216   val Loss: 28.0934   time: 510.62s   best: 28.0934
2023-10-01 15:31:04,605:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 15:31:05,551:INFO:  Epoch 29/500:  train Loss: 26.5591   val Loss: 28.0306   time: 518.56s   best: 28.0306
2023-10-01 15:39:31,199:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 15:39:31,726:INFO:  Epoch 30/500:  train Loss: 26.6632   val Loss: 27.5011   time: 505.37s   best: 27.5011
2023-10-01 15:47:58,113:INFO:  Epoch 31/500:  train Loss: 26.2189   val Loss: 27.6518   time: 506.38s   best: 27.5011
2023-10-01 15:56:26,395:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 15:56:27,573:INFO:  Epoch 32/500:  train Loss: 25.9199   val Loss: 27.4271   time: 507.79s   best: 27.4271
2023-10-01 16:05:14,045:INFO:  Epoch 33/500:  train Loss: 25.8676   val Loss: 28.4367   time: 526.47s   best: 27.4271
2023-10-01 16:13:49,421:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 16:13:49,981:INFO:  Epoch 34/500:  train Loss: 25.6771   val Loss: 27.4152   time: 515.19s   best: 27.4152
2023-10-01 16:22:13,751:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 16:22:14,156:INFO:  Epoch 35/500:  train Loss: 25.3610   val Loss: 27.2828   time: 503.23s   best: 27.2828
2023-10-01 16:30:51,245:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 16:30:51,614:INFO:  Epoch 36/500:  train Loss: 25.2875   val Loss: 26.9608   time: 516.93s   best: 26.9608
2023-10-01 16:39:10,358:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 16:39:10,980:INFO:  Epoch 37/500:  train Loss: 25.2635   val Loss: 26.7198   time: 498.64s   best: 26.7198
2023-10-01 16:47:32,727:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 16:47:34,002:INFO:  Epoch 38/500:  train Loss: 24.9859   val Loss: 26.6326   time: 501.32s   best: 26.6326
2023-10-01 16:55:51,342:INFO:  Epoch 39/500:  train Loss: 24.8014   val Loss: 27.0938   time: 497.34s   best: 26.6326
2023-10-01 17:04:28,698:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 17:04:29,625:INFO:  Epoch 40/500:  train Loss: 24.5584   val Loss: 26.5463   time: 517.15s   best: 26.5463
2023-10-01 17:12:47,687:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 17:12:48,004:INFO:  Epoch 41/500:  train Loss: 24.6483   val Loss: 26.3492   time: 497.79s   best: 26.3492
2023-10-01 17:21:14,972:INFO:  Epoch 42/500:  train Loss: 24.3498   val Loss: 27.3518   time: 506.97s   best: 26.3492
2023-10-01 17:29:38,115:INFO:  Epoch 43/500:  train Loss: 24.3282   val Loss: 26.4351   time: 503.14s   best: 26.3492
2023-10-01 17:38:06,562:INFO:  Epoch 44/500:  train Loss: 24.1992   val Loss: 26.4232   time: 508.42s   best: 26.3492
2023-10-01 17:46:28,644:INFO:  Epoch 45/500:  train Loss: 24.2047   val Loss: 26.5371   time: 502.05s   best: 26.3492
2023-10-01 17:54:50,096:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 17:54:50,695:INFO:  Epoch 46/500:  train Loss: 23.8995   val Loss: 26.0366   time: 500.99s   best: 26.0366
2023-10-01 18:03:17,254:INFO:  Epoch 47/500:  train Loss: 23.9872   val Loss: 26.6610   time: 506.56s   best: 26.0366
2023-10-01 18:11:48,946:INFO:  Epoch 48/500:  train Loss: 23.8549   val Loss: 26.5889   time: 511.68s   best: 26.0366
2023-10-01 18:20:10,655:INFO:  Epoch 49/500:  train Loss: 23.8734   val Loss: 26.6450   time: 501.68s   best: 26.0366
2023-10-01 18:28:37,271:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 18:28:37,866:INFO:  Epoch 50/500:  train Loss: 23.5837   val Loss: 26.0187   time: 506.41s   best: 26.0187
2023-10-01 18:37:14,019:INFO:  Epoch 51/500:  train Loss: 23.4727   val Loss: 26.3121   time: 516.12s   best: 26.0187
2023-10-01 18:45:34,724:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 18:45:35,693:INFO:  Epoch 52/500:  train Loss: 23.3552   val Loss: 25.8941   time: 500.56s   best: 25.8941
2023-10-01 18:54:00,930:INFO:  Epoch 53/500:  train Loss: 23.4598   val Loss: 26.1589   time: 505.24s   best: 25.8941
2023-10-01 19:02:29,769:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 19:02:30,295:INFO:  Epoch 54/500:  train Loss: 23.2684   val Loss: 25.6039   time: 508.48s   best: 25.6039
2023-10-01 19:11:01,532:INFO:  Epoch 55/500:  train Loss: 23.1586   val Loss: 26.0599   time: 511.22s   best: 25.6039
2023-10-01 19:19:31,952:INFO:  Epoch 56/500:  train Loss: 23.0615   val Loss: 26.3093   time: 510.37s   best: 25.6039
2023-10-01 19:28:02,675:INFO:  Epoch 57/500:  train Loss: 22.9300   val Loss: 27.0220   time: 510.71s   best: 25.6039
2023-10-01 19:36:28,362:INFO:  Epoch 58/500:  train Loss: 23.0214   val Loss: 26.0057   time: 505.67s   best: 25.6039
2023-10-01 19:44:41,995:INFO:  Epoch 59/500:  train Loss: 22.8772   val Loss: 25.7673   time: 493.62s   best: 25.6039
2023-10-01 19:53:13,497:INFO:  Epoch 60/500:  train Loss: 22.8050   val Loss: 25.8325   time: 511.49s   best: 25.6039
2023-10-01 20:01:49,582:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 20:01:50,421:INFO:  Epoch 61/500:  train Loss: 23.1398   val Loss: 25.3559   time: 515.29s   best: 25.3559
2023-10-01 20:10:21,677:INFO:  Epoch 62/500:  train Loss: 22.7165   val Loss: 25.6739   time: 511.25s   best: 25.3559
2023-10-01 20:18:48,874:INFO:  Epoch 63/500:  train Loss: 22.4911   val Loss: 25.4733   time: 507.17s   best: 25.3559
2023-10-01 20:27:20,053:INFO:  Epoch 64/500:  train Loss: 22.7001   val Loss: 25.6861   time: 511.01s   best: 25.3559
2023-10-01 20:36:10,036:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 20:36:10,709:INFO:  Epoch 65/500:  train Loss: 22.6255   val Loss: 25.3436   time: 529.63s   best: 25.3436
2023-10-01 20:44:44,062:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 20:44:44,849:INFO:  Epoch 66/500:  train Loss: 22.3576   val Loss: 25.1915   time: 512.94s   best: 25.1915
2023-10-01 20:53:06,172:INFO:  Epoch 67/500:  train Loss: 22.3288   val Loss: 25.3389   time: 501.31s   best: 25.1915
2023-10-01 21:01:37,274:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 21:01:37,827:INFO:  Epoch 68/500:  train Loss: 22.6354   val Loss: 25.1423   time: 510.84s   best: 25.1423
2023-10-01 21:09:58,745:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 21:09:59,087:INFO:  Epoch 69/500:  train Loss: 22.1083   val Loss: 24.9909   time: 500.47s   best: 24.9909
2023-10-01 21:18:00,872:INFO:  Epoch 70/500:  train Loss: 22.1383   val Loss: 25.2131   time: 481.77s   best: 24.9909
2023-10-01 21:26:21,308:INFO:  Epoch 71/500:  train Loss: 22.1980   val Loss: 25.4631   time: 500.43s   best: 24.9909
2023-10-01 21:34:49,263:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-01 21:34:49,910:INFO:  Epoch 72/500:  train Loss: 21.9746   val Loss: 24.5315   time: 507.72s   best: 24.5315
2023-10-01 21:43:07,281:INFO:  Epoch 73/500:  train Loss: 22.0137   val Loss: 25.4998   time: 497.37s   best: 24.5315
2023-10-01 21:51:38,000:INFO:  Epoch 74/500:  train Loss: 22.0894   val Loss: 24.8400   time: 510.70s   best: 24.5315
2023-10-01 21:59:55,342:INFO:  Epoch 75/500:  train Loss: 21.8428   val Loss: 24.6580   time: 497.30s   best: 24.5315
2023-10-01 22:08:20,079:INFO:  Epoch 76/500:  train Loss: 22.0283   val Loss: 24.9615   time: 504.73s   best: 24.5315
2023-10-01 22:16:53,264:INFO:  Epoch 77/500:  train Loss: 21.9796   val Loss: 25.3129   time: 513.16s   best: 24.5315
2023-10-01 22:25:19,291:INFO:  Epoch 78/500:  train Loss: 21.6559   val Loss: 25.3118   time: 506.02s   best: 24.5315
2023-10-01 22:33:50,252:INFO:  Epoch 79/500:  train Loss: 21.6228   val Loss: 25.2755   time: 510.95s   best: 24.5315
2023-10-01 22:42:21,632:INFO:  Epoch 80/500:  train Loss: 21.7664   val Loss: 24.7926   time: 511.34s   best: 24.5315
2023-10-01 22:50:49,205:INFO:  Epoch 81/500:  train Loss: 21.5626   val Loss: 25.4826   time: 507.41s   best: 24.5315
2023-10-01 22:59:14,986:INFO:  Epoch 82/500:  train Loss: 21.5478   val Loss: 24.9222   time: 505.74s   best: 24.5315
2023-10-01 23:07:20,924:INFO:  Epoch 83/500:  train Loss: 21.6067   val Loss: 24.8766   time: 485.91s   best: 24.5315
2023-10-01 23:15:48,565:INFO:  Epoch 84/500:  train Loss: 21.4171   val Loss: 24.9162   time: 507.62s   best: 24.5315
2023-10-01 23:24:15,241:INFO:  Epoch 85/500:  train Loss: 21.5131   val Loss: 25.7883   time: 506.54s   best: 24.5315
2023-10-01 23:32:52,500:INFO:  Epoch 86/500:  train Loss: 21.3525   val Loss: 24.8152   time: 517.25s   best: 24.5315
2023-10-01 23:41:27,039:INFO:  Epoch 87/500:  train Loss: 21.3662   val Loss: 24.8075   time: 514.52s   best: 24.5315
2023-10-01 23:49:58,992:INFO:  Epoch 88/500:  train Loss: 21.4129   val Loss: 25.0340   time: 511.93s   best: 24.5315
2023-10-01 23:58:30,093:INFO:  Epoch 89/500:  train Loss: 21.3972   val Loss: 25.3138   time: 511.09s   best: 24.5315
2023-10-02 00:07:14,369:INFO:  Epoch 90/500:  train Loss: 21.2348   val Loss: 24.5562   time: 524.26s   best: 24.5315
2023-10-02 00:15:49,980:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-02 00:15:50,947:INFO:  Epoch 91/500:  train Loss: 21.1342   val Loss: 24.4690   time: 515.17s   best: 24.4690
2023-10-02 00:24:15,263:INFO:  Epoch 92/500:  train Loss: 21.2582   val Loss: 25.4950   time: 504.31s   best: 24.4690
2023-10-02 00:32:33,515:INFO:  Epoch 93/500:  train Loss: 21.3750   val Loss: 24.5106   time: 498.22s   best: 24.4690
2023-10-02 00:41:06,910:INFO:  Epoch 94/500:  train Loss: 21.2425   val Loss: 24.8869   time: 513.38s   best: 24.4690
2023-10-02 00:49:32,144:INFO:  Epoch 95/500:  train Loss: 21.2365   val Loss: 24.7947   time: 505.22s   best: 24.4690
2023-10-02 00:57:58,331:INFO:  Epoch 96/500:  train Loss: 21.2216   val Loss: 24.8206   time: 506.16s   best: 24.4690
2023-10-02 01:06:32,202:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-02 01:06:33,334:INFO:  Epoch 97/500:  train Loss: 20.9716   val Loss: 24.3241   time: 513.57s   best: 24.3241
2023-10-02 01:15:04,944:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-02 01:15:05,785:INFO:  Epoch 98/500:  train Loss: 21.4037   val Loss: 24.1564   time: 510.94s   best: 24.1564
2023-10-02 01:23:49,948:INFO:  Epoch 99/500:  train Loss: 21.1125   val Loss: 24.6895   time: 524.15s   best: 24.1564
2023-10-02 01:32:46,048:INFO:  Epoch 100/500:  train Loss: 21.1102   val Loss: 25.9033   time: 535.87s   best: 24.1564
2023-10-02 01:41:23,350:INFO:  Epoch 101/500:  train Loss: 20.8591   val Loss: 24.2624   time: 517.27s   best: 24.1564
2023-10-02 01:50:06,860:INFO:  Epoch 102/500:  train Loss: 20.8747   val Loss: 24.8505   time: 523.49s   best: 24.1564
2023-10-02 01:59:10,130:INFO:  Epoch 103/500:  train Loss: 21.2736   val Loss: 25.9192   time: 543.26s   best: 24.1564
2023-10-02 02:08:09,169:INFO:  Epoch 104/500:  train Loss: 20.8832   val Loss: 24.1981   time: 539.02s   best: 24.1564
2023-10-02 02:16:41,807:INFO:  Epoch 105/500:  train Loss: 20.9251   val Loss: 24.7046   time: 512.59s   best: 24.1564
2023-10-02 02:25:04,104:INFO:  Epoch 106/500:  train Loss: 20.8434   val Loss: 24.9462   time: 502.28s   best: 24.1564
2023-10-02 02:33:28,428:INFO:  Epoch 107/500:  train Loss: 20.7871   val Loss: 24.5573   time: 504.29s   best: 24.1564
2023-10-02 02:41:54,893:INFO:  Epoch 108/500:  train Loss: 20.7195   val Loss: 24.5748   time: 506.44s   best: 24.1564
2023-10-02 02:50:36,515:INFO:  Epoch 109/500:  train Loss: 21.0366   val Loss: 24.5689   time: 521.61s   best: 24.1564
2023-10-02 02:59:12,625:INFO:  Epoch 110/500:  train Loss: 20.6698   val Loss: 24.3219   time: 516.09s   best: 24.1564
2023-10-02 03:07:39,045:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-02 03:07:39,759:INFO:  Epoch 111/500:  train Loss: 20.6918   val Loss: 24.0169   time: 506.18s   best: 24.0169
2023-10-02 03:16:13,222:INFO:  Epoch 112/500:  train Loss: 20.6472   val Loss: 24.5356   time: 513.46s   best: 24.0169
2023-10-02 03:24:54,190:INFO:  Epoch 113/500:  train Loss: 20.7311   val Loss: 24.1532   time: 520.93s   best: 24.0169
2023-10-02 03:33:37,068:INFO:  Epoch 114/500:  train Loss: 20.6005   val Loss: 24.3626   time: 522.85s   best: 24.0169
2023-10-02 03:42:24,416:INFO:  Epoch 115/500:  train Loss: 20.6208   val Loss: 25.8720   time: 527.34s   best: 24.0169
2023-10-02 03:50:55,328:INFO:  Epoch 116/500:  train Loss: 20.4961   val Loss: 24.2870   time: 510.90s   best: 24.0169
2023-10-02 03:59:13,869:INFO:  Epoch 117/500:  train Loss: 20.4587   val Loss: 24.9554   time: 498.52s   best: 24.0169
2023-10-02 04:07:53,787:INFO:  Epoch 118/500:  train Loss: 20.4937   val Loss: 24.4088   time: 519.89s   best: 24.0169
2023-10-02 04:16:29,715:INFO:  Epoch 119/500:  train Loss: 20.6030   val Loss: 26.3245   time: 515.92s   best: 24.0169
2023-10-02 04:24:54,672:INFO:  Epoch 120/500:  train Loss: 20.6105   val Loss: 24.2515   time: 504.93s   best: 24.0169
2023-10-02 04:33:35,451:INFO:  Epoch 121/500:  train Loss: 20.3593   val Loss: 24.3677   time: 520.76s   best: 24.0169
2023-10-02 04:41:59,961:INFO:  Epoch 122/500:  train Loss: 20.3728   val Loss: 24.4883   time: 504.48s   best: 24.0169
2023-10-02 04:50:21,581:INFO:  Epoch 123/500:  train Loss: 20.2522   val Loss: 24.1983   time: 501.57s   best: 24.0169
2023-10-02 04:58:43,576:INFO:  Epoch 124/500:  train Loss: 20.4570   val Loss: 24.0402   time: 501.97s   best: 24.0169
2023-10-02 05:07:12,994:INFO:  Epoch 125/500:  train Loss: 20.8463   val Loss: 24.1815   time: 509.40s   best: 24.0169
2023-10-02 05:15:36,025:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-02 05:15:36,399:INFO:  Epoch 126/500:  train Loss: 20.4031   val Loss: 23.9692   time: 502.81s   best: 23.9692
2023-10-02 05:23:56,755:INFO:  Epoch 127/500:  train Loss: 20.2041   val Loss: 24.1699   time: 500.34s   best: 23.9692
2023-10-02 05:32:11,026:INFO:  Epoch 128/500:  train Loss: 20.3648   val Loss: 24.2892   time: 494.24s   best: 23.9692
2023-10-02 05:40:46,378:INFO:  Epoch 129/500:  train Loss: 20.1308   val Loss: 24.1026   time: 515.32s   best: 23.9692
2023-10-02 05:49:10,266:INFO:  Epoch 130/500:  train Loss: 20.1858   val Loss: 24.2678   time: 503.85s   best: 23.9692
2023-10-02 05:57:36,089:INFO:  Epoch 131/500:  train Loss: 20.4374   val Loss: 24.0235   time: 505.81s   best: 23.9692
2023-10-02 06:06:11,938:INFO:  Epoch 132/500:  train Loss: 20.0704   val Loss: 27.8028   time: 515.80s   best: 23.9692
2023-10-02 06:14:47,681:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-02 06:14:48,381:INFO:  Epoch 133/500:  train Loss: 20.1916   val Loss: 23.7943   time: 515.06s   best: 23.7943
2023-10-02 06:23:20,584:INFO:  Epoch 134/500:  train Loss: 20.1513   val Loss: 24.2287   time: 512.20s   best: 23.7943
2023-10-02 06:31:55,355:INFO:  Epoch 135/500:  train Loss: 20.1924   val Loss: 24.1125   time: 514.75s   best: 23.7943
2023-10-02 06:40:32,829:INFO:  Epoch 136/500:  train Loss: 20.1421   val Loss: 23.9197   time: 517.44s   best: 23.7943
2023-10-02 06:49:06,681:INFO:  Epoch 137/500:  train Loss: 20.0390   val Loss: 24.4190   time: 513.83s   best: 23.7943
2023-10-02 06:57:33,259:INFO:  Epoch 138/500:  train Loss: 19.9666   val Loss: 24.1296   time: 506.57s   best: 23.7943
2023-10-02 07:05:52,816:INFO:  Epoch 139/500:  train Loss: 20.7126   val Loss: 24.0676   time: 499.52s   best: 23.7943
2023-10-02 07:14:13,152:INFO:  Epoch 140/500:  train Loss: 20.1742   val Loss: 24.2150   time: 500.33s   best: 23.7943
2023-10-02 07:22:46,202:INFO:  Epoch 141/500:  train Loss: 20.1126   val Loss: 24.2361   time: 513.03s   best: 23.7943
2023-10-02 07:31:13,851:INFO:  Epoch 142/500:  train Loss: 20.1782   val Loss: 24.0067   time: 507.64s   best: 23.7943
2023-10-02 07:39:42,747:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-02 07:39:43,379:INFO:  Epoch 143/500:  train Loss: 19.9101   val Loss: 23.5031   time: 508.72s   best: 23.5031
2023-10-02 07:48:19,787:INFO:  Epoch 144/500:  train Loss: 19.8970   val Loss: 23.9272   time: 516.40s   best: 23.5031
2023-10-02 07:56:56,472:INFO:  Epoch 145/500:  train Loss: 19.9146   val Loss: 24.1143   time: 516.64s   best: 23.5031
2023-10-02 08:05:21,588:INFO:  Epoch 146/500:  train Loss: 19.9767   val Loss: 23.8456   time: 505.10s   best: 23.5031
2023-10-02 08:13:59,952:INFO:  Epoch 147/500:  train Loss: 19.9786   val Loss: 26.7614   time: 518.33s   best: 23.5031
2023-10-02 08:22:25,901:INFO:  Epoch 148/500:  train Loss: 19.9947   val Loss: 24.3396   time: 505.93s   best: 23.5031
2023-10-02 08:31:15,807:INFO:  Epoch 149/500:  train Loss: 20.0625   val Loss: 24.0750   time: 529.89s   best: 23.5031
2023-10-02 08:39:44,930:INFO:  Epoch 150/500:  train Loss: 19.9961   val Loss: 26.0404   time: 509.10s   best: 23.5031
2023-10-02 08:48:36,280:INFO:  Epoch 151/500:  train Loss: 20.2698   val Loss: 24.1571   time: 531.34s   best: 23.5031
2023-10-02 08:57:06,322:INFO:  Epoch 152/500:  train Loss: 19.8311   val Loss: 23.6736   time: 510.03s   best: 23.5031
2023-10-02 09:05:48,515:INFO:  Epoch 153/500:  train Loss: 20.1447   val Loss: 24.0163   time: 522.18s   best: 23.5031
2023-10-02 09:14:25,142:INFO:  Epoch 154/500:  train Loss: 19.8546   val Loss: 23.7608   time: 516.61s   best: 23.5031
2023-10-02 09:22:54,480:INFO:  Epoch 155/500:  train Loss: 19.8986   val Loss: 26.4559   time: 509.32s   best: 23.5031
2023-10-02 09:31:26,521:INFO:  Epoch 156/500:  train Loss: 19.8018   val Loss: 27.4005   time: 511.86s   best: 23.5031
2023-10-02 09:40:13,179:INFO:  Epoch 157/500:  train Loss: 19.8146   val Loss: 23.8201   time: 526.64s   best: 23.5031
2023-10-02 09:48:46,913:INFO:  Epoch 158/500:  train Loss: 19.6907   val Loss: 24.0918   time: 513.70s   best: 23.5031
2023-10-02 09:57:25,775:INFO:  Epoch 159/500:  train Loss: 19.8005   val Loss: 23.7984   time: 518.85s   best: 23.5031
2023-10-02 10:06:15,365:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-02 10:06:18,694:INFO:  Epoch 160/500:  train Loss: 19.8104   val Loss: 23.3516   time: 529.53s   best: 23.3516
2023-10-02 10:15:10,772:INFO:  Epoch 161/500:  train Loss: 19.7045   val Loss: 23.7144   time: 532.06s   best: 23.3516
2023-10-02 10:23:53,945:INFO:  Epoch 162/500:  train Loss: 19.7408   val Loss: 23.7179   time: 523.16s   best: 23.3516
2023-10-02 10:32:38,748:INFO:  Epoch 163/500:  train Loss: 19.5981   val Loss: 24.2813   time: 524.76s   best: 23.3516
2023-10-02 10:41:14,541:INFO:  Epoch 164/500:  train Loss: 19.6854   val Loss: 23.6880   time: 515.77s   best: 23.3516
2023-10-02 10:49:57,475:INFO:  Epoch 165/500:  train Loss: 19.6370   val Loss: 23.9913   time: 522.91s   best: 23.3516
2023-10-02 10:58:31,186:INFO:  Epoch 166/500:  train Loss: 19.6246   val Loss: 24.0959   time: 513.70s   best: 23.3516
2023-10-02 11:07:00,559:INFO:  Epoch 167/500:  train Loss: 19.5533   val Loss: 23.4941   time: 509.35s   best: 23.3516
2023-10-02 11:15:35,632:INFO:  Epoch 168/500:  train Loss: 19.7163   val Loss: 26.1564   time: 515.05s   best: 23.3516
2023-10-02 11:24:17,014:INFO:  Epoch 169/500:  train Loss: 19.9064   val Loss: 24.0610   time: 521.37s   best: 23.3516
2023-10-02 11:32:55,313:INFO:  Epoch 170/500:  train Loss: 19.5514   val Loss: 24.0322   time: 518.27s   best: 23.3516
2023-10-02 11:41:37,419:INFO:  Epoch 171/500:  train Loss: 19.6099   val Loss: 23.5658   time: 522.06s   best: 23.3516
2023-10-02 11:50:21,113:INFO:  Epoch 172/500:  train Loss: 19.4402   val Loss: 23.8109   time: 523.66s   best: 23.3516
2023-10-02 11:59:07,423:INFO:  Epoch 173/500:  train Loss: 19.6731   val Loss: 24.4588   time: 526.29s   best: 23.3516
2023-10-02 12:07:36,331:INFO:  Epoch 174/500:  train Loss: 19.4399   val Loss: 24.1877   time: 508.88s   best: 23.3516
2023-10-02 12:16:13,111:INFO:  Epoch 175/500:  train Loss: 19.5129   val Loss: 23.8907   time: 516.77s   best: 23.3516
2023-10-02 12:24:55,382:INFO:  Epoch 176/500:  train Loss: 19.4883   val Loss: 23.8025   time: 522.26s   best: 23.3516
2023-10-02 12:33:36,469:INFO:  Epoch 177/500:  train Loss: 19.4563   val Loss: 28.7247   time: 521.07s   best: 23.3516
2023-10-02 12:42:10,650:INFO:  Epoch 178/500:  train Loss: 19.8515   val Loss: 24.0774   time: 514.15s   best: 23.3516
2023-10-02 12:51:08,904:INFO:  Epoch 179/500:  train Loss: 19.5281   val Loss: 23.9258   time: 538.24s   best: 23.3516
2023-10-02 12:59:29,682:INFO:  Epoch 180/500:  train Loss: 19.5142   val Loss: 23.7166   time: 500.75s   best: 23.3516
2023-10-02 13:08:01,154:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-02 13:08:01,727:INFO:  Epoch 181/500:  train Loss: 19.2989   val Loss: 23.1727   time: 511.32s   best: 23.1727
2023-10-02 13:16:26,019:INFO:  Epoch 182/500:  train Loss: 19.3685   val Loss: 23.5207   time: 504.28s   best: 23.1727
2023-10-02 13:25:10,269:INFO:  Epoch 183/500:  train Loss: 19.4837   val Loss: 23.7987   time: 524.22s   best: 23.1727
2023-10-02 13:34:38,641:INFO:  Epoch 184/500:  train Loss: 19.4953   val Loss: 24.0718   time: 568.35s   best: 23.1727
2023-10-02 13:43:43,692:INFO:  Epoch 185/500:  train Loss: 19.7433   val Loss: 23.8065   time: 545.01s   best: 23.1727
2023-10-02 13:53:08,099:INFO:  Epoch 186/500:  train Loss: 19.2985   val Loss: 23.9885   time: 564.39s   best: 23.1727
2023-10-02 14:02:39,962:INFO:  Epoch 187/500:  train Loss: 19.4714   val Loss: 23.7392   time: 571.85s   best: 23.1727
2023-10-02 14:11:54,849:INFO:  Epoch 188/500:  train Loss: 19.1874   val Loss: 23.5204   time: 554.87s   best: 23.1727
2023-10-02 14:21:18,309:INFO:  Epoch 189/500:  train Loss: 19.2407   val Loss: 23.8255   time: 563.45s   best: 23.1727
2023-10-02 14:30:46,818:INFO:  Epoch 190/500:  train Loss: 19.1600   val Loss: 24.2954   time: 568.50s   best: 23.1727
2023-10-02 14:39:54,840:INFO:  Epoch 191/500:  train Loss: 19.2496   val Loss: 23.8443   time: 547.98s   best: 23.1727
2023-10-02 14:49:12,877:INFO:  Epoch 192/500:  train Loss: 19.2121   val Loss: 24.0960   time: 558.00s   best: 23.1727
2023-10-02 14:58:33,001:INFO:  Epoch 193/500:  train Loss: 19.3665   val Loss: 24.1933   time: 560.12s   best: 23.1727
2023-10-02 15:07:47,056:INFO:  Epoch 194/500:  train Loss: 19.2866   val Loss: 23.7583   time: 554.03s   best: 23.1727
2023-10-02 15:17:17,922:INFO:  Epoch 195/500:  train Loss: 19.3422   val Loss: 24.3816   time: 570.79s   best: 23.1727
2023-10-02 15:26:22,909:INFO:  Epoch 196/500:  train Loss: 19.0913   val Loss: 24.1873   time: 544.94s   best: 23.1727
2023-10-02 15:35:23,352:INFO:  Epoch 197/500:  train Loss: 19.1086   val Loss: 23.5596   time: 540.43s   best: 23.1727
2023-10-02 15:44:46,292:INFO:  Epoch 198/500:  train Loss: 19.0880   val Loss: 23.5963   time: 562.90s   best: 23.1727
2023-10-02 15:53:55,797:INFO:  Epoch 199/500:  train Loss: 19.1364   val Loss: 23.5939   time: 549.46s   best: 23.1727
2023-10-02 16:03:01,027:INFO:  Epoch 200/500:  train Loss: 19.0492   val Loss: 23.9363   time: 545.22s   best: 23.1727
2023-10-02 16:12:10,841:INFO:  Epoch 201/500:  train Loss: 19.2510   val Loss: 23.4147   time: 549.78s   best: 23.1727
2023-10-02 16:21:23,390:INFO:  Epoch 202/500:  train Loss: 19.2095   val Loss: 23.3483   time: 552.48s   best: 23.1727
2023-10-02 16:30:47,602:INFO:  Epoch 203/500:  train Loss: 19.2376   val Loss: 23.8089   time: 564.18s   best: 23.1727
2023-10-02 16:39:58,880:INFO:  Epoch 204/500:  train Loss: 19.2517   val Loss: 23.8282   time: 551.25s   best: 23.1727
2023-10-02 16:48:57,001:INFO:  Epoch 205/500:  train Loss: 18.9737   val Loss: 24.1859   time: 538.10s   best: 23.1727
2023-10-02 16:57:53,906:INFO:  Epoch 206/500:  train Loss: 18.9012   val Loss: 23.6859   time: 536.75s   best: 23.1727
2023-10-02 17:06:59,456:INFO:  Epoch 207/500:  train Loss: 19.1046   val Loss: 23.6068   time: 545.54s   best: 23.1727
2023-10-02 17:15:52,621:INFO:  Epoch 208/500:  train Loss: 19.1553   val Loss: 23.5882   time: 533.13s   best: 23.1727
2023-10-02 17:24:53,811:INFO:  Epoch 209/500:  train Loss: 19.0424   val Loss: 23.9239   time: 541.17s   best: 23.1727
2023-10-02 17:33:58,208:INFO:  Epoch 210/500:  train Loss: 19.0878   val Loss: 24.0252   time: 544.35s   best: 23.1727
2023-10-02 17:42:55,865:INFO:  Epoch 211/500:  train Loss: 19.0550   val Loss: 23.8038   time: 537.64s   best: 23.1727
2023-10-02 17:52:21,734:INFO:  Epoch 212/500:  train Loss: 18.9249   val Loss: 23.8004   time: 565.81s   best: 23.1727
2023-10-02 18:01:25,146:INFO:  Epoch 213/500:  train Loss: 18.8817   val Loss: 23.3556   time: 543.40s   best: 23.1727
2023-10-02 18:10:42,142:INFO:  Epoch 214/500:  train Loss: 19.5432   val Loss: 23.9345   time: 556.98s   best: 23.1727
2023-10-02 18:19:39,415:INFO:  Epoch 215/500:  train Loss: 19.0512   val Loss: 23.2248   time: 537.25s   best: 23.1727
2023-10-02 18:28:37,875:INFO:  Epoch 216/500:  train Loss: 18.9361   val Loss: 23.2711   time: 538.44s   best: 23.1727
2023-10-02 18:37:55,792:INFO:  Epoch 217/500:  train Loss: 19.0138   val Loss: 23.3056   time: 557.90s   best: 23.1727
2023-10-02 18:47:01,512:INFO:  Epoch 218/500:  train Loss: 18.9480   val Loss: 24.0886   time: 545.71s   best: 23.1727
2023-10-02 18:56:26,403:INFO:  Epoch 219/500:  train Loss: 18.8832   val Loss: 23.7001   time: 564.88s   best: 23.1727
2023-10-02 19:05:27,236:INFO:  Epoch 220/500:  train Loss: 18.8506   val Loss: 23.3707   time: 540.79s   best: 23.1727
2023-10-02 19:14:42,654:INFO:  Epoch 221/500:  train Loss: 19.0385   val Loss: 23.7357   time: 555.38s   best: 23.1727
2023-10-02 19:23:35,977:INFO:  Epoch 222/500:  train Loss: 19.7430   val Loss: 23.5596   time: 533.31s   best: 23.1727
2023-10-02 19:32:31,361:INFO:  Epoch 223/500:  train Loss: 19.1290   val Loss: 23.3086   time: 535.37s   best: 23.1727
2023-10-02 19:41:30,290:INFO:  Epoch 224/500:  train Loss: 18.9831   val Loss: 23.7194   time: 538.90s   best: 23.1727
2023-10-02 19:50:42,047:INFO:  Epoch 225/500:  train Loss: 18.9264   val Loss: 23.4914   time: 551.74s   best: 23.1727
2023-10-02 19:59:43,667:INFO:  Epoch 226/500:  train Loss: 18.9591   val Loss: 23.6154   time: 541.61s   best: 23.1727
2023-10-02 20:08:50,444:INFO:  Epoch 227/500:  train Loss: 18.9171   val Loss: 23.6597   time: 546.77s   best: 23.1727
2023-10-02 20:17:54,548:INFO:  Epoch 228/500:  train Loss: 19.0887   val Loss: 24.1711   time: 544.06s   best: 23.1727
2023-10-02 20:27:10,621:INFO:  Epoch 229/500:  train Loss: 19.0236   val Loss: 23.7418   time: 556.04s   best: 23.1727
2023-10-02 20:36:03,026:INFO:  Epoch 230/500:  train Loss: 19.0108   val Loss: 23.7780   time: 532.38s   best: 23.1727
2023-10-02 20:44:57,862:INFO:  Epoch 231/500:  train Loss: 18.8836   val Loss: 23.7618   time: 534.83s   best: 23.1727
2023-10-02 20:54:15,197:INFO:  Epoch 232/500:  train Loss: 18.8209   val Loss: 23.5618   time: 557.14s   best: 23.1727
2023-10-02 21:03:14,042:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-02 21:03:14,617:INFO:  Epoch 233/500:  train Loss: 18.9720   val Loss: 22.9850   time: 538.19s   best: 22.9850
2023-10-02 21:12:05,151:INFO:  Epoch 234/500:  train Loss: 18.8891   val Loss: 23.5751   time: 530.52s   best: 22.9850
2023-10-02 21:21:14,346:INFO:  Epoch 235/500:  train Loss: 18.8662   val Loss: 23.8428   time: 549.14s   best: 22.9850
2023-10-02 21:30:08,038:INFO:  Epoch 236/500:  train Loss: 18.7615   val Loss: 23.4717   time: 533.65s   best: 22.9850
2023-10-02 21:39:02,694:INFO:  Epoch 237/500:  train Loss: 19.1901   val Loss: 23.8393   time: 534.65s   best: 22.9850
2023-10-02 21:48:05,420:INFO:  Epoch 238/500:  train Loss: 18.9813   val Loss: 23.2686   time: 542.69s   best: 22.9850
2023-10-02 21:56:55,071:INFO:  Epoch 239/500:  train Loss: 18.7440   val Loss: 23.3430   time: 529.63s   best: 22.9850
2023-10-02 22:05:53,127:INFO:  Epoch 240/500:  train Loss: 18.7430   val Loss: 23.6568   time: 538.04s   best: 22.9850
2023-10-02 22:14:41,010:INFO:  Epoch 241/500:  train Loss: 18.9232   val Loss: 23.7659   time: 527.88s   best: 22.9850
2023-10-02 22:23:56,038:INFO:  Epoch 242/500:  train Loss: 18.6525   val Loss: 23.7925   time: 555.00s   best: 22.9850
2023-10-02 22:32:51,953:INFO:  Epoch 243/500:  train Loss: 18.7401   val Loss: 23.4737   time: 535.89s   best: 22.9850
2023-10-02 22:41:53,451:INFO:  Epoch 244/500:  train Loss: 18.6818   val Loss: 23.5866   time: 541.47s   best: 22.9850
2023-10-02 22:51:17,917:INFO:  Epoch 245/500:  train Loss: 18.8284   val Loss: 23.7732   time: 564.44s   best: 22.9850
2023-10-02 23:00:42,495:INFO:  Epoch 246/500:  train Loss: 18.9050   val Loss: 23.9965   time: 564.57s   best: 22.9850
2023-10-02 23:09:33,947:INFO:  Epoch 247/500:  train Loss: 18.8553   val Loss: 23.6808   time: 531.38s   best: 22.9850
2023-10-02 23:18:45,390:INFO:  Epoch 248/500:  train Loss: 18.6805   val Loss: 23.5082   time: 551.43s   best: 22.9850
2023-10-02 23:27:56,049:INFO:  Epoch 249/500:  train Loss: 18.6843   val Loss: 23.6800   time: 550.61s   best: 22.9850
2023-10-02 23:36:54,547:INFO:  Epoch 250/500:  train Loss: 18.6266   val Loss: 23.8682   time: 538.48s   best: 22.9850
2023-10-02 23:45:47,098:INFO:  Epoch 251/500:  train Loss: 19.4135   val Loss: 23.4431   time: 532.53s   best: 22.9850
2023-10-02 23:54:54,131:INFO:  Epoch 252/500:  train Loss: 18.9654   val Loss: 24.0063   time: 547.02s   best: 22.9850
2023-10-03 00:03:48,476:INFO:  Epoch 253/500:  train Loss: 18.5856   val Loss: 23.2840   time: 534.31s   best: 22.9850
2023-10-03 00:12:41,368:INFO:  Epoch 254/500:  train Loss: 18.5753   val Loss: 23.4691   time: 532.89s   best: 22.9850
2023-10-03 00:21:29,383:INFO:  Epoch 255/500:  train Loss: 18.5738   val Loss: 23.7285   time: 528.00s   best: 22.9850
2023-10-03 00:30:29,578:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-03 00:30:30,131:INFO:  Epoch 256/500:  train Loss: 18.9720   val Loss: 22.9201   time: 538.92s   best: 22.9201
2023-10-03 00:39:33,437:INFO:  Epoch 257/500:  train Loss: 18.6070   val Loss: 23.7263   time: 543.30s   best: 22.9201
2023-10-03 00:48:26,251:INFO:  Epoch 258/500:  train Loss: 18.5486   val Loss: 23.9967   time: 532.81s   best: 22.9201
2023-10-03 00:57:27,039:INFO:  Epoch 259/500:  train Loss: 18.6177   val Loss: 23.4882   time: 540.75s   best: 22.9201
2023-10-03 01:06:22,725:INFO:  Epoch 260/500:  train Loss: 18.6650   val Loss: 23.5673   time: 535.65s   best: 22.9201
2023-10-03 01:15:10,970:INFO:  Epoch 261/500:  train Loss: 18.7623   val Loss: 24.1293   time: 528.23s   best: 22.9201
2023-10-03 01:24:00,244:INFO:  Epoch 262/500:  train Loss: 18.5345   val Loss: 23.1395   time: 529.26s   best: 22.9201
2023-10-03 01:33:10,655:INFO:  Epoch 263/500:  train Loss: 18.6820   val Loss: 25.4266   time: 550.39s   best: 22.9201
2023-10-03 01:41:56,474:INFO:  Epoch 264/500:  train Loss: 18.5080   val Loss: 23.1714   time: 525.80s   best: 22.9201
2023-10-03 01:50:50,554:INFO:  Epoch 265/500:  train Loss: 18.4928   val Loss: 24.4969   time: 534.05s   best: 22.9201
2023-10-03 02:00:09,930:INFO:  Epoch 266/500:  train Loss: 18.6583   val Loss: 23.9338   time: 559.36s   best: 22.9201
2023-10-03 02:09:02,699:INFO:  Epoch 267/500:  train Loss: 18.7848   val Loss: 27.0391   time: 532.74s   best: 22.9201
2023-10-03 02:17:57,961:INFO:  Epoch 268/500:  train Loss: 18.7430   val Loss: 23.6281   time: 535.23s   best: 22.9201
2023-10-03 02:26:48,841:INFO:  Epoch 269/500:  train Loss: 18.4448   val Loss: 23.3973   time: 530.85s   best: 22.9201
2023-10-03 02:36:09,929:INFO:  Epoch 270/500:  train Loss: 18.6761   val Loss: 23.6904   time: 561.07s   best: 22.9201
2023-10-03 02:45:12,171:INFO:  Epoch 271/500:  train Loss: 18.5109   val Loss: 23.2769   time: 542.23s   best: 22.9201
2023-10-03 02:54:22,192:INFO:  Epoch 272/500:  train Loss: 19.0182   val Loss: 25.7463   time: 549.94s   best: 22.9201
2023-10-03 03:03:25,684:INFO:  Epoch 273/500:  train Loss: 18.6049   val Loss: 23.6573   time: 543.46s   best: 22.9201
2023-10-03 03:12:18,066:INFO:  Epoch 274/500:  train Loss: 18.9686   val Loss: 23.9632   time: 532.37s   best: 22.9201
2023-10-03 03:21:18,080:INFO:  Epoch 275/500:  train Loss: 18.6666   val Loss: 23.3394   time: 539.97s   best: 22.9201
2023-10-03 03:30:11,156:INFO:  Epoch 276/500:  train Loss: 18.5470   val Loss: 23.4253   time: 533.05s   best: 22.9201
2023-10-03 03:39:09,031:INFO:  Epoch 277/500:  train Loss: 18.5416   val Loss: 23.5131   time: 537.86s   best: 22.9201
2023-10-03 03:48:14,584:INFO:  Epoch 278/500:  train Loss: 18.4400   val Loss: 23.5081   time: 545.53s   best: 22.9201
2023-10-03 03:57:11,550:INFO:  Epoch 279/500:  train Loss: 18.6689   val Loss: 23.5537   time: 536.91s   best: 22.9201
2023-10-03 04:06:08,321:INFO:  Epoch 280/500:  train Loss: 18.4638   val Loss: 23.8139   time: 536.76s   best: 22.9201
2023-10-03 04:15:10,196:INFO:  Epoch 281/500:  train Loss: 18.4075   val Loss: 23.9250   time: 541.85s   best: 22.9201
2023-10-03 04:24:01,623:INFO:  Epoch 282/500:  train Loss: 18.3520   val Loss: 23.5760   time: 531.38s   best: 22.9201
2023-10-03 04:32:53,437:INFO:  Epoch 283/500:  train Loss: 18.3903   val Loss: 22.9597   time: 531.79s   best: 22.9201
2023-10-03 04:41:58,080:INFO:  Epoch 284/500:  train Loss: 18.3934   val Loss: 23.9505   time: 544.63s   best: 22.9201
2023-10-03 04:50:59,932:INFO:  Epoch 285/500:  train Loss: 18.6048   val Loss: 23.6592   time: 541.81s   best: 22.9201
2023-10-03 04:59:57,485:INFO:  Epoch 286/500:  train Loss: 18.5201   val Loss: 23.2959   time: 537.53s   best: 22.9201
2023-10-03 05:08:47,526:INFO:  Epoch 287/500:  train Loss: 18.3239   val Loss: 24.7888   time: 530.03s   best: 22.9201
2023-10-03 05:17:50,955:INFO:  Epoch 288/500:  train Loss: 19.2023   val Loss: 23.2699   time: 543.42s   best: 22.9201
2023-10-03 05:26:39,699:INFO:  Epoch 289/500:  train Loss: 18.3895   val Loss: 23.1455   time: 528.58s   best: 22.9201
2023-10-03 05:35:26,347:INFO:  Epoch 290/500:  train Loss: 18.3849   val Loss: 23.2634   time: 526.63s   best: 22.9201
2023-10-03 05:44:16,480:INFO:  Epoch 291/500:  train Loss: 18.6845   val Loss: 23.4758   time: 530.10s   best: 22.9201
2023-10-03 05:53:13,904:INFO:  Epoch 292/500:  train Loss: 18.4553   val Loss: 25.3571   time: 537.41s   best: 22.9201
2023-10-03 06:02:10,605:INFO:  Epoch 293/500:  train Loss: 18.5524   val Loss: 23.7790   time: 536.68s   best: 22.9201
2023-10-03 06:11:06,985:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-03 06:11:07,554:INFO:  Epoch 294/500:  train Loss: 18.8769   val Loss: 22.8470   time: 536.09s   best: 22.8470
2023-10-03 06:20:06,239:INFO:  Epoch 295/500:  train Loss: 18.6173   val Loss: 22.9410   time: 538.68s   best: 22.8470
2023-10-03 06:29:04,854:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-03 06:29:06,139:INFO:  Epoch 296/500:  train Loss: 18.3491   val Loss: 22.5551   time: 538.20s   best: 22.5551
2023-10-03 06:37:56,179:INFO:  Epoch 297/500:  train Loss: 18.3222   val Loss: 24.0591   time: 530.04s   best: 22.5551
2023-10-03 06:47:10,824:INFO:  Epoch 298/500:  train Loss: 18.8748   val Loss: 24.3858   time: 554.45s   best: 22.5551
2023-10-03 06:56:14,808:INFO:  Epoch 299/500:  train Loss: 18.4077   val Loss: 23.1969   time: 543.97s   best: 22.5551
2023-10-03 07:05:34,465:INFO:  Epoch 300/500:  train Loss: 18.2763   val Loss: 23.7353   time: 559.64s   best: 22.5551
2023-10-03 07:14:41,709:INFO:  Epoch 301/500:  train Loss: 18.5507   val Loss: 23.5592   time: 547.23s   best: 22.5551
2023-10-03 07:23:41,879:INFO:  Epoch 302/500:  train Loss: 18.2750   val Loss: 23.9963   time: 540.14s   best: 22.5551
2023-10-03 07:32:56,990:INFO:  Epoch 303/500:  train Loss: 18.2050   val Loss: 23.5117   time: 555.10s   best: 22.5551
2023-10-03 07:42:20,622:INFO:  Epoch 304/500:  train Loss: 18.4259   val Loss: 23.3774   time: 563.62s   best: 22.5551
2023-10-03 07:51:44,429:INFO:  Epoch 305/500:  train Loss: 18.2913   val Loss: 23.5307   time: 563.75s   best: 22.5551
2023-10-03 08:01:01,713:INFO:  Epoch 306/500:  train Loss: 18.3369   val Loss: 24.1807   time: 557.26s   best: 22.5551
2023-10-03 08:10:27,709:INFO:  Epoch 307/500:  train Loss: 18.4776   val Loss: 23.4955   time: 565.98s   best: 22.5551
2023-10-03 08:19:49,038:INFO:  Epoch 308/500:  train Loss: 18.2674   val Loss: 23.8311   time: 561.27s   best: 22.5551
2023-10-03 08:28:58,460:INFO:  Epoch 309/500:  train Loss: 18.4847   val Loss: 23.3132   time: 549.40s   best: 22.5551
2023-10-03 08:38:30,577:INFO:  Epoch 310/500:  train Loss: 18.3516   val Loss: 23.4920   time: 571.93s   best: 22.5551
2023-10-03 08:47:41,823:INFO:  Epoch 311/500:  train Loss: 18.2648   val Loss: 23.4352   time: 551.21s   best: 22.5551
2023-10-03 08:57:09,713:INFO:  Epoch 312/500:  train Loss: 18.2275   val Loss: 23.4834   time: 567.88s   best: 22.5551
2023-10-03 09:06:47,761:INFO:  Epoch 313/500:  train Loss: 18.1485   val Loss: 23.3986   time: 578.01s   best: 22.5551
2023-10-03 09:16:23,636:INFO:  Epoch 314/500:  train Loss: 18.3378   val Loss: 23.3051   time: 575.80s   best: 22.5551
2023-10-03 09:25:23,618:INFO:  Epoch 315/500:  train Loss: 18.2842   val Loss: 23.5804   time: 539.96s   best: 22.5551
2023-10-03 09:34:42,066:INFO:  Epoch 316/500:  train Loss: 18.3772   val Loss: 23.4858   time: 558.44s   best: 22.5551
2023-10-03 09:43:54,318:INFO:  Epoch 317/500:  train Loss: 18.2100   val Loss: 23.6101   time: 552.23s   best: 22.5551
2023-10-03 09:53:16,921:INFO:  Epoch 318/500:  train Loss: 18.2098   val Loss: 22.9380   time: 562.54s   best: 22.5551
2023-10-03 10:02:36,949:INFO:  Epoch 319/500:  train Loss: 18.4792   val Loss: 23.2959   time: 559.93s   best: 22.5551
2023-10-03 10:11:59,998:INFO:  Epoch 320/500:  train Loss: 18.2051   val Loss: 23.7552   time: 563.04s   best: 22.5551
2023-10-03 10:21:31,283:INFO:  Epoch 321/500:  train Loss: 18.0738   val Loss: 22.8984   time: 571.27s   best: 22.5551
2023-10-03 10:30:57,168:INFO:  Epoch 322/500:  train Loss: 18.2414   val Loss: 23.2541   time: 565.86s   best: 22.5551
2023-10-03 10:40:17,312:INFO:  Epoch 323/500:  train Loss: 18.1118   val Loss: 24.8050   time: 560.13s   best: 22.5551
2023-10-03 10:49:44,319:INFO:  Epoch 324/500:  train Loss: 18.2352   val Loss: 23.7863   time: 566.96s   best: 22.5551
2023-10-03 10:58:54,291:INFO:  Epoch 325/500:  train Loss: 18.5950   val Loss: 24.2934   time: 549.93s   best: 22.5551
2023-10-03 11:08:33,126:INFO:  Epoch 326/500:  train Loss: 18.2498   val Loss: 23.5991   time: 578.82s   best: 22.5551
2023-10-03 11:18:02,909:INFO:  Epoch 327/500:  train Loss: 18.6091   val Loss: 23.9349   time: 569.77s   best: 22.5551
2023-10-03 11:28:13,514:INFO:  Epoch 328/500:  train Loss: 18.6862   val Loss: 23.6809   time: 610.44s   best: 22.5551
2023-10-03 11:38:02,679:INFO:  Epoch 329/500:  train Loss: 18.2168   val Loss: 23.5087   time: 589.15s   best: 22.5551
2023-10-03 11:47:24,811:INFO:  Epoch 330/500:  train Loss: 18.1554   val Loss: 23.5175   time: 562.07s   best: 22.5551
2023-10-03 11:56:43,061:INFO:  Epoch 331/500:  train Loss: 18.2917   val Loss: 23.4557   time: 558.23s   best: 22.5551
2023-10-03 12:06:27,874:INFO:  Epoch 332/500:  train Loss: 18.2613   val Loss: 23.6371   time: 584.80s   best: 22.5551
2023-10-03 12:15:59,916:INFO:  Epoch 333/500:  train Loss: 18.0769   val Loss: 23.6858   time: 571.79s   best: 22.5551
2023-10-03 12:25:10,379:INFO:  Epoch 334/500:  train Loss: 18.2128   val Loss: 38.9721   time: 550.42s   best: 22.5551
2023-10-03 12:34:46,575:INFO:  Epoch 335/500:  train Loss: 18.6733   val Loss: 23.2378   time: 576.18s   best: 22.5551
2023-10-03 12:44:18,052:INFO:  Epoch 336/500:  train Loss: 18.0571   val Loss: 22.9986   time: 571.44s   best: 22.5551
2023-10-03 12:53:41,090:INFO:  Epoch 337/500:  train Loss: 18.1477   val Loss: 23.4608   time: 562.99s   best: 22.5551
2023-10-03 13:02:52,146:INFO:  Epoch 338/500:  train Loss: 18.3127   val Loss: 23.2612   time: 550.91s   best: 22.5551
2023-10-03 13:12:22,225:INFO:  Epoch 339/500:  train Loss: 18.2282   val Loss: 22.9460   time: 570.07s   best: 22.5551
2023-10-03 13:22:05,403:INFO:  Epoch 340/500:  train Loss: 18.2279   val Loss: 23.5836   time: 583.12s   best: 22.5551
2023-10-03 13:31:59,743:INFO:  Epoch 341/500:  train Loss: 18.1115   val Loss: 23.5258   time: 594.32s   best: 22.5551
2023-10-03 13:41:45,398:INFO:  Epoch 342/500:  train Loss: 18.1861   val Loss: 23.1842   time: 585.63s   best: 22.5551
2023-10-03 13:51:00,404:INFO:  Epoch 343/500:  train Loss: 18.1093   val Loss: 22.7954   time: 554.97s   best: 22.5551
2023-10-03 14:00:22,611:INFO:  Epoch 344/500:  train Loss: 18.2093   val Loss: 24.3208   time: 562.17s   best: 22.5551
2023-10-03 14:10:08,873:INFO:  Epoch 345/500:  train Loss: 18.4471   val Loss: 23.2251   time: 586.08s   best: 22.5551
2023-10-03 14:19:32,670:INFO:  Epoch 346/500:  train Loss: 18.7472   val Loss: 23.1102   time: 563.77s   best: 22.5551
2023-10-03 14:28:54,529:INFO:  Epoch 347/500:  train Loss: 18.2712   val Loss: 22.9471   time: 561.84s   best: 22.5551
2023-10-03 14:38:45,955:INFO:  Epoch 348/500:  train Loss: 18.0166   val Loss: 23.0838   time: 591.41s   best: 22.5551
2023-10-03 14:48:43,901:INFO:  Epoch 349/500:  train Loss: 18.2621   val Loss: 23.6619   time: 597.92s   best: 22.5551
2023-10-03 14:58:23,782:INFO:  Epoch 350/500:  train Loss: 18.0626   val Loss: 24.4085   time: 579.86s   best: 22.5551
2023-10-03 15:08:27,757:INFO:  Epoch 351/500:  train Loss: 18.0766   val Loss: 23.0111   time: 603.96s   best: 22.5551
2023-10-03 15:17:45,298:INFO:  Epoch 352/500:  train Loss: 18.0957   val Loss: 23.3410   time: 557.51s   best: 22.5551
2023-10-03 15:26:50,155:INFO:  Epoch 353/500:  train Loss: 18.1304   val Loss: 23.0448   time: 544.85s   best: 22.5551
2023-10-03 15:36:20,468:INFO:  Epoch 354/500:  train Loss: 18.2483   val Loss: 23.0880   time: 570.30s   best: 22.5551
2023-10-03 15:45:18,805:INFO:  Epoch 355/500:  train Loss: 18.5134   val Loss: 24.1909   time: 538.31s   best: 22.5551
2023-10-03 15:54:44,370:INFO:  Epoch 356/500:  train Loss: 18.1509   val Loss: 23.6385   time: 565.55s   best: 22.5551
2023-10-03 16:04:12,285:INFO:  Epoch 357/500:  train Loss: 18.0102   val Loss: 23.1130   time: 567.89s   best: 22.5551
2023-10-03 16:13:23,606:INFO:  Epoch 358/500:  train Loss: 18.2651   val Loss: 24.5367   time: 551.31s   best: 22.5551
2023-10-03 16:23:03,552:INFO:  Epoch 359/500:  train Loss: 18.4958   val Loss: 23.3589   time: 579.93s   best: 22.5551
2023-10-03 16:33:07,518:INFO:  Epoch 360/500:  train Loss: 18.3065   val Loss: 23.1138   time: 603.93s   best: 22.5551
2023-10-03 16:42:45,495:INFO:  Epoch 361/500:  train Loss: 18.0093   val Loss: 23.3800   time: 577.96s   best: 22.5551
2023-10-03 16:52:51,142:INFO:  Epoch 362/500:  train Loss: 18.0889   val Loss: 23.3406   time: 605.60s   best: 22.5551
2023-10-03 17:02:13,750:INFO:  Epoch 363/500:  train Loss: 17.9967   val Loss: 23.0210   time: 562.59s   best: 22.5551
2023-10-03 17:11:21,255:INFO:  Epoch 364/500:  train Loss: 17.8887   val Loss: 22.8660   time: 547.49s   best: 22.5551
2023-10-03 17:20:35,751:INFO:  Epoch 365/500:  train Loss: 18.0944   val Loss: 23.5474   time: 554.49s   best: 22.5551
2023-10-03 17:30:34,784:INFO:  Epoch 366/500:  train Loss: 18.1342   val Loss: 23.1103   time: 598.96s   best: 22.5551
2023-10-03 17:39:34,182:INFO:  Epoch 367/500:  train Loss: 17.9770   val Loss: 22.8574   time: 539.36s   best: 22.5551
2023-10-03 17:48:55,170:INFO:  Epoch 368/500:  train Loss: 18.0310   val Loss: 23.5249   time: 560.96s   best: 22.5551
2023-10-03 17:57:54,601:INFO:  Epoch 369/500:  train Loss: 18.1088   val Loss: 23.1546   time: 539.40s   best: 22.5551
2023-10-03 18:07:14,039:INFO:  Epoch 370/500:  train Loss: 17.8430   val Loss: 22.6238   time: 559.43s   best: 22.5551
2023-10-03 18:16:13,535:INFO:  Epoch 371/500:  train Loss: 18.1289   val Loss: 22.8373   time: 539.48s   best: 22.5551
2023-10-03 18:25:14,706:INFO:  Epoch 372/500:  train Loss: 17.9531   val Loss: 26.9551   time: 541.15s   best: 22.5551
2023-10-03 18:34:25,384:INFO:  Epoch 373/500:  train Loss: 17.9728   val Loss: 23.4919   time: 550.65s   best: 22.5551
2023-10-03 18:43:38,475:INFO:  Epoch 374/500:  train Loss: 18.0370   val Loss: 23.3112   time: 553.05s   best: 22.5551
2023-10-03 18:52:51,630:INFO:  Epoch 375/500:  train Loss: 18.0584   val Loss: 22.9247   time: 553.12s   best: 22.5551
2023-10-03 19:02:24,512:INFO:  Epoch 376/500:  train Loss: 18.0601   val Loss: 25.5543   time: 572.85s   best: 22.5551
2023-10-03 19:11:29,546:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-03 19:11:30,527:INFO:  Epoch 377/500:  train Loss: 17.8511   val Loss: 22.5180   time: 544.53s   best: 22.5180
2023-10-03 19:20:56,245:INFO:  Epoch 378/500:  train Loss: 18.0337   val Loss: 22.9129   time: 565.72s   best: 22.5180
2023-10-03 19:29:55,079:INFO:  Epoch 379/500:  train Loss: 18.0010   val Loss: 23.3810   time: 538.81s   best: 22.5180
2023-10-03 19:39:22,583:INFO:  Epoch 380/500:  train Loss: 17.9435   val Loss: 23.2126   time: 567.47s   best: 22.5180
2023-10-03 19:48:19,403:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-03 19:48:20,198:INFO:  Epoch 381/500:  train Loss: 18.1314   val Loss: 22.5080   time: 536.15s   best: 22.5080
2023-10-03 19:57:42,263:INFO:  Epoch 382/500:  train Loss: 17.8918   val Loss: 23.4636   time: 561.87s   best: 22.5080
2023-10-03 20:06:43,197:INFO:  Epoch 383/500:  train Loss: 17.9342   val Loss: 23.4850   time: 540.92s   best: 22.5080
2023-10-03 20:16:42,872:INFO:  Epoch 384/500:  train Loss: 17.9398   val Loss: 23.0913   time: 599.64s   best: 22.5080
2023-10-03 20:44:04,548:INFO:  Epoch 385/500:  train Loss: 17.9988   val Loss: 22.5996   time: 1641.64s   best: 22.5080
2023-10-03 20:55:36,872:INFO:  Epoch 386/500:  train Loss: 17.9342   val Loss: 22.7420   time: 692.31s   best: 22.5080
2023-10-03 21:04:41,545:INFO:  Epoch 387/500:  train Loss: 18.0109   val Loss: 23.7702   time: 544.65s   best: 22.5080
2023-10-03 21:13:37,253:INFO:  Epoch 388/500:  train Loss: 18.0867   val Loss: 23.1252   time: 535.68s   best: 22.5080
2023-10-03 21:22:56,399:INFO:  Epoch 389/500:  train Loss: 17.9776   val Loss: 23.4850   time: 559.12s   best: 22.5080
2023-10-03 21:31:59,380:INFO:  Epoch 390/500:  train Loss: 17.9273   val Loss: 23.1827   time: 542.93s   best: 22.5080
2023-10-03 21:41:09,484:INFO:  Epoch 391/500:  train Loss: 18.0425   val Loss: 23.1037   time: 550.09s   best: 22.5080
2023-10-03 21:50:06,877:INFO:  Epoch 392/500:  train Loss: 17.9233   val Loss: 23.4830   time: 537.37s   best: 22.5080
2023-10-03 21:59:22,644:INFO:  Epoch 393/500:  train Loss: 17.9265   val Loss: 24.4523   time: 555.73s   best: 22.5080
2023-10-03 22:08:24,542:INFO:  Epoch 394/500:  train Loss: 18.1513   val Loss: 23.4924   time: 541.89s   best: 22.5080
2023-10-03 22:17:28,101:INFO:  Epoch 395/500:  train Loss: 18.0079   val Loss: 23.6354   time: 543.47s   best: 22.5080
2023-10-03 22:26:37,097:INFO:  Epoch 396/500:  train Loss: 17.9452   val Loss: 23.7581   time: 548.95s   best: 22.5080
2023-10-03 22:35:36,772:INFO:  Epoch 397/500:  train Loss: 17.8691   val Loss: 23.1013   time: 539.64s   best: 22.5080
2023-10-03 22:44:56,102:INFO:  Epoch 398/500:  train Loss: 18.0679   val Loss: 23.2412   time: 559.30s   best: 22.5080
2023-10-03 22:54:05,205:INFO:  Epoch 399/500:  train Loss: 18.0197   val Loss: 23.4442   time: 549.06s   best: 22.5080
2023-10-03 23:03:15,708:INFO:  Epoch 400/500:  train Loss: 17.8845   val Loss: 23.6084   time: 550.41s   best: 22.5080
2023-10-03 23:12:17,672:INFO:  Epoch 401/500:  train Loss: 17.9528   val Loss: 23.1249   time: 541.94s   best: 22.5080
2023-10-03 23:21:43,007:INFO:  Epoch 402/500:  train Loss: 17.9480   val Loss: 23.1787   time: 565.29s   best: 22.5080
2023-10-03 23:30:44,448:INFO:  Epoch 403/500:  train Loss: 17.9029   val Loss: 22.8182   time: 541.41s   best: 22.5080
2023-10-03 23:40:09,069:INFO:  Epoch 404/500:  train Loss: 17.8095   val Loss: 22.8762   time: 564.57s   best: 22.5080
2023-10-03 23:49:03,826:INFO:  Epoch 405/500:  train Loss: 17.8685   val Loss: 23.7317   time: 534.73s   best: 22.5080
2023-10-03 23:58:05,033:INFO:  Epoch 406/500:  train Loss: 17.8818   val Loss: 23.2024   time: 541.18s   best: 22.5080
2023-10-04 00:07:29,199:INFO:  Epoch 407/500:  train Loss: 17.7764   val Loss: 23.1508   time: 564.15s   best: 22.5080
2023-10-04 00:16:24,408:INFO:  Epoch 408/500:  train Loss: 17.8012   val Loss: 23.1885   time: 535.18s   best: 22.5080
2023-10-04 00:26:07,486:INFO:  Epoch 409/500:  train Loss: 18.1365   val Loss: 22.9340   time: 583.04s   best: 22.5080
2023-10-04 00:38:49,721:INFO:  Epoch 410/500:  train Loss: 18.4112   val Loss: 23.0919   time: 762.19s   best: 22.5080
2023-10-04 01:21:15,454:INFO:  Epoch 411/500:  train Loss: 18.0093   val Loss: 22.8746   time: 2545.69s   best: 22.5080
2023-10-04 01:36:03,440:INFO:  Epoch 412/500:  train Loss: 17.8865   val Loss: 23.4158   time: 887.92s   best: 22.5080
2023-10-04 01:47:31,955:INFO:  Epoch 413/500:  train Loss: 17.8589   val Loss: 22.9977   time: 688.49s   best: 22.5080
2023-10-04 01:57:49,441:INFO:  Epoch 414/500:  train Loss: 17.7985   val Loss: 22.8656   time: 617.46s   best: 22.5080
2023-10-04 02:06:45,591:INFO:  Epoch 415/500:  train Loss: 17.9453   val Loss: 23.1305   time: 536.13s   best: 22.5080
2023-10-04 02:15:36,524:INFO:  Epoch 416/500:  train Loss: 17.7511   val Loss: 23.5194   time: 530.92s   best: 22.5080
2023-10-04 02:24:45,466:INFO:  Epoch 417/500:  train Loss: 18.2167   val Loss: 27.7184   time: 548.89s   best: 22.5080
2023-10-04 02:33:39,011:INFO:  Epoch 418/500:  train Loss: 18.1252   val Loss: 23.5675   time: 533.52s   best: 22.5080
2023-10-04 02:42:43,957:INFO:  Epoch 419/500:  train Loss: 17.9539   val Loss: 23.4349   time: 544.90s   best: 22.5080
2023-10-04 02:51:59,081:INFO:  Epoch 420/500:  train Loss: 18.1465   val Loss: 22.9513   time: 554.93s   best: 22.5080
2023-10-04 03:01:04,229:INFO:  Epoch 421/500:  train Loss: 17.8130   val Loss: 22.8870   time: 545.11s   best: 22.5080
2023-10-04 03:10:18,018:INFO:  Epoch 422/500:  train Loss: 18.0260   val Loss: 22.7492   time: 553.77s   best: 22.5080
2023-10-04 03:19:12,149:INFO:  Epoch 423/500:  train Loss: 17.7719   val Loss: 23.2544   time: 534.09s   best: 22.5080
2023-10-04 03:28:07,259:INFO:  Epoch 424/500:  train Loss: 17.6890   val Loss: 22.6819   time: 535.07s   best: 22.5080
2023-10-04 03:37:40,004:INFO:  Epoch 425/500:  train Loss: 17.8534   val Loss: 23.3727   time: 572.73s   best: 22.5080
2023-10-04 03:46:43,672:INFO:  Epoch 426/500:  train Loss: 17.7142   val Loss: 22.9577   time: 543.64s   best: 22.5080
2023-10-04 03:55:48,110:INFO:  Epoch 427/500:  train Loss: 17.8867   val Loss: 23.2305   time: 544.42s   best: 22.5080
2023-10-04 04:04:41,875:INFO:  Epoch 428/500:  train Loss: 17.7312   val Loss: 23.2620   time: 533.75s   best: 22.5080
2023-10-04 04:13:42,344:INFO:  Epoch 429/500:  train Loss: 17.8068   val Loss: 23.1283   time: 540.45s   best: 22.5080
2023-10-04 04:22:52,325:INFO:  Epoch 430/500:  train Loss: 17.7836   val Loss: 23.2427   time: 549.96s   best: 22.5080
2023-10-04 04:31:46,531:INFO:  Epoch 431/500:  train Loss: 17.8338   val Loss: 22.9221   time: 534.16s   best: 22.5080
2023-10-04 04:40:43,096:INFO:  Epoch 432/500:  train Loss: 18.2399   val Loss: 23.1982   time: 536.55s   best: 22.5080
2023-10-04 04:49:53,009:INFO:  Epoch 433/500:  train Loss: 17.9514   val Loss: 22.5937   time: 549.82s   best: 22.5080
2023-10-04 04:58:50,477:INFO:  Epoch 434/500:  train Loss: 17.6085   val Loss: 22.8825   time: 537.43s   best: 22.5080
2023-10-04 05:07:51,046:INFO:  Epoch 435/500:  train Loss: 17.8555   val Loss: 22.9815   time: 540.53s   best: 22.5080
2023-10-04 05:16:57,112:INFO:  Epoch 436/500:  train Loss: 17.7379   val Loss: 22.7508   time: 546.05s   best: 22.5080
2023-10-04 05:26:02,702:INFO:  Epoch 437/500:  train Loss: 17.9865   val Loss: 22.9841   time: 545.57s   best: 22.5080
2023-10-04 05:35:24,065:INFO:  Epoch 438/500:  train Loss: 17.7027   val Loss: 22.8649   time: 561.35s   best: 22.5080
2023-10-04 05:44:24,537:INFO:  Epoch 439/500:  train Loss: 18.1427   val Loss: 22.9995   time: 540.43s   best: 22.5080
2023-10-04 05:53:20,759:INFO:  Epoch 440/500:  train Loss: 17.7233   val Loss: 23.0391   time: 536.20s   best: 22.5080
2023-10-04 06:02:25,211:INFO:  Epoch 441/500:  train Loss: 17.6213   val Loss: 22.7690   time: 544.44s   best: 22.5080
2023-10-04 06:11:14,742:INFO:  Epoch 442/500:  train Loss: 18.0398   val Loss: 22.9218   time: 529.27s   best: 22.5080
2023-10-04 06:20:17,207:INFO:  Epoch 443/500:  train Loss: 17.7316   val Loss: 22.9081   time: 542.43s   best: 22.5080
2023-10-04 06:29:37,264:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-04 06:29:37,689:INFO:  Epoch 444/500:  train Loss: 17.7812   val Loss: 22.3776   time: 559.73s   best: 22.3776
2023-10-04 06:39:14,237:INFO:  Epoch 445/500:  train Loss: 17.5957   val Loss: 23.0045   time: 576.30s   best: 22.3776
2023-10-04 06:48:20,748:INFO:  Epoch 446/500:  train Loss: 17.8617   val Loss: 28.0241   time: 546.50s   best: 22.3776
2023-10-04 06:57:37,450:INFO:  Epoch 447/500:  train Loss: 18.5413   val Loss: 22.8583   time: 556.68s   best: 22.3776
2023-10-04 07:06:32,235:INFO:  Epoch 448/500:  train Loss: 17.9499   val Loss: 22.7160   time: 534.66s   best: 22.3776
2023-10-04 07:15:30,391:INFO:  Epoch 449/500:  train Loss: 17.6597   val Loss: 22.8693   time: 538.12s   best: 22.3776
2023-10-04 07:24:44,163:INFO:  Epoch 450/500:  train Loss: 17.7648   val Loss: 23.8284   time: 553.73s   best: 22.3776
2023-10-04 07:33:45,355:INFO:  Epoch 451/500:  train Loss: 17.5632   val Loss: 23.2340   time: 541.16s   best: 22.3776
2023-10-04 07:43:04,779:INFO:  Epoch 452/500:  train Loss: 17.6168   val Loss: 22.9984   time: 559.41s   best: 22.3776
2023-10-04 07:52:05,912:INFO:  Epoch 453/500:  train Loss: 17.9256   val Loss: 23.0834   time: 541.06s   best: 22.3776
2023-10-04 08:01:32,574:INFO:  Epoch 454/500:  train Loss: 17.9813   val Loss: 22.9907   time: 566.64s   best: 22.3776
2023-10-04 08:10:48,105:INFO:  Epoch 455/500:  train Loss: 17.6123   val Loss: 23.1367   time: 555.51s   best: 22.3776
2023-10-04 08:24:10,067:INFO:  Epoch 456/500:  train Loss: 17.6665   val Loss: 22.8101   time: 801.93s   best: 22.3776
2023-10-04 08:35:05,571:INFO:  Epoch 457/500:  train Loss: 17.6971   val Loss: 22.9450   time: 655.47s   best: 22.3776
2023-10-04 09:28:23,851:INFO:  Epoch 458/500:  train Loss: 17.6597   val Loss: 22.9059   time: 3198.21s   best: 22.3776
2023-10-04 09:44:10,268:INFO:  Epoch 459/500:  train Loss: 17.6416   val Loss: 23.2844   time: 946.40s   best: 22.3776
2023-10-04 09:56:02,892:INFO:  Epoch 460/500:  train Loss: 17.5394   val Loss: 23.0403   time: 712.56s   best: 22.3776
2023-10-04 10:06:07,910:INFO:  Epoch 461/500:  train Loss: 18.0611   val Loss: 23.5398   time: 604.97s   best: 22.3776
2023-10-04 10:15:30,947:INFO:  Epoch 462/500:  train Loss: 17.8105   val Loss: 23.5906   time: 562.99s   best: 22.3776
2023-10-04 10:24:15,023:INFO:  Epoch 463/500:  train Loss: 17.6653   val Loss: 22.9691   time: 524.06s   best: 22.3776
2023-10-04 10:32:10,290:INFO:  Epoch 464/500:  train Loss: 17.5305   val Loss: 24.2740   time: 475.25s   best: 22.3776
2023-10-04 10:41:01,681:INFO:  Epoch 465/500:  train Loss: 17.8052   val Loss: 23.3420   time: 531.19s   best: 22.3776
2023-10-04 10:49:56,911:INFO:  Epoch 466/500:  train Loss: 17.6226   val Loss: 23.0893   time: 535.21s   best: 22.3776
2023-10-04 10:59:07,232:INFO:  Epoch 467/500:  train Loss: 17.7103   val Loss: 22.9250   time: 550.30s   best: 22.3776
2023-10-04 11:08:05,237:INFO:  Epoch 468/500:  train Loss: 17.6223   val Loss: 23.7964   time: 537.99s   best: 22.3776
2023-10-04 11:17:32,220:INFO:  Epoch 469/500:  train Loss: 17.5470   val Loss: 23.2754   time: 566.91s   best: 22.3776
2023-10-04 11:26:31,705:INFO:  Epoch 470/500:  train Loss: 17.7995   val Loss: 28.1981   time: 539.46s   best: 22.3776
2023-10-04 11:35:39,494:INFO:  Epoch 471/500:  train Loss: 17.7367   val Loss: 23.5109   time: 547.76s   best: 22.3776
2023-10-04 11:44:46,652:INFO:  Epoch 472/500:  train Loss: 17.5747   val Loss: 23.1010   time: 547.11s   best: 22.3776
2023-10-04 11:54:08,386:INFO:  Epoch 473/500:  train Loss: 17.5684   val Loss: 22.6903   time: 561.71s   best: 22.3776
2023-10-04 12:03:29,090:INFO:  Epoch 474/500:  train Loss: 17.4857   val Loss: 22.7441   time: 560.67s   best: 22.3776
2023-10-04 12:13:06,446:INFO:  Epoch 475/500:  train Loss: 17.5950   val Loss: 23.2820   time: 577.32s   best: 22.3776
2023-10-04 12:22:21,997:INFO:  Epoch 476/500:  train Loss: 17.7310   val Loss: 22.8738   time: 555.51s   best: 22.3776
2023-10-04 12:31:34,693:INFO:  Epoch 477/500:  train Loss: 17.5430   val Loss: 23.2746   time: 552.69s   best: 22.3776
2023-10-04 12:40:50,347:INFO:  Epoch 478/500:  train Loss: 17.6344   val Loss: 22.8158   time: 555.60s   best: 22.3776
2023-10-04 12:50:14,041:INFO:  Epoch 479/500:  train Loss: 17.4899   val Loss: 23.1192   time: 563.62s   best: 22.3776
2023-10-04 12:59:29,656:INFO:  Epoch 480/500:  train Loss: 17.4308   val Loss: 23.3634   time: 555.58s   best: 22.3776
2023-10-04 13:08:41,183:INFO:  Epoch 481/500:  train Loss: 17.8468   val Loss: 23.1968   time: 551.52s   best: 22.3776
2023-10-04 13:18:18,623:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder bidirectional_15cb.pt
2023-10-04 13:18:19,185:INFO:  Epoch 482/500:  train Loss: 17.5135   val Loss: 21.9715   time: 577.10s   best: 21.9715
2023-10-04 13:27:40,802:INFO:  Epoch 483/500:  train Loss: 17.5945   val Loss: 22.8547   time: 561.61s   best: 21.9715
2023-10-04 13:36:58,352:INFO:  Epoch 484/500:  train Loss: 17.6380   val Loss: 22.8789   time: 557.51s   best: 21.9715
2023-10-04 13:46:21,163:INFO:  Epoch 485/500:  train Loss: 17.5590   val Loss: 22.8283   time: 562.78s   best: 21.9715
2023-10-04 13:55:29,410:INFO:  Epoch 486/500:  train Loss: 17.6303   val Loss: 22.8350   time: 548.19s   best: 21.9715
2023-10-04 14:04:48,190:INFO:  Epoch 487/500:  train Loss: 17.4607   val Loss: 23.2091   time: 558.76s   best: 21.9715
2023-10-04 14:13:45,714:INFO:  Epoch 488/500:  train Loss: 17.5712   val Loss: 22.6770   time: 537.49s   best: 21.9715
2023-10-04 14:23:08,327:INFO:  Epoch 489/500:  train Loss: 17.5205   val Loss: 22.7715   time: 562.58s   best: 21.9715
2023-10-04 14:32:29,222:INFO:  Epoch 490/500:  train Loss: 17.5556   val Loss: 23.1640   time: 560.86s   best: 21.9715
2023-10-04 14:41:38,876:INFO:  Epoch 491/500:  train Loss: 17.5535   val Loss: 23.2690   time: 549.46s   best: 21.9715
2023-10-04 14:50:55,581:INFO:  Epoch 492/500:  train Loss: 17.5071   val Loss: 22.8719   time: 556.68s   best: 21.9715
2023-10-04 15:00:16,652:INFO:  Epoch 493/500:  train Loss: 17.4845   val Loss: 23.0042   time: 561.00s   best: 21.9715
2023-10-04 15:09:27,798:INFO:  Epoch 494/500:  train Loss: 17.5222   val Loss: 23.3961   time: 551.11s   best: 21.9715
2023-10-04 15:18:41,052:INFO:  Epoch 495/500:  train Loss: 17.6272   val Loss: 22.8465   time: 553.22s   best: 21.9715
2023-10-04 15:27:42,968:INFO:  Epoch 496/500:  train Loss: 17.7904   val Loss: 22.9297   time: 541.90s   best: 21.9715
2023-10-04 15:37:05,835:INFO:  Epoch 497/500:  train Loss: 17.6020   val Loss: 22.7233   time: 562.85s   best: 21.9715
2023-10-04 15:46:14,411:INFO:  Epoch 498/500:  train Loss: 17.4931   val Loss: 22.9629   time: 548.55s   best: 21.9715
2023-10-04 15:55:32,757:INFO:  Epoch 499/500:  train Loss: 17.4604   val Loss: 24.1211   time: 558.33s   best: 21.9715
2023-10-04 16:04:37,388:INFO:  Epoch 500/500:  train Loss: 17.4146   val Loss: 22.7713   time: 544.61s   best: 21.9715
2023-10-04 16:04:37,404:INFO:  -----> Training complete in 4599m 17s   best validation loss: 21.9715
 
2023-10-05 14:11:36,737:INFO:  Starting experiment lstm autoencoder debug (1 layer)
2023-10-05 14:11:36,748:INFO:  Defining the model
2023-10-05 14:11:36,805:INFO:  Reading the dataset
2023-10-05 14:12:29,967:INFO:  Starting experiment lstm autoencoder debug (1 layer)
2023-10-05 14:12:29,967:INFO:  Defining the model
2023-10-05 14:12:30,010:INFO:  Reading the dataset
2023-10-05 14:13:44,461:INFO:  Starting experiment lstm autoencoder debug (1 layer)
2023-10-05 14:13:44,462:INFO:  Defining the model
2023-10-05 14:13:44,504:INFO:  Reading the dataset
2023-10-05 14:14:20,098:INFO:  Starting experiment lstm autoencoder debug (1 layer)
2023-10-05 14:14:20,098:INFO:  Defining the model
2023-10-05 14:14:20,142:INFO:  Reading the dataset
2023-10-05 14:14:28,198:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:28,222:INFO:  Epoch 1/500:  train Loss: 99.4631   val Loss: 100.0160   time: 1.84s   best: 100.0160
2023-10-05 14:14:28,504:INFO:  Epoch 2/500:  train Loss: 99.3234   val Loss: 100.0739   time: 0.28s   best: 100.0160
2023-10-05 14:14:28,770:INFO:  Epoch 3/500:  train Loss: 99.3904   val Loss: 100.0485   time: 0.26s   best: 100.0160
2023-10-05 14:14:29,025:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:29,134:INFO:  Epoch 4/500:  train Loss: 98.6182   val Loss: 95.0785   time: 0.25s   best: 95.0785
2023-10-05 14:14:29,403:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:29,425:INFO:  Epoch 5/500:  train Loss: 98.1001   val Loss: 94.4990   time: 0.25s   best: 94.4990
2023-10-05 14:14:29,693:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:29,776:INFO:  Epoch 6/500:  train Loss: 97.0286   val Loss: 93.6931   time: 0.26s   best: 93.6931
2023-10-05 14:14:30,030:INFO:  Epoch 7/500:  train Loss: 97.2911   val Loss: 96.4655   time: 0.25s   best: 93.6931
2023-10-05 14:14:30,297:INFO:  Epoch 8/500:  train Loss: 95.8209   val Loss: 96.0179   time: 0.26s   best: 93.6931
2023-10-05 14:14:30,562:INFO:  Epoch 9/500:  train Loss: 95.7020   val Loss: 95.4546   time: 0.26s   best: 93.6931
2023-10-05 14:14:30,831:INFO:  Epoch 10/500:  train Loss: 95.0439   val Loss: 94.5761   time: 0.26s   best: 93.6931
2023-10-05 14:14:31,083:INFO:  Epoch 11/500:  train Loss: 94.2452   val Loss: 94.1807   time: 0.25s   best: 93.6931
2023-10-05 14:14:31,355:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:31,386:INFO:  Epoch 12/500:  train Loss: 93.1443   val Loss: 91.7885   time: 0.27s   best: 91.7885
2023-10-05 14:14:31,638:INFO:  Epoch 13/500:  train Loss: 93.5478   val Loss: 93.9240   time: 0.25s   best: 91.7885
2023-10-05 14:14:31,904:INFO:  Epoch 14/500:  train Loss: 94.3017   val Loss: 94.3658   time: 0.26s   best: 91.7885
2023-10-05 14:14:32,156:INFO:  Epoch 15/500:  train Loss: 93.9617   val Loss: 94.4576   time: 0.25s   best: 91.7885
2023-10-05 14:14:32,438:INFO:  Epoch 16/500:  train Loss: 93.7692   val Loss: 94.2840   time: 0.28s   best: 91.7885
2023-10-05 14:14:32,692:INFO:  Epoch 17/500:  train Loss: 93.7351   val Loss: 94.1146   time: 0.25s   best: 91.7885
2023-10-05 14:14:32,960:INFO:  Epoch 18/500:  train Loss: 94.1839   val Loss: 94.0111   time: 0.26s   best: 91.7885
2023-10-05 14:14:33,214:INFO:  Epoch 19/500:  train Loss: 94.2000   val Loss: 93.9939   time: 0.25s   best: 91.7885
2023-10-05 14:14:33,482:INFO:  Epoch 20/500:  train Loss: 93.5503   val Loss: 93.2604   time: 0.26s   best: 91.7885
2023-10-05 14:14:33,736:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:33,757:INFO:  Epoch 21/500:  train Loss: 93.2416   val Loss: 91.3418   time: 0.25s   best: 91.3418
2023-10-05 14:14:34,024:INFO:  Epoch 22/500:  train Loss: 93.1564   val Loss: 92.2915   time: 0.26s   best: 91.3418
2023-10-05 14:14:34,275:INFO:  Epoch 23/500:  train Loss: 94.3710   val Loss: 94.2637   time: 0.25s   best: 91.3418
2023-10-05 14:14:34,559:INFO:  Epoch 24/500:  train Loss: 94.2306   val Loss: 94.3660   time: 0.28s   best: 91.3418
2023-10-05 14:14:34,812:INFO:  Epoch 25/500:  train Loss: 94.2342   val Loss: 94.5093   time: 0.25s   best: 91.3418
2023-10-05 14:14:35,081:INFO:  Epoch 26/500:  train Loss: 94.7559   val Loss: 94.5729   time: 0.26s   best: 91.3418
2023-10-05 14:14:35,337:INFO:  Epoch 27/500:  train Loss: 94.5322   val Loss: 94.5766   time: 0.25s   best: 91.3418
2023-10-05 14:14:35,605:INFO:  Epoch 28/500:  train Loss: 94.6450   val Loss: 94.5169   time: 0.26s   best: 91.3418
2023-10-05 14:14:35,857:INFO:  Epoch 29/500:  train Loss: 94.5669   val Loss: 94.4439   time: 0.25s   best: 91.3418
2023-10-05 14:14:36,124:INFO:  Epoch 30/500:  train Loss: 94.4822   val Loss: 94.3558   time: 0.26s   best: 91.3418
2023-10-05 14:14:36,404:INFO:  Epoch 31/500:  train Loss: 94.1749   val Loss: 94.2572   time: 0.27s   best: 91.3418
2023-10-05 14:14:36,659:INFO:  Epoch 32/500:  train Loss: 94.4699   val Loss: 94.1979   time: 0.25s   best: 91.3418
2023-10-05 14:14:36,930:INFO:  Epoch 33/500:  train Loss: 94.4235   val Loss: 94.0635   time: 0.27s   best: 91.3418
2023-10-05 14:14:37,186:INFO:  Epoch 34/500:  train Loss: 93.6757   val Loss: 93.8445   time: 0.25s   best: 91.3418
2023-10-05 14:14:37,453:INFO:  Epoch 35/500:  train Loss: 94.1294   val Loss: 93.6651   time: 0.26s   best: 91.3418
2023-10-05 14:14:37,706:INFO:  Epoch 36/500:  train Loss: 93.9504   val Loss: 93.5210   time: 0.25s   best: 91.3418
2023-10-05 14:14:37,995:INFO:  Epoch 37/500:  train Loss: 93.6435   val Loss: 93.3250   time: 0.29s   best: 91.3418
2023-10-05 14:14:38,247:INFO:  Epoch 38/500:  train Loss: 92.8035   val Loss: 93.1132   time: 0.25s   best: 91.3418
2023-10-05 14:14:38,530:INFO:  Epoch 39/500:  train Loss: 93.3119   val Loss: 92.7937   time: 0.28s   best: 91.3418
2023-10-05 14:14:38,781:INFO:  Epoch 40/500:  train Loss: 92.6132   val Loss: 92.4474   time: 0.25s   best: 91.3418
2023-10-05 14:14:39,052:INFO:  Epoch 41/500:  train Loss: 92.4358   val Loss: 92.0209   time: 0.27s   best: 91.3418
2023-10-05 14:14:39,307:INFO:  Epoch 42/500:  train Loss: 92.1007   val Loss: 91.4869   time: 0.25s   best: 91.3418
2023-10-05 14:14:39,577:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:39,639:INFO:  Epoch 43/500:  train Loss: 91.0624   val Loss: 90.7015   time: 0.26s   best: 90.7015
2023-10-05 14:14:39,892:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:39,914:INFO:  Epoch 44/500:  train Loss: 89.9338   val Loss: 89.8675   time: 0.25s   best: 89.8675
2023-10-05 14:14:40,179:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:40,226:INFO:  Epoch 45/500:  train Loss: 89.5744   val Loss: 89.0057   time: 0.26s   best: 89.0057
2023-10-05 14:14:40,490:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:40,550:INFO:  Epoch 46/500:  train Loss: 88.7958   val Loss: 88.3932   time: 0.26s   best: 88.3932
2023-10-05 14:14:40,806:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:40,828:INFO:  Epoch 47/500:  train Loss: 88.0364   val Loss: 87.8114   time: 0.25s   best: 87.8114
2023-10-05 14:14:41,095:INFO:  Epoch 48/500:  train Loss: 88.0765   val Loss: 87.8616   time: 0.26s   best: 87.8114
2023-10-05 14:14:41,353:INFO:  Epoch 49/500:  train Loss: 88.6020   val Loss: 88.9177   time: 0.26s   best: 87.8114
2023-10-05 14:14:41,623:INFO:  Epoch 50/500:  train Loss: 89.3048   val Loss: 88.5272   time: 0.27s   best: 87.8114
2023-10-05 14:14:41,876:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:41,937:INFO:  Epoch 51/500:  train Loss: 87.8034   val Loss: 87.5585   time: 0.25s   best: 87.5585
2023-10-05 14:14:42,203:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:42,224:INFO:  Epoch 52/500:  train Loss: 88.1422   val Loss: 87.2734   time: 0.26s   best: 87.2734
2023-10-05 14:14:42,488:INFO:  Epoch 53/500:  train Loss: 88.1468   val Loss: 88.4784   time: 0.26s   best: 87.2734
2023-10-05 14:14:42,760:INFO:  Epoch 54/500:  train Loss: 89.3629   val Loss: 88.8879   time: 0.27s   best: 87.2734
2023-10-05 14:14:43,014:INFO:  Epoch 55/500:  train Loss: 88.1910   val Loss: 87.5549   time: 0.25s   best: 87.2734
2023-10-05 14:14:43,285:INFO:  Epoch 56/500:  train Loss: 88.1811   val Loss: 87.6804   time: 0.27s   best: 87.2734
2023-10-05 14:14:43,538:INFO:  Epoch 57/500:  train Loss: 88.3034   val Loss: 88.0888   time: 0.25s   best: 87.2734
2023-10-05 14:14:43,803:INFO:  Epoch 58/500:  train Loss: 88.3762   val Loss: 87.8781   time: 0.26s   best: 87.2734
2023-10-05 14:14:44,057:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:44,108:INFO:  Epoch 59/500:  train Loss: 87.2733   val Loss: 87.1742   time: 0.25s   best: 87.1742
2023-10-05 14:14:44,380:INFO:  Epoch 60/500:  train Loss: 87.8944   val Loss: 87.3858   time: 0.27s   best: 87.1742
2023-10-05 14:14:44,642:INFO:  Epoch 61/500:  train Loss: 88.0430   val Loss: 87.2818   time: 0.26s   best: 87.1742
2023-10-05 14:14:44,915:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:44,952:INFO:  Epoch 62/500:  train Loss: 86.9853   val Loss: 86.6906   time: 0.27s   best: 86.6906
2023-10-05 14:14:45,222:INFO:  Epoch 63/500:  train Loss: 88.1501   val Loss: 87.0192   time: 0.27s   best: 86.6906
2023-10-05 14:14:45,475:INFO:  Epoch 64/500:  train Loss: 87.4041   val Loss: 86.7081   time: 0.25s   best: 86.6906
2023-10-05 14:14:45,744:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:45,765:INFO:  Epoch 65/500:  train Loss: 86.8647   val Loss: 86.4062   time: 0.26s   best: 86.4062
2023-10-05 14:14:46,018:INFO:  Epoch 66/500:  train Loss: 87.4164   val Loss: 86.9188   time: 0.25s   best: 86.4062
2023-10-05 14:14:46,285:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:46,344:INFO:  Epoch 67/500:  train Loss: 86.8298   val Loss: 86.2709   time: 0.26s   best: 86.2709
2023-10-05 14:14:46,612:INFO:  Epoch 68/500:  train Loss: 87.1084   val Loss: 86.4178   time: 0.26s   best: 86.2709
2023-10-05 14:14:46,881:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:46,902:INFO:  Epoch 69/500:  train Loss: 86.7274   val Loss: 85.7470   time: 0.26s   best: 85.7470
2023-10-05 14:14:47,157:INFO:  Epoch 70/500:  train Loss: 87.0726   val Loss: 86.3708   time: 0.25s   best: 85.7470
2023-10-05 14:14:47,426:INFO:  Epoch 71/500:  train Loss: 86.7808   val Loss: 86.1321   time: 0.26s   best: 85.7470
2023-10-05 14:14:47,679:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:47,764:INFO:  Epoch 72/500:  train Loss: 86.7053   val Loss: 85.4700   time: 0.25s   best: 85.4700
2023-10-05 14:14:48,032:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:48,097:INFO:  Epoch 73/500:  train Loss: 86.2421   val Loss: 85.3160   time: 0.26s   best: 85.3160
2023-10-05 14:14:48,366:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:48,388:INFO:  Epoch 74/500:  train Loss: 85.9988   val Loss: 85.2922   time: 0.26s   best: 85.2922
2023-10-05 14:14:48,652:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:48,721:INFO:  Epoch 75/500:  train Loss: 85.6594   val Loss: 85.1879   time: 0.26s   best: 85.1879
2023-10-05 14:14:48,988:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:49,010:INFO:  Epoch 76/500:  train Loss: 85.8254   val Loss: 85.1321   time: 0.26s   best: 85.1321
2023-10-05 14:14:49,268:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:49,354:INFO:  Epoch 77/500:  train Loss: 84.9467   val Loss: 85.0488   time: 0.25s   best: 85.0488
2023-10-05 14:14:49,606:INFO:  Epoch 78/500:  train Loss: 86.1095   val Loss: 85.5924   time: 0.25s   best: 85.0488
2023-10-05 14:14:49,873:INFO:  Epoch 79/500:  train Loss: 86.5847   val Loss: 86.3014   time: 0.26s   best: 85.0488
2023-10-05 14:14:50,127:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:50,166:INFO:  Epoch 80/500:  train Loss: 86.1002   val Loss: 84.4503   time: 0.25s   best: 84.4503
2023-10-05 14:14:50,436:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:50,463:INFO:  Epoch 81/500:  train Loss: 84.8907   val Loss: 84.1453   time: 0.26s   best: 84.1453
2023-10-05 14:14:50,718:INFO:  Epoch 82/500:  train Loss: 84.8244   val Loss: 84.7944   time: 0.25s   best: 84.1453
2023-10-05 14:14:50,988:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:51,056:INFO:  Epoch 83/500:  train Loss: 85.0667   val Loss: 84.0823   time: 0.26s   best: 84.0823
2023-10-05 14:14:51,315:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:51,337:INFO:  Epoch 84/500:  train Loss: 84.4484   val Loss: 83.5954   time: 0.25s   best: 83.5954
2023-10-05 14:14:51,602:INFO:  Epoch 85/500:  train Loss: 84.4049   val Loss: 83.7139   time: 0.26s   best: 83.5954
2023-10-05 14:14:51,854:INFO:  Epoch 86/500:  train Loss: 84.2220   val Loss: 83.6843   time: 0.25s   best: 83.5954
2023-10-05 14:14:52,121:INFO:  Epoch 87/500:  train Loss: 84.2743   val Loss: 83.8907   time: 0.26s   best: 83.5954
2023-10-05 14:14:52,379:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:52,456:INFO:  Epoch 88/500:  train Loss: 83.8812   val Loss: 83.2383   time: 0.25s   best: 83.2383
2023-10-05 14:14:52,713:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:52,758:INFO:  Epoch 89/500:  train Loss: 83.4411   val Loss: 82.8540   time: 0.25s   best: 82.8540
2023-10-05 14:14:53,022:INFO:  Epoch 90/500:  train Loss: 83.6452   val Loss: 83.1291   time: 0.26s   best: 82.8540
2023-10-05 14:14:53,277:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:53,298:INFO:  Epoch 91/500:  train Loss: 83.7640   val Loss: 82.5641   time: 0.25s   best: 82.5641
2023-10-05 14:14:53,551:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:53,590:INFO:  Epoch 92/500:  train Loss: 83.1059   val Loss: 82.4777   time: 0.25s   best: 82.4777
2023-10-05 14:14:53,840:INFO:  Epoch 93/500:  train Loss: 82.6740   val Loss: 82.7721   time: 0.25s   best: 82.4777
2023-10-05 14:14:54,089:INFO:  Epoch 94/500:  train Loss: 83.2167   val Loss: 83.1939   time: 0.25s   best: 82.4777
2023-10-05 14:14:54,354:INFO:  Epoch 95/500:  train Loss: 83.2847   val Loss: 82.5482   time: 0.26s   best: 82.4777
2023-10-05 14:14:54,816:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:54,857:INFO:  Epoch 96/500:  train Loss: 84.3202   val Loss: 81.9367   time: 0.45s   best: 81.9367
2023-10-05 14:14:55,158:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:55,212:INFO:  Epoch 97/500:  train Loss: 83.0541   val Loss: 81.6162   time: 0.30s   best: 81.6162
2023-10-05 14:14:55,467:INFO:  Epoch 98/500:  train Loss: 82.3461   val Loss: 82.1152   time: 0.25s   best: 81.6162
2023-10-05 14:14:55,734:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:55,784:INFO:  Epoch 99/500:  train Loss: 82.5380   val Loss: 81.4202   time: 0.26s   best: 81.4202
2023-10-05 14:14:56,638:INFO:  Epoch 100/500:  train Loss: 83.5700   val Loss: 81.5318   time: 0.85s   best: 81.4202
2023-10-05 14:14:56,930:INFO:  Epoch 101/500:  train Loss: 82.7654   val Loss: 81.7081   time: 0.28s   best: 81.4202
2023-10-05 14:14:57,237:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:57,268:INFO:  Epoch 102/500:  train Loss: 82.5534   val Loss: 81.2080   time: 0.30s   best: 81.2080
2023-10-05 14:14:57,524:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:57,572:INFO:  Epoch 103/500:  train Loss: 82.4892   val Loss: 80.5831   time: 0.25s   best: 80.5831
2023-10-05 14:14:57,856:INFO:  Epoch 104/500:  train Loss: 82.3536   val Loss: 81.3859   time: 0.27s   best: 80.5831
2023-10-05 14:14:58,117:INFO:  Epoch 105/500:  train Loss: 82.1326   val Loss: 81.0017   time: 0.25s   best: 80.5831
2023-10-05 14:14:58,388:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:58,468:INFO:  Epoch 106/500:  train Loss: 81.1440   val Loss: 80.5621   time: 0.27s   best: 80.5621
2023-10-05 14:14:58,730:INFO:  Epoch 107/500:  train Loss: 81.7672   val Loss: 82.2677   time: 0.25s   best: 80.5621
2023-10-05 14:14:59,014:INFO:  Epoch 108/500:  train Loss: 82.2849   val Loss: 81.2152   time: 0.27s   best: 80.5621
2023-10-05 14:14:59,284:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:59,359:INFO:  Epoch 109/500:  train Loss: 82.1274   val Loss: 80.4526   time: 0.25s   best: 80.4526
2023-10-05 14:14:59,613:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:14:59,750:INFO:  Epoch 110/500:  train Loss: 81.9911   val Loss: 80.0912   time: 0.25s   best: 80.0912
2023-10-05 14:15:00,012:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:00,042:INFO:  Epoch 111/500:  train Loss: 80.9921   val Loss: 79.7375   time: 0.26s   best: 79.7375
2023-10-05 14:15:00,317:INFO:  Epoch 112/500:  train Loss: 80.9900   val Loss: 80.2601   time: 0.25s   best: 79.7375
2023-10-05 14:15:00,581:INFO:  Epoch 113/500:  train Loss: 81.0888   val Loss: 80.2801   time: 0.25s   best: 79.7375
2023-10-05 14:15:00,857:INFO:  Epoch 114/500:  train Loss: 81.4811   val Loss: 81.8211   time: 0.26s   best: 79.7375
2023-10-05 14:15:01,126:INFO:  Epoch 115/500:  train Loss: 86.0526   val Loss: 88.2383   time: 0.25s   best: 79.7375
2023-10-05 14:15:01,402:INFO:  Epoch 116/500:  train Loss: 87.9447   val Loss: 86.2802   time: 0.26s   best: 79.7375
2023-10-05 14:15:01,665:INFO:  Epoch 117/500:  train Loss: 85.6624   val Loss: 86.0699   time: 0.25s   best: 79.7375
2023-10-05 14:15:01,939:INFO:  Epoch 118/500:  train Loss: 87.0586   val Loss: 85.7180   time: 0.26s   best: 79.7375
2023-10-05 14:15:02,193:INFO:  Epoch 119/500:  train Loss: 85.7170   val Loss: 83.9737   time: 0.25s   best: 79.7375
2023-10-05 14:15:02,478:INFO:  Epoch 120/500:  train Loss: 84.1483   val Loss: 82.9600   time: 0.27s   best: 79.7375
2023-10-05 14:15:02,740:INFO:  Epoch 121/500:  train Loss: 82.5877   val Loss: 81.3975   time: 0.25s   best: 79.7375
2023-10-05 14:15:03,094:INFO:  Epoch 122/500:  train Loss: 81.5341   val Loss: 80.3006   time: 0.34s   best: 79.7375
2023-10-05 14:15:03,353:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:03,383:INFO:  Epoch 123/500:  train Loss: 81.0089   val Loss: 79.6876   time: 0.25s   best: 79.6876
2023-10-05 14:15:03,650:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:03,700:INFO:  Epoch 124/500:  train Loss: 80.1894   val Loss: 79.1815   time: 0.26s   best: 79.1815
2023-10-05 14:15:03,970:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:03,994:INFO:  Epoch 125/500:  train Loss: 79.8651   val Loss: 78.9220   time: 0.26s   best: 78.9220
2023-10-05 14:15:04,254:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:06,179:INFO:  Epoch 126/500:  train Loss: 80.0262   val Loss: 78.7766   time: 0.26s   best: 78.7766
2023-10-05 14:15:06,500:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:06,566:INFO:  Epoch 127/500:  train Loss: 79.5065   val Loss: 78.7728   time: 0.32s   best: 78.7728
2023-10-05 14:15:06,829:INFO:  Epoch 128/500:  train Loss: 80.5886   val Loss: 79.0403   time: 0.25s   best: 78.7728
2023-10-05 14:15:07,109:INFO:  Epoch 129/500:  train Loss: 79.9047   val Loss: 78.9222   time: 0.27s   best: 78.7728
2023-10-05 14:15:07,374:INFO:  Epoch 130/500:  train Loss: 79.6403   val Loss: 79.0501   time: 0.25s   best: 78.7728
2023-10-05 14:15:07,649:INFO:  Epoch 131/500:  train Loss: 79.4021   val Loss: 79.0506   time: 0.26s   best: 78.7728
2023-10-05 14:15:07,899:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:07,923:INFO:  Epoch 132/500:  train Loss: 79.3550   val Loss: 78.6804   time: 0.24s   best: 78.6804
2023-10-05 14:15:08,200:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:08,269:INFO:  Epoch 133/500:  train Loss: 78.9580   val Loss: 78.3405   time: 0.27s   best: 78.3405
2023-10-05 14:15:08,541:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:08,573:INFO:  Epoch 134/500:  train Loss: 79.1568   val Loss: 78.2315   time: 0.27s   best: 78.2315
2023-10-05 14:15:08,835:INFO:  Epoch 135/500:  train Loss: 79.8626   val Loss: 78.3776   time: 0.25s   best: 78.2315
2023-10-05 14:15:09,110:INFO:  Epoch 136/500:  train Loss: 79.7826   val Loss: 78.2930   time: 0.26s   best: 78.2315
2023-10-05 14:15:09,374:INFO:  Epoch 137/500:  train Loss: 78.4730   val Loss: 78.5160   time: 0.25s   best: 78.2315
2023-10-05 14:15:09,646:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:10,002:INFO:  Epoch 138/500:  train Loss: 78.9099   val Loss: 78.0664   time: 0.27s   best: 78.0664
2023-10-05 14:15:10,262:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:10,342:INFO:  Epoch 139/500:  train Loss: 78.9161   val Loss: 77.2727   time: 0.25s   best: 77.2727
2023-10-05 14:15:10,621:INFO:  Epoch 140/500:  train Loss: 79.0970   val Loss: 77.7834   time: 0.27s   best: 77.2727
2023-10-05 14:15:10,881:INFO:  Epoch 141/500:  train Loss: 78.9399   val Loss: 77.9006   time: 0.25s   best: 77.2727
2023-10-05 14:15:11,161:INFO:  Epoch 142/500:  train Loss: 78.6955   val Loss: 77.6792   time: 0.27s   best: 77.2727
2023-10-05 14:15:11,423:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:11,448:INFO:  Epoch 143/500:  train Loss: 78.4150   val Loss: 76.9223   time: 0.26s   best: 76.9223
2023-10-05 14:15:11,719:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:12,251:INFO:  Epoch 144/500:  train Loss: 78.1278   val Loss: 76.8098   time: 0.27s   best: 76.8098
2023-10-05 14:15:12,516:INFO:  Epoch 145/500:  train Loss: 78.8289   val Loss: 76.8871   time: 0.25s   best: 76.8098
2023-10-05 14:15:12,787:INFO:  Epoch 146/500:  train Loss: 77.6065   val Loss: 77.3843   time: 0.26s   best: 76.8098
2023-10-05 14:15:13,046:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:13,077:INFO:  Epoch 147/500:  train Loss: 77.6762   val Loss: 76.5124   time: 0.25s   best: 76.5124
2023-10-05 14:15:13,358:INFO:  Epoch 148/500:  train Loss: 79.6671   val Loss: 76.9245   time: 0.27s   best: 76.5124
2023-10-05 14:15:13,618:INFO:  Epoch 149/500:  train Loss: 78.0214   val Loss: 76.7464   time: 0.25s   best: 76.5124
2023-10-05 14:15:13,894:INFO:  Epoch 150/500:  train Loss: 79.7979   val Loss: 77.4968   time: 0.26s   best: 76.5124
2023-10-05 14:15:14,154:INFO:  Epoch 151/500:  train Loss: 77.8640   val Loss: 77.5677   time: 0.25s   best: 76.5124
2023-10-05 14:15:14,435:INFO:  Epoch 152/500:  train Loss: 77.7018   val Loss: 76.9667   time: 0.27s   best: 76.5124
2023-10-05 14:15:14,705:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:14,950:INFO:  Epoch 153/500:  train Loss: 78.4967   val Loss: 76.0984   time: 0.26s   best: 76.0984
2023-10-05 14:15:15,217:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:15,283:INFO:  Epoch 154/500:  train Loss: 77.3925   val Loss: 75.7620   time: 0.26s   best: 75.7620
2023-10-05 14:15:15,538:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:15,568:INFO:  Epoch 155/500:  train Loss: 80.7047   val Loss: 75.7025   time: 0.25s   best: 75.7025
2023-10-05 14:15:15,833:INFO:  Epoch 156/500:  train Loss: 77.3846   val Loss: 77.1175   time: 0.26s   best: 75.7025
2023-10-05 14:15:16,101:INFO:  Epoch 157/500:  train Loss: 78.3338   val Loss: 76.8537   time: 0.26s   best: 75.7025
2023-10-05 14:15:16,378:INFO:  Epoch 158/500:  train Loss: 78.9177   val Loss: 78.1201   time: 0.26s   best: 75.7025
2023-10-05 14:15:16,643:INFO:  Epoch 159/500:  train Loss: 78.5797   val Loss: 78.0827   time: 0.25s   best: 75.7025
2023-10-05 14:15:16,916:INFO:  Epoch 160/500:  train Loss: 79.0204   val Loss: 77.3471   time: 0.26s   best: 75.7025
2023-10-05 14:15:17,182:INFO:  Epoch 161/500:  train Loss: 78.3093   val Loss: 76.9937   time: 0.25s   best: 75.7025
2023-10-05 14:15:17,460:INFO:  Epoch 162/500:  train Loss: 77.7173   val Loss: 76.0240   time: 0.27s   best: 75.7025
2023-10-05 14:15:17,714:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:17,767:INFO:  Epoch 163/500:  train Loss: 76.7862   val Loss: 74.9978   time: 0.25s   best: 74.9978
2023-10-05 14:15:18,041:INFO:  Epoch 164/500:  train Loss: 77.1602   val Loss: 75.0150   time: 0.26s   best: 74.9978
2023-10-05 14:15:18,308:INFO:  Epoch 165/500:  train Loss: 76.4955   val Loss: 75.4730   time: 0.25s   best: 74.9978
2023-10-05 14:15:18,581:INFO:  Epoch 166/500:  train Loss: 76.1032   val Loss: 75.0627   time: 0.26s   best: 74.9978
2023-10-05 14:15:18,851:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:18,882:INFO:  Epoch 167/500:  train Loss: 76.9790   val Loss: 74.4005   time: 0.26s   best: 74.4005
2023-10-05 14:15:19,141:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:19,255:INFO:  Epoch 168/500:  train Loss: 75.9541   val Loss: 74.3001   time: 0.25s   best: 74.3001
2023-10-05 14:15:19,519:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:19,556:INFO:  Epoch 169/500:  train Loss: 75.6729   val Loss: 74.0562   time: 0.26s   best: 74.0562
2023-10-05 14:15:19,817:INFO:  Epoch 170/500:  train Loss: 75.1275   val Loss: 75.3724   time: 0.25s   best: 74.0562
2023-10-05 14:15:20,092:INFO:  Epoch 171/500:  train Loss: 75.5557   val Loss: 75.2428   time: 0.26s   best: 74.0562
2023-10-05 14:15:20,354:INFO:  Epoch 172/500:  train Loss: 77.1988   val Loss: 74.9634   time: 0.25s   best: 74.0562
2023-10-05 14:15:20,628:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:20,659:INFO:  Epoch 173/500:  train Loss: 75.3676   val Loss: 73.5395   time: 0.27s   best: 73.5395
2023-10-05 14:15:20,934:INFO:  Epoch 174/500:  train Loss: 76.5261   val Loss: 75.6154   time: 0.26s   best: 73.5395
2023-10-05 14:15:21,201:INFO:  Epoch 175/500:  train Loss: 78.5166   val Loss: 75.7340   time: 0.25s   best: 73.5395
2023-10-05 14:15:21,480:INFO:  Epoch 176/500:  train Loss: 77.2702   val Loss: 75.5053   time: 0.27s   best: 73.5395
2023-10-05 14:15:21,743:INFO:  Epoch 177/500:  train Loss: 79.1575   val Loss: 80.5701   time: 0.25s   best: 73.5395
2023-10-05 14:15:22,018:INFO:  Epoch 178/500:  train Loss: 80.7565   val Loss: 79.6671   time: 0.26s   best: 73.5395
2023-10-05 14:15:22,278:INFO:  Epoch 179/500:  train Loss: 81.4157   val Loss: 79.9919   time: 0.25s   best: 73.5395
2023-10-05 14:15:22,552:INFO:  Epoch 180/500:  train Loss: 80.0644   val Loss: 78.2779   time: 0.27s   best: 73.5395
2023-10-05 14:15:22,821:INFO:  Epoch 181/500:  train Loss: 79.1441   val Loss: 76.7680   time: 0.26s   best: 73.5395
2023-10-05 14:15:23,103:INFO:  Epoch 182/500:  train Loss: 76.9670   val Loss: 75.3510   time: 0.27s   best: 73.5395
2023-10-05 14:15:23,367:INFO:  Epoch 183/500:  train Loss: 76.1154   val Loss: 73.9963   time: 0.25s   best: 73.5395
2023-10-05 14:15:23,634:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:23,707:INFO:  Epoch 184/500:  train Loss: 74.8536   val Loss: 73.2573   time: 0.26s   best: 73.2573
2023-10-05 14:15:23,967:INFO:  Epoch 185/500:  train Loss: 75.0926   val Loss: 73.6843   time: 0.25s   best: 73.2573
2023-10-05 14:15:24,235:INFO:  Epoch 186/500:  train Loss: 75.1156   val Loss: 73.3194   time: 0.26s   best: 73.2573
2023-10-05 14:15:24,511:INFO:  Epoch 187/500:  train Loss: 76.4381   val Loss: 73.5354   time: 0.26s   best: 73.2573
2023-10-05 14:15:24,782:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:24,813:INFO:  Epoch 188/500:  train Loss: 74.6547   val Loss: 72.8670   time: 0.27s   best: 72.8670
2023-10-05 14:15:25,124:INFO:  Epoch 189/500:  train Loss: 74.6504   val Loss: 73.8427   time: 0.30s   best: 72.8670
2023-10-05 14:15:25,558:INFO:  Epoch 190/500:  train Loss: 75.4452   val Loss: 73.6766   time: 0.42s   best: 72.8670
2023-10-05 14:15:25,822:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:25,874:INFO:  Epoch 191/500:  train Loss: 76.2703   val Loss: 72.6455   time: 0.26s   best: 72.6455
2023-10-05 14:15:26,150:INFO:  Epoch 192/500:  train Loss: 74.1260   val Loss: 72.7500   time: 0.26s   best: 72.6455
2023-10-05 14:15:26,411:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:26,499:INFO:  Epoch 193/500:  train Loss: 74.3584   val Loss: 72.5809   time: 0.26s   best: 72.5809
2023-10-05 14:15:26,773:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:26,804:INFO:  Epoch 194/500:  train Loss: 74.7140   val Loss: 72.4741   time: 0.27s   best: 72.4741
2023-10-05 14:15:27,066:INFO:  Epoch 195/500:  train Loss: 73.8919   val Loss: 72.6059   time: 0.25s   best: 72.4741
2023-10-05 14:15:27,343:INFO:  Epoch 196/500:  train Loss: 74.5528   val Loss: 73.3668   time: 0.26s   best: 72.4741
2023-10-05 14:15:27,604:INFO:  Epoch 197/500:  train Loss: 74.9004   val Loss: 72.9805   time: 0.26s   best: 72.4741
2023-10-05 14:15:27,872:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:27,923:INFO:  Epoch 198/500:  train Loss: 74.7055   val Loss: 72.2198   time: 0.26s   best: 72.2198
2023-10-05 14:15:28,207:INFO:  Epoch 199/500:  train Loss: 73.6688   val Loss: 72.3559   time: 0.27s   best: 72.2198
2023-10-05 14:15:28,505:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:28,581:INFO:  Epoch 200/500:  train Loss: 74.0836   val Loss: 72.1258   time: 0.29s   best: 72.1258
2023-10-05 14:15:28,858:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:29,137:INFO:  Epoch 201/500:  train Loss: 73.7788   val Loss: 72.1128   time: 0.27s   best: 72.1128
2023-10-05 14:15:29,401:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:29,496:INFO:  Epoch 202/500:  train Loss: 73.8603   val Loss: 71.9683   time: 0.26s   best: 71.9683
2023-10-05 14:15:29,774:INFO:  Epoch 203/500:  train Loss: 73.5867   val Loss: 73.5654   time: 0.27s   best: 71.9683
2023-10-05 14:15:30,035:INFO:  Epoch 204/500:  train Loss: 74.1549   val Loss: 73.3893   time: 0.25s   best: 71.9683
2023-10-05 14:15:30,311:INFO:  Epoch 205/500:  train Loss: 74.6380   val Loss: 72.4584   time: 0.26s   best: 71.9683
2023-10-05 14:15:30,576:INFO:  Epoch 206/500:  train Loss: 73.3400   val Loss: 72.2366   time: 0.25s   best: 71.9683
2023-10-05 14:15:30,852:INFO:  Epoch 207/500:  train Loss: 74.2633   val Loss: 74.1266   time: 0.26s   best: 71.9683
2023-10-05 14:15:31,112:INFO:  Epoch 208/500:  train Loss: 75.2797   val Loss: 73.1713   time: 0.25s   best: 71.9683
2023-10-05 14:15:31,387:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:31,418:INFO:  Epoch 209/500:  train Loss: 75.3119   val Loss: 71.3935   time: 0.27s   best: 71.3935
2023-10-05 14:15:31,684:INFO:  Epoch 210/500:  train Loss: 74.1213   val Loss: 71.5895   time: 0.25s   best: 71.3935
2023-10-05 14:15:31,953:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:32,013:INFO:  Epoch 211/500:  train Loss: 72.4124   val Loss: 71.1925   time: 0.26s   best: 71.1925
2023-10-05 14:15:32,267:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:32,332:INFO:  Epoch 212/500:  train Loss: 74.1428   val Loss: 70.8157   time: 0.25s   best: 70.8157
2023-10-05 14:15:32,599:INFO:  Epoch 213/500:  train Loss: 74.2143   val Loss: 71.9733   time: 0.26s   best: 70.8157
2023-10-05 14:15:32,874:INFO:  Epoch 214/500:  train Loss: 72.3584   val Loss: 71.0155   time: 0.26s   best: 70.8157
2023-10-05 14:15:33,133:INFO:  Epoch 215/500:  train Loss: 73.8125   val Loss: 71.7155   time: 0.25s   best: 70.8157
2023-10-05 14:15:33,411:INFO:  Epoch 216/500:  train Loss: 72.8115   val Loss: 71.4134   time: 0.27s   best: 70.8157
2023-10-05 14:15:33,701:INFO:  Epoch 217/500:  train Loss: 73.9911   val Loss: 72.2366   time: 0.26s   best: 70.8157
2023-10-05 14:15:33,975:INFO:  Epoch 218/500:  train Loss: 73.6883   val Loss: 71.0830   time: 0.26s   best: 70.8157
2023-10-05 14:15:34,227:INFO:  Epoch 219/500:  train Loss: 73.3222   val Loss: 70.8768   time: 0.25s   best: 70.8157
2023-10-05 14:15:34,507:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:34,538:INFO:  Epoch 220/500:  train Loss: 71.4496   val Loss: 70.2298   time: 0.27s   best: 70.2298
2023-10-05 14:15:34,800:INFO:  Epoch 221/500:  train Loss: 73.6027   val Loss: 71.3393   time: 0.25s   best: 70.2298
2023-10-05 14:15:35,073:INFO:  Epoch 222/500:  train Loss: 73.8965   val Loss: 72.3338   time: 0.26s   best: 70.2298
2023-10-05 14:15:35,336:INFO:  Epoch 223/500:  train Loss: 74.4082   val Loss: 72.5375   time: 0.25s   best: 70.2298
2023-10-05 14:15:35,619:INFO:  Epoch 224/500:  train Loss: 73.4230   val Loss: 71.1214   time: 0.27s   best: 70.2298
2023-10-05 14:15:35,872:INFO:  Epoch 225/500:  train Loss: 72.7382   val Loss: 70.4564   time: 0.25s   best: 70.2298
2023-10-05 14:15:36,153:INFO:  Epoch 226/500:  train Loss: 71.6417   val Loss: 70.6403   time: 0.27s   best: 70.2298
2023-10-05 14:15:36,418:INFO:  Epoch 227/500:  train Loss: 72.8300   val Loss: 71.0890   time: 0.25s   best: 70.2298
2023-10-05 14:15:36,695:INFO:  Epoch 228/500:  train Loss: 71.5797   val Loss: 70.3353   time: 0.26s   best: 70.2298
2023-10-05 14:15:36,962:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:37,012:INFO:  Epoch 229/500:  train Loss: 71.9870   val Loss: 70.0611   time: 0.26s   best: 70.0611
2023-10-05 14:15:37,272:INFO:  Epoch 230/500:  train Loss: 71.5307   val Loss: 70.8289   time: 0.25s   best: 70.0611
2023-10-05 14:15:37,548:INFO:  Epoch 231/500:  train Loss: 72.7315   val Loss: 70.5893   time: 0.26s   best: 70.0611
2023-10-05 14:15:37,807:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:37,838:INFO:  Epoch 232/500:  train Loss: 71.4915   val Loss: 69.8888   time: 0.25s   best: 69.8888
2023-10-05 14:15:38,106:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:38,151:INFO:  Epoch 233/500:  train Loss: 71.1749   val Loss: 69.2964   time: 0.26s   best: 69.2964
2023-10-05 14:15:38,404:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:38,444:INFO:  Epoch 234/500:  train Loss: 70.5293   val Loss: 69.2605   time: 0.25s   best: 69.2605
2023-10-05 14:15:38,720:INFO:  Epoch 235/500:  train Loss: 71.8619   val Loss: 70.6682   time: 0.26s   best: 69.2605
2023-10-05 14:15:38,980:INFO:  Epoch 236/500:  train Loss: 71.6831   val Loss: 70.8244   time: 0.25s   best: 69.2605
2023-10-05 14:15:39,255:INFO:  Epoch 237/500:  train Loss: 71.8565   val Loss: 72.1469   time: 0.26s   best: 69.2605
2023-10-05 14:15:39,517:INFO:  Epoch 238/500:  train Loss: 72.5199   val Loss: 71.5245   time: 0.25s   best: 69.2605
2023-10-05 14:15:39,799:INFO:  Epoch 239/500:  train Loss: 73.8425   val Loss: 71.4792   time: 0.27s   best: 69.2605
2023-10-05 14:15:40,072:INFO:  Epoch 240/500:  train Loss: 73.0829   val Loss: 70.6712   time: 0.25s   best: 69.2605
2023-10-05 14:15:40,332:INFO:  Epoch 241/500:  train Loss: 79.1686   val Loss: 77.4806   time: 0.25s   best: 69.2605
2023-10-05 14:15:40,612:INFO:  Epoch 242/500:  train Loss: 83.0987   val Loss: 80.0917   time: 0.27s   best: 69.2605
2023-10-05 14:15:40,875:INFO:  Epoch 243/500:  train Loss: 77.5993   val Loss: 77.7523   time: 0.25s   best: 69.2605
2023-10-05 14:15:41,149:INFO:  Epoch 244/500:  train Loss: 79.1130   val Loss: 75.6405   time: 0.26s   best: 69.2605
2023-10-05 14:15:41,410:INFO:  Epoch 245/500:  train Loss: 75.8066   val Loss: 74.6752   time: 0.25s   best: 69.2605
2023-10-05 14:15:41,698:INFO:  Epoch 246/500:  train Loss: 76.4172   val Loss: 73.9266   time: 0.28s   best: 69.2605
2023-10-05 14:15:41,960:INFO:  Epoch 247/500:  train Loss: 74.1642   val Loss: 73.0874   time: 0.25s   best: 69.2605
2023-10-05 14:15:42,235:INFO:  Epoch 248/500:  train Loss: 73.9329   val Loss: 73.2192   time: 0.26s   best: 69.2605
2023-10-05 14:15:42,500:INFO:  Epoch 249/500:  train Loss: 74.0136   val Loss: 72.5494   time: 0.25s   best: 69.2605
2023-10-05 14:15:42,778:INFO:  Epoch 250/500:  train Loss: 73.1166   val Loss: 72.7411   time: 0.26s   best: 69.2605
2023-10-05 14:15:43,037:INFO:  Epoch 251/500:  train Loss: 74.4372   val Loss: 71.6695   time: 0.25s   best: 69.2605
2023-10-05 14:15:43,314:INFO:  Epoch 252/500:  train Loss: 73.1669   val Loss: 71.4024   time: 0.26s   best: 69.2605
2023-10-05 14:15:43,575:INFO:  Epoch 253/500:  train Loss: 72.6431   val Loss: 69.9904   time: 0.25s   best: 69.2605
2023-10-05 14:15:43,858:INFO:  Epoch 254/500:  train Loss: 71.3577   val Loss: 69.7972   time: 0.27s   best: 69.2605
2023-10-05 14:15:44,117:INFO:  Epoch 255/500:  train Loss: 71.3934   val Loss: 70.1023   time: 0.25s   best: 69.2605
2023-10-05 14:15:44,384:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:44,415:INFO:  Epoch 256/500:  train Loss: 71.4294   val Loss: 68.9187   time: 0.26s   best: 68.9187
2023-10-05 14:15:44,682:INFO:  Epoch 257/500:  train Loss: 72.1564   val Loss: 70.7620   time: 0.25s   best: 68.9187
2023-10-05 14:15:44,957:INFO:  Epoch 258/500:  train Loss: 73.0398   val Loss: 71.5178   time: 0.26s   best: 68.9187
2023-10-05 14:15:45,216:INFO:  Epoch 259/500:  train Loss: 72.4896   val Loss: 71.7808   time: 0.25s   best: 68.9187
2023-10-05 14:15:45,496:INFO:  Epoch 260/500:  train Loss: 80.8370   val Loss: 79.8325   time: 0.27s   best: 68.9187
2023-10-05 14:15:45,774:INFO:  Epoch 261/500:  train Loss: 76.9481   val Loss: 74.7171   time: 0.25s   best: 68.9187
2023-10-05 14:15:46,035:INFO:  Epoch 262/500:  train Loss: 77.1425   val Loss: 73.6622   time: 0.25s   best: 68.9187
2023-10-05 14:15:46,309:INFO:  Epoch 263/500:  train Loss: 73.6559   val Loss: 72.0769   time: 0.26s   best: 68.9187
2023-10-05 14:15:46,574:INFO:  Epoch 264/500:  train Loss: 72.5670   val Loss: 75.5042   time: 0.25s   best: 68.9187
2023-10-05 14:15:46,851:INFO:  Epoch 265/500:  train Loss: 78.1799   val Loss: 74.0152   time: 0.26s   best: 68.9187
2023-10-05 14:15:47,111:INFO:  Epoch 266/500:  train Loss: 74.6174   val Loss: 71.6273   time: 0.25s   best: 68.9187
2023-10-05 14:15:47,388:INFO:  Epoch 267/500:  train Loss: 74.2853   val Loss: 70.7283   time: 0.26s   best: 68.9187
2023-10-05 14:15:47,656:INFO:  Epoch 268/500:  train Loss: 72.2247   val Loss: 71.0270   time: 0.26s   best: 68.9187
2023-10-05 14:15:47,930:INFO:  Epoch 269/500:  train Loss: 72.4545   val Loss: 69.9327   time: 0.26s   best: 68.9187
2023-10-05 14:15:48,190:INFO:  Epoch 270/500:  train Loss: 70.6406   val Loss: 71.6802   time: 0.25s   best: 68.9187
2023-10-05 14:15:48,471:INFO:  Epoch 271/500:  train Loss: 71.7024   val Loss: 69.3320   time: 0.27s   best: 68.9187
2023-10-05 14:15:48,733:INFO:  Epoch 272/500:  train Loss: 70.4382   val Loss: 71.0502   time: 0.25s   best: 68.9187
2023-10-05 14:15:49,009:INFO:  Epoch 273/500:  train Loss: 70.9053   val Loss: 69.7358   time: 0.26s   best: 68.9187
2023-10-05 14:15:49,268:INFO:  Epoch 274/500:  train Loss: 72.9025   val Loss: 69.4386   time: 0.25s   best: 68.9187
2023-10-05 14:15:49,545:INFO:  Epoch 275/500:  train Loss: 70.7853   val Loss: 69.2806   time: 0.26s   best: 68.9187
2023-10-05 14:15:49,807:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:49,862:INFO:  Epoch 276/500:  train Loss: 71.5206   val Loss: 68.3627   time: 0.26s   best: 68.3627
2023-10-05 14:15:50,130:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:15:50,161:INFO:  Epoch 277/500:  train Loss: 70.0052   val Loss: 67.1988   time: 0.26s   best: 67.1988
2023-10-05 14:15:50,422:INFO:  Epoch 278/500:  train Loss: 69.5098   val Loss: 68.9099   time: 0.25s   best: 67.1988
2023-10-05 14:15:50,702:INFO:  Epoch 279/500:  train Loss: 69.7721   val Loss: 77.0134   time: 0.27s   best: 67.1988
2023-10-05 14:15:50,956:INFO:  Epoch 280/500:  train Loss: 95.3366   val Loss: 94.5298   time: 0.25s   best: 67.1988
2023-10-05 14:15:51,238:INFO:  Epoch 281/500:  train Loss: 93.6189   val Loss: 94.1443   time: 0.27s   best: 67.1988
2023-10-05 14:15:51,518:INFO:  Epoch 282/500:  train Loss: 94.2981   val Loss: 95.1080   time: 0.27s   best: 67.1988
2023-10-05 14:15:51,786:INFO:  Epoch 283/500:  train Loss: 94.7538   val Loss: 94.9139   time: 0.26s   best: 67.1988
2023-10-05 14:15:52,062:INFO:  Epoch 284/500:  train Loss: 94.3426   val Loss: 94.0741   time: 0.26s   best: 67.1988
2023-10-05 14:15:52,323:INFO:  Epoch 285/500:  train Loss: 93.5760   val Loss: 92.8677   time: 0.25s   best: 67.1988
2023-10-05 14:15:52,596:INFO:  Epoch 286/500:  train Loss: 92.9938   val Loss: 92.8398   time: 0.27s   best: 67.1988
2023-10-05 14:15:52,868:INFO:  Epoch 287/500:  train Loss: 93.1614   val Loss: 93.2276   time: 0.26s   best: 67.1988
2023-10-05 14:15:53,142:INFO:  Epoch 288/500:  train Loss: 93.4256   val Loss: 93.2511   time: 0.26s   best: 67.1988
2023-10-05 14:15:53,403:INFO:  Epoch 289/500:  train Loss: 93.3796   val Loss: 93.2283   time: 0.25s   best: 67.1988
2023-10-05 14:15:53,685:INFO:  Epoch 290/500:  train Loss: 92.8335   val Loss: 92.6180   time: 0.27s   best: 67.1988
2023-10-05 14:15:53,947:INFO:  Epoch 291/500:  train Loss: 92.1564   val Loss: 91.6699   time: 0.25s   best: 67.1988
2023-10-05 14:15:54,227:INFO:  Epoch 292/500:  train Loss: 91.9109   val Loss: 91.4017   time: 0.26s   best: 67.1988
2023-10-05 14:15:54,496:INFO:  Epoch 293/500:  train Loss: 91.5349   val Loss: 91.4663   time: 0.26s   best: 67.1988
2023-10-05 14:15:54,773:INFO:  Epoch 294/500:  train Loss: 91.6806   val Loss: 91.4161   time: 0.26s   best: 67.1988
2023-10-05 14:15:55,032:INFO:  Epoch 295/500:  train Loss: 91.2889   val Loss: 90.9617   time: 0.25s   best: 67.1988
2023-10-05 14:15:55,308:INFO:  Epoch 296/500:  train Loss: 90.7925   val Loss: 90.4781   time: 0.26s   best: 67.1988
2023-10-05 14:15:55,570:INFO:  Epoch 297/500:  train Loss: 90.3764   val Loss: 90.0593   time: 0.25s   best: 67.1988
2023-10-05 14:15:56,017:INFO:  Epoch 298/500:  train Loss: 90.0835   val Loss: 89.8104   time: 0.43s   best: 67.1988
2023-10-05 14:15:56,316:INFO:  Epoch 299/500:  train Loss: 89.6519   val Loss: 89.4386   time: 0.29s   best: 67.1988
2023-10-05 14:15:56,618:INFO:  Epoch 300/500:  train Loss: 89.1964   val Loss: 88.6185   time: 0.30s   best: 67.1988
2023-10-05 14:15:56,918:INFO:  Epoch 301/500:  train Loss: 88.0786   val Loss: 87.6704   time: 0.29s   best: 67.1988
2023-10-05 14:15:57,177:INFO:  Epoch 302/500:  train Loss: 87.8156   val Loss: 86.7196   time: 0.25s   best: 67.1988
2023-10-05 14:15:57,454:INFO:  Epoch 303/500:  train Loss: 86.7970   val Loss: 85.6248   time: 0.26s   best: 67.1988
2023-10-05 14:15:57,715:INFO:  Epoch 304/500:  train Loss: 85.3937   val Loss: 84.2856   time: 0.25s   best: 67.1988
2023-10-05 14:15:57,991:INFO:  Epoch 305/500:  train Loss: 84.0700   val Loss: 82.0265   time: 0.26s   best: 67.1988
2023-10-05 14:15:58,272:INFO:  Epoch 306/500:  train Loss: 82.2538   val Loss: 80.0367   time: 0.27s   best: 67.1988
2023-10-05 14:15:58,537:INFO:  Epoch 307/500:  train Loss: 79.9080   val Loss: 77.7385   time: 0.25s   best: 67.1988
2023-10-05 14:15:58,813:INFO:  Epoch 308/500:  train Loss: 77.5430   val Loss: 75.4850   time: 0.26s   best: 67.1988
2023-10-05 14:15:59,073:INFO:  Epoch 309/500:  train Loss: 75.3702   val Loss: 73.7351   time: 0.25s   best: 67.1988
2023-10-05 14:15:59,348:INFO:  Epoch 310/500:  train Loss: 74.0771   val Loss: 72.3966   time: 0.26s   best: 67.1988
2023-10-05 14:15:59,609:INFO:  Epoch 311/500:  train Loss: 72.7544   val Loss: 72.4784   time: 0.25s   best: 67.1988
2023-10-05 14:15:59,885:INFO:  Epoch 312/500:  train Loss: 73.9949   val Loss: 71.8456   time: 0.26s   best: 67.1988
2023-10-05 14:16:00,152:INFO:  Epoch 313/500:  train Loss: 72.2455   val Loss: 70.5927   time: 0.25s   best: 67.1988
2023-10-05 14:16:00,428:INFO:  Epoch 314/500:  train Loss: 73.8054   val Loss: 72.0546   time: 0.26s   best: 67.1988
2023-10-05 14:16:00,695:INFO:  Epoch 315/500:  train Loss: 73.1894   val Loss: 71.6388   time: 0.25s   best: 67.1988
2023-10-05 14:16:00,969:INFO:  Epoch 316/500:  train Loss: 73.6710   val Loss: 73.2248   time: 0.26s   best: 67.1988
2023-10-05 14:16:01,228:INFO:  Epoch 317/500:  train Loss: 74.8445   val Loss: 73.8475   time: 0.25s   best: 67.1988
2023-10-05 14:16:01,510:INFO:  Epoch 318/500:  train Loss: 74.1907   val Loss: 72.9627   time: 0.27s   best: 67.1988
2023-10-05 14:16:01,769:INFO:  Epoch 319/500:  train Loss: 74.7428   val Loss: 75.4840   time: 0.25s   best: 67.1988
2023-10-05 14:16:02,046:INFO:  Epoch 320/500:  train Loss: 77.0689   val Loss: 76.6570   time: 0.26s   best: 67.1988
2023-10-05 14:16:02,314:INFO:  Epoch 321/500:  train Loss: 77.3838   val Loss: 72.4368   time: 0.26s   best: 67.1988
2023-10-05 14:16:02,594:INFO:  Epoch 322/500:  train Loss: 73.4373   val Loss: 71.7386   time: 0.27s   best: 67.1988
2023-10-05 14:16:02,856:INFO:  Epoch 323/500:  train Loss: 73.3407   val Loss: 71.0489   time: 0.25s   best: 67.1988
2023-10-05 14:16:03,132:INFO:  Epoch 324/500:  train Loss: 71.8102   val Loss: 70.4102   time: 0.26s   best: 67.1988
2023-10-05 14:16:03,392:INFO:  Epoch 325/500:  train Loss: 71.4991   val Loss: 70.3263   time: 0.25s   best: 67.1988
2023-10-05 14:16:03,668:INFO:  Epoch 326/500:  train Loss: 71.0988   val Loss: 69.8593   time: 0.26s   best: 67.1988
2023-10-05 14:16:03,929:INFO:  Epoch 327/500:  train Loss: 70.7552   val Loss: 69.5019   time: 0.25s   best: 67.1988
2023-10-05 14:16:04,212:INFO:  Epoch 328/500:  train Loss: 70.0731   val Loss: 68.9396   time: 0.27s   best: 67.1988
2023-10-05 14:16:04,476:INFO:  Epoch 329/500:  train Loss: 71.7198   val Loss: 70.4070   time: 0.25s   best: 67.1988
2023-10-05 14:16:04,755:INFO:  Epoch 330/500:  train Loss: 72.0554   val Loss: 69.9550   time: 0.27s   best: 67.1988
2023-10-05 14:16:05,009:INFO:  Epoch 331/500:  train Loss: 72.8257   val Loss: 71.6590   time: 0.25s   best: 67.1988
2023-10-05 14:16:05,290:INFO:  Epoch 332/500:  train Loss: 72.9583   val Loss: 70.9955   time: 0.27s   best: 67.1988
2023-10-05 14:16:05,567:INFO:  Epoch 333/500:  train Loss: 73.9887   val Loss: 69.5322   time: 0.25s   best: 67.1988
2023-10-05 14:16:05,828:INFO:  Epoch 334/500:  train Loss: 73.5463   val Loss: 70.1260   time: 0.25s   best: 67.1988
2023-10-05 14:16:06,102:INFO:  Epoch 335/500:  train Loss: 71.3206   val Loss: 69.1882   time: 0.26s   best: 67.1988
2023-10-05 14:16:06,371:INFO:  Epoch 336/500:  train Loss: 69.5426   val Loss: 68.1112   time: 0.26s   best: 67.1988
2023-10-05 14:16:06,643:INFO:  Epoch 337/500:  train Loss: 69.2209   val Loss: 68.2179   time: 0.27s   best: 67.1988
2023-10-05 14:16:06,912:INFO:  Epoch 338/500:  train Loss: 68.8263   val Loss: 67.5806   time: 0.26s   best: 67.1988
2023-10-05 14:16:07,185:INFO:  Epoch 339/500:  train Loss: 68.8062   val Loss: 67.8225   time: 0.26s   best: 67.1988
2023-10-05 14:16:07,447:INFO:  Epoch 340/500:  train Loss: 68.9916   val Loss: 68.5188   time: 0.25s   best: 67.1988
2023-10-05 14:16:07,724:INFO:  Epoch 341/500:  train Loss: 70.6641   val Loss: 71.8133   time: 0.26s   best: 67.1988
2023-10-05 14:16:07,985:INFO:  Epoch 342/500:  train Loss: 71.8915   val Loss: 67.8334   time: 0.25s   best: 67.1988
2023-10-05 14:16:08,267:INFO:  Epoch 343/500:  train Loss: 73.8306   val Loss: 67.8762   time: 0.27s   best: 67.1988
2023-10-05 14:16:08,531:INFO:  Epoch 344/500:  train Loss: 72.1045   val Loss: 68.8000   time: 0.25s   best: 67.1988
2023-10-05 14:16:08,808:INFO:  Epoch 345/500:  train Loss: 69.7754   val Loss: 69.6922   time: 0.26s   best: 67.1988
2023-10-05 14:16:09,068:INFO:  Epoch 346/500:  train Loss: 70.2446   val Loss: 69.3592   time: 0.25s   best: 67.1988
2023-10-05 14:16:09,343:INFO:  Epoch 347/500:  train Loss: 72.4548   val Loss: 69.0915   time: 0.26s   best: 67.1988
2023-10-05 14:16:09,606:INFO:  Epoch 348/500:  train Loss: 72.1624   val Loss: 72.5800   time: 0.25s   best: 67.1988
2023-10-05 14:16:09,880:INFO:  Epoch 349/500:  train Loss: 73.1421   val Loss: 71.2050   time: 0.26s   best: 67.1988
2023-10-05 14:16:10,140:INFO:  Epoch 350/500:  train Loss: 71.5472   val Loss: 70.0924   time: 0.25s   best: 67.1988
2023-10-05 14:16:10,424:INFO:  Epoch 351/500:  train Loss: 71.2223   val Loss: 69.9151   time: 0.27s   best: 67.1988
2023-10-05 14:16:10,690:INFO:  Epoch 352/500:  train Loss: 71.0643   val Loss: 71.2187   time: 0.25s   best: 67.1988
2023-10-05 14:16:10,966:INFO:  Epoch 353/500:  train Loss: 71.9592   val Loss: 70.1380   time: 0.26s   best: 67.1988
2023-10-05 14:16:11,225:INFO:  Epoch 354/500:  train Loss: 71.0480   val Loss: 68.6274   time: 0.25s   best: 67.1988
2023-10-05 14:16:11,501:INFO:  Epoch 355/500:  train Loss: 70.7554   val Loss: 69.3100   time: 0.27s   best: 67.1988
2023-10-05 14:16:11,769:INFO:  Epoch 356/500:  train Loss: 70.0630   val Loss: 67.7390   time: 0.26s   best: 67.1988
2023-10-05 14:16:12,045:INFO:  Epoch 357/500:  train Loss: 70.2699   val Loss: 68.7284   time: 0.26s   best: 67.1988
2023-10-05 14:16:12,319:INFO:  Epoch 358/500:  train Loss: 70.9766   val Loss: 67.6096   time: 0.25s   best: 67.1988
2023-10-05 14:16:12,594:INFO:  Epoch 359/500:  train Loss: 70.9817   val Loss: 67.9099   time: 0.26s   best: 67.1988
2023-10-05 14:16:12,869:INFO:  Epoch 360/500:  train Loss: 70.2973   val Loss: 67.9079   time: 0.26s   best: 67.1988
2023-10-05 14:16:13,129:INFO:  Epoch 361/500:  train Loss: 70.9396   val Loss: 69.5440   time: 0.25s   best: 67.1988
2023-10-05 14:16:13,408:INFO:  Epoch 362/500:  train Loss: 71.5712   val Loss: 69.1534   time: 0.27s   best: 67.1988
2023-10-05 14:16:13,670:INFO:  Epoch 363/500:  train Loss: 70.4803   val Loss: 72.2798   time: 0.25s   best: 67.1988
2023-10-05 14:16:13,945:INFO:  Epoch 364/500:  train Loss: 73.5752   val Loss: 71.5711   time: 0.26s   best: 67.1988
2023-10-05 14:16:14,212:INFO:  Epoch 365/500:  train Loss: 73.4629   val Loss: 71.2158   time: 0.25s   best: 67.1988
2023-10-05 14:16:14,490:INFO:  Epoch 366/500:  train Loss: 71.3128   val Loss: 69.5703   time: 0.27s   best: 67.1988
2023-10-05 14:16:14,753:INFO:  Epoch 367/500:  train Loss: 72.7317   val Loss: 70.0570   time: 0.25s   best: 67.1988
2023-10-05 14:16:15,028:INFO:  Epoch 368/500:  train Loss: 72.1361   val Loss: 69.0752   time: 0.26s   best: 67.1988
2023-10-05 14:16:15,286:INFO:  Epoch 369/500:  train Loss: 72.0191   val Loss: 70.5571   time: 0.25s   best: 67.1988
2023-10-05 14:16:15,564:INFO:  Epoch 370/500:  train Loss: 71.6389   val Loss: 69.3154   time: 0.26s   best: 67.1988
2023-10-05 14:16:15,823:INFO:  Epoch 371/500:  train Loss: 71.4948   val Loss: 68.6783   time: 0.25s   best: 67.1988
2023-10-05 14:16:16,100:INFO:  Epoch 372/500:  train Loss: 70.9043   val Loss: 68.1004   time: 0.26s   best: 67.1988
2023-10-05 14:16:16,369:INFO:  Epoch 373/500:  train Loss: 69.3836   val Loss: 69.3414   time: 0.26s   best: 67.1988
2023-10-05 14:16:16,648:INFO:  Epoch 374/500:  train Loss: 69.4508   val Loss: 68.1444   time: 0.27s   best: 67.1988
2023-10-05 14:16:16,910:INFO:  Epoch 375/500:  train Loss: 69.6312   val Loss: 68.3711   time: 0.25s   best: 67.1988
2023-10-05 14:16:17,184:INFO:  Epoch 376/500:  train Loss: 68.9603   val Loss: 67.7441   time: 0.26s   best: 67.1988
2023-10-05 14:16:17,445:INFO:  Epoch 377/500:  train Loss: 70.1107   val Loss: 67.8705   time: 0.25s   best: 67.1988
2023-10-05 14:16:17,715:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:17,746:INFO:  Epoch 378/500:  train Loss: 68.1783   val Loss: 67.0988   time: 0.26s   best: 67.0988
2023-10-05 14:16:18,007:INFO:  Epoch 379/500:  train Loss: 71.3804   val Loss: 68.2152   time: 0.25s   best: 67.0988
2023-10-05 14:16:18,290:INFO:  Epoch 380/500:  train Loss: 69.7842   val Loss: 69.0476   time: 0.27s   best: 67.0988
2023-10-05 14:16:18,569:INFO:  Epoch 381/500:  train Loss: 71.1403   val Loss: 68.3766   time: 0.27s   best: 67.0988
2023-10-05 14:16:18,823:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:18,963:INFO:  Epoch 382/500:  train Loss: 68.9511   val Loss: 67.0881   time: 0.25s   best: 67.0881
2023-10-05 14:16:19,232:INFO:  Epoch 383/500:  train Loss: 70.4524   val Loss: 71.0852   time: 0.26s   best: 67.0881
2023-10-05 14:16:19,493:INFO:  Epoch 384/500:  train Loss: 80.3225   val Loss: 76.8248   time: 0.25s   best: 67.0881
2023-10-05 14:16:19,769:INFO:  Epoch 385/500:  train Loss: 76.9013   val Loss: 75.1888   time: 0.26s   best: 67.0881
2023-10-05 14:16:20,029:INFO:  Epoch 386/500:  train Loss: 77.7863   val Loss: 73.5355   time: 0.25s   best: 67.0881
2023-10-05 14:16:20,311:INFO:  Epoch 387/500:  train Loss: 74.2213   val Loss: 71.9471   time: 0.27s   best: 67.0881
2023-10-05 14:16:20,576:INFO:  Epoch 388/500:  train Loss: 72.2963   val Loss: 70.5266   time: 0.25s   best: 67.0881
2023-10-05 14:16:20,853:INFO:  Epoch 389/500:  train Loss: 70.9871   val Loss: 68.4492   time: 0.26s   best: 67.0881
2023-10-05 14:16:21,113:INFO:  Epoch 390/500:  train Loss: 69.4967   val Loss: 67.6551   time: 0.25s   best: 67.0881
2023-10-05 14:16:21,393:INFO:  Epoch 391/500:  train Loss: 68.3788   val Loss: 67.0993   time: 0.27s   best: 67.0881
2023-10-05 14:16:21,647:INFO:  Epoch 392/500:  train Loss: 68.7102   val Loss: 68.8032   time: 0.25s   best: 67.0881
2023-10-05 14:16:21,931:INFO:  Epoch 393/500:  train Loss: 70.3255   val Loss: 68.2246   time: 0.27s   best: 67.0881
2023-10-05 14:16:22,197:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:22,237:INFO:  Epoch 394/500:  train Loss: 68.8061   val Loss: 66.3277   time: 0.26s   best: 66.3277
2023-10-05 14:16:22,500:INFO:  Epoch 395/500:  train Loss: 67.2487   val Loss: 66.4600   time: 0.25s   best: 66.3277
2023-10-05 14:16:22,775:INFO:  Epoch 396/500:  train Loss: 68.4573   val Loss: 67.6096   time: 0.26s   best: 66.3277
2023-10-05 14:16:23,027:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:23,082:INFO:  Epoch 397/500:  train Loss: 69.7756   val Loss: 65.1178   time: 0.25s   best: 65.1178
2023-10-05 14:16:23,355:INFO:  Epoch 398/500:  train Loss: 66.6839   val Loss: 66.5705   time: 0.26s   best: 65.1178
2023-10-05 14:16:23,616:INFO:  Epoch 399/500:  train Loss: 67.0641   val Loss: 65.7535   time: 0.25s   best: 65.1178
2023-10-05 14:16:23,935:INFO:  Epoch 400/500:  train Loss: 67.1054   val Loss: 66.1932   time: 0.30s   best: 65.1178
2023-10-05 14:16:24,206:INFO:  Epoch 401/500:  train Loss: 66.1482   val Loss: 65.4327   time: 0.26s   best: 65.1178
2023-10-05 14:16:24,493:INFO:  Epoch 402/500:  train Loss: 67.2918   val Loss: 66.2796   time: 0.27s   best: 65.1178
2023-10-05 14:16:24,748:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:24,818:INFO:  Epoch 403/500:  train Loss: 66.1299   val Loss: 64.5918   time: 0.25s   best: 64.5918
2023-10-05 14:16:25,078:INFO:  Epoch 404/500:  train Loss: 67.1638   val Loss: 67.5785   time: 0.25s   best: 64.5918
2023-10-05 14:16:25,350:INFO:  Epoch 405/500:  train Loss: 68.0307   val Loss: 66.9714   time: 0.26s   best: 64.5918
2023-10-05 14:16:25,615:INFO:  Epoch 406/500:  train Loss: 68.3980   val Loss: 67.2688   time: 0.25s   best: 64.5918
2023-10-05 14:16:25,889:INFO:  Epoch 407/500:  train Loss: 68.3607   val Loss: 65.3817   time: 0.26s   best: 64.5918
2023-10-05 14:16:26,150:INFO:  Epoch 408/500:  train Loss: 69.1699   val Loss: 66.1732   time: 0.25s   best: 64.5918
2023-10-05 14:16:26,589:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:26,630:INFO:  Epoch 409/500:  train Loss: 68.4644   val Loss: 64.1063   time: 0.43s   best: 64.1063
2023-10-05 14:16:26,934:INFO:  Epoch 410/500:  train Loss: 69.1647   val Loss: 71.3792   time: 0.29s   best: 64.1063
2023-10-05 14:16:27,193:INFO:  Epoch 411/500:  train Loss: 73.4409   val Loss: 70.4979   time: 0.25s   best: 64.1063
2023-10-05 14:16:27,470:INFO:  Epoch 412/500:  train Loss: 71.1750   val Loss: 67.8540   time: 0.26s   best: 64.1063
2023-10-05 14:16:27,730:INFO:  Epoch 413/500:  train Loss: 69.9054   val Loss: 67.2986   time: 0.25s   best: 64.1063
2023-10-05 14:16:28,005:INFO:  Epoch 414/500:  train Loss: 67.5935   val Loss: 65.8261   time: 0.26s   best: 64.1063
2023-10-05 14:16:28,265:INFO:  Epoch 415/500:  train Loss: 67.0677   val Loss: 65.3107   time: 0.25s   best: 64.1063
2023-10-05 14:16:28,544:INFO:  Epoch 416/500:  train Loss: 66.9360   val Loss: 64.4460   time: 0.27s   best: 64.1063
2023-10-05 14:16:28,811:INFO:  Epoch 417/500:  train Loss: 65.3885   val Loss: 66.5213   time: 0.26s   best: 64.1063
2023-10-05 14:16:29,085:INFO:  Epoch 418/500:  train Loss: 68.2950   val Loss: 67.9259   time: 0.26s   best: 64.1063
2023-10-05 14:16:29,344:INFO:  Epoch 419/500:  train Loss: 67.7016   val Loss: 67.0920   time: 0.25s   best: 64.1063
2023-10-05 14:16:29,622:INFO:  Epoch 420/500:  train Loss: 67.6316   val Loss: 65.6736   time: 0.26s   best: 64.1063
2023-10-05 14:16:29,881:INFO:  Epoch 421/500:  train Loss: 66.2275   val Loss: 65.7062   time: 0.25s   best: 64.1063
2023-10-05 14:16:30,157:INFO:  Epoch 422/500:  train Loss: 66.5715   val Loss: 64.7883   time: 0.26s   best: 64.1063
2023-10-05 14:16:30,416:INFO:  Epoch 423/500:  train Loss: 65.9452   val Loss: 65.2379   time: 0.25s   best: 64.1063
2023-10-05 14:16:30,696:INFO:  Epoch 424/500:  train Loss: 68.8949   val Loss: 68.4028   time: 0.27s   best: 64.1063
2023-10-05 14:16:30,967:INFO:  Epoch 425/500:  train Loss: 70.4178   val Loss: 70.2007   time: 0.26s   best: 64.1063
2023-10-05 14:16:31,241:INFO:  Epoch 426/500:  train Loss: 70.1282   val Loss: 68.8288   time: 0.26s   best: 64.1063
2023-10-05 14:16:31,506:INFO:  Epoch 427/500:  train Loss: 69.5020   val Loss: 66.3687   time: 0.25s   best: 64.1063
2023-10-05 14:16:31,783:INFO:  Epoch 428/500:  train Loss: 68.1551   val Loss: 65.9667   time: 0.26s   best: 64.1063
2023-10-05 14:16:32,058:INFO:  Epoch 429/500:  train Loss: 66.4613   val Loss: 64.9423   time: 0.25s   best: 64.1063
2023-10-05 14:16:32,319:INFO:  Epoch 430/500:  train Loss: 66.9900   val Loss: 66.1871   time: 0.25s   best: 64.1063
2023-10-05 14:16:32,591:INFO:  Epoch 431/500:  train Loss: 67.2993   val Loss: 65.9226   time: 0.27s   best: 64.1063
2023-10-05 14:16:32,864:INFO:  Epoch 432/500:  train Loss: 66.7831   val Loss: 65.6864   time: 0.26s   best: 64.1063
2023-10-05 14:16:33,131:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:33,538:INFO:  Epoch 433/500:  train Loss: 65.9862   val Loss: 63.2769   time: 0.26s   best: 63.2769
2023-10-05 14:16:33,802:INFO:  Epoch 434/500:  train Loss: 66.0012   val Loss: 65.3204   time: 0.25s   best: 63.2769
2023-10-05 14:16:34,063:INFO:  Epoch 435/500:  train Loss: 66.6238   val Loss: 63.6408   time: 0.25s   best: 63.2769
2023-10-05 14:16:34,337:INFO:  Epoch 436/500:  train Loss: 66.0351   val Loss: 64.7579   time: 0.26s   best: 63.2769
2023-10-05 14:16:34,602:INFO:  Epoch 437/500:  train Loss: 65.7955   val Loss: 64.4090   time: 0.25s   best: 63.2769
2023-10-05 14:16:34,885:INFO:  Epoch 438/500:  train Loss: 65.9559   val Loss: 66.3933   time: 0.27s   best: 63.2769
2023-10-05 14:16:35,146:INFO:  Epoch 439/500:  train Loss: 67.0722   val Loss: 65.6055   time: 0.25s   best: 63.2769
2023-10-05 14:16:35,421:INFO:  Epoch 440/500:  train Loss: 66.1427   val Loss: 63.6223   time: 0.26s   best: 63.2769
2023-10-05 14:16:35,696:INFO:  Epoch 441/500:  train Loss: 67.8051   val Loss: 66.9128   time: 0.25s   best: 63.2769
2023-10-05 14:16:35,957:INFO:  Epoch 442/500:  train Loss: 67.5327   val Loss: 65.2942   time: 0.25s   best: 63.2769
2023-10-05 14:16:36,225:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:36,309:INFO:  Epoch 443/500:  train Loss: 67.7310   val Loss: 63.2283   time: 0.26s   best: 63.2283
2023-10-05 14:16:36,567:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:36,598:INFO:  Epoch 444/500:  train Loss: 65.4961   val Loss: 63.0489   time: 0.25s   best: 63.0489
2023-10-05 14:16:36,872:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:36,940:INFO:  Epoch 445/500:  train Loss: 63.6461   val Loss: 62.9588   time: 0.27s   best: 62.9588
2023-10-05 14:16:37,193:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:37,256:INFO:  Epoch 446/500:  train Loss: 64.6311   val Loss: 62.5820   time: 0.25s   best: 62.5820
2023-10-05 14:16:37,516:INFO:  Epoch 447/500:  train Loss: 64.3563   val Loss: 62.7681   time: 0.25s   best: 62.5820
2023-10-05 14:16:37,817:INFO:  Epoch 448/500:  train Loss: 64.4404   val Loss: 62.8453   time: 0.29s   best: 62.5820
2023-10-05 14:16:38,070:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:38,115:INFO:  Epoch 449/500:  train Loss: 63.5436   val Loss: 62.5091   time: 0.25s   best: 62.5091
2023-10-05 14:16:38,387:INFO:  Epoch 450/500:  train Loss: 65.1181   val Loss: 64.8317   time: 0.26s   best: 62.5091
2023-10-05 14:16:38,653:INFO:  Epoch 451/500:  train Loss: 64.7302   val Loss: 64.3132   time: 0.25s   best: 62.5091
2023-10-05 14:16:38,933:INFO:  Epoch 452/500:  train Loss: 66.2053   val Loss: 65.3790   time: 0.27s   best: 62.5091
2023-10-05 14:16:39,193:INFO:  Epoch 453/500:  train Loss: 66.3632   val Loss: 63.6909   time: 0.25s   best: 62.5091
2023-10-05 14:16:39,468:INFO:  Epoch 454/500:  train Loss: 65.3475   val Loss: 64.1378   time: 0.26s   best: 62.5091
2023-10-05 14:16:39,729:INFO:  Epoch 455/500:  train Loss: 66.5147   val Loss: 62.7376   time: 0.25s   best: 62.5091
2023-10-05 14:16:40,004:INFO:  Epoch 456/500:  train Loss: 65.3079   val Loss: 63.5095   time: 0.26s   best: 62.5091
2023-10-05 14:16:40,256:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:40,293:INFO:  Epoch 457/500:  train Loss: 64.9076   val Loss: 62.4660   time: 0.25s   best: 62.4660
2023-10-05 14:16:40,571:INFO:  Epoch 458/500:  train Loss: 63.8389   val Loss: 62.8762   time: 0.27s   best: 62.4660
2023-10-05 14:16:40,835:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:40,866:INFO:  Epoch 459/500:  train Loss: 63.7209   val Loss: 62.2397   time: 0.26s   best: 62.2397
2023-10-05 14:16:41,141:INFO:  Epoch 460/500:  train Loss: 64.2104   val Loss: 64.9289   time: 0.26s   best: 62.2397
2023-10-05 14:16:41,412:INFO:  Epoch 461/500:  train Loss: 64.9920   val Loss: 64.1197   time: 0.25s   best: 62.2397
2023-10-05 14:16:41,679:INFO:  Epoch 462/500:  train Loss: 67.0136   val Loss: 63.3221   time: 0.26s   best: 62.2397
2023-10-05 14:16:41,960:INFO:  Epoch 463/500:  train Loss: 64.8751   val Loss: 63.9014   time: 0.27s   best: 62.2397
2023-10-05 14:16:42,221:INFO:  Epoch 464/500:  train Loss: 66.8402   val Loss: 66.6630   time: 0.25s   best: 62.2397
2023-10-05 14:16:42,495:INFO:  Epoch 465/500:  train Loss: 66.5403   val Loss: 63.9203   time: 0.26s   best: 62.2397
2023-10-05 14:16:42,759:INFO:  Epoch 466/500:  train Loss: 66.0245   val Loss: 64.3299   time: 0.25s   best: 62.2397
2023-10-05 14:16:43,046:INFO:  Epoch 467/500:  train Loss: 63.8018   val Loss: 63.6902   time: 0.27s   best: 62.2397
2023-10-05 14:16:43,297:INFO:  Epoch 468/500:  train Loss: 64.7092   val Loss: 63.0405   time: 0.25s   best: 62.2397
2023-10-05 14:16:43,553:INFO:  Epoch 469/500:  train Loss: 64.1487   val Loss: 65.3316   time: 0.25s   best: 62.2397
2023-10-05 14:16:43,826:INFO:  Epoch 470/500:  train Loss: 65.6752   val Loss: 63.5684   time: 0.26s   best: 62.2397
2023-10-05 14:16:44,110:INFO:  Epoch 471/500:  train Loss: 66.9653   val Loss: 64.6798   time: 0.27s   best: 62.2397
2023-10-05 14:16:44,370:INFO:  Epoch 472/500:  train Loss: 69.2862   val Loss: 67.7259   time: 0.25s   best: 62.2397
2023-10-05 14:16:44,651:INFO:  Epoch 473/500:  train Loss: 66.4258   val Loss: 64.7093   time: 0.27s   best: 62.2397
2023-10-05 14:16:44,918:INFO:  Epoch 474/500:  train Loss: 66.5579   val Loss: 66.4647   time: 0.25s   best: 62.2397
2023-10-05 14:16:45,193:INFO:  Epoch 475/500:  train Loss: 65.8574   val Loss: 63.7758   time: 0.26s   best: 62.2397
2023-10-05 14:16:45,455:INFO:  Epoch 476/500:  train Loss: 65.0575   val Loss: 65.7534   time: 0.25s   best: 62.2397
2023-10-05 14:16:45,731:INFO:  Epoch 477/500:  train Loss: 64.3377   val Loss: 63.3354   time: 0.26s   best: 62.2397
2023-10-05 14:16:45,990:INFO:  Epoch 478/500:  train Loss: 64.5476   val Loss: 64.4057   time: 0.25s   best: 62.2397
2023-10-05 14:16:46,259:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:46,324:INFO:  Epoch 479/500:  train Loss: 66.1284   val Loss: 61.8470   time: 0.26s   best: 61.8470
2023-10-05 14:16:46,603:INFO:  Epoch 480/500:  train Loss: 64.6673   val Loss: 65.0098   time: 0.25s   best: 61.8470
2023-10-05 14:16:46,873:INFO:  Epoch 481/500:  train Loss: 63.7059   val Loss: 63.6119   time: 0.26s   best: 61.8470
2023-10-05 14:16:47,149:INFO:  Epoch 482/500:  train Loss: 67.1065   val Loss: 66.2502   time: 0.26s   best: 61.8470
2023-10-05 14:16:47,409:INFO:  Epoch 483/500:  train Loss: 67.2926   val Loss: 66.4200   time: 0.25s   best: 61.8470
2023-10-05 14:16:47,685:INFO:  Epoch 484/500:  train Loss: 68.8747   val Loss: 64.7718   time: 0.26s   best: 61.8470
2023-10-05 14:16:47,945:INFO:  Epoch 485/500:  train Loss: 65.9166   val Loss: 63.5389   time: 0.25s   best: 61.8470
2023-10-05 14:16:48,211:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:48,295:INFO:  Epoch 486/500:  train Loss: 65.4418   val Loss: 61.3027   time: 0.26s   best: 61.3027
2023-10-05 14:16:48,557:INFO:  Epoch 487/500:  train Loss: 63.6773   val Loss: 61.5797   time: 0.25s   best: 61.3027
2023-10-05 14:16:48,832:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_99c7.pt
2023-10-05 14:16:48,864:INFO:  Epoch 488/500:  train Loss: 62.9913   val Loss: 60.5370   time: 0.27s   best: 60.5370
2023-10-05 14:16:49,124:INFO:  Epoch 489/500:  train Loss: 62.9701   val Loss: 60.8072   time: 0.25s   best: 60.5370
2023-10-05 14:16:49,398:INFO:  Epoch 490/500:  train Loss: 64.8903   val Loss: 64.4665   time: 0.26s   best: 60.5370
2023-10-05 14:16:49,660:INFO:  Epoch 491/500:  train Loss: 64.5802   val Loss: 63.3017   time: 0.25s   best: 60.5370
2023-10-05 14:16:49,928:INFO:  Epoch 492/500:  train Loss: 64.5023   val Loss: 66.2662   time: 0.26s   best: 60.5370
2023-10-05 14:16:50,195:INFO:  Epoch 493/500:  train Loss: 66.0979   val Loss: 64.6731   time: 0.25s   best: 60.5370
2023-10-05 14:16:50,471:INFO:  Epoch 494/500:  train Loss: 68.2322   val Loss: 65.3903   time: 0.26s   best: 60.5370
2023-10-05 14:16:50,743:INFO:  Epoch 495/500:  train Loss: 63.8925   val Loss: 62.5732   time: 0.25s   best: 60.5370
2023-10-05 14:16:51,019:INFO:  Epoch 496/500:  train Loss: 64.1883   val Loss: 63.1876   time: 0.26s   best: 60.5370
2023-10-05 14:16:51,291:INFO:  Epoch 497/500:  train Loss: 66.1639   val Loss: 65.0689   time: 0.26s   best: 60.5370
2023-10-05 14:16:51,556:INFO:  Epoch 498/500:  train Loss: 67.0510   val Loss: 64.6485   time: 0.25s   best: 60.5370
2023-10-05 14:16:51,832:INFO:  Epoch 499/500:  train Loss: 65.3456   val Loss: 64.0420   time: 0.26s   best: 60.5370
2023-10-05 14:16:52,134:INFO:  Epoch 500/500:  train Loss: 65.4157   val Loss: 63.4187   time: 0.29s   best: 60.5370
2023-10-05 14:16:52,135:INFO:  -----> Training complete in 2m 26s   best validation loss: 60.5370
 
2023-10-05 15:02:51,527:INFO:  Starting experiment lstm autoencoder debug (1 layer)
2023-10-05 15:02:51,537:INFO:  Defining the model
2023-10-05 15:02:51,681:INFO:  Reading the dataset
2023-10-05 15:03:07,120:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:07,142:INFO:  Epoch 1/500:  train Loss: 98.5859   val Loss: 98.6795   time: 5.76s   best: 98.6795
2023-10-05 15:03:07,443:INFO:  Epoch 2/500:  train Loss: 99.0957   val Loss: 100.0307   time: 0.30s   best: 98.6795
2023-10-05 15:03:07,741:INFO:  Epoch 3/500:  train Loss: 98.8620   val Loss: 100.0217   time: 0.30s   best: 98.6795
2023-10-05 15:03:08,056:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:08,076:INFO:  Epoch 4/500:  train Loss: 98.7415   val Loss: 97.8754   time: 0.31s   best: 97.8754
2023-10-05 15:03:08,391:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:08,410:INFO:  Epoch 5/500:  train Loss: 97.7699   val Loss: 94.6087   time: 0.31s   best: 94.6087
2023-10-05 15:03:08,711:INFO:  Epoch 6/500:  train Loss: 97.5821   val Loss: 97.2707   time: 0.30s   best: 94.6087
2023-10-05 15:03:09,015:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:09,037:INFO:  Epoch 7/500:  train Loss: 96.9796   val Loss: 94.5822   time: 0.30s   best: 94.5822
2023-10-05 15:03:09,348:INFO:  Epoch 8/500:  train Loss: 96.5590   val Loss: 96.5162   time: 0.31s   best: 94.5822
2023-10-05 15:03:09,648:INFO:  Epoch 9/500:  train Loss: 96.3362   val Loss: 96.0425   time: 0.30s   best: 94.5822
2023-10-05 15:03:09,959:INFO:  Epoch 10/500:  train Loss: 95.8340   val Loss: 95.5023   time: 0.31s   best: 94.5822
2023-10-05 15:03:10,261:INFO:  Epoch 11/500:  train Loss: 94.6899   val Loss: 94.6665   time: 0.30s   best: 94.5822
2023-10-05 15:03:10,563:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:10,581:INFO:  Epoch 12/500:  train Loss: 94.1624   val Loss: 93.8432   time: 0.30s   best: 93.8432
2023-10-05 15:03:10,892:INFO:  Epoch 13/500:  train Loss: 93.4261   val Loss: 94.1923   time: 0.31s   best: 93.8432
2023-10-05 15:03:11,198:INFO:  Epoch 14/500:  train Loss: 93.9341   val Loss: 94.2891   time: 0.30s   best: 93.8432
2023-10-05 15:03:11,502:INFO:  Epoch 15/500:  train Loss: 93.9118   val Loss: 94.2911   time: 0.30s   best: 93.8432
2023-10-05 15:03:11,805:INFO:  Epoch 16/500:  train Loss: 93.8679   val Loss: 94.3245   time: 0.30s   best: 93.8432
2023-10-05 15:03:12,119:INFO:  Epoch 17/500:  train Loss: 94.5050   val Loss: 94.3739   time: 0.31s   best: 93.8432
2023-10-05 15:03:12,429:INFO:  Epoch 18/500:  train Loss: 94.8492   val Loss: 94.4622   time: 0.31s   best: 93.8432
2023-10-05 15:03:12,729:INFO:  Epoch 19/500:  train Loss: 94.7589   val Loss: 94.5465   time: 0.30s   best: 93.8432
2023-10-05 15:03:13,039:INFO:  Epoch 20/500:  train Loss: 94.9531   val Loss: 94.5983   time: 0.31s   best: 93.8432
2023-10-05 15:03:13,342:INFO:  Epoch 21/500:  train Loss: 94.9731   val Loss: 94.5329   time: 0.30s   best: 93.8432
2023-10-05 15:03:13,653:INFO:  Epoch 22/500:  train Loss: 94.7548   val Loss: 94.4732   time: 0.31s   best: 93.8432
2023-10-05 15:03:13,963:INFO:  Epoch 23/500:  train Loss: 94.6198   val Loss: 94.3208   time: 0.31s   best: 93.8432
2023-10-05 15:03:14,266:INFO:  Epoch 24/500:  train Loss: 94.2883   val Loss: 94.1957   time: 0.30s   best: 93.8432
2023-10-05 15:03:14,567:INFO:  Epoch 25/500:  train Loss: 94.2045   val Loss: 94.0136   time: 0.30s   best: 93.8432
2023-10-05 15:03:14,868:INFO:  Epoch 26/500:  train Loss: 94.1550   val Loss: 93.9051   time: 0.30s   best: 93.8432
2023-10-05 15:03:15,185:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:15,205:INFO:  Epoch 27/500:  train Loss: 93.8195   val Loss: 93.8117   time: 0.31s   best: 93.8117
2023-10-05 15:03:15,516:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:15,537:INFO:  Epoch 28/500:  train Loss: 93.7558   val Loss: 93.7733   time: 0.31s   best: 93.7733
2023-10-05 15:03:15,840:INFO:  Epoch 29/500:  train Loss: 94.2279   val Loss: 93.7924   time: 0.30s   best: 93.7733
2023-10-05 15:03:16,152:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:16,171:INFO:  Epoch 30/500:  train Loss: 94.0634   val Loss: 93.7490   time: 0.31s   best: 93.7490
2023-10-05 15:03:16,481:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:16,499:INFO:  Epoch 31/500:  train Loss: 94.0387   val Loss: 93.5931   time: 0.31s   best: 93.5931
2023-10-05 15:03:16,800:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:16,819:INFO:  Epoch 32/500:  train Loss: 93.9666   val Loss: 93.3976   time: 0.30s   best: 93.3976
2023-10-05 15:03:17,129:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:17,149:INFO:  Epoch 33/500:  train Loss: 93.3477   val Loss: 93.0522   time: 0.31s   best: 93.0522
2023-10-05 15:03:17,449:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:17,469:INFO:  Epoch 34/500:  train Loss: 93.0163   val Loss: 92.6472   time: 0.30s   best: 92.6472
2023-10-05 15:03:17,776:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:17,799:INFO:  Epoch 35/500:  train Loss: 92.4573   val Loss: 92.0448   time: 0.30s   best: 92.0448
2023-10-05 15:03:18,112:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:18,132:INFO:  Epoch 36/500:  train Loss: 92.2687   val Loss: 91.4869   time: 0.31s   best: 91.4869
2023-10-05 15:03:18,434:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:18,454:INFO:  Epoch 37/500:  train Loss: 91.4223   val Loss: 90.8910   time: 0.30s   best: 90.8910
2023-10-05 15:03:18,766:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:18,786:INFO:  Epoch 38/500:  train Loss: 90.5979   val Loss: 90.0927   time: 0.31s   best: 90.0927
2023-10-05 15:03:19,100:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:19,119:INFO:  Epoch 39/500:  train Loss: 89.9376   val Loss: 89.0753   time: 0.31s   best: 89.0753
2023-10-05 15:03:19,420:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:19,440:INFO:  Epoch 40/500:  train Loss: 89.0654   val Loss: 88.4237   time: 0.30s   best: 88.4237
2023-10-05 15:03:19,740:INFO:  Epoch 41/500:  train Loss: 88.7605   val Loss: 88.5555   time: 0.30s   best: 88.4237
2023-10-05 15:03:20,056:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:20,076:INFO:  Epoch 42/500:  train Loss: 88.7263   val Loss: 88.1444   time: 0.31s   best: 88.1444
2023-10-05 15:03:20,377:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:20,396:INFO:  Epoch 43/500:  train Loss: 87.6529   val Loss: 87.3190   time: 0.30s   best: 87.3190
2023-10-05 15:03:20,697:INFO:  Epoch 44/500:  train Loss: 87.5778   val Loss: 87.5808   time: 0.30s   best: 87.3190
2023-10-05 15:03:20,999:INFO:  Epoch 45/500:  train Loss: 88.6014   val Loss: 88.5415   time: 0.30s   best: 87.3190
2023-10-05 15:03:21,313:INFO:  Epoch 46/500:  train Loss: 88.7410   val Loss: 87.5135   time: 0.31s   best: 87.3190
2023-10-05 15:03:21,620:INFO:  Epoch 47/500:  train Loss: 87.7781   val Loss: 88.1744   time: 0.30s   best: 87.3190
2023-10-05 15:03:21,924:INFO:  Epoch 48/500:  train Loss: 87.8708   val Loss: 87.3493   time: 0.30s   best: 87.3190
2023-10-05 15:03:22,230:INFO:  Epoch 49/500:  train Loss: 87.9886   val Loss: 87.4744   time: 0.30s   best: 87.3190
2023-10-05 15:03:22,531:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:22,550:INFO:  Epoch 50/500:  train Loss: 87.5463   val Loss: 87.1219   time: 0.30s   best: 87.1219
2023-10-05 15:03:22,861:INFO:  Epoch 51/500:  train Loss: 87.5912   val Loss: 88.1340   time: 0.31s   best: 87.1219
2023-10-05 15:03:23,176:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:23,195:INFO:  Epoch 52/500:  train Loss: 87.6409   val Loss: 86.7881   time: 0.31s   best: 86.7881
2023-10-05 15:03:23,496:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:23,524:INFO:  Epoch 53/500:  train Loss: 87.4429   val Loss: 86.7645   time: 0.30s   best: 86.7645
2023-10-05 15:03:23,828:INFO:  Epoch 54/500:  train Loss: 87.1260   val Loss: 86.8367   time: 0.30s   best: 86.7645
2023-10-05 15:03:24,142:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:24,161:INFO:  Epoch 55/500:  train Loss: 86.7410   val Loss: 86.4900   time: 0.30s   best: 86.4900
2023-10-05 15:03:24,461:INFO:  Epoch 56/500:  train Loss: 87.2476   val Loss: 87.2621   time: 0.30s   best: 86.4900
2023-10-05 15:03:24,769:INFO:  Epoch 57/500:  train Loss: 87.7903   val Loss: 86.5922   time: 0.30s   best: 86.4900
2023-10-05 15:03:25,074:INFO:  Epoch 58/500:  train Loss: 86.8961   val Loss: 86.7726   time: 0.30s   best: 86.4900
2023-10-05 15:03:25,380:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:25,399:INFO:  Epoch 59/500:  train Loss: 87.3780   val Loss: 86.0022   time: 0.30s   best: 86.0022
2023-10-05 15:03:25,709:INFO:  Epoch 60/500:  train Loss: 86.1824   val Loss: 86.1969   time: 0.31s   best: 86.0022
2023-10-05 15:03:26,012:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:26,032:INFO:  Epoch 61/500:  train Loss: 86.2905   val Loss: 85.7629   time: 0.30s   best: 85.7629
2023-10-05 15:03:26,339:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:26,358:INFO:  Epoch 62/500:  train Loss: 86.4196   val Loss: 85.3517   time: 0.30s   best: 85.3517
2023-10-05 15:03:26,659:INFO:  Epoch 63/500:  train Loss: 86.1012   val Loss: 85.5762   time: 0.30s   best: 85.3517
2023-10-05 15:03:26,975:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:26,993:INFO:  Epoch 64/500:  train Loss: 85.9324   val Loss: 84.8348   time: 0.31s   best: 84.8348
2023-10-05 15:03:27,384:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:27,404:INFO:  Epoch 65/500:  train Loss: 85.4151   val Loss: 84.6936   time: 0.39s   best: 84.6936
2023-10-05 15:03:27,715:INFO:  Epoch 66/500:  train Loss: 85.7207   val Loss: 85.3364   time: 0.30s   best: 84.6936
2023-10-05 15:03:28,016:INFO:  Epoch 67/500:  train Loss: 85.6304   val Loss: 85.0014   time: 0.30s   best: 84.6936
2023-10-05 15:03:28,324:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:28,344:INFO:  Epoch 68/500:  train Loss: 85.6573   val Loss: 84.5235   time: 0.30s   best: 84.5235
2023-10-05 15:03:28,645:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:28,664:INFO:  Epoch 69/500:  train Loss: 85.4306   val Loss: 84.2404   time: 0.30s   best: 84.2404
2023-10-05 15:03:28,973:INFO:  Epoch 70/500:  train Loss: 84.7370   val Loss: 85.0415   time: 0.31s   best: 84.2404
2023-10-05 15:03:29,287:INFO:  Epoch 71/500:  train Loss: 85.5922   val Loss: 84.8125   time: 0.31s   best: 84.2404
2023-10-05 15:03:29,586:INFO:  Epoch 72/500:  train Loss: 85.2703   val Loss: 84.7495   time: 0.30s   best: 84.2404
2023-10-05 15:03:29,892:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:29,911:INFO:  Epoch 73/500:  train Loss: 84.7440   val Loss: 84.0816   time: 0.30s   best: 84.0816
2023-10-05 15:03:30,216:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:30,236:INFO:  Epoch 74/500:  train Loss: 84.4730   val Loss: 83.8732   time: 0.30s   best: 83.8732
2023-10-05 15:03:30,542:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:30,562:INFO:  Epoch 75/500:  train Loss: 84.2804   val Loss: 83.6276   time: 0.30s   best: 83.6276
2023-10-05 15:03:30,871:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:30,890:INFO:  Epoch 76/500:  train Loss: 83.7011   val Loss: 83.2287   time: 0.31s   best: 83.2287
2023-10-05 15:03:31,192:INFO:  Epoch 77/500:  train Loss: 84.5104   val Loss: 83.4945   time: 0.30s   best: 83.2287
2023-10-05 15:03:31,498:INFO:  Epoch 78/500:  train Loss: 84.3832   val Loss: 83.4218   time: 0.30s   best: 83.2287
2023-10-05 15:03:31,807:INFO:  Epoch 79/500:  train Loss: 83.9953   val Loss: 83.6445   time: 0.31s   best: 83.2287
2023-10-05 15:03:32,108:INFO:  Epoch 80/500:  train Loss: 83.7724   val Loss: 83.4158   time: 0.30s   best: 83.2287
2023-10-05 15:03:32,419:INFO:  Epoch 81/500:  train Loss: 83.9479   val Loss: 83.2793   time: 0.31s   best: 83.2287
2023-10-05 15:03:32,720:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:32,739:INFO:  Epoch 82/500:  train Loss: 83.8230   val Loss: 82.6791   time: 0.30s   best: 82.6791
2023-10-05 15:03:33,052:INFO:  Epoch 83/500:  train Loss: 83.6943   val Loss: 82.6807   time: 0.31s   best: 82.6791
2023-10-05 15:03:33,365:INFO:  Epoch 84/500:  train Loss: 83.6624   val Loss: 85.3359   time: 0.31s   best: 82.6791
2023-10-05 15:03:33,664:INFO:  Epoch 85/500:  train Loss: 85.6361   val Loss: 84.3742   time: 0.30s   best: 82.6791
2023-10-05 15:03:33,973:INFO:  Epoch 86/500:  train Loss: 84.7082   val Loss: 83.0673   time: 0.31s   best: 82.6791
2023-10-05 15:03:34,272:INFO:  Epoch 87/500:  train Loss: 84.3334   val Loss: 83.2267   time: 0.30s   best: 82.6791
2023-10-05 15:03:34,579:INFO:  Epoch 88/500:  train Loss: 83.3025   val Loss: 83.2303   time: 0.30s   best: 82.6791
2023-10-05 15:03:34,888:INFO:  Epoch 89/500:  train Loss: 84.3399   val Loss: 83.3336   time: 0.31s   best: 82.6791
2023-10-05 15:03:35,188:INFO:  Epoch 90/500:  train Loss: 83.8630   val Loss: 82.8237   time: 0.30s   best: 82.6791
2023-10-05 15:03:35,570:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:35,591:INFO:  Epoch 91/500:  train Loss: 83.8959   val Loss: 81.9437   time: 0.31s   best: 81.9437
2023-10-05 15:03:35,903:INFO:  Epoch 92/500:  train Loss: 82.7899   val Loss: 82.3674   time: 0.31s   best: 81.9437
2023-10-05 15:03:36,207:INFO:  Epoch 93/500:  train Loss: 83.1933   val Loss: 85.1420   time: 0.30s   best: 81.9437
2023-10-05 15:03:36,517:INFO:  Epoch 94/500:  train Loss: 84.0197   val Loss: 82.0723   time: 0.31s   best: 81.9437
2023-10-05 15:03:36,817:INFO:  Epoch 95/500:  train Loss: 83.2788   val Loss: 82.7818   time: 0.30s   best: 81.9437
2023-10-05 15:03:37,119:INFO:  Epoch 96/500:  train Loss: 84.4497   val Loss: 81.9989   time: 0.30s   best: 81.9437
2023-10-05 15:03:37,434:INFO:  Epoch 97/500:  train Loss: 84.3865   val Loss: 86.0482   time: 0.31s   best: 81.9437
2023-10-05 15:03:37,736:INFO:  Epoch 98/500:  train Loss: 86.4815   val Loss: 85.5103   time: 0.30s   best: 81.9437
2023-10-05 15:03:38,046:INFO:  Epoch 99/500:  train Loss: 85.9133   val Loss: 84.5721   time: 0.31s   best: 81.9437
2023-10-05 15:03:39,084:INFO:  Epoch 100/500:  train Loss: 85.4895   val Loss: 84.4794   time: 1.03s   best: 81.9437
2023-10-05 15:03:39,401:INFO:  Epoch 101/500:  train Loss: 84.6624   val Loss: 83.2950   time: 0.31s   best: 81.9437
2023-10-05 15:03:39,720:INFO:  Epoch 102/500:  train Loss: 83.4927   val Loss: 82.7298   time: 0.31s   best: 81.9437
2023-10-05 15:03:40,042:INFO:  Epoch 103/500:  train Loss: 83.2757   val Loss: 82.0661   time: 0.31s   best: 81.9437
2023-10-05 15:03:40,344:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:40,370:INFO:  Epoch 104/500:  train Loss: 82.4061   val Loss: 81.6051   time: 0.30s   best: 81.6051
2023-10-05 15:03:40,676:INFO:  Epoch 105/500:  train Loss: 82.0058   val Loss: 81.6888   time: 0.30s   best: 81.6051
2023-10-05 15:03:40,988:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:41,015:INFO:  Epoch 106/500:  train Loss: 82.2570   val Loss: 81.4996   time: 0.30s   best: 81.4996
2023-10-05 15:03:41,324:INFO:  Epoch 107/500:  train Loss: 82.4786   val Loss: 81.5007   time: 0.30s   best: 81.4996
2023-10-05 15:03:41,636:INFO:  Epoch 108/500:  train Loss: 82.2259   val Loss: 81.5274   time: 0.30s   best: 81.4996
2023-10-05 15:03:41,940:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:41,965:INFO:  Epoch 109/500:  train Loss: 82.2929   val Loss: 81.0758   time: 0.30s   best: 81.0758
2023-10-05 15:03:42,278:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:42,304:INFO:  Epoch 110/500:  train Loss: 82.1096   val Loss: 80.8362   time: 0.31s   best: 80.8362
2023-10-05 15:03:42,613:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:42,639:INFO:  Epoch 111/500:  train Loss: 81.5567   val Loss: 80.0442   time: 0.30s   best: 80.0442
2023-10-05 15:03:42,947:INFO:  Epoch 112/500:  train Loss: 80.6834   val Loss: 80.2435   time: 0.30s   best: 80.0442
2023-10-05 15:03:43,267:INFO:  Epoch 113/500:  train Loss: 81.8591   val Loss: 81.6906   time: 0.31s   best: 80.0442
2023-10-05 15:03:43,587:INFO:  Epoch 114/500:  train Loss: 81.3723   val Loss: 80.3948   time: 0.31s   best: 80.0442
2023-10-05 15:03:43,898:INFO:  Epoch 115/500:  train Loss: 81.3440   val Loss: 80.2062   time: 0.30s   best: 80.0442
2023-10-05 15:03:44,206:INFO:  Epoch 116/500:  train Loss: 81.2967   val Loss: 80.2793   time: 0.30s   best: 80.0442
2023-10-05 15:03:44,508:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:44,534:INFO:  Epoch 117/500:  train Loss: 81.2358   val Loss: 79.8923   time: 0.30s   best: 79.8923
2023-10-05 15:03:44,844:INFO:  Epoch 118/500:  train Loss: 82.1112   val Loss: 80.0571   time: 0.30s   best: 79.8923
2023-10-05 15:03:45,162:INFO:  Epoch 119/500:  train Loss: 83.9870   val Loss: 86.3530   time: 0.31s   best: 79.8923
2023-10-05 15:03:45,472:INFO:  Epoch 120/500:  train Loss: 86.4184   val Loss: 84.7046   time: 0.30s   best: 79.8923
2023-10-05 15:03:45,780:INFO:  Epoch 121/500:  train Loss: 85.5146   val Loss: 83.9797   time: 0.30s   best: 79.8923
2023-10-05 15:03:46,100:INFO:  Epoch 122/500:  train Loss: 85.1321   val Loss: 83.9961   time: 0.31s   best: 79.8923
2023-10-05 15:03:46,407:INFO:  Epoch 123/500:  train Loss: 83.7460   val Loss: 82.0635   time: 0.30s   best: 79.8923
2023-10-05 15:03:46,715:INFO:  Epoch 124/500:  train Loss: 81.6810   val Loss: 80.5460   time: 0.31s   best: 79.8923
2023-10-05 15:03:47,015:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:47,041:INFO:  Epoch 125/500:  train Loss: 81.1522   val Loss: 79.7363   time: 0.30s   best: 79.7363
2023-10-05 15:03:47,364:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:47,391:INFO:  Epoch 126/500:  train Loss: 80.8301   val Loss: 79.0387   time: 0.32s   best: 79.0387
2023-10-05 15:03:47,700:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:47,726:INFO:  Epoch 127/500:  train Loss: 80.6758   val Loss: 78.9939   time: 0.31s   best: 78.9939
2023-10-05 15:03:48,032:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:48,058:INFO:  Epoch 128/500:  train Loss: 80.1727   val Loss: 78.9771   time: 0.30s   best: 78.9771
2023-10-05 15:03:48,377:INFO:  Epoch 129/500:  train Loss: 80.6445   val Loss: 80.1564   time: 0.31s   best: 78.9771
2023-10-05 15:03:48,687:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:48,713:INFO:  Epoch 130/500:  train Loss: 80.6210   val Loss: 78.6572   time: 0.31s   best: 78.6572
2023-10-05 15:03:49,021:INFO:  Epoch 131/500:  train Loss: 80.3498   val Loss: 79.2449   time: 0.30s   best: 78.6572
2023-10-05 15:03:49,339:INFO:  Epoch 132/500:  train Loss: 80.0969   val Loss: 78.8363   time: 0.31s   best: 78.6572
2023-10-05 15:03:49,649:INFO:  Epoch 133/500:  train Loss: 80.4317   val Loss: 78.8227   time: 0.30s   best: 78.6572
2023-10-05 15:03:49,969:INFO:  Epoch 134/500:  train Loss: 80.4214   val Loss: 79.8715   time: 0.31s   best: 78.6572
2023-10-05 15:03:50,286:INFO:  Epoch 135/500:  train Loss: 80.2922   val Loss: 79.2081   time: 0.31s   best: 78.6572
2023-10-05 15:03:50,589:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:50,615:INFO:  Epoch 136/500:  train Loss: 80.3518   val Loss: 78.0513   time: 0.30s   best: 78.0513
2023-10-05 15:03:50,933:INFO:  Epoch 137/500:  train Loss: 79.5664   val Loss: 79.0287   time: 0.31s   best: 78.0513
2023-10-05 15:03:51,251:INFO:  Epoch 138/500:  train Loss: 80.9471   val Loss: 78.5338   time: 0.31s   best: 78.0513
2023-10-05 15:03:51,558:INFO:  Epoch 139/500:  train Loss: 82.2349   val Loss: 84.6076   time: 0.30s   best: 78.0513
2023-10-05 15:03:51,872:INFO:  Epoch 140/500:  train Loss: 85.6502   val Loss: 84.5189   time: 0.30s   best: 78.0513
2023-10-05 15:03:52,179:INFO:  Epoch 141/500:  train Loss: 85.3169   val Loss: 84.9318   time: 0.30s   best: 78.0513
2023-10-05 15:03:52,497:INFO:  Epoch 142/500:  train Loss: 85.1981   val Loss: 84.3509   time: 0.31s   best: 78.0513
2023-10-05 15:03:52,814:INFO:  Epoch 143/500:  train Loss: 84.1534   val Loss: 83.1295   time: 0.31s   best: 78.0513
2023-10-05 15:03:53,123:INFO:  Epoch 144/500:  train Loss: 83.3630   val Loss: 82.1953   time: 0.30s   best: 78.0513
2023-10-05 15:03:53,444:INFO:  Epoch 145/500:  train Loss: 82.2516   val Loss: 80.3845   time: 0.31s   best: 78.0513
2023-10-05 15:03:53,761:INFO:  Epoch 146/500:  train Loss: 80.8042   val Loss: 79.2760   time: 0.31s   best: 78.0513
2023-10-05 15:03:54,069:INFO:  Epoch 147/500:  train Loss: 79.9786   val Loss: 78.6184   time: 0.30s   best: 78.0513
2023-10-05 15:03:54,379:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:54,405:INFO:  Epoch 148/500:  train Loss: 79.2013   val Loss: 78.0071   time: 0.31s   best: 78.0071
2023-10-05 15:03:54,706:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:54,733:INFO:  Epoch 149/500:  train Loss: 78.6740   val Loss: 77.7819   time: 0.30s   best: 77.7819
2023-10-05 15:03:55,047:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:55,075:INFO:  Epoch 150/500:  train Loss: 78.6542   val Loss: 77.3822   time: 0.31s   best: 77.3822
2023-10-05 15:03:55,396:INFO:  Epoch 151/500:  train Loss: 78.9195   val Loss: 79.8609   time: 0.31s   best: 77.3822
2023-10-05 15:03:55,703:INFO:  Epoch 152/500:  train Loss: 80.7687   val Loss: 78.4901   time: 0.30s   best: 77.3822
2023-10-05 15:03:56,015:INFO:  Epoch 153/500:  train Loss: 79.2463   val Loss: 77.9126   time: 0.30s   best: 77.3822
2023-10-05 15:03:56,332:INFO:  Epoch 154/500:  train Loss: 79.1712   val Loss: 78.3068   time: 0.31s   best: 77.3822
2023-10-05 15:03:56,633:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:56,659:INFO:  Epoch 155/500:  train Loss: 78.4667   val Loss: 77.1609   time: 0.30s   best: 77.1609
2023-10-05 15:03:56,974:INFO:  Epoch 156/500:  train Loss: 78.6799   val Loss: 77.4373   time: 0.30s   best: 77.1609
2023-10-05 15:03:57,282:INFO:  Epoch 157/500:  train Loss: 78.5675   val Loss: 77.2276   time: 0.30s   best: 77.1609
2023-10-05 15:03:57,673:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:57,699:INFO:  Epoch 158/500:  train Loss: 78.4982   val Loss: 76.7885   time: 0.39s   best: 76.7885
2023-10-05 15:03:58,009:INFO:  Epoch 159/500:  train Loss: 78.1228   val Loss: 77.0332   time: 0.30s   best: 76.7885
2023-10-05 15:03:58,309:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:58,336:INFO:  Epoch 160/500:  train Loss: 78.4378   val Loss: 76.6671   time: 0.30s   best: 76.6671
2023-10-05 15:03:58,649:INFO:  Epoch 161/500:  train Loss: 77.8320   val Loss: 76.9531   time: 0.30s   best: 76.6671
2023-10-05 15:03:58,965:INFO:  Epoch 162/500:  train Loss: 79.1846   val Loss: 77.0942   time: 0.31s   best: 76.6671
2023-10-05 15:03:59,274:INFO:  Epoch 163/500:  train Loss: 78.7958   val Loss: 77.6843   time: 0.30s   best: 76.6671
2023-10-05 15:03:59,581:INFO:  Epoch 164/500:  train Loss: 78.5236   val Loss: 77.6579   time: 0.30s   best: 76.6671
2023-10-05 15:03:59,893:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:03:59,920:INFO:  Epoch 165/500:  train Loss: 77.8957   val Loss: 76.6251   time: 0.31s   best: 76.6251
2023-10-05 15:04:00,227:INFO:  Epoch 166/500:  train Loss: 77.7035   val Loss: 76.6457   time: 0.30s   best: 76.6251
2023-10-05 15:04:00,528:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:00,595:INFO:  Epoch 167/500:  train Loss: 77.8208   val Loss: 76.5811   time: 0.30s   best: 76.5811
2023-10-05 15:04:00,906:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:00,932:INFO:  Epoch 168/500:  train Loss: 77.8275   val Loss: 76.5675   time: 0.30s   best: 76.5675
2023-10-05 15:04:01,232:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:01,258:INFO:  Epoch 169/500:  train Loss: 77.1880   val Loss: 76.0527   time: 0.30s   best: 76.0527
2023-10-05 15:04:01,566:INFO:  Epoch 170/500:  train Loss: 76.7866   val Loss: 76.2402   time: 0.30s   best: 76.0527
2023-10-05 15:04:01,883:INFO:  Epoch 171/500:  train Loss: 77.5597   val Loss: 77.1984   time: 0.31s   best: 76.0527
2023-10-05 15:04:02,206:INFO:  Epoch 172/500:  train Loss: 78.8567   val Loss: 78.4654   time: 0.31s   best: 76.0527
2023-10-05 15:04:02,523:INFO:  Epoch 173/500:  train Loss: 79.7615   val Loss: 79.1171   time: 0.31s   best: 76.0527
2023-10-05 15:04:02,829:INFO:  Epoch 174/500:  train Loss: 79.4587   val Loss: 77.4800   time: 0.30s   best: 76.0527
2023-10-05 15:04:03,139:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:03,166:INFO:  Epoch 175/500:  train Loss: 77.0360   val Loss: 75.6788   time: 0.31s   best: 75.6788
2023-10-05 15:04:03,481:INFO:  Epoch 176/500:  train Loss: 78.2878   val Loss: 75.7823   time: 0.31s   best: 75.6788
2023-10-05 15:04:03,789:INFO:  Epoch 177/500:  train Loss: 77.1365   val Loss: 75.8330   time: 0.30s   best: 75.6788
2023-10-05 15:04:04,100:INFO:  Epoch 178/500:  train Loss: 77.1671   val Loss: 76.1627   time: 0.30s   best: 75.6788
2023-10-05 15:04:04,402:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:04,429:INFO:  Epoch 179/500:  train Loss: 76.7770   val Loss: 75.2497   time: 0.30s   best: 75.2497
2023-10-05 15:04:04,746:INFO:  Epoch 180/500:  train Loss: 77.0281   val Loss: 75.3778   time: 0.31s   best: 75.2497
2023-10-05 15:04:05,064:INFO:  Epoch 181/500:  train Loss: 76.8833   val Loss: 75.8797   time: 0.31s   best: 75.2497
2023-10-05 15:04:05,372:INFO:  Epoch 182/500:  train Loss: 76.7928   val Loss: 77.7122   time: 0.30s   best: 75.2497
2023-10-05 15:04:05,691:INFO:  Epoch 183/500:  train Loss: 78.4122   val Loss: 77.1843   time: 0.31s   best: 75.2497
2023-10-05 15:04:06,012:INFO:  Epoch 184/500:  train Loss: 77.7977   val Loss: 77.4354   time: 0.30s   best: 75.2497
2023-10-05 15:04:06,313:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:06,340:INFO:  Epoch 185/500:  train Loss: 77.2904   val Loss: 75.1159   time: 0.30s   best: 75.1159
2023-10-05 15:04:06,655:INFO:  Epoch 186/500:  train Loss: 77.2172   val Loss: 76.6566   time: 0.31s   best: 75.1159
2023-10-05 15:04:06,962:INFO:  Epoch 187/500:  train Loss: 76.7978   val Loss: 75.9946   time: 0.30s   best: 75.1159
2023-10-05 15:04:07,282:INFO:  Epoch 188/500:  train Loss: 76.9193   val Loss: 75.6357   time: 0.31s   best: 75.1159
2023-10-05 15:04:07,597:INFO:  Epoch 189/500:  train Loss: 76.9964   val Loss: 75.5131   time: 0.31s   best: 75.1159
2023-10-05 15:04:07,901:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:07,927:INFO:  Epoch 190/500:  train Loss: 77.6861   val Loss: 74.3976   time: 0.30s   best: 74.3976
2023-10-05 15:04:08,247:INFO:  Epoch 191/500:  train Loss: 76.2278   val Loss: 75.5316   time: 0.31s   best: 74.3976
2023-10-05 15:04:08,555:INFO:  Epoch 192/500:  train Loss: 76.5168   val Loss: 75.2491   time: 0.30s   best: 74.3976
2023-10-05 15:04:08,872:INFO:  Epoch 193/500:  train Loss: 75.8930   val Loss: 74.6036   time: 0.31s   best: 74.3976
2023-10-05 15:04:09,188:INFO:  Epoch 194/500:  train Loss: 75.8946   val Loss: 75.1839   time: 0.31s   best: 74.3976
2023-10-05 15:04:09,496:INFO:  Epoch 195/500:  train Loss: 82.4106   val Loss: 74.6481   time: 0.30s   best: 74.3976
2023-10-05 15:04:09,804:INFO:  Epoch 196/500:  train Loss: 83.9779   val Loss: 89.1323   time: 0.30s   best: 74.3976
2023-10-05 15:04:10,121:INFO:  Epoch 197/500:  train Loss: 89.0375   val Loss: 88.5333   time: 0.31s   best: 74.3976
2023-10-05 15:04:10,429:INFO:  Epoch 198/500:  train Loss: 88.6742   val Loss: 88.7403   time: 0.30s   best: 74.3976
2023-10-05 15:04:10,744:INFO:  Epoch 199/500:  train Loss: 89.0692   val Loss: 89.1389   time: 0.30s   best: 74.3976
2023-10-05 15:04:11,097:INFO:  Epoch 200/500:  train Loss: 89.5174   val Loss: 89.2737   time: 0.34s   best: 74.3976
2023-10-05 15:04:11,413:INFO:  Epoch 201/500:  train Loss: 89.6008   val Loss: 89.2142   time: 0.31s   best: 74.3976
2023-10-05 15:04:11,734:INFO:  Epoch 202/500:  train Loss: 89.5915   val Loss: 89.1399   time: 0.31s   best: 74.3976
2023-10-05 15:04:12,043:INFO:  Epoch 203/500:  train Loss: 89.1383   val Loss: 88.7512   time: 0.30s   best: 74.3976
2023-10-05 15:04:12,351:INFO:  Epoch 204/500:  train Loss: 88.5937   val Loss: 88.2866   time: 0.30s   best: 74.3976
2023-10-05 15:04:12,668:INFO:  Epoch 205/500:  train Loss: 88.5427   val Loss: 88.0789   time: 0.30s   best: 74.3976
2023-10-05 15:04:12,974:INFO:  Epoch 206/500:  train Loss: 88.4301   val Loss: 87.7636   time: 0.30s   best: 74.3976
2023-10-05 15:04:13,294:INFO:  Epoch 207/500:  train Loss: 88.0642   val Loss: 87.3735   time: 0.31s   best: 74.3976
2023-10-05 15:04:13,601:INFO:  Epoch 208/500:  train Loss: 87.4704   val Loss: 86.7737   time: 0.30s   best: 74.3976
2023-10-05 15:04:13,926:INFO:  Epoch 209/500:  train Loss: 86.9713   val Loss: 86.0997   time: 0.31s   best: 74.3976
2023-10-05 15:04:14,242:INFO:  Epoch 210/500:  train Loss: 86.2284   val Loss: 85.3484   time: 0.31s   best: 74.3976
2023-10-05 15:04:14,549:INFO:  Epoch 211/500:  train Loss: 85.4099   val Loss: 84.4234   time: 0.30s   best: 74.3976
2023-10-05 15:04:14,866:INFO:  Epoch 212/500:  train Loss: 84.5995   val Loss: 83.3135   time: 0.31s   best: 74.3976
2023-10-05 15:04:15,175:INFO:  Epoch 213/500:  train Loss: 83.4130   val Loss: 81.9697   time: 0.30s   best: 74.3976
2023-10-05 15:04:15,488:INFO:  Epoch 214/500:  train Loss: 82.1093   val Loss: 80.4051   time: 0.31s   best: 74.3976
2023-10-05 15:04:15,804:INFO:  Epoch 215/500:  train Loss: 81.3900   val Loss: 79.1034   time: 0.31s   best: 74.3976
2023-10-05 15:04:16,112:INFO:  Epoch 216/500:  train Loss: 79.5949   val Loss: 77.5057   time: 0.30s   best: 74.3976
2023-10-05 15:04:16,428:INFO:  Epoch 217/500:  train Loss: 78.3245   val Loss: 77.2353   time: 0.31s   best: 74.3976
2023-10-05 15:04:16,740:INFO:  Epoch 218/500:  train Loss: 77.7050   val Loss: 77.0354   time: 0.30s   best: 74.3976
2023-10-05 15:04:17,059:INFO:  Epoch 219/500:  train Loss: 77.5908   val Loss: 77.2324   time: 0.31s   best: 74.3976
2023-10-05 15:04:17,377:INFO:  Epoch 220/500:  train Loss: 77.6646   val Loss: 75.6705   time: 0.31s   best: 74.3976
2023-10-05 15:04:17,688:INFO:  Epoch 221/500:  train Loss: 76.7108   val Loss: 75.1011   time: 0.30s   best: 74.3976
2023-10-05 15:04:18,005:INFO:  Epoch 222/500:  train Loss: 76.9793   val Loss: 75.8411   time: 0.31s   best: 74.3976
2023-10-05 15:04:18,326:INFO:  Epoch 223/500:  train Loss: 76.8698   val Loss: 75.1471   time: 0.31s   best: 74.3976
2023-10-05 15:04:18,627:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:18,654:INFO:  Epoch 224/500:  train Loss: 76.0369   val Loss: 74.3720   time: 0.30s   best: 74.3720
2023-10-05 15:04:18,962:INFO:  Epoch 225/500:  train Loss: 75.7200   val Loss: 74.9878   time: 0.30s   best: 74.3720
2023-10-05 15:04:19,269:INFO:  Epoch 226/500:  train Loss: 76.3968   val Loss: 75.7100   time: 0.30s   best: 74.3720
2023-10-05 15:04:19,587:INFO:  Epoch 227/500:  train Loss: 77.5822   val Loss: 76.2152   time: 0.31s   best: 74.3720
2023-10-05 15:04:19,908:INFO:  Epoch 228/500:  train Loss: 77.1624   val Loss: 76.2228   time: 0.31s   best: 74.3720
2023-10-05 15:04:20,216:INFO:  Epoch 229/500:  train Loss: 76.7899   val Loss: 76.0038   time: 0.30s   best: 74.3720
2023-10-05 15:04:20,529:INFO:  Epoch 230/500:  train Loss: 76.4692   val Loss: 75.0578   time: 0.30s   best: 74.3720
2023-10-05 15:04:20,847:INFO:  Epoch 231/500:  train Loss: 75.4385   val Loss: 74.6164   time: 0.30s   best: 74.3720
2023-10-05 15:04:21,150:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:21,177:INFO:  Epoch 232/500:  train Loss: 75.8492   val Loss: 74.3203   time: 0.30s   best: 74.3203
2023-10-05 15:04:21,484:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:21,511:INFO:  Epoch 233/500:  train Loss: 75.5662   val Loss: 73.8857   time: 0.30s   best: 73.8857
2023-10-05 15:04:21,822:INFO:  Epoch 234/500:  train Loss: 75.5910   val Loss: 75.3323   time: 0.30s   best: 73.8857
2023-10-05 15:04:22,140:INFO:  Epoch 235/500:  train Loss: 76.5125   val Loss: 75.5748   time: 0.31s   best: 73.8857
2023-10-05 15:04:22,459:INFO:  Epoch 236/500:  train Loss: 75.7445   val Loss: 74.6082   time: 0.31s   best: 73.8857
2023-10-05 15:04:22,765:INFO:  Epoch 237/500:  train Loss: 75.2316   val Loss: 74.4318   time: 0.30s   best: 73.8857
2023-10-05 15:04:23,078:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:23,107:INFO:  Epoch 238/500:  train Loss: 75.1158   val Loss: 73.7241   time: 0.31s   best: 73.7241
2023-10-05 15:04:23,417:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:23,508:INFO:  Epoch 239/500:  train Loss: 74.0580   val Loss: 73.3896   time: 0.31s   best: 73.3896
2023-10-05 15:04:23,815:INFO:  Epoch 240/500:  train Loss: 74.1311   val Loss: 74.6615   time: 0.30s   best: 73.3896
2023-10-05 15:04:24,123:INFO:  Epoch 241/500:  train Loss: 75.8099   val Loss: 73.9430   time: 0.30s   best: 73.3896
2023-10-05 15:04:24,441:INFO:  Epoch 242/500:  train Loss: 74.9623   val Loss: 74.2484   time: 0.31s   best: 73.3896
2023-10-05 15:04:24,748:INFO:  Epoch 243/500:  train Loss: 74.7018   val Loss: 74.2380   time: 0.30s   best: 73.3896
2023-10-05 15:04:25,065:INFO:  Epoch 244/500:  train Loss: 74.9369   val Loss: 73.6872   time: 0.31s   best: 73.3896
2023-10-05 15:04:25,375:INFO:  Epoch 245/500:  train Loss: 74.8652   val Loss: 73.5960   time: 0.30s   best: 73.3896
2023-10-05 15:04:25,695:INFO:  Epoch 246/500:  train Loss: 74.0546   val Loss: 73.6929   time: 0.31s   best: 73.3896
2023-10-05 15:04:26,009:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:26,036:INFO:  Epoch 247/500:  train Loss: 74.8505   val Loss: 73.2675   time: 0.31s   best: 73.2675
2023-10-05 15:04:26,339:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:26,365:INFO:  Epoch 248/500:  train Loss: 74.1995   val Loss: 73.2158   time: 0.30s   best: 73.2158
2023-10-05 15:04:26,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:26,688:INFO:  Epoch 249/500:  train Loss: 73.9566   val Loss: 72.6153   time: 0.30s   best: 72.6153
2023-10-05 15:04:26,989:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:27,011:INFO:  Epoch 250/500:  train Loss: 73.8511   val Loss: 71.9210   time: 0.30s   best: 71.9210
2023-10-05 15:04:27,327:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:27,353:INFO:  Epoch 251/500:  train Loss: 73.0024   val Loss: 71.8399   time: 0.31s   best: 71.8399
2023-10-05 15:04:27,690:INFO:  Epoch 252/500:  train Loss: 73.7170   val Loss: 72.4090   time: 0.31s   best: 71.8399
2023-10-05 15:04:28,082:INFO:  Epoch 253/500:  train Loss: 73.3034   val Loss: 73.1030   time: 0.38s   best: 71.8399
2023-10-05 15:04:28,390:INFO:  Epoch 254/500:  train Loss: 74.0773   val Loss: 72.3281   time: 0.30s   best: 71.8399
2023-10-05 15:04:28,707:INFO:  Epoch 255/500:  train Loss: 73.8427   val Loss: 72.2397   time: 0.31s   best: 71.8399
2023-10-05 15:04:29,026:INFO:  Epoch 256/500:  train Loss: 73.9264   val Loss: 72.9301   time: 0.30s   best: 71.8399
2023-10-05 15:04:29,330:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:29,357:INFO:  Epoch 257/500:  train Loss: 72.8040   val Loss: 70.6556   time: 0.30s   best: 70.6556
2023-10-05 15:04:29,664:INFO:  Epoch 258/500:  train Loss: 72.2271   val Loss: 71.4443   time: 0.30s   best: 70.6556
2023-10-05 15:04:29,975:INFO:  Epoch 259/500:  train Loss: 72.4855   val Loss: 71.8231   time: 0.30s   best: 70.6556
2023-10-05 15:04:30,291:INFO:  Epoch 260/500:  train Loss: 72.6392   val Loss: 71.4728   time: 0.31s   best: 70.6556
2023-10-05 15:04:30,613:INFO:  Epoch 261/500:  train Loss: 72.8179   val Loss: 72.0498   time: 0.31s   best: 70.6556
2023-10-05 15:04:30,920:INFO:  Epoch 262/500:  train Loss: 73.6312   val Loss: 72.4361   time: 0.30s   best: 70.6556
2023-10-05 15:04:31,233:INFO:  Epoch 263/500:  train Loss: 72.6937   val Loss: 71.3551   time: 0.30s   best: 70.6556
2023-10-05 15:04:31,540:INFO:  Epoch 264/500:  train Loss: 72.7400   val Loss: 72.4630   time: 0.30s   best: 70.6556
2023-10-05 15:04:31,848:INFO:  Epoch 265/500:  train Loss: 73.0053   val Loss: 72.0188   time: 0.30s   best: 70.6556
2023-10-05 15:04:32,174:INFO:  Epoch 266/500:  train Loss: 72.1618   val Loss: 72.8825   time: 0.32s   best: 70.6556
2023-10-05 15:04:32,482:INFO:  Epoch 267/500:  train Loss: 74.3578   val Loss: 72.9901   time: 0.30s   best: 70.6556
2023-10-05 15:04:32,790:INFO:  Epoch 268/500:  train Loss: 73.2358   val Loss: 71.3103   time: 0.30s   best: 70.6556
2023-10-05 15:04:33,111:INFO:  Epoch 269/500:  train Loss: 75.3317   val Loss: 73.0128   time: 0.30s   best: 70.6556
2023-10-05 15:04:33,420:INFO:  Epoch 270/500:  train Loss: 75.9517   val Loss: 75.1596   time: 0.30s   best: 70.6556
2023-10-05 15:04:33,737:INFO:  Epoch 271/500:  train Loss: 76.8445   val Loss: 76.9297   time: 0.31s   best: 70.6556
2023-10-05 15:04:34,050:INFO:  Epoch 272/500:  train Loss: 75.7240   val Loss: 75.2949   time: 0.30s   best: 70.6556
2023-10-05 15:04:34,368:INFO:  Epoch 273/500:  train Loss: 74.8129   val Loss: 72.9078   time: 0.31s   best: 70.6556
2023-10-05 15:04:34,684:INFO:  Epoch 274/500:  train Loss: 73.2758   val Loss: 70.9345   time: 0.31s   best: 70.6556
2023-10-05 15:04:34,992:INFO:  Epoch 275/500:  train Loss: 72.5547   val Loss: 72.5259   time: 0.30s   best: 70.6556
2023-10-05 15:04:35,302:INFO:  Epoch 276/500:  train Loss: 74.5111   val Loss: 73.7331   time: 0.30s   best: 70.6556
2023-10-05 15:04:35,610:INFO:  Epoch 277/500:  train Loss: 73.1387   val Loss: 71.3972   time: 0.30s   best: 70.6556
2023-10-05 15:04:35,931:INFO:  Epoch 278/500:  train Loss: 72.2572   val Loss: 71.2411   time: 0.31s   best: 70.6556
2023-10-05 15:04:36,252:INFO:  Epoch 279/500:  train Loss: 73.1908   val Loss: 72.7294   time: 0.31s   best: 70.6556
2023-10-05 15:04:36,562:INFO:  Epoch 280/500:  train Loss: 73.7661   val Loss: 73.1304   time: 0.30s   best: 70.6556
2023-10-05 15:04:36,878:INFO:  Epoch 281/500:  train Loss: 73.1606   val Loss: 72.1044   time: 0.31s   best: 70.6556
2023-10-05 15:04:37,199:INFO:  Epoch 282/500:  train Loss: 72.9851   val Loss: 72.1232   time: 0.31s   best: 70.6556
2023-10-05 15:04:37,507:INFO:  Epoch 283/500:  train Loss: 74.6304   val Loss: 75.4140   time: 0.30s   best: 70.6556
2023-10-05 15:04:37,824:INFO:  Epoch 284/500:  train Loss: 74.6233   val Loss: 74.2731   time: 0.31s   best: 70.6556
2023-10-05 15:04:38,135:INFO:  Epoch 285/500:  train Loss: 73.5412   val Loss: 71.5654   time: 0.30s   best: 70.6556
2023-10-05 15:04:38,449:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:38,484:INFO:  Epoch 286/500:  train Loss: 72.3513   val Loss: 70.1183   time: 0.31s   best: 70.1183
2023-10-05 15:04:38,801:INFO:  Epoch 287/500:  train Loss: 71.4100   val Loss: 71.0319   time: 0.31s   best: 70.1183
2023-10-05 15:04:39,111:INFO:  Epoch 288/500:  train Loss: 72.1267   val Loss: 70.2605   time: 0.30s   best: 70.1183
2023-10-05 15:04:39,429:INFO:  Epoch 289/500:  train Loss: 73.7424   val Loss: 74.2128   time: 0.31s   best: 70.1183
2023-10-05 15:04:39,730:INFO:  Epoch 290/500:  train Loss: 74.4726   val Loss: 73.1615   time: 0.30s   best: 70.1183
2023-10-05 15:04:40,052:INFO:  Epoch 291/500:  train Loss: 73.7182   val Loss: 73.5259   time: 0.31s   best: 70.1183
2023-10-05 15:04:40,377:INFO:  Epoch 292/500:  train Loss: 74.3575   val Loss: 75.2170   time: 0.31s   best: 70.1183
2023-10-05 15:04:40,684:INFO:  Epoch 293/500:  train Loss: 73.8148   val Loss: 71.6105   time: 0.30s   best: 70.1183
2023-10-05 15:04:41,002:INFO:  Epoch 294/500:  train Loss: 73.1528   val Loss: 71.1181   time: 0.31s   best: 70.1183
2023-10-05 15:04:41,321:INFO:  Epoch 295/500:  train Loss: 72.1064   val Loss: 70.8787   time: 0.31s   best: 70.1183
2023-10-05 15:04:41,628:INFO:  Epoch 296/500:  train Loss: 71.4352   val Loss: 70.9536   time: 0.30s   best: 70.1183
2023-10-05 15:04:41,940:INFO:  Epoch 297/500:  train Loss: 71.0619   val Loss: 70.3859   time: 0.30s   best: 70.1183
2023-10-05 15:04:42,256:INFO:  Epoch 298/500:  train Loss: 73.6984   val Loss: 76.3680   time: 0.30s   best: 70.1183
2023-10-05 15:04:42,574:INFO:  Epoch 299/500:  train Loss: 76.4885   val Loss: 75.6920   time: 0.31s   best: 70.1183
2023-10-05 15:04:42,928:INFO:  Epoch 300/500:  train Loss: 76.2925   val Loss: 73.6216   time: 0.34s   best: 70.1183
2023-10-05 15:04:43,246:INFO:  Epoch 301/500:  train Loss: 73.5702   val Loss: 72.0458   time: 0.31s   best: 70.1183
2023-10-05 15:04:43,563:INFO:  Epoch 302/500:  train Loss: 73.9256   val Loss: 70.9250   time: 0.31s   best: 70.1183
2023-10-05 15:04:43,880:INFO:  Epoch 303/500:  train Loss: 72.7946   val Loss: 72.4894   time: 0.31s   best: 70.1183
2023-10-05 15:04:44,193:INFO:  Epoch 304/500:  train Loss: 71.8108   val Loss: 71.2883   time: 0.30s   best: 70.1183
2023-10-05 15:04:44,510:INFO:  Epoch 305/500:  train Loss: 72.9015   val Loss: 72.6769   time: 0.31s   best: 70.1183
2023-10-05 15:04:44,817:INFO:  Epoch 306/500:  train Loss: 72.0668   val Loss: 71.0118   time: 0.30s   best: 70.1183
2023-10-05 15:04:45,136:INFO:  Epoch 307/500:  train Loss: 71.8834   val Loss: 71.9338   time: 0.31s   best: 70.1183
2023-10-05 15:04:45,455:INFO:  Epoch 308/500:  train Loss: 71.6471   val Loss: 71.9848   time: 0.31s   best: 70.1183
2023-10-05 15:04:45,762:INFO:  Epoch 309/500:  train Loss: 72.2086   val Loss: 73.0865   time: 0.30s   best: 70.1183
2023-10-05 15:04:46,075:INFO:  Epoch 310/500:  train Loss: 72.7753   val Loss: 73.0560   time: 0.31s   best: 70.1183
2023-10-05 15:04:46,382:INFO:  Epoch 311/500:  train Loss: 73.0524   val Loss: 71.1564   time: 0.31s   best: 70.1183
2023-10-05 15:04:46,701:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:46,727:INFO:  Epoch 312/500:  train Loss: 70.9292   val Loss: 69.3122   time: 0.31s   best: 69.3122
2023-10-05 15:04:47,039:INFO:  Epoch 313/500:  train Loss: 74.4462   val Loss: 73.1672   time: 0.30s   best: 69.3122
2023-10-05 15:04:47,347:INFO:  Epoch 314/500:  train Loss: 86.6801   val Loss: 90.6563   time: 0.30s   best: 69.3122
2023-10-05 15:04:47,666:INFO:  Epoch 315/500:  train Loss: 89.3673   val Loss: 89.2508   time: 0.31s   best: 69.3122
2023-10-05 15:04:47,988:INFO:  Epoch 316/500:  train Loss: 89.1797   val Loss: 89.2877   time: 0.31s   best: 69.3122
2023-10-05 15:04:48,295:INFO:  Epoch 317/500:  train Loss: 90.0176   val Loss: 89.2515   time: 0.30s   best: 69.3122
2023-10-05 15:04:48,602:INFO:  Epoch 318/500:  train Loss: 90.1706   val Loss: 89.4227   time: 0.30s   best: 69.3122
2023-10-05 15:04:48,908:INFO:  Epoch 319/500:  train Loss: 89.7039   val Loss: 88.7560   time: 0.30s   best: 69.3122
2023-10-05 15:04:49,229:INFO:  Epoch 320/500:  train Loss: 88.3391   val Loss: 87.5398   time: 0.31s   best: 69.3122
2023-10-05 15:04:49,540:INFO:  Epoch 321/500:  train Loss: 87.7458   val Loss: 86.3683   time: 0.31s   best: 69.3122
2023-10-05 15:04:49,851:INFO:  Epoch 322/500:  train Loss: 86.4928   val Loss: 84.9089   time: 0.30s   best: 69.3122
2023-10-05 15:04:50,171:INFO:  Epoch 323/500:  train Loss: 84.7548   val Loss: 83.2107   time: 0.31s   best: 69.3122
2023-10-05 15:04:50,490:INFO:  Epoch 324/500:  train Loss: 82.8763   val Loss: 80.8262   time: 0.30s   best: 69.3122
2023-10-05 15:04:50,798:INFO:  Epoch 325/500:  train Loss: 80.7043   val Loss: 78.6202   time: 0.30s   best: 69.3122
2023-10-05 15:04:51,117:INFO:  Epoch 326/500:  train Loss: 78.8529   val Loss: 76.7066   time: 0.31s   best: 69.3122
2023-10-05 15:04:51,419:INFO:  Epoch 327/500:  train Loss: 77.5842   val Loss: 76.9609   time: 0.30s   best: 69.3122
2023-10-05 15:04:51,741:INFO:  Epoch 328/500:  train Loss: 75.3950   val Loss: 75.3979   time: 0.31s   best: 69.3122
2023-10-05 15:04:52,058:INFO:  Epoch 329/500:  train Loss: 76.2908   val Loss: 75.3294   time: 0.31s   best: 69.3122
2023-10-05 15:04:52,366:INFO:  Epoch 330/500:  train Loss: 75.2699   val Loss: 73.4873   time: 0.30s   best: 69.3122
2023-10-05 15:04:52,672:INFO:  Epoch 331/500:  train Loss: 74.1030   val Loss: 72.3106   time: 0.30s   best: 69.3122
2023-10-05 15:04:52,980:INFO:  Epoch 332/500:  train Loss: 73.2796   val Loss: 72.7067   time: 0.30s   best: 69.3122
2023-10-05 15:04:53,297:INFO:  Epoch 333/500:  train Loss: 72.9433   val Loss: 71.5766   time: 0.31s   best: 69.3122
2023-10-05 15:04:53,614:INFO:  Epoch 334/500:  train Loss: 73.6007   val Loss: 71.4434   time: 0.31s   best: 69.3122
2023-10-05 15:04:53,921:INFO:  Epoch 335/500:  train Loss: 72.6206   val Loss: 70.8006   time: 0.30s   best: 69.3122
2023-10-05 15:04:54,232:INFO:  Epoch 336/500:  train Loss: 71.4987   val Loss: 69.6680   time: 0.30s   best: 69.3122
2023-10-05 15:04:54,539:INFO:  Epoch 337/500:  train Loss: 71.9738   val Loss: 72.2769   time: 0.30s   best: 69.3122
2023-10-05 15:04:54,856:INFO:  Epoch 338/500:  train Loss: 71.3513   val Loss: 71.2092   time: 0.31s   best: 69.3122
2023-10-05 15:04:55,172:INFO:  Epoch 339/500:  train Loss: 72.3159   val Loss: 72.1631   time: 0.31s   best: 69.3122
2023-10-05 15:04:55,479:INFO:  Epoch 340/500:  train Loss: 71.7262   val Loss: 70.4676   time: 0.30s   best: 69.3122
2023-10-05 15:04:55,791:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:04:55,817:INFO:  Epoch 341/500:  train Loss: 70.4095   val Loss: 68.4077   time: 0.31s   best: 68.4077
2023-10-05 15:04:56,137:INFO:  Epoch 342/500:  train Loss: 71.2800   val Loss: 70.2578   time: 0.31s   best: 68.4077
2023-10-05 15:04:56,446:INFO:  Epoch 343/500:  train Loss: 70.8044   val Loss: 70.5106   time: 0.30s   best: 68.4077
2023-10-05 15:04:56,754:INFO:  Epoch 344/500:  train Loss: 71.2399   val Loss: 69.8744   time: 0.30s   best: 68.4077
2023-10-05 15:04:57,063:INFO:  Epoch 345/500:  train Loss: 72.0381   val Loss: 72.2236   time: 0.30s   best: 68.4077
2023-10-05 15:04:57,386:INFO:  Epoch 346/500:  train Loss: 71.5991   val Loss: 70.7621   time: 0.31s   best: 68.4077
2023-10-05 15:04:57,703:INFO:  Epoch 347/500:  train Loss: 73.6095   val Loss: 73.0611   time: 0.31s   best: 68.4077
2023-10-05 15:04:58,042:INFO:  Epoch 348/500:  train Loss: 73.9686   val Loss: 74.2542   time: 0.33s   best: 68.4077
2023-10-05 15:04:58,411:INFO:  Epoch 349/500:  train Loss: 74.8855   val Loss: 72.5623   time: 0.36s   best: 68.4077
2023-10-05 15:04:58,727:INFO:  Epoch 350/500:  train Loss: 73.0194   val Loss: 71.9693   time: 0.31s   best: 68.4077
2023-10-05 15:04:59,033:INFO:  Epoch 351/500:  train Loss: 87.5585   val Loss: 81.7764   time: 0.30s   best: 68.4077
2023-10-05 15:04:59,344:INFO:  Epoch 352/500:  train Loss: 94.3556   val Loss: 94.4475   time: 0.30s   best: 68.4077
2023-10-05 15:04:59,652:INFO:  Epoch 353/500:  train Loss: 97.1595   val Loss: 95.5765   time: 0.30s   best: 68.4077
2023-10-05 15:04:59,969:INFO:  Epoch 354/500:  train Loss: 97.0787   val Loss: 96.2870   time: 0.31s   best: 68.4077
2023-10-05 15:05:00,292:INFO:  Epoch 355/500:  train Loss: 96.3777   val Loss: 94.8997   time: 0.31s   best: 68.4077
2023-10-05 15:05:00,600:INFO:  Epoch 356/500:  train Loss: 95.5358   val Loss: 94.5857   time: 0.30s   best: 68.4077
2023-10-05 15:05:00,916:INFO:  Epoch 357/500:  train Loss: 94.4526   val Loss: 93.3915   time: 0.31s   best: 68.4077
2023-10-05 15:05:01,235:INFO:  Epoch 358/500:  train Loss: 96.0695   val Loss: 96.2257   time: 0.30s   best: 68.4077
2023-10-05 15:05:01,540:INFO:  Epoch 359/500:  train Loss: 95.8937   val Loss: 95.2429   time: 0.30s   best: 68.4077
2023-10-05 15:05:01,854:INFO:  Epoch 360/500:  train Loss: 95.1521   val Loss: 94.5142   time: 0.30s   best: 68.4077
2023-10-05 15:05:02,164:INFO:  Epoch 361/500:  train Loss: 94.7647   val Loss: 94.2233   time: 0.30s   best: 68.4077
2023-10-05 15:05:02,477:INFO:  Epoch 362/500:  train Loss: 94.9077   val Loss: 94.2517   time: 0.30s   best: 68.4077
2023-10-05 15:05:02,796:INFO:  Epoch 363/500:  train Loss: 94.2212   val Loss: 93.9335   time: 0.31s   best: 68.4077
2023-10-05 15:05:03,103:INFO:  Epoch 364/500:  train Loss: 94.6522   val Loss: 93.9250   time: 0.30s   best: 68.4077
2023-10-05 15:05:03,418:INFO:  Epoch 365/500:  train Loss: 94.5884   val Loss: 94.4254   time: 0.31s   best: 68.4077
2023-10-05 15:05:03,725:INFO:  Epoch 366/500:  train Loss: 94.1943   val Loss: 93.5475   time: 0.30s   best: 68.4077
2023-10-05 15:05:04,039:INFO:  Epoch 367/500:  train Loss: 94.3458   val Loss: 93.8326   time: 0.31s   best: 68.4077
2023-10-05 15:05:04,367:INFO:  Epoch 368/500:  train Loss: 93.8166   val Loss: 93.6043   time: 0.31s   best: 68.4077
2023-10-05 15:05:04,675:INFO:  Epoch 369/500:  train Loss: 93.6725   val Loss: 93.5061   time: 0.30s   best: 68.4077
2023-10-05 15:05:04,983:INFO:  Epoch 370/500:  train Loss: 94.2451   val Loss: 93.7668   time: 0.30s   best: 68.4077
2023-10-05 15:05:05,290:INFO:  Epoch 371/500:  train Loss: 93.7568   val Loss: 93.3253   time: 0.30s   best: 68.4077
2023-10-05 15:05:05,608:INFO:  Epoch 372/500:  train Loss: 93.4924   val Loss: 93.0511   time: 0.31s   best: 68.4077
2023-10-05 15:05:05,920:INFO:  Epoch 373/500:  train Loss: 93.4203   val Loss: 92.9364   time: 0.31s   best: 68.4077
2023-10-05 15:05:06,237:INFO:  Epoch 374/500:  train Loss: 93.4308   val Loss: 93.0203   time: 0.31s   best: 68.4077
2023-10-05 15:05:06,550:INFO:  Epoch 375/500:  train Loss: 93.4186   val Loss: 93.1277   time: 0.30s   best: 68.4077
2023-10-05 15:05:06,868:INFO:  Epoch 376/500:  train Loss: 93.2460   val Loss: 92.6294   time: 0.30s   best: 68.4077
2023-10-05 15:05:07,178:INFO:  Epoch 377/500:  train Loss: 92.7065   val Loss: 92.1172   time: 0.30s   best: 68.4077
2023-10-05 15:05:07,495:INFO:  Epoch 378/500:  train Loss: 92.0687   val Loss: 91.6851   time: 0.31s   best: 68.4077
2023-10-05 15:05:07,802:INFO:  Epoch 379/500:  train Loss: 91.8010   val Loss: 91.5112   time: 0.30s   best: 68.4077
2023-10-05 15:05:08,120:INFO:  Epoch 380/500:  train Loss: 91.5368   val Loss: 91.1793   time: 0.31s   best: 68.4077
2023-10-05 15:05:08,440:INFO:  Epoch 381/500:  train Loss: 91.0924   val Loss: 90.1390   time: 0.31s   best: 68.4077
2023-10-05 15:05:08,747:INFO:  Epoch 382/500:  train Loss: 90.0724   val Loss: 89.2299   time: 0.30s   best: 68.4077
2023-10-05 15:05:09,057:INFO:  Epoch 383/500:  train Loss: 88.9943   val Loss: 88.1595   time: 0.30s   best: 68.4077
2023-10-05 15:05:09,366:INFO:  Epoch 384/500:  train Loss: 87.7438   val Loss: 86.4685   time: 0.30s   best: 68.4077
2023-10-05 15:05:09,683:INFO:  Epoch 385/500:  train Loss: 85.5523   val Loss: 84.2690   time: 0.31s   best: 68.4077
2023-10-05 15:05:10,000:INFO:  Epoch 386/500:  train Loss: 85.4646   val Loss: 84.1201   time: 0.31s   best: 68.4077
2023-10-05 15:05:10,312:INFO:  Epoch 387/500:  train Loss: 84.8522   val Loss: 83.7748   time: 0.30s   best: 68.4077
2023-10-05 15:05:10,629:INFO:  Epoch 388/500:  train Loss: 83.6174   val Loss: 82.1723   time: 0.31s   best: 68.4077
2023-10-05 15:05:10,937:INFO:  Epoch 389/500:  train Loss: 82.9912   val Loss: 81.5297   time: 0.30s   best: 68.4077
2023-10-05 15:05:11,251:INFO:  Epoch 390/500:  train Loss: 82.0988   val Loss: 82.4207   time: 0.30s   best: 68.4077
2023-10-05 15:05:11,566:INFO:  Epoch 391/500:  train Loss: 82.0030   val Loss: 79.8634   time: 0.31s   best: 68.4077
2023-10-05 15:05:11,874:INFO:  Epoch 392/500:  train Loss: 81.4736   val Loss: 80.5944   time: 0.30s   best: 68.4077
2023-10-05 15:05:12,183:INFO:  Epoch 393/500:  train Loss: 81.1559   val Loss: 79.9683   time: 0.30s   best: 68.4077
2023-10-05 15:05:12,502:INFO:  Epoch 394/500:  train Loss: 79.6206   val Loss: 78.9325   time: 0.31s   best: 68.4077
2023-10-05 15:05:12,809:INFO:  Epoch 395/500:  train Loss: 80.0603   val Loss: 78.8385   time: 0.30s   best: 68.4077
2023-10-05 15:05:13,127:INFO:  Epoch 396/500:  train Loss: 79.4403   val Loss: 77.6256   time: 0.31s   best: 68.4077
2023-10-05 15:05:13,435:INFO:  Epoch 397/500:  train Loss: 77.5107   val Loss: 74.5461   time: 0.30s   best: 68.4077
2023-10-05 15:05:13,753:INFO:  Epoch 398/500:  train Loss: 77.3861   val Loss: 77.2335   time: 0.31s   best: 68.4077
2023-10-05 15:05:14,069:INFO:  Epoch 399/500:  train Loss: 78.4822   val Loss: 77.4945   time: 0.31s   best: 68.4077
2023-10-05 15:05:14,419:INFO:  Epoch 400/500:  train Loss: 76.7539   val Loss: 75.5069   time: 0.34s   best: 68.4077
2023-10-05 15:05:14,736:INFO:  Epoch 401/500:  train Loss: 76.3624   val Loss: 74.0867   time: 0.31s   best: 68.4077
2023-10-05 15:05:15,054:INFO:  Epoch 402/500:  train Loss: 74.8739   val Loss: 74.5366   time: 0.31s   best: 68.4077
2023-10-05 15:05:15,364:INFO:  Epoch 403/500:  train Loss: 75.4908   val Loss: 74.9669   time: 0.30s   best: 68.4077
2023-10-05 15:05:15,681:INFO:  Epoch 404/500:  train Loss: 74.8629   val Loss: 73.3531   time: 0.31s   best: 68.4077
2023-10-05 15:05:15,985:INFO:  Epoch 405/500:  train Loss: 74.5128   val Loss: 73.7560   time: 0.30s   best: 68.4077
2023-10-05 15:05:16,303:INFO:  Epoch 406/500:  train Loss: 74.6098   val Loss: 73.0554   time: 0.31s   best: 68.4077
2023-10-05 15:05:16,620:INFO:  Epoch 407/500:  train Loss: 78.7889   val Loss: 83.4358   time: 0.31s   best: 68.4077
2023-10-05 15:05:16,934:INFO:  Epoch 408/500:  train Loss: 82.5917   val Loss: 80.9050   time: 0.30s   best: 68.4077
2023-10-05 15:05:17,246:INFO:  Epoch 409/500:  train Loss: 78.9659   val Loss: 77.2344   time: 0.30s   best: 68.4077
2023-10-05 15:05:17,553:INFO:  Epoch 410/500:  train Loss: 76.4813   val Loss: 73.6910   time: 0.30s   best: 68.4077
2023-10-05 15:05:17,871:INFO:  Epoch 411/500:  train Loss: 76.4698   val Loss: 74.1232   time: 0.31s   best: 68.4077
2023-10-05 15:05:18,187:INFO:  Epoch 412/500:  train Loss: 73.9133   val Loss: 74.3860   time: 0.31s   best: 68.4077
2023-10-05 15:05:18,498:INFO:  Epoch 413/500:  train Loss: 74.6818   val Loss: 72.8502   time: 0.30s   best: 68.4077
2023-10-05 15:05:18,813:INFO:  Epoch 414/500:  train Loss: 72.2936   val Loss: 73.9260   time: 0.30s   best: 68.4077
2023-10-05 15:05:19,131:INFO:  Epoch 415/500:  train Loss: 74.6367   val Loss: 71.9674   time: 0.30s   best: 68.4077
2023-10-05 15:05:19,438:INFO:  Epoch 416/500:  train Loss: 75.3348   val Loss: 75.7321   time: 0.30s   best: 68.4077
2023-10-05 15:05:19,756:INFO:  Epoch 417/500:  train Loss: 77.6594   val Loss: 78.3885   time: 0.31s   best: 68.4077
2023-10-05 15:05:20,063:INFO:  Epoch 418/500:  train Loss: 77.8597   val Loss: 76.4216   time: 0.30s   best: 68.4077
2023-10-05 15:05:20,384:INFO:  Epoch 419/500:  train Loss: 75.9488   val Loss: 76.6646   time: 0.31s   best: 68.4077
2023-10-05 15:05:20,703:INFO:  Epoch 420/500:  train Loss: 77.3304   val Loss: 75.7698   time: 0.31s   best: 68.4077
2023-10-05 15:05:21,011:INFO:  Epoch 421/500:  train Loss: 73.5860   val Loss: 73.4560   time: 0.30s   best: 68.4077
2023-10-05 15:05:21,316:INFO:  Epoch 422/500:  train Loss: 74.7386   val Loss: 73.9623   time: 0.30s   best: 68.4077
2023-10-05 15:05:21,630:INFO:  Epoch 423/500:  train Loss: 74.0888   val Loss: 72.9732   time: 0.30s   best: 68.4077
2023-10-05 15:05:21,947:INFO:  Epoch 424/500:  train Loss: 73.3334   val Loss: 72.0850   time: 0.31s   best: 68.4077
2023-10-05 15:05:22,264:INFO:  Epoch 425/500:  train Loss: 72.8736   val Loss: 70.5791   time: 0.31s   best: 68.4077
2023-10-05 15:05:22,577:INFO:  Epoch 426/500:  train Loss: 71.8254   val Loss: 70.9533   time: 0.30s   best: 68.4077
2023-10-05 15:05:22,894:INFO:  Epoch 427/500:  train Loss: 72.5517   val Loss: 71.6973   time: 0.31s   best: 68.4077
2023-10-05 15:05:23,215:INFO:  Epoch 428/500:  train Loss: 71.5822   val Loss: 70.6040   time: 0.30s   best: 68.4077
2023-10-05 15:05:23,523:INFO:  Epoch 429/500:  train Loss: 71.2894   val Loss: 70.4240   time: 0.30s   best: 68.4077
2023-10-05 15:05:23,839:INFO:  Epoch 430/500:  train Loss: 70.5888   val Loss: 69.6405   time: 0.31s   best: 68.4077
2023-10-05 15:05:24,140:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:05:24,167:INFO:  Epoch 431/500:  train Loss: 69.9776   val Loss: 68.2735   time: 0.30s   best: 68.2735
2023-10-05 15:05:24,485:INFO:  Epoch 432/500:  train Loss: 71.3557   val Loss: 69.3987   time: 0.31s   best: 68.2735
2023-10-05 15:05:24,795:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (1 layer)_d009.pt
2023-10-05 15:05:24,821:INFO:  Epoch 433/500:  train Loss: 69.8043   val Loss: 66.6975   time: 0.31s   best: 66.6975
2023-10-05 15:05:25,129:INFO:  Epoch 434/500:  train Loss: 71.1936   val Loss: 69.4950   time: 0.30s   best: 66.6975
2023-10-05 15:05:25,449:INFO:  Epoch 435/500:  train Loss: 75.6261   val Loss: 78.9156   time: 0.31s   best: 66.6975
2023-10-05 15:05:25,755:INFO:  Epoch 436/500:  train Loss: 81.7752   val Loss: 81.2994   time: 0.30s   best: 66.6975
2023-10-05 15:05:26,074:INFO:  Epoch 437/500:  train Loss: 80.0242   val Loss: 79.1517   time: 0.31s   best: 66.6975
2023-10-05 15:05:26,395:INFO:  Epoch 438/500:  train Loss: 78.6139   val Loss: 76.1891   time: 0.31s   best: 66.6975
2023-10-05 15:05:26,703:INFO:  Epoch 439/500:  train Loss: 76.4136   val Loss: 74.2020   time: 0.30s   best: 66.6975
2023-10-05 15:05:27,021:INFO:  Epoch 440/500:  train Loss: 73.6524   val Loss: 72.1047   time: 0.31s   best: 66.6975
2023-10-05 15:05:27,341:INFO:  Epoch 441/500:  train Loss: 71.6887   val Loss: 71.4632   time: 0.31s   best: 66.6975
2023-10-05 15:05:27,648:INFO:  Epoch 442/500:  train Loss: 74.1034   val Loss: 75.3638   time: 0.30s   best: 66.6975
2023-10-05 15:05:27,956:INFO:  Epoch 443/500:  train Loss: 77.5565   val Loss: 80.9558   time: 0.30s   best: 66.6975
2023-10-05 15:05:28,264:INFO:  Epoch 444/500:  train Loss: 76.7660   val Loss: 74.7937   time: 0.30s   best: 66.6975
2023-10-05 15:05:28,660:INFO:  Epoch 445/500:  train Loss: 74.5554   val Loss: 72.3038   time: 0.39s   best: 66.6975
2023-10-05 15:05:28,976:INFO:  Epoch 446/500:  train Loss: 72.6757   val Loss: 71.5282   time: 0.31s   best: 66.6975
2023-10-05 15:05:29,287:INFO:  Epoch 447/500:  train Loss: 70.8884   val Loss: 69.7145   time: 0.30s   best: 66.6975
2023-10-05 15:05:29,606:INFO:  Epoch 448/500:  train Loss: 71.4719   val Loss: 72.3493   time: 0.31s   best: 66.6975
2023-10-05 15:05:29,925:INFO:  Epoch 449/500:  train Loss: 72.9101   val Loss: 72.7046   time: 0.31s   best: 66.6975
2023-10-05 15:05:30,234:INFO:  Epoch 450/500:  train Loss: 72.6501   val Loss: 70.8255   time: 0.30s   best: 66.6975
2023-10-05 15:05:30,557:INFO:  Epoch 451/500:  train Loss: 71.2072   val Loss: 70.5332   time: 0.31s   best: 66.6975
2023-10-05 15:05:30,864:INFO:  Epoch 452/500:  train Loss: 70.7580   val Loss: 69.3502   time: 0.30s   best: 66.6975
2023-10-05 15:05:31,174:INFO:  Epoch 453/500:  train Loss: 70.9595   val Loss: 68.8225   time: 0.30s   best: 66.6975
2023-10-05 15:05:31,488:INFO:  Epoch 454/500:  train Loss: 70.3247   val Loss: 69.4152   time: 0.31s   best: 66.6975
2023-10-05 15:05:31,802:INFO:  Epoch 455/500:  train Loss: 69.8585   val Loss: 69.5481   time: 0.30s   best: 66.6975
2023-10-05 15:05:32,110:INFO:  Epoch 456/500:  train Loss: 73.9249   val Loss: 75.1250   time: 0.30s   best: 66.6975
2023-10-05 15:05:32,428:INFO:  Epoch 457/500:  train Loss: 75.3542   val Loss: 75.4077   time: 0.31s   best: 66.6975
2023-10-05 15:05:32,740:INFO:  Epoch 458/500:  train Loss: 70.9717   val Loss: 69.1418   time: 0.30s   best: 66.6975
2023-10-05 15:05:33,056:INFO:  Epoch 459/500:  train Loss: 71.3897   val Loss: 71.3802   time: 0.31s   best: 66.6975
2023-10-05 15:05:33,366:INFO:  Epoch 460/500:  train Loss: 71.3149   val Loss: 70.2526   time: 0.30s   best: 66.6975
2023-10-05 15:05:33,685:INFO:  Epoch 461/500:  train Loss: 74.1566   val Loss: 75.2841   time: 0.31s   best: 66.6975
2023-10-05 15:05:34,001:INFO:  Epoch 462/500:  train Loss: 75.9460   val Loss: 74.8610   time: 0.31s   best: 66.6975
2023-10-05 15:05:34,310:INFO:  Epoch 463/500:  train Loss: 73.9432   val Loss: 72.0988   time: 0.30s   best: 66.6975
2023-10-05 15:05:34,626:INFO:  Epoch 464/500:  train Loss: 71.9570   val Loss: 69.7443   time: 0.31s   best: 66.6975
2023-10-05 15:05:34,934:INFO:  Epoch 465/500:  train Loss: 70.1230   val Loss: 69.3982   time: 0.30s   best: 66.6975
2023-10-05 15:05:35,256:INFO:  Epoch 466/500:  train Loss: 70.1549   val Loss: 69.7073   time: 0.31s   best: 66.6975
2023-10-05 15:05:35,573:INFO:  Epoch 467/500:  train Loss: 70.5635   val Loss: 69.2003   time: 0.31s   best: 66.6975
2023-10-05 15:05:35,875:INFO:  Epoch 468/500:  train Loss: 68.6842   val Loss: 67.4714   time: 0.30s   best: 66.6975
2023-10-05 15:05:36,185:INFO:  Epoch 469/500:  train Loss: 68.9934   val Loss: 69.2073   time: 0.30s   best: 66.6975
2023-10-05 15:05:36,518:INFO:  Epoch 470/500:  train Loss: 70.0115   val Loss: 68.0699   time: 0.31s   best: 66.6975
2023-10-05 15:05:36,822:INFO:  Epoch 471/500:  train Loss: 70.7382   val Loss: 70.9708   time: 0.30s   best: 66.6975
2023-10-05 15:05:37,146:INFO:  Epoch 472/500:  train Loss: 71.5238   val Loss: 71.9467   time: 0.31s   best: 66.6975
2023-10-05 15:05:37,456:INFO:  Epoch 473/500:  train Loss: 72.5553   val Loss: 75.9294   time: 0.30s   best: 66.6975
2023-10-05 15:05:37,772:INFO:  Epoch 474/500:  train Loss: 74.2625   val Loss: 73.5267   time: 0.31s   best: 66.6975
2023-10-05 15:05:38,091:INFO:  Epoch 475/500:  train Loss: 71.9655   val Loss: 70.7789   time: 0.31s   best: 66.6975
2023-10-05 15:05:38,399:INFO:  Epoch 476/500:  train Loss: 69.6628   val Loss: 68.9583   time: 0.30s   best: 66.6975
2023-10-05 15:05:38,731:INFO:  Epoch 477/500:  train Loss: 70.4245   val Loss: 69.6412   time: 0.32s   best: 66.6975
2023-10-05 15:05:39,039:INFO:  Epoch 478/500:  train Loss: 76.1144   val Loss: 67.6041   time: 0.30s   best: 66.6975
2023-10-05 15:05:39,361:INFO:  Epoch 479/500:  train Loss: 80.8537   val Loss: 88.7688   time: 0.31s   best: 66.6975
2023-10-05 15:05:39,678:INFO:  Epoch 480/500:  train Loss: 90.5997   val Loss: 91.1071   time: 0.31s   best: 66.6975
2023-10-05 15:05:39,985:INFO:  Epoch 481/500:  train Loss: 92.3440   val Loss: 91.8687   time: 0.30s   best: 66.6975
2023-10-05 15:05:40,303:INFO:  Epoch 482/500:  train Loss: 92.4846   val Loss: 91.8646   time: 0.31s   best: 66.6975
2023-10-05 15:05:40,626:INFO:  Epoch 483/500:  train Loss: 91.9486   val Loss: 91.0129   time: 0.31s   best: 66.6975
2023-10-05 15:05:40,933:INFO:  Epoch 484/500:  train Loss: 91.0207   val Loss: 90.0361   time: 0.30s   best: 66.6975
2023-10-05 15:05:41,251:INFO:  Epoch 485/500:  train Loss: 90.1135   val Loss: 89.0897   time: 0.31s   best: 66.6975
2023-10-05 15:05:41,558:INFO:  Epoch 486/500:  train Loss: 88.7796   val Loss: 87.5918   time: 0.30s   best: 66.6975
2023-10-05 15:05:41,876:INFO:  Epoch 487/500:  train Loss: 87.0455   val Loss: 85.8374   time: 0.31s   best: 66.6975
2023-10-05 15:05:42,192:INFO:  Epoch 488/500:  train Loss: 85.1952   val Loss: 83.7015   time: 0.31s   best: 66.6975
2023-10-05 15:05:42,501:INFO:  Epoch 489/500:  train Loss: 83.5793   val Loss: 82.0335   time: 0.30s   best: 66.6975
2023-10-05 15:05:42,821:INFO:  Epoch 490/500:  train Loss: 80.8979   val Loss: 79.7112   time: 0.31s   best: 66.6975
2023-10-05 15:05:43,129:INFO:  Epoch 491/500:  train Loss: 78.9584   val Loss: 74.5978   time: 0.30s   best: 66.6975
2023-10-05 15:05:43,449:INFO:  Epoch 492/500:  train Loss: 77.8527   val Loss: 77.0040   time: 0.31s   best: 66.6975
2023-10-05 15:05:43,766:INFO:  Epoch 493/500:  train Loss: 77.3629   val Loss: 76.3185   time: 0.31s   best: 66.6975
2023-10-05 15:05:44,074:INFO:  Epoch 494/500:  train Loss: 76.2194   val Loss: 75.1127   time: 0.30s   best: 66.6975
2023-10-05 15:05:44,394:INFO:  Epoch 495/500:  train Loss: 74.7254   val Loss: 73.6329   time: 0.31s   best: 66.6975
2023-10-05 15:05:44,714:INFO:  Epoch 496/500:  train Loss: 73.4446   val Loss: 72.9955   time: 0.30s   best: 66.6975
2023-10-05 15:05:45,016:INFO:  Epoch 497/500:  train Loss: 73.0761   val Loss: 71.3089   time: 0.30s   best: 66.6975
2023-10-05 15:05:45,333:INFO:  Epoch 498/500:  train Loss: 69.8837   val Loss: 68.2260   time: 0.31s   best: 66.6975
2023-10-05 15:05:45,641:INFO:  Epoch 499/500:  train Loss: 70.6241   val Loss: 70.1611   time: 0.30s   best: 66.6975
2023-10-05 15:05:45,989:INFO:  Epoch 500/500:  train Loss: 70.9560   val Loss: 69.9905   time: 0.34s   best: 66.6975
2023-10-05 15:05:45,990:INFO:  -----> Training complete in 2m 45s   best validation loss: 66.6975
 
2023-10-06 08:54:07,816:INFO:  Starting experiment lstm autoencoder (+ hidden vector)
2023-10-06 08:54:07,833:INFO:  Defining the model
2023-10-06 08:54:08,535:INFO:  Reading the dataset
2023-10-06 09:44:26,826:INFO:  Starting experiment lstm autoencoder (+ hidden vector)
2023-10-06 09:44:26,843:INFO:  Defining the model
2023-10-06 09:44:27,211:INFO:  Reading the dataset
2023-10-06 10:30:33,736:INFO:  Starting experiment lstm autoencoder (+ hidden vector)
2023-10-06 10:30:33,748:INFO:  Defining the model
2023-10-06 10:30:34,340:INFO:  Reading the dataset
2023-10-06 10:30:57,009:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:30:57,108:INFO:  Epoch 1/500:  train Loss: 107.1186   val Loss: 98.0850   time: 6.97s   best: 98.0850
2023-10-06 10:30:57,348:INFO:  Epoch 2/500:  train Loss: 99.1265   val Loss: 100.1165   time: 0.24s   best: 98.0850
2023-10-06 10:30:57,583:INFO:  Epoch 3/500:  train Loss: 99.9731   val Loss: 100.2381   time: 0.23s   best: 98.0850
2023-10-06 10:30:57,821:INFO:  Epoch 4/500:  train Loss: 100.2189   val Loss: 100.2900   time: 0.24s   best: 98.0850
2023-10-06 10:30:58,057:INFO:  Epoch 5/500:  train Loss: 100.2778   val Loss: 100.3125   time: 0.24s   best: 98.0850
2023-10-06 10:30:58,300:INFO:  Epoch 6/500:  train Loss: 100.3029   val Loss: 100.3168   time: 0.24s   best: 98.0850
2023-10-06 10:30:58,573:INFO:  Epoch 7/500:  train Loss: 100.2688   val Loss: 100.3108   time: 0.24s   best: 98.0850
2023-10-06 10:30:58,813:INFO:  Epoch 8/500:  train Loss: 100.2896   val Loss: 100.2975   time: 0.24s   best: 98.0850
2023-10-06 10:30:59,047:INFO:  Epoch 9/500:  train Loss: 100.2450   val Loss: 100.2798   time: 0.23s   best: 98.0850
2023-10-06 10:30:59,283:INFO:  Epoch 10/500:  train Loss: 100.1698   val Loss: 100.2461   time: 0.23s   best: 98.0850
2023-10-06 10:30:59,535:INFO:  Epoch 11/500:  train Loss: 100.1419   val Loss: 100.2014   time: 0.25s   best: 98.0850
2023-10-06 10:30:59,772:INFO:  Epoch 12/500:  train Loss: 99.8615   val Loss: 100.0726   time: 0.24s   best: 98.0850
2023-10-06 10:31:00,006:INFO:  Epoch 13/500:  train Loss: 98.9496   val Loss: 98.7630   time: 0.23s   best: 98.0850
2023-10-06 10:31:00,253:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:00,272:INFO:  Epoch 14/500:  train Loss: 96.5172   val Loss: 96.1372   time: 0.24s   best: 96.1372
2023-10-06 10:31:00,514:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:00,532:INFO:  Epoch 15/500:  train Loss: 95.1248   val Loss: 94.3717   time: 0.24s   best: 94.3717
2023-10-06 10:31:00,793:INFO:  Epoch 16/500:  train Loss: 94.7508   val Loss: 94.6037   time: 0.26s   best: 94.3717
2023-10-06 10:31:01,031:INFO:  Epoch 17/500:  train Loss: 94.7764   val Loss: 94.5979   time: 0.24s   best: 94.3717
2023-10-06 10:31:01,269:INFO:  Epoch 18/500:  train Loss: 94.6399   val Loss: 94.7042   time: 0.24s   best: 94.3717
2023-10-06 10:31:01,503:INFO:  Epoch 19/500:  train Loss: 94.3637   val Loss: 94.4712   time: 0.23s   best: 94.3717
2023-10-06 10:31:01,742:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:01,761:INFO:  Epoch 20/500:  train Loss: 94.6488   val Loss: 94.3314   time: 0.23s   best: 94.3314
2023-10-06 10:31:02,000:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:02,019:INFO:  Epoch 21/500:  train Loss: 94.5071   val Loss: 94.2600   time: 0.24s   best: 94.2600
2023-10-06 10:31:02,264:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:02,282:INFO:  Epoch 22/500:  train Loss: 94.2117   val Loss: 94.2241   time: 0.24s   best: 94.2241
2023-10-06 10:31:02,520:INFO:  Epoch 23/500:  train Loss: 94.8909   val Loss: 94.3262   time: 0.24s   best: 94.2241
2023-10-06 10:31:02,757:INFO:  Epoch 24/500:  train Loss: 94.5001   val Loss: 94.3931   time: 0.24s   best: 94.2241
2023-10-06 10:31:02,991:INFO:  Epoch 25/500:  train Loss: 94.6767   val Loss: 94.4081   time: 0.23s   best: 94.2241
2023-10-06 10:31:03,225:INFO:  Epoch 26/500:  train Loss: 94.4723   val Loss: 94.4484   time: 0.23s   best: 94.2241
2023-10-06 10:31:03,463:INFO:  Epoch 27/500:  train Loss: 94.7944   val Loss: 94.4981   time: 0.24s   best: 94.2241
2023-10-06 10:31:03,700:INFO:  Epoch 28/500:  train Loss: 94.8151   val Loss: 94.3590   time: 0.24s   best: 94.2241
2023-10-06 10:31:03,938:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:03,956:INFO:  Epoch 29/500:  train Loss: 94.0414   val Loss: 94.0755   time: 0.23s   best: 94.0755
2023-10-06 10:31:04,194:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:04,212:INFO:  Epoch 30/500:  train Loss: 94.0683   val Loss: 93.8697   time: 0.23s   best: 93.8697
2023-10-06 10:31:04,450:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:04,468:INFO:  Epoch 31/500:  train Loss: 94.3552   val Loss: 93.7669   time: 0.23s   best: 93.7669
2023-10-06 10:31:04,707:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:04,725:INFO:  Epoch 32/500:  train Loss: 93.8175   val Loss: 93.7415   time: 0.23s   best: 93.7415
2023-10-06 10:31:04,963:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:04,982:INFO:  Epoch 33/500:  train Loss: 93.9039   val Loss: 93.6386   time: 0.23s   best: 93.6386
2023-10-06 10:31:05,224:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:05,244:INFO:  Epoch 34/500:  train Loss: 93.6757   val Loss: 93.5251   time: 0.24s   best: 93.5251
2023-10-06 10:31:05,483:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:05,502:INFO:  Epoch 35/500:  train Loss: 93.5482   val Loss: 93.3432   time: 0.24s   best: 93.3432
2023-10-06 10:31:05,745:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:05,764:INFO:  Epoch 36/500:  train Loss: 93.2659   val Loss: 93.0718   time: 0.24s   best: 93.0718
2023-10-06 10:31:06,004:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:06,022:INFO:  Epoch 37/500:  train Loss: 93.5524   val Loss: 92.8275   time: 0.24s   best: 92.8275
2023-10-06 10:31:06,262:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:06,280:INFO:  Epoch 38/500:  train Loss: 92.9120   val Loss: 92.4409   time: 0.24s   best: 92.4409
2023-10-06 10:31:06,518:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:06,538:INFO:  Epoch 39/500:  train Loss: 92.6741   val Loss: 92.1659   time: 0.23s   best: 92.1659
2023-10-06 10:31:06,777:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:06,796:INFO:  Epoch 40/500:  train Loss: 92.5469   val Loss: 91.6793   time: 0.23s   best: 91.6793
2023-10-06 10:31:07,036:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:07,054:INFO:  Epoch 41/500:  train Loss: 91.6321   val Loss: 91.1217   time: 0.24s   best: 91.1217
2023-10-06 10:31:07,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:07,313:INFO:  Epoch 42/500:  train Loss: 90.7248   val Loss: 90.3262   time: 0.24s   best: 90.3262
2023-10-06 10:31:07,552:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:07,571:INFO:  Epoch 43/500:  train Loss: 90.2729   val Loss: 89.4967   time: 0.23s   best: 89.4967
2023-10-06 10:31:07,811:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:07,829:INFO:  Epoch 44/500:  train Loss: 89.4839   val Loss: 89.2010   time: 0.24s   best: 89.2010
2023-10-06 10:31:08,068:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:08,086:INFO:  Epoch 45/500:  train Loss: 89.0023   val Loss: 88.6217   time: 0.23s   best: 88.6217
2023-10-06 10:31:08,327:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:08,345:INFO:  Epoch 46/500:  train Loss: 88.9123   val Loss: 88.4473   time: 0.24s   best: 88.4473
2023-10-06 10:31:08,590:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:08,608:INFO:  Epoch 47/500:  train Loss: 88.3865   val Loss: 88.0992   time: 0.24s   best: 88.0992
2023-10-06 10:31:08,849:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:08,868:INFO:  Epoch 48/500:  train Loss: 88.2622   val Loss: 87.4981   time: 0.24s   best: 87.4981
2023-10-06 10:31:09,109:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:09,127:INFO:  Epoch 49/500:  train Loss: 87.2933   val Loss: 87.2859   time: 0.24s   best: 87.2859
2023-10-06 10:31:09,364:INFO:  Epoch 50/500:  train Loss: 87.9833   val Loss: 88.2367   time: 0.24s   best: 87.2859
2023-10-06 10:31:09,601:INFO:  Epoch 51/500:  train Loss: 87.7850   val Loss: 88.2563   time: 0.24s   best: 87.2859
2023-10-06 10:31:09,840:INFO:  Epoch 52/500:  train Loss: 89.5044   val Loss: 88.5777   time: 0.24s   best: 87.2859
2023-10-06 10:31:10,080:INFO:  Epoch 53/500:  train Loss: 89.8456   val Loss: 90.4982   time: 0.24s   best: 87.2859
2023-10-06 10:31:10,319:INFO:  Epoch 54/500:  train Loss: 90.2254   val Loss: 89.4886   time: 0.24s   best: 87.2859
2023-10-06 10:31:10,557:INFO:  Epoch 55/500:  train Loss: 89.1438   val Loss: 88.1044   time: 0.24s   best: 87.2859
2023-10-06 10:31:10,791:INFO:  Epoch 56/500:  train Loss: 88.0992   val Loss: 88.0207   time: 0.23s   best: 87.2859
2023-10-06 10:31:11,026:INFO:  Epoch 57/500:  train Loss: 87.8106   val Loss: 87.6742   time: 0.23s   best: 87.2859
2023-10-06 10:31:11,263:INFO:  Epoch 58/500:  train Loss: 88.2164   val Loss: 87.9864   time: 0.24s   best: 87.2859
2023-10-06 10:31:11,506:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:11,598:INFO:  Epoch 59/500:  train Loss: 87.7501   val Loss: 86.6681   time: 0.24s   best: 86.6681
2023-10-06 10:31:11,838:INFO:  Epoch 60/500:  train Loss: 86.6589   val Loss: 87.1552   time: 0.24s   best: 86.6681
2023-10-06 10:31:12,073:INFO:  Epoch 61/500:  train Loss: 87.5786   val Loss: 87.0370   time: 0.23s   best: 86.6681
2023-10-06 10:31:12,311:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:12,329:INFO:  Epoch 62/500:  train Loss: 87.3415   val Loss: 86.2335   time: 0.23s   best: 86.2335
2023-10-06 10:31:12,571:INFO:  Epoch 63/500:  train Loss: 87.1269   val Loss: 87.4190   time: 0.24s   best: 86.2335
2023-10-06 10:31:12,804:INFO:  Epoch 64/500:  train Loss: 88.0537   val Loss: 87.9192   time: 0.23s   best: 86.2335
2023-10-06 10:31:13,042:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:13,060:INFO:  Epoch 65/500:  train Loss: 87.7971   val Loss: 85.9398   time: 0.23s   best: 85.9398
2023-10-06 10:31:13,297:INFO:  Epoch 66/500:  train Loss: 87.5504   val Loss: 86.5714   time: 0.24s   best: 85.9398
2023-10-06 10:31:13,538:INFO:  Epoch 67/500:  train Loss: 86.9724   val Loss: 86.4445   time: 0.24s   best: 85.9398
2023-10-06 10:31:13,771:INFO:  Epoch 68/500:  train Loss: 86.9022   val Loss: 86.1636   time: 0.23s   best: 85.9398
2023-10-06 10:31:14,027:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:14,046:INFO:  Epoch 69/500:  train Loss: 86.5365   val Loss: 85.9106   time: 0.25s   best: 85.9106
2023-10-06 10:31:14,286:INFO:  Epoch 70/500:  train Loss: 86.0078   val Loss: 85.9140   time: 0.24s   best: 85.9106
2023-10-06 10:31:14,522:INFO:  Epoch 71/500:  train Loss: 86.2308   val Loss: 85.9649   time: 0.24s   best: 85.9106
2023-10-06 10:31:14,774:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:14,793:INFO:  Epoch 72/500:  train Loss: 86.2292   val Loss: 85.1466   time: 0.25s   best: 85.1466
2023-10-06 10:31:15,055:INFO:  Epoch 73/500:  train Loss: 86.1158   val Loss: 85.1844   time: 0.26s   best: 85.1466
2023-10-06 10:31:15,293:INFO:  Epoch 74/500:  train Loss: 85.5967   val Loss: 85.4185   time: 0.24s   best: 85.1466
2023-10-06 10:31:15,539:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:16,383:INFO:  Epoch 75/500:  train Loss: 85.6952   val Loss: 84.8361   time: 0.24s   best: 84.8361
2023-10-06 10:31:16,627:INFO:  Epoch 76/500:  train Loss: 85.5754   val Loss: 85.7773   time: 0.24s   best: 84.8361
2023-10-06 10:31:16,876:INFO:  Epoch 77/500:  train Loss: 86.3676   val Loss: 84.9303   time: 0.25s   best: 84.8361
2023-10-06 10:31:17,121:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:17,218:INFO:  Epoch 78/500:  train Loss: 85.6163   val Loss: 84.4834   time: 0.24s   best: 84.4834
2023-10-06 10:31:17,464:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:17,482:INFO:  Epoch 79/500:  train Loss: 84.5386   val Loss: 84.4537   time: 0.24s   best: 84.4537
2023-10-06 10:31:17,723:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:17,741:INFO:  Epoch 80/500:  train Loss: 85.1113   val Loss: 84.3095   time: 0.24s   best: 84.3095
2023-10-06 10:31:17,980:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:17,999:INFO:  Epoch 81/500:  train Loss: 84.0145   val Loss: 84.0196   time: 0.23s   best: 84.0196
2023-10-06 10:31:18,237:INFO:  Epoch 82/500:  train Loss: 84.6230   val Loss: 84.2405   time: 0.24s   best: 84.0196
2023-10-06 10:31:18,476:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:18,494:INFO:  Epoch 83/500:  train Loss: 85.1854   val Loss: 83.5007   time: 0.23s   best: 83.5007
2023-10-06 10:31:18,741:INFO:  Epoch 84/500:  train Loss: 84.2959   val Loss: 84.2732   time: 0.25s   best: 83.5007
2023-10-06 10:31:18,980:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:18,998:INFO:  Epoch 85/500:  train Loss: 84.5523   val Loss: 83.4614   time: 0.23s   best: 83.4614
2023-10-06 10:31:19,242:INFO:  Epoch 86/500:  train Loss: 83.9723   val Loss: 83.6990   time: 0.24s   best: 83.4614
2023-10-06 10:31:19,482:INFO:  Epoch 87/500:  train Loss: 86.1261   val Loss: 88.0559   time: 0.24s   best: 83.4614
2023-10-06 10:31:19,733:INFO:  Epoch 88/500:  train Loss: 88.1157   val Loss: 86.7158   time: 0.25s   best: 83.4614
2023-10-06 10:31:19,973:INFO:  Epoch 89/500:  train Loss: 87.3434   val Loss: 85.8847   time: 0.24s   best: 83.4614
2023-10-06 10:31:20,214:INFO:  Epoch 90/500:  train Loss: 86.4849   val Loss: 85.7999   time: 0.24s   best: 83.4614
2023-10-06 10:31:20,455:INFO:  Epoch 91/500:  train Loss: 84.9474   val Loss: 83.6754   time: 0.24s   best: 83.4614
2023-10-06 10:31:20,697:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:20,717:INFO:  Epoch 92/500:  train Loss: 84.2592   val Loss: 83.3547   time: 0.24s   best: 83.3547
2023-10-06 10:31:20,957:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:20,975:INFO:  Epoch 93/500:  train Loss: 83.6785   val Loss: 82.9158   time: 0.24s   best: 82.9158
2023-10-06 10:31:21,217:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:21,252:INFO:  Epoch 94/500:  train Loss: 83.7503   val Loss: 82.6695   time: 0.24s   best: 82.6695
2023-10-06 10:31:21,492:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:21,511:INFO:  Epoch 95/500:  train Loss: 83.1526   val Loss: 82.3743   time: 0.24s   best: 82.3743
2023-10-06 10:31:21,748:INFO:  Epoch 96/500:  train Loss: 82.7754   val Loss: 82.3811   time: 0.24s   best: 82.3743
2023-10-06 10:31:21,982:INFO:  Epoch 97/500:  train Loss: 82.6076   val Loss: 82.5564   time: 0.23s   best: 82.3743
2023-10-06 10:31:22,217:INFO:  Epoch 98/500:  train Loss: 84.2240   val Loss: 84.5429   time: 0.23s   best: 82.3743
2023-10-06 10:31:22,458:INFO:  Epoch 99/500:  train Loss: 86.3941   val Loss: 88.0644   time: 0.24s   best: 82.3743
2023-10-06 10:31:23,391:INFO:  Epoch 100/500:  train Loss: 88.7338   val Loss: 87.1974   time: 0.93s   best: 82.3743
2023-10-06 10:31:23,634:INFO:  Epoch 101/500:  train Loss: 86.1150   val Loss: 83.6941   time: 0.24s   best: 82.3743
2023-10-06 10:31:23,875:INFO:  Epoch 102/500:  train Loss: 84.8351   val Loss: 84.2493   time: 0.24s   best: 82.3743
2023-10-06 10:31:24,109:INFO:  Epoch 103/500:  train Loss: 83.5261   val Loss: 83.1302   time: 0.23s   best: 82.3743
2023-10-06 10:31:24,356:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:24,444:INFO:  Epoch 104/500:  train Loss: 83.0710   val Loss: 82.0816   time: 0.24s   best: 82.0816
2023-10-06 10:31:24,688:INFO:  Epoch 105/500:  train Loss: 83.3482   val Loss: 82.6594   time: 0.24s   best: 82.0816
2023-10-06 10:31:24,924:INFO:  Epoch 106/500:  train Loss: 83.4657   val Loss: 82.8848   time: 0.23s   best: 82.0816
2023-10-06 10:31:25,157:INFO:  Epoch 107/500:  train Loss: 83.2290   val Loss: 83.3091   time: 0.23s   best: 82.0816
2023-10-06 10:31:25,397:INFO:  Epoch 108/500:  train Loss: 83.7237   val Loss: 83.0846   time: 0.24s   best: 82.0816
2023-10-06 10:31:25,631:INFO:  Epoch 109/500:  train Loss: 83.2172   val Loss: 82.5938   time: 0.23s   best: 82.0816
2023-10-06 10:31:25,865:INFO:  Epoch 110/500:  train Loss: 82.5851   val Loss: 83.1672   time: 0.23s   best: 82.0816
2023-10-06 10:31:26,173:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:26,191:INFO:  Epoch 111/500:  train Loss: 83.3371   val Loss: 81.7823   time: 0.23s   best: 81.7823
2023-10-06 10:31:26,427:INFO:  Epoch 112/500:  train Loss: 82.9897   val Loss: 82.2145   time: 0.23s   best: 81.7823
2023-10-06 10:31:26,676:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:26,700:INFO:  Epoch 113/500:  train Loss: 82.0709   val Loss: 81.2425   time: 0.24s   best: 81.2425
2023-10-06 10:31:26,938:INFO:  Epoch 114/500:  train Loss: 82.1384   val Loss: 81.7762   time: 0.24s   best: 81.2425
2023-10-06 10:31:27,172:INFO:  Epoch 115/500:  train Loss: 82.9024   val Loss: 82.2403   time: 0.23s   best: 81.2425
2023-10-06 10:31:27,407:INFO:  Epoch 116/500:  train Loss: 82.5954   val Loss: 82.3481   time: 0.23s   best: 81.2425
2023-10-06 10:31:27,644:INFO:  Epoch 117/500:  train Loss: 82.8560   val Loss: 81.6994   time: 0.24s   best: 81.2425
2023-10-06 10:31:27,884:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:27,902:INFO:  Epoch 118/500:  train Loss: 81.9953   val Loss: 81.0729   time: 0.24s   best: 81.0729
2023-10-06 10:31:28,141:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:28,160:INFO:  Epoch 119/500:  train Loss: 81.9728   val Loss: 80.6437   time: 0.23s   best: 80.6437
2023-10-06 10:31:28,394:INFO:  Epoch 120/500:  train Loss: 82.7384   val Loss: 81.0108   time: 0.23s   best: 80.6437
2023-10-06 10:31:28,630:INFO:  Epoch 121/500:  train Loss: 85.9579   val Loss: 89.4634   time: 0.23s   best: 80.6437
2023-10-06 10:31:28,866:INFO:  Epoch 122/500:  train Loss: 88.8740   val Loss: 87.2446   time: 0.23s   best: 80.6437
2023-10-06 10:31:29,099:INFO:  Epoch 123/500:  train Loss: 87.1678   val Loss: 86.2959   time: 0.23s   best: 80.6437
2023-10-06 10:31:29,510:INFO:  Epoch 124/500:  train Loss: 86.9305   val Loss: 86.5092   time: 0.24s   best: 80.6437
2023-10-06 10:31:29,751:INFO:  Epoch 125/500:  train Loss: 85.5808   val Loss: 84.2462   time: 0.24s   best: 80.6437
2023-10-06 10:31:29,992:INFO:  Epoch 126/500:  train Loss: 83.4807   val Loss: 82.5111   time: 0.24s   best: 80.6437
2023-10-06 10:31:30,230:INFO:  Epoch 127/500:  train Loss: 82.7419   val Loss: 82.0179   time: 0.24s   best: 80.6437
2023-10-06 10:31:30,481:INFO:  Epoch 128/500:  train Loss: 82.1379   val Loss: 81.0571   time: 0.25s   best: 80.6437
2023-10-06 10:31:30,721:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:31,639:INFO:  Epoch 129/500:  train Loss: 81.4089   val Loss: 80.1870   time: 0.24s   best: 80.1870
2023-10-06 10:31:31,890:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:31,996:INFO:  Epoch 130/500:  train Loss: 80.6029   val Loss: 80.0369   time: 0.25s   best: 80.0369
2023-10-06 10:31:32,278:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:32,297:INFO:  Epoch 131/500:  train Loss: 80.8574   val Loss: 79.9704   time: 0.28s   best: 79.9704
2023-10-06 10:31:32,541:INFO:  Epoch 132/500:  train Loss: 80.6774   val Loss: 79.9874   time: 0.24s   best: 79.9704
2023-10-06 10:31:32,796:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:32,815:INFO:  Epoch 133/500:  train Loss: 80.4367   val Loss: 79.9459   time: 0.25s   best: 79.9459
2023-10-06 10:31:33,066:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:33,085:INFO:  Epoch 134/500:  train Loss: 80.5667   val Loss: 79.1826   time: 0.25s   best: 79.1826
2023-10-06 10:31:33,320:INFO:  Epoch 135/500:  train Loss: 80.2063   val Loss: 79.3382   time: 0.23s   best: 79.1826
2023-10-06 10:31:33,558:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:33,576:INFO:  Epoch 136/500:  train Loss: 80.3457   val Loss: 79.1577   time: 0.23s   best: 79.1577
2023-10-06 10:31:33,813:INFO:  Epoch 137/500:  train Loss: 80.1513   val Loss: 79.7922   time: 0.24s   best: 79.1577
2023-10-06 10:31:34,046:INFO:  Epoch 138/500:  train Loss: 79.8420   val Loss: 79.4254   time: 0.23s   best: 79.1577
2023-10-06 10:31:34,286:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:34,711:INFO:  Epoch 139/500:  train Loss: 79.6614   val Loss: 79.0423   time: 0.23s   best: 79.0423
2023-10-06 10:31:34,947:INFO:  Epoch 140/500:  train Loss: 80.0751   val Loss: 79.9653   time: 0.23s   best: 79.0423
2023-10-06 10:31:35,187:INFO:  Epoch 141/500:  train Loss: 79.8274   val Loss: 79.0833   time: 0.24s   best: 79.0423
2023-10-06 10:31:35,434:INFO:  Epoch 142/500:  train Loss: 80.8698   val Loss: 79.4880   time: 0.25s   best: 79.0423
2023-10-06 10:31:35,684:INFO:  Epoch 143/500:  train Loss: 80.4423   val Loss: 79.5340   time: 0.25s   best: 79.0423
2023-10-06 10:31:35,933:INFO:  Epoch 144/500:  train Loss: 80.1595   val Loss: 80.1044   time: 0.25s   best: 79.0423
2023-10-06 10:31:36,169:INFO:  Epoch 145/500:  train Loss: 80.6904   val Loss: 80.7068   time: 0.23s   best: 79.0423
2023-10-06 10:31:36,422:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:36,446:INFO:  Epoch 146/500:  train Loss: 80.3844   val Loss: 79.0256   time: 0.25s   best: 79.0256
2023-10-06 10:31:36,685:INFO:  Epoch 147/500:  train Loss: 80.3464   val Loss: 79.2187   time: 0.24s   best: 79.0256
2023-10-06 10:31:36,936:INFO:  Epoch 148/500:  train Loss: 79.9961   val Loss: 79.2941   time: 0.25s   best: 79.0256
2023-10-06 10:31:37,177:INFO:  Epoch 149/500:  train Loss: 79.8566   val Loss: 79.2167   time: 0.24s   best: 79.0256
2023-10-06 10:31:37,425:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:37,445:INFO:  Epoch 150/500:  train Loss: 79.8929   val Loss: 78.8569   time: 0.24s   best: 78.8569
2023-10-06 10:31:37,693:INFO:  Epoch 151/500:  train Loss: 80.9187   val Loss: 79.5668   time: 0.25s   best: 78.8569
2023-10-06 10:31:37,938:INFO:  Epoch 152/500:  train Loss: 80.8283   val Loss: 78.8963   time: 0.24s   best: 78.8569
2023-10-06 10:31:38,174:INFO:  Epoch 153/500:  train Loss: 81.8399   val Loss: 83.3174   time: 0.24s   best: 78.8569
2023-10-06 10:31:38,413:INFO:  Epoch 154/500:  train Loss: 83.7224   val Loss: 83.7773   time: 0.24s   best: 78.8569
2023-10-06 10:31:38,650:INFO:  Epoch 155/500:  train Loss: 84.1694   val Loss: 83.9681   time: 0.24s   best: 78.8569
2023-10-06 10:31:38,899:INFO:  Epoch 156/500:  train Loss: 82.7400   val Loss: 81.1712   time: 0.25s   best: 78.8569
2023-10-06 10:31:39,154:INFO:  Epoch 157/500:  train Loss: 80.7246   val Loss: 79.5077   time: 0.25s   best: 78.8569
2023-10-06 10:31:39,394:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:40,527:INFO:  Epoch 158/500:  train Loss: 79.9694   val Loss: 78.5706   time: 0.23s   best: 78.5706
2023-10-06 10:31:40,766:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:40,892:INFO:  Epoch 159/500:  train Loss: 79.2780   val Loss: 77.7870   time: 0.24s   best: 77.7870
2023-10-06 10:31:41,155:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:41,175:INFO:  Epoch 160/500:  train Loss: 78.7318   val Loss: 77.5902   time: 0.26s   best: 77.5902
2023-10-06 10:31:41,414:INFO:  Epoch 161/500:  train Loss: 78.4410   val Loss: 77.7765   time: 0.24s   best: 77.5902
2023-10-06 10:31:41,672:INFO:  Epoch 162/500:  train Loss: 79.8146   val Loss: 78.2896   time: 0.25s   best: 77.5902
2023-10-06 10:31:41,914:INFO:  Epoch 163/500:  train Loss: 78.9535   val Loss: 81.7640   time: 0.24s   best: 77.5902
2023-10-06 10:31:42,155:INFO:  Epoch 164/500:  train Loss: 80.5406   val Loss: 79.5579   time: 0.24s   best: 77.5902
2023-10-06 10:31:42,396:INFO:  Epoch 165/500:  train Loss: 79.8271   val Loss: 78.3407   time: 0.24s   best: 77.5902
2023-10-06 10:31:42,638:INFO:  Epoch 166/500:  train Loss: 78.8720   val Loss: 77.9640   time: 0.24s   best: 77.5902
2023-10-06 10:31:42,879:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:42,902:INFO:  Epoch 167/500:  train Loss: 78.9983   val Loss: 76.8751   time: 0.24s   best: 76.8751
2023-10-06 10:31:43,139:INFO:  Epoch 168/500:  train Loss: 78.2395   val Loss: 77.7387   time: 0.24s   best: 76.8751
2023-10-06 10:31:43,373:INFO:  Epoch 169/500:  train Loss: 78.2364   val Loss: 77.3729   time: 0.23s   best: 76.8751
2023-10-06 10:31:43,608:INFO:  Epoch 170/500:  train Loss: 77.9404   val Loss: 77.5389   time: 0.23s   best: 76.8751
2023-10-06 10:31:43,848:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:43,868:INFO:  Epoch 171/500:  train Loss: 77.6415   val Loss: 76.5836   time: 0.24s   best: 76.5836
2023-10-06 10:31:44,106:INFO:  Epoch 172/500:  train Loss: 77.6581   val Loss: 77.4216   time: 0.24s   best: 76.5836
2023-10-06 10:31:44,366:INFO:  Epoch 173/500:  train Loss: 78.1248   val Loss: 77.4363   time: 0.26s   best: 76.5836
2023-10-06 10:31:44,671:INFO:  Epoch 174/500:  train Loss: 78.7024   val Loss: 77.7575   time: 0.24s   best: 76.5836
2023-10-06 10:31:44,921:INFO:  Epoch 175/500:  train Loss: 78.5574   val Loss: 78.5133   time: 0.25s   best: 76.5836
2023-10-06 10:31:45,181:INFO:  Epoch 176/500:  train Loss: 78.4531   val Loss: 76.8511   time: 0.26s   best: 76.5836
2023-10-06 10:31:45,418:INFO:  Epoch 177/500:  train Loss: 77.4719   val Loss: 76.7987   time: 0.24s   best: 76.5836
2023-10-06 10:31:45,654:INFO:  Epoch 178/500:  train Loss: 78.0011   val Loss: 77.0540   time: 0.24s   best: 76.5836
2023-10-06 10:31:45,891:INFO:  Epoch 179/500:  train Loss: 77.6636   val Loss: 77.3794   time: 0.24s   best: 76.5836
2023-10-06 10:31:46,132:INFO:  Epoch 180/500:  train Loss: 78.0363   val Loss: 78.5059   time: 0.24s   best: 76.5836
2023-10-06 10:31:46,369:INFO:  Epoch 181/500:  train Loss: 79.0373   val Loss: 77.7783   time: 0.24s   best: 76.5836
2023-10-06 10:31:46,613:INFO:  Epoch 182/500:  train Loss: 79.5437   val Loss: 77.7601   time: 0.24s   best: 76.5836
2023-10-06 10:31:46,850:INFO:  Epoch 183/500:  train Loss: 78.8198   val Loss: 79.7536   time: 0.24s   best: 76.5836
2023-10-06 10:31:47,086:INFO:  Epoch 184/500:  train Loss: 79.7520   val Loss: 78.6440   time: 0.23s   best: 76.5836
2023-10-06 10:31:47,330:INFO:  Epoch 185/500:  train Loss: 79.4836   val Loss: 78.9604   time: 0.24s   best: 76.5836
2023-10-06 10:31:47,574:INFO:  Epoch 186/500:  train Loss: 78.4998   val Loss: 79.0905   time: 0.24s   best: 76.5836
2023-10-06 10:31:47,818:INFO:  Epoch 187/500:  train Loss: 79.9723   val Loss: 77.7619   time: 0.24s   best: 76.5836
2023-10-06 10:31:48,062:INFO:  Epoch 188/500:  train Loss: 79.0124   val Loss: 80.2781   time: 0.24s   best: 76.5836
2023-10-06 10:31:48,306:INFO:  Epoch 189/500:  train Loss: 78.8703   val Loss: 76.7813   time: 0.24s   best: 76.5836
2023-10-06 10:31:48,546:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:48,570:INFO:  Epoch 190/500:  train Loss: 77.3746   val Loss: 75.5544   time: 0.24s   best: 75.5544
2023-10-06 10:31:48,806:INFO:  Epoch 191/500:  train Loss: 78.6864   val Loss: 76.1944   time: 0.24s   best: 75.5544
2023-10-06 10:31:49,043:INFO:  Epoch 192/500:  train Loss: 77.3396   val Loss: 76.7074   time: 0.24s   best: 75.5544
2023-10-06 10:31:49,280:INFO:  Epoch 193/500:  train Loss: 76.8622   val Loss: 75.8539   time: 0.24s   best: 75.5544
2023-10-06 10:31:49,516:INFO:  Epoch 194/500:  train Loss: 76.4005   val Loss: 75.9567   time: 0.24s   best: 75.5544
2023-10-06 10:31:49,752:INFO:  Epoch 195/500:  train Loss: 76.4794   val Loss: 76.1926   time: 0.24s   best: 75.5544
2023-10-06 10:31:49,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:50,016:INFO:  Epoch 196/500:  train Loss: 76.7452   val Loss: 75.1784   time: 0.24s   best: 75.1784
2023-10-06 10:31:50,254:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:50,281:INFO:  Epoch 197/500:  train Loss: 75.9454   val Loss: 74.9205   time: 0.23s   best: 74.9205
2023-10-06 10:31:50,525:INFO:  Epoch 198/500:  train Loss: 76.2596   val Loss: 76.5032   time: 0.24s   best: 74.9205
2023-10-06 10:31:50,765:INFO:  Epoch 199/500:  train Loss: 77.2478   val Loss: 77.3678   time: 0.24s   best: 74.9205
2023-10-06 10:31:51,043:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:51,062:INFO:  Epoch 200/500:  train Loss: 76.7472   val Loss: 74.8104   time: 0.27s   best: 74.8104
2023-10-06 10:31:51,301:INFO:  Epoch 201/500:  train Loss: 76.4631   val Loss: 76.4722   time: 0.24s   best: 74.8104
2023-10-06 10:31:51,536:INFO:  Epoch 202/500:  train Loss: 77.8616   val Loss: 76.4069   time: 0.23s   best: 74.8104
2023-10-06 10:31:51,777:INFO:  Epoch 203/500:  train Loss: 76.9364   val Loss: 76.5115   time: 0.24s   best: 74.8104
2023-10-06 10:31:52,014:INFO:  Epoch 204/500:  train Loss: 76.4244   val Loss: 76.0906   time: 0.24s   best: 74.8104
2023-10-06 10:31:52,254:INFO:  Epoch 205/500:  train Loss: 77.5501   val Loss: 77.4723   time: 0.24s   best: 74.8104
2023-10-06 10:31:52,492:INFO:  Epoch 206/500:  train Loss: 78.5319   val Loss: 78.0085   time: 0.24s   best: 74.8104
2023-10-06 10:31:52,737:INFO:  Epoch 207/500:  train Loss: 78.7056   val Loss: 78.0776   time: 0.24s   best: 74.8104
2023-10-06 10:31:52,974:INFO:  Epoch 208/500:  train Loss: 80.4199   val Loss: 78.0996   time: 0.24s   best: 74.8104
2023-10-06 10:31:53,212:INFO:  Epoch 209/500:  train Loss: 79.1964   val Loss: 78.0553   time: 0.24s   best: 74.8104
2023-10-06 10:31:53,452:INFO:  Epoch 210/500:  train Loss: 78.5607   val Loss: 79.0310   time: 0.24s   best: 74.8104
2023-10-06 10:31:53,693:INFO:  Epoch 211/500:  train Loss: 78.0360   val Loss: 75.4272   time: 0.24s   best: 74.8104
2023-10-06 10:31:53,930:INFO:  Epoch 212/500:  train Loss: 76.7189   val Loss: 75.3153   time: 0.24s   best: 74.8104
2023-10-06 10:31:54,170:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:54,189:INFO:  Epoch 213/500:  train Loss: 75.0680   val Loss: 74.3156   time: 0.24s   best: 74.3156
2023-10-06 10:31:54,424:INFO:  Epoch 214/500:  train Loss: 76.2575   val Loss: 74.7525   time: 0.23s   best: 74.3156
2023-10-06 10:31:54,661:INFO:  Epoch 215/500:  train Loss: 75.3546   val Loss: 74.8813   time: 0.24s   best: 74.3156
2023-10-06 10:31:54,898:INFO:  Epoch 216/500:  train Loss: 75.5668   val Loss: 74.7168   time: 0.23s   best: 74.3156
2023-10-06 10:31:55,133:INFO:  Epoch 217/500:  train Loss: 76.0387   val Loss: 75.7081   time: 0.23s   best: 74.3156
2023-10-06 10:31:55,370:INFO:  Epoch 218/500:  train Loss: 76.5714   val Loss: 76.0878   time: 0.23s   best: 74.3156
2023-10-06 10:31:55,605:INFO:  Epoch 219/500:  train Loss: 76.2939   val Loss: 74.4521   time: 0.23s   best: 74.3156
2023-10-06 10:31:55,846:INFO:  Epoch 220/500:  train Loss: 75.7822   val Loss: 74.6784   time: 0.24s   best: 74.3156
2023-10-06 10:31:56,082:INFO:  Epoch 221/500:  train Loss: 75.3851   val Loss: 75.6527   time: 0.23s   best: 74.3156
2023-10-06 10:31:56,320:INFO:  Epoch 222/500:  train Loss: 75.3360   val Loss: 74.3165   time: 0.24s   best: 74.3156
2023-10-06 10:31:56,559:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:56,579:INFO:  Epoch 223/500:  train Loss: 75.0823   val Loss: 73.7914   time: 0.23s   best: 73.7914
2023-10-06 10:31:56,814:INFO:  Epoch 224/500:  train Loss: 74.8819   val Loss: 74.1965   time: 0.23s   best: 73.7914
2023-10-06 10:31:57,047:INFO:  Epoch 225/500:  train Loss: 75.2707   val Loss: 75.1687   time: 0.23s   best: 73.7914
2023-10-06 10:31:57,280:INFO:  Epoch 226/500:  train Loss: 77.1739   val Loss: 76.6039   time: 0.23s   best: 73.7914
2023-10-06 10:31:57,515:INFO:  Epoch 227/500:  train Loss: 77.9472   val Loss: 77.5496   time: 0.23s   best: 73.7914
2023-10-06 10:31:57,751:INFO:  Epoch 228/500:  train Loss: 76.9975   val Loss: 75.6420   time: 0.23s   best: 73.7914
2023-10-06 10:31:57,986:INFO:  Epoch 229/500:  train Loss: 75.8998   val Loss: 74.8100   time: 0.23s   best: 73.7914
2023-10-06 10:31:58,222:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:58,242:INFO:  Epoch 230/500:  train Loss: 74.9625   val Loss: 73.4733   time: 0.23s   best: 73.4733
2023-10-06 10:31:58,480:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:31:58,499:INFO:  Epoch 231/500:  train Loss: 74.8189   val Loss: 73.2132   time: 0.23s   best: 73.2132
2023-10-06 10:31:58,734:INFO:  Epoch 232/500:  train Loss: 74.4408   val Loss: 74.1647   time: 0.23s   best: 73.2132
2023-10-06 10:31:58,968:INFO:  Epoch 233/500:  train Loss: 74.0193   val Loss: 73.9811   time: 0.23s   best: 73.2132
2023-10-06 10:31:59,201:INFO:  Epoch 234/500:  train Loss: 76.4635   val Loss: 76.2219   time: 0.23s   best: 73.2132
2023-10-06 10:31:59,434:INFO:  Epoch 235/500:  train Loss: 76.1847   val Loss: 76.8733   time: 0.23s   best: 73.2132
2023-10-06 10:31:59,668:INFO:  Epoch 236/500:  train Loss: 75.8146   val Loss: 73.7356   time: 0.23s   best: 73.2132
2023-10-06 10:31:59,907:INFO:  Epoch 237/500:  train Loss: 74.6551   val Loss: 75.7251   time: 0.24s   best: 73.2132
2023-10-06 10:32:00,143:INFO:  Epoch 238/500:  train Loss: 79.5751   val Loss: 80.1169   time: 0.23s   best: 73.2132
2023-10-06 10:32:00,383:INFO:  Epoch 239/500:  train Loss: 81.7305   val Loss: 83.8205   time: 0.24s   best: 73.2132
2023-10-06 10:32:00,620:INFO:  Epoch 240/500:  train Loss: 82.6396   val Loss: 79.4629   time: 0.24s   best: 73.2132
2023-10-06 10:32:00,856:INFO:  Epoch 241/500:  train Loss: 78.6050   val Loss: 78.4302   time: 0.23s   best: 73.2132
2023-10-06 10:32:01,091:INFO:  Epoch 242/500:  train Loss: 78.6330   val Loss: 78.1977   time: 0.23s   best: 73.2132
2023-10-06 10:32:01,327:INFO:  Epoch 243/500:  train Loss: 79.1938   val Loss: 77.5160   time: 0.23s   best: 73.2132
2023-10-06 10:32:01,563:INFO:  Epoch 244/500:  train Loss: 76.7674   val Loss: 76.8882   time: 0.23s   best: 73.2132
2023-10-06 10:32:01,799:INFO:  Epoch 245/500:  train Loss: 76.8027   val Loss: 76.9042   time: 0.23s   best: 73.2132
2023-10-06 10:32:02,035:INFO:  Epoch 246/500:  train Loss: 77.3806   val Loss: 78.1083   time: 0.23s   best: 73.2132
2023-10-06 10:32:02,270:INFO:  Epoch 247/500:  train Loss: 77.3899   val Loss: 75.9661   time: 0.23s   best: 73.2132
2023-10-06 10:32:02,507:INFO:  Epoch 248/500:  train Loss: 76.0282   val Loss: 75.3431   time: 0.24s   best: 73.2132
2023-10-06 10:32:02,743:INFO:  Epoch 249/500:  train Loss: 94.7816   val Loss: 80.5181   time: 0.23s   best: 73.2132
2023-10-06 10:32:02,983:INFO:  Epoch 250/500:  train Loss: 94.0142   val Loss: 99.1191   time: 0.24s   best: 73.2132
2023-10-06 10:32:03,219:INFO:  Epoch 251/500:  train Loss: 97.7980   val Loss: 94.3236   time: 0.23s   best: 73.2132
2023-10-06 10:32:03,461:INFO:  Epoch 252/500:  train Loss: 97.0607   val Loss: 94.7699   time: 0.24s   best: 73.2132
2023-10-06 10:32:03,698:INFO:  Epoch 253/500:  train Loss: 98.1268   val Loss: 98.8709   time: 0.24s   best: 73.2132
2023-10-06 10:32:03,934:INFO:  Epoch 254/500:  train Loss: 96.8865   val Loss: 94.6848   time: 0.23s   best: 73.2132
2023-10-06 10:32:04,170:INFO:  Epoch 255/500:  train Loss: 97.6341   val Loss: 98.2281   time: 0.23s   best: 73.2132
2023-10-06 10:32:04,406:INFO:  Epoch 256/500:  train Loss: 96.5488   val Loss: 95.0762   time: 0.23s   best: 73.2132
2023-10-06 10:32:04,659:INFO:  Epoch 257/500:  train Loss: 96.3522   val Loss: 96.2660   time: 0.24s   best: 73.2132
2023-10-06 10:32:04,990:INFO:  Epoch 258/500:  train Loss: 97.6206   val Loss: 98.0064   time: 0.33s   best: 73.2132
2023-10-06 10:32:05,368:INFO:  Epoch 259/500:  train Loss: 96.2830   val Loss: 92.3931   time: 0.23s   best: 73.2132
2023-10-06 10:32:05,620:INFO:  Epoch 260/500:  train Loss: 94.7665   val Loss: 94.8943   time: 0.24s   best: 73.2132
2023-10-06 10:32:05,864:INFO:  Epoch 261/500:  train Loss: 95.9581   val Loss: 95.9318   time: 0.24s   best: 73.2132
2023-10-06 10:32:06,120:INFO:  Epoch 262/500:  train Loss: 94.9920   val Loss: 92.8042   time: 0.25s   best: 73.2132
2023-10-06 10:32:06,364:INFO:  Epoch 263/500:  train Loss: 94.6334   val Loss: 92.0425   time: 0.24s   best: 73.2132
2023-10-06 10:32:06,619:INFO:  Epoch 264/500:  train Loss: 95.7599   val Loss: 95.7745   time: 0.25s   best: 73.2132
2023-10-06 10:32:06,864:INFO:  Epoch 265/500:  train Loss: 95.7590   val Loss: 95.4083   time: 0.24s   best: 73.2132
2023-10-06 10:32:07,120:INFO:  Epoch 266/500:  train Loss: 94.7522   val Loss: 94.5425   time: 0.25s   best: 73.2132
2023-10-06 10:32:07,362:INFO:  Epoch 267/500:  train Loss: 92.7978   val Loss: 91.4507   time: 0.24s   best: 73.2132
2023-10-06 10:32:07,613:INFO:  Epoch 268/500:  train Loss: 94.1939   val Loss: 94.4566   time: 0.25s   best: 73.2132
2023-10-06 10:32:07,864:INFO:  Epoch 269/500:  train Loss: 94.8766   val Loss: 94.6480   time: 0.24s   best: 73.2132
2023-10-06 10:32:08,123:INFO:  Epoch 270/500:  train Loss: 95.0593   val Loss: 94.5544   time: 0.25s   best: 73.2132
2023-10-06 10:32:08,360:INFO:  Epoch 271/500:  train Loss: 94.5148   val Loss: 94.3256   time: 0.24s   best: 73.2132
2023-10-06 10:32:08,626:INFO:  Epoch 272/500:  train Loss: 94.6458   val Loss: 94.2061   time: 0.25s   best: 73.2132
2023-10-06 10:32:08,877:INFO:  Epoch 273/500:  train Loss: 94.7586   val Loss: 94.3104   time: 0.24s   best: 73.2132
2023-10-06 10:32:09,135:INFO:  Epoch 274/500:  train Loss: 94.5566   val Loss: 94.2206   time: 0.24s   best: 73.2132
2023-10-06 10:32:09,387:INFO:  Epoch 275/500:  train Loss: 94.5609   val Loss: 94.0699   time: 0.24s   best: 73.2132
2023-10-06 10:32:09,650:INFO:  Epoch 276/500:  train Loss: 94.4206   val Loss: 93.9260   time: 0.25s   best: 73.2132
2023-10-06 10:32:09,894:INFO:  Epoch 277/500:  train Loss: 93.7963   val Loss: 93.5339   time: 0.24s   best: 73.2132
2023-10-06 10:32:10,148:INFO:  Epoch 278/500:  train Loss: 93.7499   val Loss: 93.5028   time: 0.25s   best: 73.2132
2023-10-06 10:32:10,393:INFO:  Epoch 279/500:  train Loss: 93.9323   val Loss: 93.4920   time: 0.24s   best: 73.2132
2023-10-06 10:32:10,647:INFO:  Epoch 280/500:  train Loss: 93.8813   val Loss: 93.4939   time: 0.25s   best: 73.2132
2023-10-06 10:32:10,892:INFO:  Epoch 281/500:  train Loss: 94.2188   val Loss: 93.9167   time: 0.24s   best: 73.2132
2023-10-06 10:32:11,146:INFO:  Epoch 282/500:  train Loss: 93.6647   val Loss: 93.3360   time: 0.25s   best: 73.2132
2023-10-06 10:32:11,388:INFO:  Epoch 283/500:  train Loss: 93.0650   val Loss: 92.9234   time: 0.24s   best: 73.2132
2023-10-06 10:32:11,638:INFO:  Epoch 284/500:  train Loss: 93.2039   val Loss: 92.8850   time: 0.25s   best: 73.2132
2023-10-06 10:32:11,880:INFO:  Epoch 285/500:  train Loss: 93.4852   val Loss: 93.3469   time: 0.24s   best: 73.2132
2023-10-06 10:32:12,130:INFO:  Epoch 286/500:  train Loss: 93.6208   val Loss: 93.4792   time: 0.25s   best: 73.2132
2023-10-06 10:32:12,371:INFO:  Epoch 287/500:  train Loss: 93.4491   val Loss: 93.1901   time: 0.24s   best: 73.2132
2023-10-06 10:32:12,612:INFO:  Epoch 288/500:  train Loss: 93.2337   val Loss: 93.0321   time: 0.24s   best: 73.2132
2023-10-06 10:32:12,857:INFO:  Epoch 289/500:  train Loss: 92.9447   val Loss: 92.6191   time: 0.24s   best: 73.2132
2023-10-06 10:32:13,095:INFO:  Epoch 290/500:  train Loss: 92.6144   val Loss: 92.3605   time: 0.24s   best: 73.2132
2023-10-06 10:32:13,338:INFO:  Epoch 291/500:  train Loss: 93.0132   val Loss: 92.6225   time: 0.24s   best: 73.2132
2023-10-06 10:32:13,574:INFO:  Epoch 292/500:  train Loss: 92.8863   val Loss: 92.6970   time: 0.23s   best: 73.2132
2023-10-06 10:32:13,811:INFO:  Epoch 293/500:  train Loss: 92.5662   val Loss: 92.3495   time: 0.24s   best: 73.2132
2023-10-06 10:32:14,047:INFO:  Epoch 294/500:  train Loss: 92.7412   val Loss: 92.2417   time: 0.23s   best: 73.2132
2023-10-06 10:32:14,283:INFO:  Epoch 295/500:  train Loss: 92.2588   val Loss: 91.9890   time: 0.23s   best: 73.2132
2023-10-06 10:32:14,521:INFO:  Epoch 296/500:  train Loss: 91.9958   val Loss: 91.7426   time: 0.24s   best: 73.2132
2023-10-06 10:32:14,757:INFO:  Epoch 297/500:  train Loss: 92.1603   val Loss: 91.7005   time: 0.23s   best: 73.2132
2023-10-06 10:32:14,993:INFO:  Epoch 298/500:  train Loss: 92.2259   val Loss: 92.0002   time: 0.23s   best: 73.2132
2023-10-06 10:32:15,237:INFO:  Epoch 299/500:  train Loss: 92.1391   val Loss: 91.7206   time: 0.24s   best: 73.2132
2023-10-06 10:32:15,519:INFO:  Epoch 300/500:  train Loss: 91.3824   val Loss: 91.0161   time: 0.27s   best: 73.2132
2023-10-06 10:32:15,782:INFO:  Epoch 301/500:  train Loss: 91.1710   val Loss: 90.7337   time: 0.25s   best: 73.2132
2023-10-06 10:32:16,027:INFO:  Epoch 302/500:  train Loss: 90.5066   val Loss: 90.3182   time: 0.24s   best: 73.2132
2023-10-06 10:32:16,281:INFO:  Epoch 303/500:  train Loss: 90.4377   val Loss: 90.0281   time: 0.25s   best: 73.2132
2023-10-06 10:32:16,525:INFO:  Epoch 304/500:  train Loss: 90.1773   val Loss: 89.6218   time: 0.24s   best: 73.2132
2023-10-06 10:32:16,779:INFO:  Epoch 305/500:  train Loss: 89.5856   val Loss: 89.0345   time: 0.25s   best: 73.2132
2023-10-06 10:32:17,023:INFO:  Epoch 306/500:  train Loss: 89.1763   val Loss: 88.4164   time: 0.24s   best: 73.2132
2023-10-06 10:32:17,279:INFO:  Epoch 307/500:  train Loss: 88.4390   val Loss: 87.8005   time: 0.25s   best: 73.2132
2023-10-06 10:32:17,527:INFO:  Epoch 308/500:  train Loss: 88.0277   val Loss: 87.0631   time: 0.24s   best: 73.2132
2023-10-06 10:32:17,778:INFO:  Epoch 309/500:  train Loss: 87.2747   val Loss: 85.9247   time: 0.25s   best: 73.2132
2023-10-06 10:32:18,022:INFO:  Epoch 310/500:  train Loss: 85.9763   val Loss: 84.3312   time: 0.24s   best: 73.2132
2023-10-06 10:32:18,276:INFO:  Epoch 311/500:  train Loss: 84.1152   val Loss: 82.1801   time: 0.25s   best: 73.2132
2023-10-06 10:32:18,520:INFO:  Epoch 312/500:  train Loss: 82.2172   val Loss: 81.3019   time: 0.24s   best: 73.2132
2023-10-06 10:32:18,774:INFO:  Epoch 313/500:  train Loss: 81.6928   val Loss: 79.6018   time: 0.25s   best: 73.2132
2023-10-06 10:32:19,018:INFO:  Epoch 314/500:  train Loss: 79.5425   val Loss: 78.6926   time: 0.24s   best: 73.2132
2023-10-06 10:32:19,268:INFO:  Epoch 315/500:  train Loss: 78.5002   val Loss: 78.0230   time: 0.24s   best: 73.2132
2023-10-06 10:32:19,507:INFO:  Epoch 316/500:  train Loss: 78.3925   val Loss: 77.8580   time: 0.24s   best: 73.2132
2023-10-06 10:32:19,745:INFO:  Epoch 317/500:  train Loss: 77.0456   val Loss: 75.1833   time: 0.24s   best: 73.2132
2023-10-06 10:32:19,989:INFO:  Epoch 318/500:  train Loss: 76.3605   val Loss: 74.7741   time: 0.24s   best: 73.2132
2023-10-06 10:32:20,236:INFO:  Epoch 319/500:  train Loss: 75.6942   val Loss: 74.4149   time: 0.24s   best: 73.2132
2023-10-06 10:32:20,481:INFO:  Epoch 320/500:  train Loss: 75.2712   val Loss: 74.1187   time: 0.24s   best: 73.2132
2023-10-06 10:32:20,724:INFO:  Epoch 321/500:  train Loss: 75.7659   val Loss: 75.3581   time: 0.24s   best: 73.2132
2023-10-06 10:32:20,970:INFO:  Epoch 322/500:  train Loss: 77.0034   val Loss: 76.5121   time: 0.24s   best: 73.2132
2023-10-06 10:32:21,215:INFO:  Epoch 323/500:  train Loss: 77.2973   val Loss: 76.4981   time: 0.24s   best: 73.2132
2023-10-06 10:32:21,458:INFO:  Epoch 324/500:  train Loss: 76.4044   val Loss: 75.9530   time: 0.24s   best: 73.2132
2023-10-06 10:32:21,692:INFO:  Epoch 325/500:  train Loss: 77.5553   val Loss: 76.9030   time: 0.23s   best: 73.2132
2023-10-06 10:32:21,937:INFO:  Epoch 326/500:  train Loss: 76.7457   val Loss: 78.3176   time: 0.24s   best: 73.2132
2023-10-06 10:32:22,174:INFO:  Epoch 327/500:  train Loss: 79.2343   val Loss: 78.4626   time: 0.24s   best: 73.2132
2023-10-06 10:32:22,420:INFO:  Epoch 328/500:  train Loss: 79.3807   val Loss: 76.8198   time: 0.24s   best: 73.2132
2023-10-06 10:32:22,657:INFO:  Epoch 329/500:  train Loss: 79.0361   val Loss: 78.5619   time: 0.24s   best: 73.2132
2023-10-06 10:32:22,902:INFO:  Epoch 330/500:  train Loss: 77.0845   val Loss: 75.7180   time: 0.24s   best: 73.2132
2023-10-06 10:32:23,138:INFO:  Epoch 331/500:  train Loss: 75.8236   val Loss: 73.7120   time: 0.23s   best: 73.2132
2023-10-06 10:32:23,388:INFO:  Epoch 332/500:  train Loss: 74.0164   val Loss: 73.7680   time: 0.24s   best: 73.2132
2023-10-06 10:32:23,628:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:23,652:INFO:  Epoch 333/500:  train Loss: 74.4179   val Loss: 72.5225   time: 0.23s   best: 72.5225
2023-10-06 10:32:23,897:INFO:  Epoch 334/500:  train Loss: 74.8178   val Loss: 72.7385   time: 0.24s   best: 72.5225
2023-10-06 10:32:24,136:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:24,156:INFO:  Epoch 335/500:  train Loss: 73.2969   val Loss: 72.4612   time: 0.23s   best: 72.4612
2023-10-06 10:32:24,400:INFO:  Epoch 336/500:  train Loss: 73.2207   val Loss: 72.8969   time: 0.24s   best: 72.4612
2023-10-06 10:32:24,635:INFO:  Epoch 337/500:  train Loss: 73.5784   val Loss: 73.1535   time: 0.23s   best: 72.4612
2023-10-06 10:32:24,869:INFO:  Epoch 338/500:  train Loss: 74.3930   val Loss: 74.3667   time: 0.23s   best: 72.4612
2023-10-06 10:32:25,116:INFO:  Epoch 339/500:  train Loss: 74.6802   val Loss: 74.2788   time: 0.24s   best: 72.4612
2023-10-06 10:32:25,352:INFO:  Epoch 340/500:  train Loss: 74.3153   val Loss: 75.0348   time: 0.23s   best: 72.4612
2023-10-06 10:32:25,602:INFO:  Epoch 341/500:  train Loss: 75.4551   val Loss: 74.2476   time: 0.25s   best: 72.4612
2023-10-06 10:32:25,838:INFO:  Epoch 342/500:  train Loss: 73.8939   val Loss: 72.9811   time: 0.23s   best: 72.4612
2023-10-06 10:32:26,087:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:26,106:INFO:  Epoch 343/500:  train Loss: 73.4751   val Loss: 72.2987   time: 0.24s   best: 72.2987
2023-10-06 10:32:26,344:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:26,363:INFO:  Epoch 344/500:  train Loss: 72.6519   val Loss: 71.3887   time: 0.23s   best: 71.3887
2023-10-06 10:32:26,608:INFO:  Epoch 345/500:  train Loss: 72.2002   val Loss: 71.9881   time: 0.24s   best: 71.3887
2023-10-06 10:32:26,843:INFO:  Epoch 346/500:  train Loss: 73.7838   val Loss: 72.2952   time: 0.23s   best: 71.3887
2023-10-06 10:32:27,088:INFO:  Epoch 347/500:  train Loss: 72.8733   val Loss: 72.6495   time: 0.24s   best: 71.3887
2023-10-06 10:32:27,324:INFO:  Epoch 348/500:  train Loss: 76.5510   val Loss: 77.5738   time: 0.23s   best: 71.3887
2023-10-06 10:32:27,571:INFO:  Epoch 349/500:  train Loss: 78.3852   val Loss: 77.5395   time: 0.25s   best: 71.3887
2023-10-06 10:32:27,807:INFO:  Epoch 350/500:  train Loss: 78.3186   val Loss: 76.1205   time: 0.23s   best: 71.3887
2023-10-06 10:32:28,052:INFO:  Epoch 351/500:  train Loss: 76.0525   val Loss: 74.9036   time: 0.24s   best: 71.3887
2023-10-06 10:32:28,288:INFO:  Epoch 352/500:  train Loss: 75.1581   val Loss: 73.9445   time: 0.23s   best: 71.3887
2023-10-06 10:32:28,534:INFO:  Epoch 353/500:  train Loss: 74.6173   val Loss: 73.6232   time: 0.25s   best: 71.3887
2023-10-06 10:32:28,771:INFO:  Epoch 354/500:  train Loss: 73.7814   val Loss: 72.7371   time: 0.24s   best: 71.3887
2023-10-06 10:32:29,016:INFO:  Epoch 355/500:  train Loss: 74.5762   val Loss: 74.5217   time: 0.24s   best: 71.3887
2023-10-06 10:32:29,252:INFO:  Epoch 356/500:  train Loss: 74.7673   val Loss: 75.9978   time: 0.24s   best: 71.3887
2023-10-06 10:32:29,488:INFO:  Epoch 357/500:  train Loss: 73.9175   val Loss: 72.3526   time: 0.23s   best: 71.3887
2023-10-06 10:32:29,740:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:30,253:INFO:  Epoch 358/500:  train Loss: 73.0379   val Loss: 71.1333   time: 0.25s   best: 71.1333
2023-10-06 10:32:30,490:INFO:  Epoch 359/500:  train Loss: 72.1086   val Loss: 71.4114   time: 0.24s   best: 71.1333
2023-10-06 10:32:30,725:INFO:  Epoch 360/500:  train Loss: 73.1349   val Loss: 73.5124   time: 0.23s   best: 71.1333
2023-10-06 10:32:30,958:INFO:  Epoch 361/500:  train Loss: 76.5937   val Loss: 75.5546   time: 0.23s   best: 71.1333
2023-10-06 10:32:31,191:INFO:  Epoch 362/500:  train Loss: 76.1375   val Loss: 77.9906   time: 0.23s   best: 71.1333
2023-10-06 10:32:31,426:INFO:  Epoch 363/500:  train Loss: 75.3971   val Loss: 78.1430   time: 0.23s   best: 71.1333
2023-10-06 10:32:31,661:INFO:  Epoch 364/500:  train Loss: 84.9307   val Loss: 88.5562   time: 0.23s   best: 71.1333
2023-10-06 10:32:31,903:INFO:  Epoch 365/500:  train Loss: 86.8048   val Loss: 86.2036   time: 0.24s   best: 71.1333
2023-10-06 10:32:32,144:INFO:  Epoch 366/500:  train Loss: 85.6593   val Loss: 84.6721   time: 0.23s   best: 71.1333
2023-10-06 10:32:32,384:INFO:  Epoch 367/500:  train Loss: 86.8191   val Loss: 86.0807   time: 0.23s   best: 71.1333
2023-10-06 10:32:32,631:INFO:  Epoch 368/500:  train Loss: 83.6556   val Loss: 81.3306   time: 0.24s   best: 71.1333
2023-10-06 10:32:32,871:INFO:  Epoch 369/500:  train Loss: 80.5481   val Loss: 79.4118   time: 0.24s   best: 71.1333
2023-10-06 10:32:33,117:INFO:  Epoch 370/500:  train Loss: 81.6063   val Loss: 80.3707   time: 0.24s   best: 71.1333
2023-10-06 10:32:33,357:INFO:  Epoch 371/500:  train Loss: 81.8221   val Loss: 81.2710   time: 0.23s   best: 71.1333
2023-10-06 10:32:33,609:INFO:  Epoch 372/500:  train Loss: 80.7746   val Loss: 78.9729   time: 0.25s   best: 71.1333
2023-10-06 10:32:33,850:INFO:  Epoch 373/500:  train Loss: 77.8338   val Loss: 76.3232   time: 0.24s   best: 71.1333
2023-10-06 10:32:34,086:INFO:  Epoch 374/500:  train Loss: 76.6317   val Loss: 74.8995   time: 0.23s   best: 71.1333
2023-10-06 10:32:34,335:INFO:  Epoch 375/500:  train Loss: 76.4704   val Loss: 75.3083   time: 0.24s   best: 71.1333
2023-10-06 10:32:34,576:INFO:  Epoch 376/500:  train Loss: 75.9803   val Loss: 74.5617   time: 0.23s   best: 71.1333
2023-10-06 10:32:34,817:INFO:  Epoch 377/500:  train Loss: 74.8892   val Loss: 74.7783   time: 0.24s   best: 71.1333
2023-10-06 10:32:35,058:INFO:  Epoch 378/500:  train Loss: 77.1115   val Loss: 76.7220   time: 0.23s   best: 71.1333
2023-10-06 10:32:35,297:INFO:  Epoch 379/500:  train Loss: 74.5049   val Loss: 73.6467   time: 0.23s   best: 71.1333
2023-10-06 10:32:35,533:INFO:  Epoch 380/500:  train Loss: 73.9976   val Loss: 72.4804   time: 0.23s   best: 71.1333
2023-10-06 10:32:35,819:INFO:  Epoch 381/500:  train Loss: 72.3758   val Loss: 71.6202   time: 0.28s   best: 71.1333
2023-10-06 10:32:36,062:INFO:  Epoch 382/500:  train Loss: 73.0325   val Loss: 72.6156   time: 0.24s   best: 71.1333
2023-10-06 10:32:36,304:INFO:  Epoch 383/500:  train Loss: 74.0944   val Loss: 73.7525   time: 0.24s   best: 71.1333
2023-10-06 10:32:36,543:INFO:  Epoch 384/500:  train Loss: 73.0817   val Loss: 72.4290   time: 0.24s   best: 71.1333
2023-10-06 10:32:36,779:INFO:  Epoch 385/500:  train Loss: 73.8190   val Loss: 72.3228   time: 0.23s   best: 71.1333
2023-10-06 10:32:37,015:INFO:  Epoch 386/500:  train Loss: 73.7036   val Loss: 74.2440   time: 0.23s   best: 71.1333
2023-10-06 10:32:37,257:INFO:  Epoch 387/500:  train Loss: 73.5086   val Loss: 72.2206   time: 0.24s   best: 71.1333
2023-10-06 10:32:37,498:INFO:  Epoch 388/500:  train Loss: 74.3719   val Loss: 72.2456   time: 0.24s   best: 71.1333
2023-10-06 10:32:37,742:INFO:  Epoch 389/500:  train Loss: 73.5506   val Loss: 74.5607   time: 0.24s   best: 71.1333
2023-10-06 10:32:37,982:INFO:  Epoch 390/500:  train Loss: 73.4667   val Loss: 73.3310   time: 0.23s   best: 71.1333
2023-10-06 10:32:38,226:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:38,249:INFO:  Epoch 391/500:  train Loss: 76.2740   val Loss: 70.3221   time: 0.24s   best: 70.3221
2023-10-06 10:32:38,484:INFO:  Epoch 392/500:  train Loss: 76.2736   val Loss: 78.6659   time: 0.23s   best: 70.3221
2023-10-06 10:32:38,727:INFO:  Epoch 393/500:  train Loss: 78.8135   val Loss: 78.7531   time: 0.24s   best: 70.3221
2023-10-06 10:32:38,963:INFO:  Epoch 394/500:  train Loss: 78.2475   val Loss: 76.1924   time: 0.23s   best: 70.3221
2023-10-06 10:32:39,199:INFO:  Epoch 395/500:  train Loss: 75.6244   val Loss: 73.8309   time: 0.24s   best: 70.3221
2023-10-06 10:32:39,435:INFO:  Epoch 396/500:  train Loss: 74.6667   val Loss: 72.7151   time: 0.23s   best: 70.3221
2023-10-06 10:32:39,672:INFO:  Epoch 397/500:  train Loss: 73.4078   val Loss: 73.3280   time: 0.24s   best: 70.3221
2023-10-06 10:32:39,909:INFO:  Epoch 398/500:  train Loss: 75.5530   val Loss: 76.1609   time: 0.24s   best: 70.3221
2023-10-06 10:32:40,144:INFO:  Epoch 399/500:  train Loss: 74.6302   val Loss: 75.0389   time: 0.23s   best: 70.3221
2023-10-06 10:32:40,411:INFO:  Epoch 400/500:  train Loss: 76.2180   val Loss: 79.1644   time: 0.27s   best: 70.3221
2023-10-06 10:32:40,649:INFO:  Epoch 401/500:  train Loss: 75.0759   val Loss: 74.7113   time: 0.24s   best: 70.3221
2023-10-06 10:32:40,888:INFO:  Epoch 402/500:  train Loss: 73.2274   val Loss: 71.7424   time: 0.24s   best: 70.3221
2023-10-06 10:32:41,126:INFO:  Epoch 403/500:  train Loss: 72.2614   val Loss: 71.5758   time: 0.24s   best: 70.3221
2023-10-06 10:32:41,371:INFO:  Epoch 404/500:  train Loss: 75.1306   val Loss: 75.2493   time: 0.24s   best: 70.3221
2023-10-06 10:32:41,607:INFO:  Epoch 405/500:  train Loss: 75.3243   val Loss: 74.8422   time: 0.23s   best: 70.3221
2023-10-06 10:32:41,846:INFO:  Epoch 406/500:  train Loss: 73.9913   val Loss: 74.0142   time: 0.24s   best: 70.3221
2023-10-06 10:32:42,081:INFO:  Epoch 407/500:  train Loss: 73.6509   val Loss: 73.1812   time: 0.23s   best: 70.3221
2023-10-06 10:32:42,325:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:42,344:INFO:  Epoch 408/500:  train Loss: 72.7462   val Loss: 70.2501   time: 0.24s   best: 70.2501
2023-10-06 10:32:42,584:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:42,603:INFO:  Epoch 409/500:  train Loss: 70.5414   val Loss: 69.7485   time: 0.24s   best: 69.7485
2023-10-06 10:32:42,846:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:42,865:INFO:  Epoch 410/500:  train Loss: 70.7625   val Loss: 68.9476   time: 0.24s   best: 68.9476
2023-10-06 10:32:43,099:INFO:  Epoch 411/500:  train Loss: 68.9224   val Loss: 69.4469   time: 0.23s   best: 68.9476
2023-10-06 10:32:43,338:INFO:  Epoch 412/500:  train Loss: 70.5468   val Loss: 71.2764   time: 0.24s   best: 68.9476
2023-10-06 10:32:43,574:INFO:  Epoch 413/500:  train Loss: 76.0961   val Loss: 72.1828   time: 0.23s   best: 68.9476
2023-10-06 10:32:43,811:INFO:  Epoch 414/500:  train Loss: 72.7567   val Loss: 72.7022   time: 0.24s   best: 68.9476
2023-10-06 10:32:44,047:INFO:  Epoch 415/500:  train Loss: 73.9663   val Loss: 74.5515   time: 0.23s   best: 68.9476
2023-10-06 10:32:44,283:INFO:  Epoch 416/500:  train Loss: 71.9180   val Loss: 71.8950   time: 0.23s   best: 68.9476
2023-10-06 10:32:45,905:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:46,814:INFO:  Epoch 417/500:  train Loss: 70.9832   val Loss: 68.5013   time: 1.62s   best: 68.5013
2023-10-06 10:32:47,054:INFO:  Epoch 418/500:  train Loss: 70.0039   val Loss: 69.7919   time: 0.24s   best: 68.5013
2023-10-06 10:32:47,288:INFO:  Epoch 419/500:  train Loss: 70.5665   val Loss: 71.2503   time: 0.23s   best: 68.5013
2023-10-06 10:32:47,522:INFO:  Epoch 420/500:  train Loss: 75.1102   val Loss: 79.6329   time: 0.23s   best: 68.5013
2023-10-06 10:32:47,756:INFO:  Epoch 421/500:  train Loss: 74.6789   val Loss: 72.6566   time: 0.23s   best: 68.5013
2023-10-06 10:32:47,991:INFO:  Epoch 422/500:  train Loss: 71.2170   val Loss: 72.8515   time: 0.23s   best: 68.5013
2023-10-06 10:32:48,224:INFO:  Epoch 423/500:  train Loss: 72.3280   val Loss: 70.5932   time: 0.23s   best: 68.5013
2023-10-06 10:32:48,484:INFO:  Epoch 424/500:  train Loss: 75.3839   val Loss: 74.8515   time: 0.26s   best: 68.5013
2023-10-06 10:32:48,727:INFO:  Epoch 425/500:  train Loss: 75.2147   val Loss: 75.9692   time: 0.24s   best: 68.5013
2023-10-06 10:32:48,992:INFO:  Epoch 426/500:  train Loss: 73.9035   val Loss: 74.3353   time: 0.26s   best: 68.5013
2023-10-06 10:32:49,229:INFO:  Epoch 427/500:  train Loss: 72.2096   val Loss: 70.8791   time: 0.24s   best: 68.5013
2023-10-06 10:32:49,480:INFO:  Epoch 428/500:  train Loss: 70.6933   val Loss: 69.7499   time: 0.25s   best: 68.5013
2023-10-06 10:32:49,722:INFO:  Epoch 429/500:  train Loss: 72.4586   val Loss: 70.0336   time: 0.24s   best: 68.5013
2023-10-06 10:32:49,970:INFO:  Epoch 430/500:  train Loss: 73.6462   val Loss: 73.4149   time: 0.24s   best: 68.5013
2023-10-06 10:32:50,216:INFO:  Epoch 431/500:  train Loss: 75.1618   val Loss: 73.6722   time: 0.24s   best: 68.5013
2023-10-06 10:32:50,464:INFO:  Epoch 432/500:  train Loss: 72.4841   val Loss: 71.2055   time: 0.24s   best: 68.5013
2023-10-06 10:32:50,707:INFO:  Epoch 433/500:  train Loss: 71.6690   val Loss: 70.3730   time: 0.24s   best: 68.5013
2023-10-06 10:32:50,956:INFO:  Epoch 434/500:  train Loss: 70.4041   val Loss: 69.6470   time: 0.24s   best: 68.5013
2023-10-06 10:32:51,199:INFO:  Epoch 435/500:  train Loss: 70.7652   val Loss: 70.3359   time: 0.24s   best: 68.5013
2023-10-06 10:32:51,438:INFO:  Epoch 436/500:  train Loss: 70.2013   val Loss: 68.6874   time: 0.24s   best: 68.5013
2023-10-06 10:32:51,683:INFO:  Epoch 437/500:  train Loss: 70.3946   val Loss: 70.9932   time: 0.24s   best: 68.5013
2023-10-06 10:32:51,923:INFO:  Epoch 438/500:  train Loss: 74.1511   val Loss: 73.0227   time: 0.23s   best: 68.5013
2023-10-06 10:32:52,165:INFO:  Epoch 439/500:  train Loss: 72.8926   val Loss: 72.3831   time: 0.24s   best: 68.5013
2023-10-06 10:32:52,408:INFO:  Epoch 440/500:  train Loss: 71.3801   val Loss: 70.8291   time: 0.24s   best: 68.5013
2023-10-06 10:32:52,652:INFO:  Epoch 441/500:  train Loss: 71.4887   val Loss: 71.7632   time: 0.24s   best: 68.5013
2023-10-06 10:32:52,896:INFO:  Epoch 442/500:  train Loss: 71.4416   val Loss: 72.0375   time: 0.24s   best: 68.5013
2023-10-06 10:32:53,140:INFO:  Epoch 443/500:  train Loss: 73.7015   val Loss: 75.9902   time: 0.24s   best: 68.5013
2023-10-06 10:32:53,383:INFO:  Epoch 444/500:  train Loss: 72.2728   val Loss: 70.5771   time: 0.24s   best: 68.5013
2023-10-06 10:32:53,624:INFO:  Epoch 445/500:  train Loss: 70.1195   val Loss: 69.7659   time: 0.24s   best: 68.5013
2023-10-06 10:32:53,866:INFO:  Epoch 446/500:  train Loss: 70.7979   val Loss: 68.8132   time: 0.24s   best: 68.5013
2023-10-06 10:32:54,109:INFO:  Epoch 447/500:  train Loss: 69.7362   val Loss: 69.8142   time: 0.24s   best: 68.5013
2023-10-06 10:32:54,352:INFO:  Epoch 448/500:  train Loss: 70.6310   val Loss: 69.1333   time: 0.24s   best: 68.5013
2023-10-06 10:32:54,591:INFO:  Epoch 449/500:  train Loss: 70.1845   val Loss: 70.2161   time: 0.24s   best: 68.5013
2023-10-06 10:32:54,830:INFO:  Epoch 450/500:  train Loss: 72.5282   val Loss: 70.9149   time: 0.24s   best: 68.5013
2023-10-06 10:32:55,077:INFO:  Epoch 451/500:  train Loss: 71.1840   val Loss: 71.7355   time: 0.24s   best: 68.5013
2023-10-06 10:32:55,320:INFO:  Epoch 452/500:  train Loss: 75.3105   val Loss: 71.3271   time: 0.24s   best: 68.5013
2023-10-06 10:32:55,565:INFO:  Epoch 453/500:  train Loss: 77.6154   val Loss: 81.4939   time: 0.24s   best: 68.5013
2023-10-06 10:32:55,817:INFO:  Epoch 454/500:  train Loss: 77.0328   val Loss: 75.1931   time: 0.25s   best: 68.5013
2023-10-06 10:32:56,062:INFO:  Epoch 455/500:  train Loss: 74.7289   val Loss: 70.3835   time: 0.24s   best: 68.5013
2023-10-06 10:32:56,306:INFO:  Epoch 456/500:  train Loss: 70.7329   val Loss: 72.0644   time: 0.24s   best: 68.5013
2023-10-06 10:32:56,697:INFO:  Epoch 457/500:  train Loss: 72.1840   val Loss: 73.1935   time: 0.39s   best: 68.5013
2023-10-06 10:32:56,933:INFO:  Epoch 458/500:  train Loss: 72.8293   val Loss: 74.3905   time: 0.23s   best: 68.5013
2023-10-06 10:32:57,169:INFO:  Epoch 459/500:  train Loss: 73.8380   val Loss: 74.4755   time: 0.23s   best: 68.5013
2023-10-06 10:32:57,405:INFO:  Epoch 460/500:  train Loss: 72.3268   val Loss: 72.8225   time: 0.23s   best: 68.5013
2023-10-06 10:32:57,640:INFO:  Epoch 461/500:  train Loss: 73.6180   val Loss: 78.0725   time: 0.23s   best: 68.5013
2023-10-06 10:32:57,876:INFO:  Epoch 462/500:  train Loss: 76.1609   val Loss: 72.2898   time: 0.23s   best: 68.5013
2023-10-06 10:32:58,120:INFO:  Epoch 463/500:  train Loss: 72.9448   val Loss: 71.4231   time: 0.24s   best: 68.5013
2023-10-06 10:32:58,355:INFO:  Epoch 464/500:  train Loss: 71.3064   val Loss: 70.5850   time: 0.23s   best: 68.5013
2023-10-06 10:32:58,592:INFO:  Epoch 465/500:  train Loss: 69.9000   val Loss: 68.7164   time: 0.24s   best: 68.5013
2023-10-06 10:32:58,831:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:58,850:INFO:  Epoch 466/500:  train Loss: 70.6524   val Loss: 67.1707   time: 0.23s   best: 67.1707
2023-10-06 10:32:59,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:59,107:INFO:  Epoch 467/500:  train Loss: 67.3651   val Loss: 66.4849   time: 0.23s   best: 66.4849
2023-10-06 10:32:59,343:INFO:  Epoch 468/500:  train Loss: 67.2911   val Loss: 67.1890   time: 0.23s   best: 66.4849
2023-10-06 10:32:59,586:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:32:59,605:INFO:  Epoch 469/500:  train Loss: 68.4196   val Loss: 65.8797   time: 0.24s   best: 65.8797
2023-10-06 10:33:01,577:INFO:  Epoch 470/500:  train Loss: 68.1581   val Loss: 68.4230   time: 1.97s   best: 65.8797
2023-10-06 10:33:01,829:INFO:  Epoch 471/500:  train Loss: 68.1068   val Loss: 68.1861   time: 0.24s   best: 65.8797
2023-10-06 10:33:02,073:INFO:  Epoch 472/500:  train Loss: 70.0689   val Loss: 70.7096   time: 0.24s   best: 65.8797
2023-10-06 10:33:02,326:INFO:  Epoch 473/500:  train Loss: 71.3753   val Loss: 70.7348   time: 0.25s   best: 65.8797
2023-10-06 10:33:02,570:INFO:  Epoch 474/500:  train Loss: 69.8867   val Loss: 69.4383   time: 0.24s   best: 65.8797
2023-10-06 10:33:02,823:INFO:  Epoch 475/500:  train Loss: 70.7846   val Loss: 70.3189   time: 0.25s   best: 65.8797
2023-10-06 10:33:03,066:INFO:  Epoch 476/500:  train Loss: 71.3592   val Loss: 70.0382   time: 0.24s   best: 65.8797
2023-10-06 10:33:03,316:INFO:  Epoch 477/500:  train Loss: 70.9346   val Loss: 67.9477   time: 0.25s   best: 65.8797
2023-10-06 10:33:03,557:INFO:  Epoch 478/500:  train Loss: 69.7407   val Loss: 70.7451   time: 0.24s   best: 65.8797
2023-10-06 10:33:03,807:INFO:  Epoch 479/500:  train Loss: 70.5646   val Loss: 70.5506   time: 0.24s   best: 65.8797
2023-10-06 10:33:04,048:INFO:  Epoch 480/500:  train Loss: 71.2606   val Loss: 73.8270   time: 0.24s   best: 65.8797
2023-10-06 10:33:04,297:INFO:  Epoch 481/500:  train Loss: 71.9060   val Loss: 71.1425   time: 0.24s   best: 65.8797
2023-10-06 10:33:04,538:INFO:  Epoch 482/500:  train Loss: 68.0273   val Loss: 67.6565   time: 0.24s   best: 65.8797
2023-10-06 10:33:04,787:INFO:  Epoch 483/500:  train Loss: 67.2479   val Loss: 66.3382   time: 0.24s   best: 65.8797
2023-10-06 10:33:05,027:INFO:  Epoch 484/500:  train Loss: 67.1645   val Loss: 67.9265   time: 0.24s   best: 65.8797
2023-10-06 10:33:05,275:INFO:  Epoch 485/500:  train Loss: 68.9719   val Loss: 67.9517   time: 0.25s   best: 65.8797
2023-10-06 10:33:05,520:INFO:  Epoch 486/500:  train Loss: 67.0425   val Loss: 66.8230   time: 0.24s   best: 65.8797
2023-10-06 10:33:05,774:INFO:  Epoch 487/500:  train Loss: 68.2404   val Loss: 68.2729   time: 0.25s   best: 65.8797
2023-10-06 10:33:06,014:INFO:  Epoch 488/500:  train Loss: 67.9050   val Loss: 66.1708   time: 0.24s   best: 65.8797
2023-10-06 10:33:06,265:INFO:  Epoch 489/500:  train Loss: 67.5139   val Loss: 66.5294   time: 0.25s   best: 65.8797
2023-10-06 10:33:06,506:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_8994.pt
2023-10-06 10:33:06,529:INFO:  Epoch 490/500:  train Loss: 65.8131   val Loss: 63.7889   time: 0.24s   best: 63.7889
2023-10-06 10:33:06,773:INFO:  Epoch 491/500:  train Loss: 67.8039   val Loss: 68.6187   time: 0.24s   best: 63.7889
2023-10-06 10:33:07,007:INFO:  Epoch 492/500:  train Loss: 71.3954   val Loss: 71.1865   time: 0.23s   best: 63.7889
2023-10-06 10:33:07,248:INFO:  Epoch 493/500:  train Loss: 71.9388   val Loss: 71.4903   time: 0.24s   best: 63.7889
2023-10-06 10:33:07,483:INFO:  Epoch 494/500:  train Loss: 71.6898   val Loss: 75.3799   time: 0.23s   best: 63.7889
2023-10-06 10:33:07,718:INFO:  Epoch 495/500:  train Loss: 71.6133   val Loss: 67.4673   time: 0.23s   best: 63.7889
2023-10-06 10:33:07,965:INFO:  Epoch 496/500:  train Loss: 69.0486   val Loss: 67.7142   time: 0.24s   best: 63.7889
2023-10-06 10:33:08,200:INFO:  Epoch 497/500:  train Loss: 68.5556   val Loss: 67.3126   time: 0.23s   best: 63.7889
2023-10-06 10:33:08,441:INFO:  Epoch 498/500:  train Loss: 67.3951   val Loss: 66.0780   time: 0.24s   best: 63.7889
2023-10-06 10:33:08,677:INFO:  Epoch 499/500:  train Loss: 66.9243   val Loss: 67.1389   time: 0.24s   best: 63.7889
2023-10-06 10:33:08,958:INFO:  Epoch 500/500:  train Loss: 68.7668   val Loss: 68.0293   time: 0.28s   best: 63.7889
2023-10-06 10:33:08,958:INFO:  -----> Training complete in 2m 19s   best validation loss: 63.7889
 
2023-10-06 10:33:58,364:INFO:  Starting experiment lstm autoencoder (+ hidden vector)
2023-10-06 10:33:58,364:INFO:  Defining the model
2023-10-06 10:33:58,435:INFO:  Reading the dataset
2023-10-06 13:21:27,467:INFO:  Starting experiment lstm autoencoder (+ hidden vector)
2023-10-06 13:21:27,479:INFO:  Defining the model
2023-10-06 13:21:27,779:INFO:  Reading the dataset
2023-10-06 13:59:45,114:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 13:59:45,726:INFO:  Epoch 1/500:  train Loss: 75.0630   val Loss: 67.3821   time: 471.59s   best: 67.3821
2023-10-06 14:07:34,263:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 14:07:35,367:INFO:  Epoch 2/500:  train Loss: 63.4856   val Loss: 60.6919   time: 468.47s   best: 60.6919
2023-10-06 14:16:27,653:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 14:16:28,520:INFO:  Epoch 3/500:  train Loss: 57.1463   val Loss: 53.9869   time: 531.98s   best: 53.9869
2023-10-06 14:25:23,776:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 14:25:24,304:INFO:  Epoch 4/500:  train Loss: 52.0559   val Loss: 50.4997   time: 534.93s   best: 50.4997
2023-10-06 14:34:19,366:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 14:34:19,872:INFO:  Epoch 5/500:  train Loss: 47.6786   val Loss: 47.0573   time: 534.88s   best: 47.0573
2023-10-06 14:43:14,172:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 14:43:14,569:INFO:  Epoch 6/500:  train Loss: 44.0450   val Loss: 43.5376   time: 534.14s   best: 43.5376
2023-10-06 14:52:03,287:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 14:52:04,107:INFO:  Epoch 7/500:  train Loss: 41.2139   val Loss: 41.0772   time: 528.44s   best: 41.0772
2023-10-06 15:01:31,658:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 15:01:32,225:INFO:  Epoch 8/500:  train Loss: 38.9443   val Loss: 39.8534   time: 567.38s   best: 39.8534
2023-10-06 15:10:28,512:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 15:10:29,108:INFO:  Epoch 9/500:  train Loss: 37.2832   val Loss: 37.2573   time: 536.04s   best: 37.2573
2023-10-06 15:19:26,376:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 15:19:26,974:INFO:  Epoch 10/500:  train Loss: 35.7564   val Loss: 35.9524   time: 537.02s   best: 35.9524
2023-10-06 15:28:17,957:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 15:28:18,532:INFO:  Epoch 11/500:  train Loss: 34.6347   val Loss: 34.2797   time: 530.85s   best: 34.2797
2023-10-06 15:37:22,771:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 15:37:23,228:INFO:  Epoch 12/500:  train Loss: 33.5883   val Loss: 33.8515   time: 543.98s   best: 33.8515
2023-10-06 15:46:24,775:INFO:  Epoch 13/500:  train Loss: 32.7684   val Loss: 34.1318   time: 541.52s   best: 33.8515
2023-10-06 15:55:54,319:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 15:55:54,891:INFO:  Epoch 14/500:  train Loss: 32.4456   val Loss: 32.2015   time: 569.26s   best: 32.2015
2023-10-06 16:04:52,400:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 16:04:53,018:INFO:  Epoch 15/500:  train Loss: 31.5010   val Loss: 31.5012   time: 537.22s   best: 31.5012
2023-10-06 16:13:51,705:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 16:13:52,421:INFO:  Epoch 16/500:  train Loss: 30.9782   val Loss: 31.0519   time: 538.40s   best: 31.0519
2023-10-06 16:23:01,268:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 16:23:01,852:INFO:  Epoch 17/500:  train Loss: 30.4597   val Loss: 30.8140   time: 548.62s   best: 30.8140
2023-10-06 16:31:46,340:INFO:  Epoch 18/500:  train Loss: 29.9254   val Loss: 34.4006   time: 524.46s   best: 30.8140
2023-10-06 16:40:35,025:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 16:40:35,679:INFO:  Epoch 19/500:  train Loss: 29.3695   val Loss: 30.0470   time: 528.43s   best: 30.0470
2023-10-06 16:49:29,156:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 16:49:29,688:INFO:  Epoch 20/500:  train Loss: 28.9579   val Loss: 29.1161   time: 533.41s   best: 29.1161
2023-10-06 16:58:04,477:INFO:  Epoch 21/500:  train Loss: 28.8830   val Loss: 29.1233   time: 514.76s   best: 29.1161
2023-10-06 17:06:49,488:INFO:  Epoch 22/500:  train Loss: 28.2499   val Loss: 29.3785   time: 524.98s   best: 29.1161
2023-10-06 17:15:35,275:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 17:15:35,585:INFO:  Epoch 23/500:  train Loss: 28.0061   val Loss: 28.7445   time: 525.58s   best: 28.7445
2023-10-06 17:24:12,894:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 17:24:13,327:INFO:  Epoch 24/500:  train Loss: 28.0836   val Loss: 28.3485   time: 517.14s   best: 28.3485
2023-10-06 17:32:48,542:INFO:  Epoch 25/500:  train Loss: 27.5869   val Loss: 28.6003   time: 515.21s   best: 28.3485
2023-10-06 17:41:25,454:INFO:  Epoch 26/500:  train Loss: 27.1632   val Loss: 28.7636   time: 516.85s   best: 28.3485
2023-10-06 17:50:13,573:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 17:50:14,088:INFO:  Epoch 27/500:  train Loss: 26.9423   val Loss: 28.1743   time: 527.81s   best: 28.1743
2023-10-06 17:59:05,238:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 17:59:05,854:INFO:  Epoch 28/500:  train Loss: 26.8742   val Loss: 27.7617   time: 530.92s   best: 27.7617
2023-10-06 18:07:47,962:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 18:07:48,547:INFO:  Epoch 29/500:  train Loss: 26.3555   val Loss: 27.5566   time: 521.83s   best: 27.5566
2023-10-06 18:16:37,078:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 18:16:37,615:INFO:  Epoch 30/500:  train Loss: 26.1997   val Loss: 27.4897   time: 528.36s   best: 27.4897
2023-10-06 18:25:19,113:INFO:  Epoch 31/500:  train Loss: 26.1197   val Loss: 28.0131   time: 521.50s   best: 27.4897
2023-10-06 18:33:51,229:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 18:33:51,721:INFO:  Epoch 32/500:  train Loss: 25.6773   val Loss: 26.6754   time: 511.96s   best: 26.6754
2023-10-06 18:42:27,686:INFO:  Epoch 33/500:  train Loss: 25.7122   val Loss: 27.1545   time: 515.94s   best: 26.6754
2023-10-06 18:50:58,182:INFO:  Epoch 34/500:  train Loss: 25.4096   val Loss: 27.1916   time: 510.44s   best: 26.6754
2023-10-06 18:59:41,191:INFO:  Epoch 35/500:  train Loss: 25.2377   val Loss: 28.6094   time: 522.96s   best: 26.6754
2023-10-06 19:08:30,342:INFO:  Epoch 36/500:  train Loss: 25.1008   val Loss: 26.6891   time: 529.13s   best: 26.6754
2023-10-06 19:16:54,926:INFO:  Epoch 37/500:  train Loss: 24.9470   val Loss: 27.1123   time: 504.53s   best: 26.6754
2023-10-06 19:25:29,449:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 19:25:29,980:INFO:  Epoch 38/500:  train Loss: 24.7457   val Loss: 25.9532   time: 514.29s   best: 25.9532
2023-10-06 19:33:59,471:INFO:  Epoch 39/500:  train Loss: 24.6512   val Loss: 35.8664   time: 509.46s   best: 25.9532
2023-10-06 19:42:27,892:INFO:  Epoch 40/500:  train Loss: 24.6805   val Loss: 26.1371   time: 508.38s   best: 25.9532
2023-10-06 19:51:05,119:INFO:  Epoch 41/500:  train Loss: 24.3959   val Loss: 25.9661   time: 517.18s   best: 25.9532
2023-10-06 19:59:38,854:INFO:  Epoch 42/500:  train Loss: 24.1717   val Loss: 26.1310   time: 513.69s   best: 25.9532
2023-10-06 20:08:15,581:INFO:  Epoch 43/500:  train Loss: 24.2744   val Loss: 26.2870   time: 516.67s   best: 25.9532
2023-10-06 20:16:52,255:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 20:16:53,017:INFO:  Epoch 44/500:  train Loss: 24.0571   val Loss: 25.6145   time: 516.39s   best: 25.6145
2023-10-06 20:25:39,352:INFO:  Epoch 45/500:  train Loss: 23.9360   val Loss: 26.7274   time: 526.32s   best: 25.6145
2023-10-06 20:34:08,977:INFO:  Epoch 46/500:  train Loss: 23.8306   val Loss: 28.0995   time: 509.59s   best: 25.6145
2023-10-06 20:42:54,371:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 20:42:54,970:INFO:  Epoch 47/500:  train Loss: 23.8394   val Loss: 25.3889   time: 525.22s   best: 25.3889
2023-10-06 20:51:26,931:INFO:  Epoch 48/500:  train Loss: 23.9061   val Loss: 34.6312   time: 511.94s   best: 25.3889
2023-10-06 20:59:56,882:INFO:  Epoch 49/500:  train Loss: 23.7226   val Loss: 25.6881   time: 509.89s   best: 25.3889
2023-10-06 21:08:28,478:INFO:  Epoch 50/500:  train Loss: 23.5902   val Loss: 25.4735   time: 511.44s   best: 25.3889
2023-10-06 21:17:00,039:INFO:  Epoch 51/500:  train Loss: 23.2942   val Loss: 25.5221   time: 511.50s   best: 25.3889
2023-10-06 21:25:30,456:INFO:  Epoch 52/500:  train Loss: 23.2162   val Loss: 26.3213   time: 510.37s   best: 25.3889
2023-10-06 21:34:06,301:INFO:  Epoch 53/500:  train Loss: 23.1547   val Loss: 25.5206   time: 515.77s   best: 25.3889
2023-10-06 21:42:39,270:INFO:  Epoch 54/500:  train Loss: 23.2071   val Loss: 26.4128   time: 512.94s   best: 25.3889
2023-10-06 21:51:13,765:INFO:  Epoch 55/500:  train Loss: 22.9274   val Loss: 26.5524   time: 514.45s   best: 25.3889
2023-10-06 21:59:51,448:INFO:  Epoch 56/500:  train Loss: 23.0807   val Loss: 25.7566   time: 517.64s   best: 25.3889
2023-10-06 22:08:29,106:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 22:08:29,584:INFO:  Epoch 57/500:  train Loss: 22.8515   val Loss: 25.0468   time: 517.39s   best: 25.0468
2023-10-06 22:17:06,965:INFO:  Epoch 58/500:  train Loss: 22.6618   val Loss: 25.4925   time: 517.35s   best: 25.0468
2023-10-06 22:26:20,592:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 22:26:21,050:INFO:  Epoch 59/500:  train Loss: 22.8222   val Loss: 24.5373   time: 553.37s   best: 24.5373
2023-10-06 22:36:23,738:INFO:  Epoch 60/500:  train Loss: 22.6601   val Loss: 24.6631   time: 602.63s   best: 24.5373
2023-10-06 22:46:26,751:INFO:  Epoch 61/500:  train Loss: 22.6179   val Loss: 24.9510   time: 602.98s   best: 24.5373
2023-10-06 22:56:44,053:INFO:  Epoch 62/500:  train Loss: 22.3708   val Loss: 26.0623   time: 617.23s   best: 24.5373
2023-10-06 23:07:20,064:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 23:07:20,844:INFO:  Epoch 63/500:  train Loss: 22.4534   val Loss: 24.4500   time: 635.62s   best: 24.4500
2023-10-06 23:17:42,491:INFO:  Epoch 64/500:  train Loss: 22.5836   val Loss: 25.2915   time: 621.59s   best: 24.4500
2023-10-06 23:28:21,251:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-06 23:28:22,246:INFO:  Epoch 65/500:  train Loss: 22.3482   val Loss: 24.1624   time: 638.39s   best: 24.1624
2023-10-06 23:39:19,614:INFO:  Epoch 66/500:  train Loss: 22.1720   val Loss: 25.1910   time: 657.35s   best: 24.1624
2023-10-06 23:50:12,408:INFO:  Epoch 67/500:  train Loss: 22.1084   val Loss: 24.5456   time: 652.52s   best: 24.1624
2023-10-07 00:00:47,264:INFO:  Epoch 68/500:  train Loss: 22.2018   val Loss: 25.0787   time: 634.78s   best: 24.1624
2023-10-07 00:11:44,047:INFO:  Epoch 69/500:  train Loss: 22.0491   val Loss: 24.7025   time: 656.74s   best: 24.1624
2023-10-07 00:22:33,675:INFO:  Epoch 70/500:  train Loss: 21.9076   val Loss: 24.3440   time: 649.50s   best: 24.1624
2023-10-07 00:33:36,972:INFO:  Epoch 71/500:  train Loss: 21.9640   val Loss: 24.7293   time: 663.26s   best: 24.1624
2023-10-07 00:44:39,457:INFO:  Epoch 72/500:  train Loss: 22.0864   val Loss: 24.5312   time: 662.42s   best: 24.1624
2023-10-07 00:55:29,116:INFO:  Epoch 73/500:  train Loss: 21.8962   val Loss: 24.5566   time: 649.63s   best: 24.1624
2023-10-07 01:05:51,137:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-07 01:05:52,490:INFO:  Epoch 74/500:  train Loss: 21.9614   val Loss: 24.0151   time: 621.58s   best: 24.0151
2023-10-07 01:16:19,627:INFO:  Epoch 75/500:  train Loss: 21.7475   val Loss: 24.8532   time: 627.10s   best: 24.0151
2023-10-07 01:27:07,366:INFO:  Epoch 76/500:  train Loss: 21.6788   val Loss: 24.4593   time: 647.66s   best: 24.0151
2023-10-07 01:37:11,665:INFO:  Epoch 77/500:  train Loss: 21.5698   val Loss: 24.4043   time: 604.22s   best: 24.0151
2023-10-07 01:47:11,157:INFO:  Epoch 78/500:  train Loss: 21.6633   val Loss: 24.0495   time: 599.43s   best: 24.0151
2023-10-07 01:57:02,858:INFO:  Epoch 79/500:  train Loss: 21.6020   val Loss: 24.2610   time: 591.66s   best: 24.0151
2023-10-07 02:06:52,057:INFO:  Epoch 80/500:  train Loss: 21.4737   val Loss: 25.0784   time: 589.15s   best: 24.0151
2023-10-07 02:16:33,004:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-07 02:16:33,753:INFO:  Epoch 81/500:  train Loss: 21.3662   val Loss: 23.9989   time: 580.63s   best: 23.9989
2023-10-07 02:26:18,103:INFO:  Epoch 82/500:  train Loss: 21.2811   val Loss: 24.4899   time: 584.33s   best: 23.9989
2023-10-07 02:35:56,631:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-07 02:35:57,179:INFO:  Epoch 83/500:  train Loss: 21.2911   val Loss: 23.8218   time: 578.34s   best: 23.8218
2023-10-07 02:45:12,887:INFO:  Epoch 84/500:  train Loss: 21.2926   val Loss: 24.1675   time: 555.68s   best: 23.8218
2023-10-07 02:55:08,035:INFO:  Epoch 85/500:  train Loss: 21.2282   val Loss: 24.2172   time: 595.09s   best: 23.8218
2023-10-07 03:05:43,622:INFO:  Epoch 86/500:  train Loss: 21.1434   val Loss: 24.0209   time: 635.53s   best: 23.8218
2023-10-07 03:16:06,011:INFO:  Epoch 87/500:  train Loss: 21.2905   val Loss: 26.1779   time: 622.32s   best: 23.8218
2023-10-07 03:27:09,576:INFO:  Epoch 88/500:  train Loss: 21.5924   val Loss: 24.1991   time: 663.53s   best: 23.8218
2023-10-07 03:38:02,242:INFO:  Epoch 89/500:  train Loss: 21.1208   val Loss: 24.3083   time: 652.62s   best: 23.8218
2023-10-07 03:49:20,325:INFO:  Epoch 90/500:  train Loss: 21.0801   val Loss: 24.4485   time: 678.05s   best: 23.8218
2023-10-07 04:00:13,364:INFO:  Epoch 91/500:  train Loss: 21.2314   val Loss: 24.4556   time: 652.99s   best: 23.8218
2023-10-07 04:10:57,055:INFO:  Epoch 92/500:  train Loss: 21.0035   val Loss: 24.2376   time: 643.62s   best: 23.8218
2023-10-07 04:20:56,400:INFO:  Epoch 93/500:  train Loss: 21.0448   val Loss: 24.1770   time: 599.31s   best: 23.8218
2023-10-07 04:30:48,643:INFO:  Epoch 94/500:  train Loss: 21.1619   val Loss: 24.0988   time: 592.20s   best: 23.8218
2023-10-07 04:40:25,058:INFO:  Epoch 95/500:  train Loss: 21.1743   val Loss: 24.7864   time: 576.37s   best: 23.8218
2023-10-07 04:49:57,099:INFO:  Epoch 96/500:  train Loss: 21.1706   val Loss: 24.1956   time: 571.92s   best: 23.8218
2023-10-07 04:58:57,255:INFO:  Epoch 97/500:  train Loss: 20.9684   val Loss: 24.3954   time: 540.13s   best: 23.8218
2023-10-07 05:07:51,601:INFO:  Epoch 98/500:  train Loss: 20.9973   val Loss: 24.0466   time: 534.30s   best: 23.8218
2023-10-07 05:16:36,859:INFO:  Epoch 99/500:  train Loss: 21.0094   val Loss: 24.1924   time: 525.21s   best: 23.8218
2023-10-07 05:24:34,370:INFO:  Epoch 100/500:  train Loss: 20.9112   val Loss: 25.0575   time: 477.48s   best: 23.8218
2023-10-07 05:33:15,439:INFO:  Epoch 101/500:  train Loss: 20.7702   val Loss: 23.8858   time: 521.06s   best: 23.8218
2023-10-07 05:41:51,470:INFO:  Epoch 102/500:  train Loss: 20.6781   val Loss: 24.1566   time: 515.98s   best: 23.8218
2023-10-07 05:50:36,501:INFO:  Epoch 103/500:  train Loss: 20.8316   val Loss: 24.2643   time: 524.98s   best: 23.8218
2023-10-07 05:59:02,999:INFO:  Epoch 104/500:  train Loss: 20.5747   val Loss: 25.9591   time: 506.45s   best: 23.8218
2023-10-07 06:07:38,579:INFO:  Epoch 105/500:  train Loss: 20.6975   val Loss: 27.8539   time: 515.55s   best: 23.8218
2023-10-07 06:16:10,344:INFO:  Epoch 106/500:  train Loss: 20.6522   val Loss: 24.3144   time: 511.73s   best: 23.8218
2023-10-07 06:24:45,209:INFO:  Epoch 107/500:  train Loss: 20.5240   val Loss: 24.0039   time: 514.83s   best: 23.8218
2023-10-07 06:33:23,845:INFO:  Epoch 108/500:  train Loss: 20.5222   val Loss: 24.0171   time: 518.57s   best: 23.8218
2023-10-07 06:41:58,460:INFO:  Epoch 109/500:  train Loss: 20.4898   val Loss: 24.8437   time: 514.56s   best: 23.8218
2023-10-07 06:50:31,144:INFO:  Epoch 110/500:  train Loss: 20.5511   val Loss: 24.2485   time: 512.61s   best: 23.8218
2023-10-07 06:59:02,269:INFO:  Epoch 111/500:  train Loss: 20.4309   val Loss: 23.9772   time: 511.11s   best: 23.8218
2023-10-07 07:07:33,309:INFO:  Epoch 112/500:  train Loss: 20.4465   val Loss: 24.0701   time: 510.99s   best: 23.8218
2023-10-07 07:16:03,503:INFO:  Epoch 113/500:  train Loss: 20.3773   val Loss: 24.2474   time: 510.16s   best: 23.8218
2023-10-07 07:24:40,314:INFO:  Epoch 114/500:  train Loss: 20.6181   val Loss: 24.2752   time: 516.78s   best: 23.8218
2023-10-07 07:33:13,477:INFO:  Epoch 115/500:  train Loss: 20.6014   val Loss: 23.9294   time: 513.13s   best: 23.8218
2023-10-07 07:41:43,719:INFO:  Epoch 116/500:  train Loss: 20.2326   val Loss: 23.8287   time: 510.20s   best: 23.8218
2023-10-07 07:50:18,511:INFO:  Epoch 117/500:  train Loss: 20.4740   val Loss: 23.9913   time: 514.76s   best: 23.8218
2023-10-07 07:58:56,328:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-07 07:58:56,730:INFO:  Epoch 118/500:  train Loss: 20.2115   val Loss: 23.5417   time: 517.64s   best: 23.5417
2023-10-07 08:07:25,057:INFO:  Epoch 119/500:  train Loss: 20.2224   val Loss: 23.8071   time: 508.31s   best: 23.5417
2023-10-07 08:15:58,563:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-07 08:15:58,930:INFO:  Epoch 120/500:  train Loss: 20.2387   val Loss: 23.4742   time: 513.35s   best: 23.4742
2023-10-07 08:24:30,120:INFO:  Epoch 121/500:  train Loss: 20.3835   val Loss: 24.0589   time: 511.19s   best: 23.4742
2023-10-07 08:33:03,174:INFO:  Epoch 122/500:  train Loss: 20.1052   val Loss: 24.4214   time: 513.03s   best: 23.4742
2023-10-07 08:41:35,672:INFO:  Epoch 123/500:  train Loss: 20.1013   val Loss: 23.5915   time: 512.46s   best: 23.4742
2023-10-07 08:50:09,945:INFO:  Epoch 124/500:  train Loss: 20.1093   val Loss: 24.1849   time: 514.22s   best: 23.4742
2023-10-07 08:58:41,667:INFO:  Epoch 125/500:  train Loss: 20.1343   val Loss: 24.0505   time: 511.68s   best: 23.4742
2023-10-07 09:07:17,947:INFO:  Epoch 126/500:  train Loss: 20.0476   val Loss: 23.9126   time: 516.23s   best: 23.4742
2023-10-07 09:15:55,436:INFO:  Epoch 127/500:  train Loss: 19.9705   val Loss: 23.9790   time: 517.45s   best: 23.4742
2023-10-07 09:24:32,720:INFO:  Epoch 128/500:  train Loss: 19.9300   val Loss: 23.8847   time: 517.24s   best: 23.4742
2023-10-07 09:33:03,652:INFO:  Epoch 129/500:  train Loss: 19.8465   val Loss: 33.1897   time: 510.91s   best: 23.4742
2023-10-07 09:41:35,223:INFO:  Epoch 130/500:  train Loss: 19.9414   val Loss: 26.5908   time: 511.54s   best: 23.4742
2023-10-07 09:49:48,023:INFO:  Epoch 131/500:  train Loss: 19.9536   val Loss: 23.8600   time: 492.75s   best: 23.4742
2023-10-07 09:58:17,173:INFO:  Epoch 132/500:  train Loss: 19.8330   val Loss: 23.6339   time: 509.14s   best: 23.4742
2023-10-07 10:06:21,231:INFO:  Epoch 133/500:  train Loss: 19.8579   val Loss: 23.7665   time: 484.02s   best: 23.4742
2023-10-07 10:14:38,100:INFO:  Epoch 134/500:  train Loss: 19.8829   val Loss: 23.6611   time: 496.81s   best: 23.4742
2023-10-07 10:22:55,779:INFO:  Epoch 135/500:  train Loss: 19.8467   val Loss: 23.6741   time: 497.65s   best: 23.4742
2023-10-07 10:31:16,218:INFO:  Epoch 136/500:  train Loss: 19.8223   val Loss: 23.6765   time: 500.42s   best: 23.4742
2023-10-07 10:39:29,447:INFO:  Epoch 137/500:  train Loss: 19.7758   val Loss: 23.8474   time: 493.21s   best: 23.4742
2023-10-07 10:47:47,079:INFO:  Epoch 138/500:  train Loss: 20.2290   val Loss: 23.8437   time: 497.59s   best: 23.4742
2023-10-07 10:56:09,864:INFO:  Epoch 139/500:  train Loss: 19.9257   val Loss: 23.5034   time: 502.75s   best: 23.4742
2023-10-07 11:04:38,977:INFO:  Epoch 140/500:  train Loss: 19.7265   val Loss: 23.9387   time: 509.08s   best: 23.4742
2023-10-07 11:12:52,875:INFO:  Epoch 141/500:  train Loss: 19.6786   val Loss: 24.6042   time: 493.85s   best: 23.4742
2023-10-07 11:20:59,432:INFO:  Epoch 142/500:  train Loss: 19.6273   val Loss: 23.6302   time: 486.53s   best: 23.4742
2023-10-07 11:29:05,191:INFO:  Epoch 143/500:  train Loss: 19.6168   val Loss: 23.7605   time: 485.74s   best: 23.4742
2023-10-07 11:37:11,590:INFO:  Epoch 144/500:  train Loss: 19.6239   val Loss: 23.7946   time: 486.35s   best: 23.4742
2023-10-07 11:45:15,117:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-07 11:45:15,586:INFO:  Epoch 145/500:  train Loss: 19.5566   val Loss: 23.2676   time: 483.40s   best: 23.2676
2023-10-07 11:53:37,520:INFO:  Epoch 146/500:  train Loss: 20.0543   val Loss: 23.8033   time: 501.92s   best: 23.2676
2023-10-07 12:02:04,558:INFO:  Epoch 147/500:  train Loss: 19.9078   val Loss: 23.4984   time: 507.01s   best: 23.2676
2023-10-07 12:10:24,834:INFO:  Epoch 148/500:  train Loss: 19.5227   val Loss: 23.6193   time: 500.22s   best: 23.2676
2023-10-07 12:18:41,021:INFO:  Epoch 149/500:  train Loss: 19.6895   val Loss: 23.6531   time: 496.12s   best: 23.2676
2023-10-07 12:27:13,905:INFO:  Epoch 150/500:  train Loss: 19.7049   val Loss: 23.4359   time: 512.82s   best: 23.2676
2023-10-07 12:35:40,625:INFO:  Epoch 151/500:  train Loss: 19.4554   val Loss: 23.9006   time: 506.70s   best: 23.2676
2023-10-07 12:44:05,520:INFO:  Epoch 152/500:  train Loss: 19.6138   val Loss: 23.5400   time: 504.88s   best: 23.2676
2023-10-07 12:52:28,679:INFO:  Epoch 153/500:  train Loss: 19.5453   val Loss: 24.7421   time: 503.14s   best: 23.2676
2023-10-07 13:00:47,746:INFO:  Epoch 154/500:  train Loss: 19.4002   val Loss: 23.6798   time: 499.03s   best: 23.2676
2023-10-07 13:09:05,721:INFO:  Epoch 155/500:  train Loss: 19.4070   val Loss: 24.5410   time: 497.94s   best: 23.2676
2023-10-07 13:17:31,372:INFO:  Epoch 156/500:  train Loss: 19.7599   val Loss: 23.5780   time: 505.62s   best: 23.2676
2023-10-07 13:25:59,480:INFO:  Epoch 157/500:  train Loss: 19.3883   val Loss: 23.9901   time: 508.07s   best: 23.2676
2023-10-07 13:34:22,832:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-07 13:34:23,252:INFO:  Epoch 158/500:  train Loss: 19.3716   val Loss: 23.2577   time: 503.24s   best: 23.2577
2023-10-07 13:42:48,384:INFO:  Epoch 159/500:  train Loss: 19.3748   val Loss: 28.2923   time: 505.12s   best: 23.2577
2023-10-07 13:51:17,642:INFO:  Epoch 160/500:  train Loss: 19.5170   val Loss: 23.4352   time: 509.23s   best: 23.2577
2023-10-07 13:59:47,105:INFO:  Epoch 161/500:  train Loss: 19.4563   val Loss: 23.7151   time: 509.45s   best: 23.2577
2023-10-07 14:08:15,765:INFO:  Epoch 162/500:  train Loss: 19.5188   val Loss: 23.4563   time: 508.64s   best: 23.2577
2023-10-07 14:16:44,149:INFO:  Epoch 163/500:  train Loss: 19.2081   val Loss: 23.6171   time: 508.32s   best: 23.2577
2023-10-07 14:25:14,648:INFO:  Epoch 164/500:  train Loss: 19.2698   val Loss: 23.2896   time: 510.47s   best: 23.2577
2023-10-07 14:33:26,691:INFO:  Epoch 165/500:  train Loss: 19.2575   val Loss: 23.6595   time: 492.02s   best: 23.2577
2023-10-07 14:41:51,518:INFO:  Epoch 166/500:  train Loss: 19.2804   val Loss: 23.9516   time: 504.80s   best: 23.2577
2023-10-07 14:49:52,877:INFO:  Epoch 167/500:  train Loss: 19.1552   val Loss: 23.5196   time: 481.32s   best: 23.2577
2023-10-07 14:58:12,760:INFO:  Epoch 168/500:  train Loss: 19.4211   val Loss: 23.4968   time: 499.85s   best: 23.2577
2023-10-07 15:06:29,684:INFO:  Epoch 169/500:  train Loss: 19.2661   val Loss: 23.6497   time: 496.91s   best: 23.2577
2023-10-07 15:15:01,446:INFO:  Epoch 170/500:  train Loss: 19.2728   val Loss: 27.0053   time: 511.73s   best: 23.2577
2023-10-07 15:23:35,958:INFO:  Epoch 171/500:  train Loss: 19.1923   val Loss: 23.6281   time: 514.50s   best: 23.2577
2023-10-07 15:32:12,898:INFO:  Epoch 172/500:  train Loss: 19.1056   val Loss: 23.8166   time: 516.90s   best: 23.2577
2023-10-07 15:40:51,578:INFO:  Epoch 173/500:  train Loss: 19.2751   val Loss: 23.8791   time: 518.63s   best: 23.2577
2023-10-07 15:49:13,412:INFO:  Epoch 174/500:  train Loss: 19.2175   val Loss: 23.3980   time: 501.79s   best: 23.2577
2023-10-07 15:57:25,459:INFO:  Epoch 175/500:  train Loss: 19.1997   val Loss: 23.5746   time: 492.01s   best: 23.2577
2023-10-07 16:05:52,786:INFO:  Epoch 176/500:  train Loss: 19.0517   val Loss: 23.6022   time: 507.28s   best: 23.2577
2023-10-07 16:14:09,009:INFO:  Epoch 177/500:  train Loss: 19.0231   val Loss: 28.6293   time: 496.18s   best: 23.2577
2023-10-07 16:22:28,594:INFO:  Epoch 178/500:  train Loss: 19.1316   val Loss: 23.5061   time: 499.56s   best: 23.2577
2023-10-07 16:30:49,285:INFO:  Epoch 179/500:  train Loss: 19.0817   val Loss: 23.8261   time: 500.68s   best: 23.2577
2023-10-07 16:39:13,451:INFO:  Epoch 180/500:  train Loss: 19.1194   val Loss: 23.8543   time: 504.12s   best: 23.2577
2023-10-07 16:47:35,337:INFO:  Epoch 181/500:  train Loss: 19.1201   val Loss: 23.3342   time: 501.87s   best: 23.2577
2023-10-07 16:55:59,358:INFO:  Epoch 182/500:  train Loss: 19.0622   val Loss: 24.1934   time: 503.97s   best: 23.2577
2023-10-07 17:04:19,754:INFO:  Epoch 183/500:  train Loss: 19.1357   val Loss: 27.8055   time: 500.37s   best: 23.2577
2023-10-07 17:12:40,487:INFO:  Epoch 184/500:  train Loss: 19.0260   val Loss: 24.7889   time: 500.68s   best: 23.2577
2023-10-07 17:21:04,565:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-07 17:21:05,156:INFO:  Epoch 185/500:  train Loss: 19.0185   val Loss: 23.1326   time: 503.89s   best: 23.1326
2023-10-07 17:29:25,595:INFO:  Epoch 186/500:  train Loss: 18.9366   val Loss: 24.2362   time: 500.43s   best: 23.1326
2023-10-07 17:37:29,371:INFO:  Epoch 187/500:  train Loss: 18.9163   val Loss: 23.7291   time: 483.74s   best: 23.1326
2023-10-07 17:45:41,799:INFO:  Epoch 188/500:  train Loss: 19.0847   val Loss: 23.7744   time: 492.41s   best: 23.1326
2023-10-07 17:53:55,405:INFO:  Epoch 189/500:  train Loss: 18.9427   val Loss: 23.5843   time: 493.59s   best: 23.1326
2023-10-07 18:02:07,618:INFO:  Epoch 190/500:  train Loss: 18.8577   val Loss: 23.9130   time: 492.19s   best: 23.1326
2023-10-07 18:10:26,798:INFO:  Epoch 191/500:  train Loss: 18.9109   val Loss: 27.9642   time: 499.16s   best: 23.1326
2023-10-07 18:18:45,594:INFO:  Epoch 192/500:  train Loss: 18.9054   val Loss: 23.7277   time: 498.76s   best: 23.1326
2023-10-07 18:26:59,343:INFO:  Epoch 193/500:  train Loss: 18.8233   val Loss: 23.7101   time: 493.72s   best: 23.1326
2023-10-07 18:35:13,706:INFO:  Epoch 194/500:  train Loss: 18.8857   val Loss: 23.6049   time: 494.32s   best: 23.1326
2023-10-07 18:43:33,678:INFO:  Epoch 195/500:  train Loss: 18.8428   val Loss: 23.2667   time: 499.94s   best: 23.1326
2023-10-07 18:51:41,294:INFO:  Epoch 196/500:  train Loss: 18.9944   val Loss: 23.6017   time: 487.59s   best: 23.1326
2023-10-07 19:00:01,619:INFO:  Epoch 197/500:  train Loss: 18.8932   val Loss: 23.5376   time: 500.32s   best: 23.1326
2023-10-07 19:08:14,705:INFO:  Epoch 198/500:  train Loss: 19.1257   val Loss: 23.4173   time: 493.05s   best: 23.1326
2023-10-07 19:16:31,755:INFO:  Epoch 199/500:  train Loss: 18.7670   val Loss: 23.2289   time: 497.00s   best: 23.1326
2023-10-07 19:24:45,011:INFO:  Epoch 200/500:  train Loss: 18.7859   val Loss: 24.0829   time: 493.23s   best: 23.1326
2023-10-07 19:32:42,747:INFO:  Epoch 201/500:  train Loss: 18.9341   val Loss: 23.4804   time: 477.71s   best: 23.1326
2023-10-07 19:41:03,270:INFO:  Epoch 202/500:  train Loss: 18.8279   val Loss: 23.6738   time: 500.52s   best: 23.1326
2023-10-07 19:49:11,519:INFO:  Epoch 203/500:  train Loss: 18.7635   val Loss: 24.0139   time: 488.24s   best: 23.1326
2023-10-07 19:57:17,665:INFO:  Epoch 204/500:  train Loss: 18.9927   val Loss: 23.4736   time: 486.12s   best: 23.1326
2023-10-07 20:05:27,270:INFO:  Epoch 205/500:  train Loss: 18.7371   val Loss: 23.5114   time: 489.57s   best: 23.1326
2023-10-07 20:13:35,932:INFO:  Epoch 206/500:  train Loss: 18.7749   val Loss: 23.3864   time: 488.62s   best: 23.1326
2023-10-07 20:21:43,156:INFO:  Epoch 207/500:  train Loss: 18.8104   val Loss: 23.4217   time: 487.21s   best: 23.1326
2023-10-07 20:29:50,193:INFO:  Epoch 208/500:  train Loss: 18.7042   val Loss: 23.4761   time: 487.02s   best: 23.1326
2023-10-07 20:37:45,607:INFO:  Epoch 209/500:  train Loss: 18.7611   val Loss: 24.2983   time: 475.39s   best: 23.1326
2023-10-07 20:45:53,724:INFO:  Epoch 210/500:  train Loss: 18.7231   val Loss: 31.2995   time: 488.10s   best: 23.1326
2023-10-07 20:54:05,629:INFO:  Epoch 211/500:  train Loss: 18.7575   val Loss: 23.6304   time: 491.87s   best: 23.1326
2023-10-07 21:02:13,970:INFO:  Epoch 212/500:  train Loss: 18.6381   val Loss: 23.6260   time: 488.32s   best: 23.1326
2023-10-07 21:10:27,050:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-07 21:10:27,413:INFO:  Epoch 213/500:  train Loss: 18.6797   val Loss: 23.0233   time: 493.02s   best: 23.0233
2023-10-07 21:18:33,853:INFO:  Epoch 214/500:  train Loss: 18.6811   val Loss: 23.4783   time: 486.42s   best: 23.0233
2023-10-07 21:25:22,976:INFO:  Starting experiment lstm autoencoder debug
2023-10-07 21:25:22,983:INFO:  Defining the model
2023-10-07 21:25:23,031:INFO:  Reading the dataset
2023-10-07 21:25:36,722:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:36,782:INFO:  Epoch 1/500:  train Loss: 99.4516   val Loss: 100.0962   time: 4.69s   best: 100.0962
2023-10-07 21:25:37,049:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:37,073:INFO:  Epoch 2/500:  train Loss: 99.3792   val Loss: 100.0714   time: 0.26s   best: 100.0714
2023-10-07 21:25:37,361:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:37,486:INFO:  Epoch 3/500:  train Loss: 99.0645   val Loss: 99.0020   time: 0.28s   best: 99.0020
2023-10-07 21:25:37,750:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:37,824:INFO:  Epoch 4/500:  train Loss: 98.3152   val Loss: 96.0003   time: 0.26s   best: 96.0003
2023-10-07 21:25:38,095:INFO:  Epoch 5/500:  train Loss: 98.9502   val Loss: 98.0837   time: 0.27s   best: 96.0003
2023-10-07 21:25:38,390:INFO:  Epoch 6/500:  train Loss: 98.7305   val Loss: 99.4901   time: 0.29s   best: 96.0003
2023-10-07 21:25:38,653:INFO:  Epoch 7/500:  train Loss: 98.2119   val Loss: 98.3049   time: 0.26s   best: 96.0003
2023-10-07 21:25:38,940:INFO:  Epoch 8/500:  train Loss: 96.5489   val Loss: 96.4349   time: 0.28s   best: 96.0003
2023-10-07 21:25:39,205:INFO:  Epoch 9/500:  train Loss: 95.9021   val Loss: 96.0233   time: 0.26s   best: 96.0003
2023-10-07 21:25:39,503:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:39,580:INFO:  Epoch 10/500:  train Loss: 95.2345   val Loss: 95.6249   time: 0.29s   best: 95.6249
2023-10-07 21:25:39,845:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:39,982:INFO:  Epoch 11/500:  train Loss: 95.2872   val Loss: 95.0318   time: 0.26s   best: 95.0318
2023-10-07 21:25:40,261:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:40,283:INFO:  Epoch 12/500:  train Loss: 94.5333   val Loss: 94.6910   time: 0.27s   best: 94.6910
2023-10-07 21:25:40,563:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:40,595:INFO:  Epoch 13/500:  train Loss: 94.6697   val Loss: 94.4435   time: 0.27s   best: 94.4435
2023-10-07 21:25:40,869:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:41,247:INFO:  Epoch 14/500:  train Loss: 93.9354   val Loss: 94.4416   time: 0.27s   best: 94.4416
2023-10-07 21:25:41,516:INFO:  Epoch 15/500:  train Loss: 94.2397   val Loss: 94.6064   time: 0.27s   best: 94.4416
2023-10-07 21:25:41,788:INFO:  Epoch 16/500:  train Loss: 94.8373   val Loss: 94.5329   time: 0.27s   best: 94.4416
2023-10-07 21:25:42,071:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:42,095:INFO:  Epoch 17/500:  train Loss: 94.0694   val Loss: 94.3296   time: 0.28s   best: 94.3296
2023-10-07 21:25:42,377:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:42,411:INFO:  Epoch 18/500:  train Loss: 93.7392   val Loss: 93.6144   time: 0.27s   best: 93.6144
2023-10-07 21:25:42,697:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:43,015:INFO:  Epoch 19/500:  train Loss: 93.5286   val Loss: 91.1116   time: 0.28s   best: 91.1116
2023-10-07 21:25:43,287:INFO:  Epoch 20/500:  train Loss: 93.5722   val Loss: 94.0405   time: 0.27s   best: 91.1116
2023-10-07 21:25:43,573:INFO:  Epoch 21/500:  train Loss: 93.8097   val Loss: 94.2064   time: 0.28s   best: 91.1116
2023-10-07 21:25:43,842:INFO:  Epoch 22/500:  train Loss: 94.0587   val Loss: 94.2418   time: 0.26s   best: 91.1116
2023-10-07 21:25:44,131:INFO:  Epoch 23/500:  train Loss: 93.7642   val Loss: 94.2334   time: 0.29s   best: 91.1116
2023-10-07 21:25:44,407:INFO:  Epoch 24/500:  train Loss: 94.1343   val Loss: 94.2251   time: 0.27s   best: 91.1116
2023-10-07 21:25:44,692:INFO:  Epoch 25/500:  train Loss: 93.4078   val Loss: 94.0898   time: 0.28s   best: 91.1116
2023-10-07 21:25:44,955:INFO:  Epoch 26/500:  train Loss: 92.9997   val Loss: 91.9487   time: 0.26s   best: 91.1116
2023-10-07 21:25:45,243:INFO:  Epoch 27/500:  train Loss: 93.8383   val Loss: 91.6420   time: 0.28s   best: 91.1116
2023-10-07 21:25:45,524:INFO:  Epoch 28/500:  train Loss: 93.3287   val Loss: 94.0367   time: 0.28s   best: 91.1116
2023-10-07 21:25:45,816:INFO:  Epoch 29/500:  train Loss: 94.0147   val Loss: 94.2172   time: 0.29s   best: 91.1116
2023-10-07 21:25:46,078:INFO:  Epoch 30/500:  train Loss: 94.0641   val Loss: 94.2565   time: 0.26s   best: 91.1116
2023-10-07 21:25:46,377:INFO:  Epoch 31/500:  train Loss: 94.1604   val Loss: 94.2651   time: 0.30s   best: 91.1116
2023-10-07 21:25:46,652:INFO:  Epoch 32/500:  train Loss: 94.4612   val Loss: 94.2294   time: 0.26s   best: 91.1116
2023-10-07 21:25:46,929:INFO:  Epoch 33/500:  train Loss: 93.7448   val Loss: 94.1054   time: 0.27s   best: 91.1116
2023-10-07 21:25:47,209:INFO:  Epoch 34/500:  train Loss: 93.6197   val Loss: 93.9123   time: 0.28s   best: 91.1116
2023-10-07 21:25:47,484:INFO:  Epoch 35/500:  train Loss: 93.7826   val Loss: 93.6897   time: 0.27s   best: 91.1116
2023-10-07 21:25:47,783:INFO:  Epoch 36/500:  train Loss: 93.2472   val Loss: 93.5064   time: 0.29s   best: 91.1116
2023-10-07 21:25:48,047:INFO:  Epoch 37/500:  train Loss: 93.2768   val Loss: 93.3872   time: 0.26s   best: 91.1116
2023-10-07 21:25:48,346:INFO:  Epoch 38/500:  train Loss: 92.9133   val Loss: 93.4695   time: 0.29s   best: 91.1116
2023-10-07 21:25:48,618:INFO:  Epoch 39/500:  train Loss: 93.2719   val Loss: 93.4355   time: 0.27s   best: 91.1116
2023-10-07 21:25:48,902:INFO:  Epoch 40/500:  train Loss: 93.1638   val Loss: 93.3219   time: 0.28s   best: 91.1116
2023-10-07 21:25:49,165:INFO:  Epoch 41/500:  train Loss: 93.2924   val Loss: 93.1877   time: 0.26s   best: 91.1116
2023-10-07 21:25:49,456:INFO:  Epoch 42/500:  train Loss: 92.7255   val Loss: 92.9220   time: 0.29s   best: 91.1116
2023-10-07 21:25:49,721:INFO:  Epoch 43/500:  train Loss: 92.2196   val Loss: 92.2970   time: 0.26s   best: 91.1116
2023-10-07 21:25:50,015:INFO:  Epoch 44/500:  train Loss: 91.8411   val Loss: 91.5086   time: 0.29s   best: 91.1116
2023-10-07 21:25:50,286:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:50,314:INFO:  Epoch 45/500:  train Loss: 91.1313   val Loss: 90.8258   time: 0.27s   best: 90.8258
2023-10-07 21:25:50,608:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:50,668:INFO:  Epoch 46/500:  train Loss: 90.5035   val Loss: 90.2593   time: 0.29s   best: 90.2593
2023-10-07 21:25:50,949:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:51,347:INFO:  Epoch 47/500:  train Loss: 89.8334   val Loss: 89.3458   time: 0.28s   best: 89.3458
2023-10-07 21:25:51,621:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:51,703:INFO:  Epoch 48/500:  train Loss: 89.2460   val Loss: 89.0322   time: 0.27s   best: 89.0322
2023-10-07 21:25:51,982:INFO:  Epoch 49/500:  train Loss: 88.7837   val Loss: 89.0352   time: 0.28s   best: 89.0322
2023-10-07 21:25:52,260:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:52,351:INFO:  Epoch 50/500:  train Loss: 89.4607   val Loss: 88.5645   time: 0.27s   best: 88.5645
2023-10-07 21:25:52,634:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:52,696:INFO:  Epoch 51/500:  train Loss: 88.4052   val Loss: 88.1432   time: 0.28s   best: 88.1432
2023-10-07 21:25:52,987:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:53,064:INFO:  Epoch 52/500:  train Loss: 87.9989   val Loss: 88.0240   time: 0.29s   best: 88.0240
2023-10-07 21:25:53,332:INFO:  Epoch 53/500:  train Loss: 88.7795   val Loss: 88.5296   time: 0.26s   best: 88.0240
2023-10-07 21:25:53,615:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:53,703:INFO:  Epoch 54/500:  train Loss: 88.6721   val Loss: 87.9836   time: 0.28s   best: 87.9836
2023-10-07 21:25:53,968:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:54,028:INFO:  Epoch 55/500:  train Loss: 88.2984   val Loss: 87.2445   time: 0.26s   best: 87.2445
2023-10-07 21:25:54,306:INFO:  Epoch 56/500:  train Loss: 87.9187   val Loss: 87.4883   time: 0.27s   best: 87.2445
2023-10-07 21:25:54,597:INFO:  Epoch 57/500:  train Loss: 88.3150   val Loss: 87.3603   time: 0.29s   best: 87.2445
2023-10-07 21:25:54,856:INFO:  Epoch 58/500:  train Loss: 88.3378   val Loss: 88.3907   time: 0.25s   best: 87.2445
2023-10-07 21:25:55,151:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:55,257:INFO:  Epoch 59/500:  train Loss: 88.3501   val Loss: 87.2144   time: 0.29s   best: 87.2144
2023-10-07 21:25:55,527:INFO:  Epoch 60/500:  train Loss: 87.9604   val Loss: 87.4668   time: 0.27s   best: 87.2144
2023-10-07 21:25:55,809:INFO:  Epoch 61/500:  train Loss: 87.7398   val Loss: 87.7778   time: 0.28s   best: 87.2144
2023-10-07 21:25:56,100:INFO:  Epoch 62/500:  train Loss: 88.5315   val Loss: 87.9754   time: 0.29s   best: 87.2144
2023-10-07 21:25:56,377:INFO:  Epoch 63/500:  train Loss: 87.6928   val Loss: 87.7144   time: 0.27s   best: 87.2144
2023-10-07 21:25:56,662:INFO:  Epoch 64/500:  train Loss: 87.4956   val Loss: 87.4732   time: 0.28s   best: 87.2144
2023-10-07 21:25:56,927:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:57,280:INFO:  Epoch 65/500:  train Loss: 87.7619   val Loss: 87.2071   time: 0.26s   best: 87.2071
2023-10-07 21:25:57,567:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:57,618:INFO:  Epoch 66/500:  train Loss: 87.5556   val Loss: 87.2012   time: 0.28s   best: 87.2012
2023-10-07 21:25:57,895:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:58,015:INFO:  Epoch 67/500:  train Loss: 88.1051   val Loss: 87.1174   time: 0.27s   best: 87.1174
2023-10-07 21:25:58,325:INFO:  Epoch 68/500:  train Loss: 88.0604   val Loss: 87.8734   time: 0.30s   best: 87.1174
2023-10-07 21:25:58,817:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:58,845:INFO:  Epoch 69/500:  train Loss: 88.0719   val Loss: 86.7811   time: 0.49s   best: 86.7811
2023-10-07 21:25:59,119:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:59,324:INFO:  Epoch 70/500:  train Loss: 86.8844   val Loss: 86.1270   time: 0.27s   best: 86.1270
2023-10-07 21:25:59,597:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:25:59,719:INFO:  Epoch 71/500:  train Loss: 86.4923   val Loss: 86.0508   time: 0.27s   best: 86.0508
2023-10-07 21:25:59,982:INFO:  Epoch 72/500:  train Loss: 86.7298   val Loss: 86.5312   time: 0.26s   best: 86.0508
2023-10-07 21:26:00,271:INFO:  Epoch 73/500:  train Loss: 87.1593   val Loss: 86.5307   time: 0.28s   best: 86.0508
2023-10-07 21:26:00,540:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:00,564:INFO:  Epoch 74/500:  train Loss: 86.4785   val Loss: 85.5177   time: 0.26s   best: 85.5177
2023-10-07 21:26:00,885:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:00,992:INFO:  Epoch 75/500:  train Loss: 86.3864   val Loss: 85.3023   time: 0.32s   best: 85.3023
2023-10-07 21:26:01,268:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:01,331:INFO:  Epoch 76/500:  train Loss: 85.8991   val Loss: 85.0927   time: 0.27s   best: 85.0927
2023-10-07 21:26:01,599:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:01,684:INFO:  Epoch 77/500:  train Loss: 86.5722   val Loss: 85.0420   time: 0.26s   best: 85.0420
2023-10-07 21:26:01,964:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:02,065:INFO:  Epoch 78/500:  train Loss: 85.8600   val Loss: 84.8204   time: 0.27s   best: 84.8204
2023-10-07 21:26:02,335:INFO:  Epoch 79/500:  train Loss: 85.8130   val Loss: 85.4246   time: 0.27s   best: 84.8204
2023-10-07 21:26:02,620:INFO:  Epoch 80/500:  train Loss: 86.5314   val Loss: 85.9935   time: 0.28s   best: 84.8204
2023-10-07 21:26:02,920:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:02,985:INFO:  Epoch 81/500:  train Loss: 85.4196   val Loss: 84.6122   time: 0.29s   best: 84.6122
2023-10-07 21:26:03,250:INFO:  Epoch 82/500:  train Loss: 85.9115   val Loss: 85.1896   time: 0.26s   best: 84.6122
2023-10-07 21:26:03,546:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:03,575:INFO:  Epoch 83/500:  train Loss: 85.1077   val Loss: 84.0727   time: 0.29s   best: 84.0727
2023-10-07 21:26:03,840:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:03,943:INFO:  Epoch 84/500:  train Loss: 84.6816   val Loss: 83.6028   time: 0.26s   best: 83.6028
2023-10-07 21:26:04,218:INFO:  Epoch 85/500:  train Loss: 84.2283   val Loss: 83.9558   time: 0.27s   best: 83.6028
2023-10-07 21:26:04,507:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:04,547:INFO:  Epoch 86/500:  train Loss: 84.3282   val Loss: 83.2328   time: 0.28s   best: 83.2328
2023-10-07 21:26:04,812:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:04,924:INFO:  Epoch 87/500:  train Loss: 84.4637   val Loss: 83.1026   time: 0.26s   best: 83.1026
2023-10-07 21:26:05,206:INFO:  Epoch 88/500:  train Loss: 83.7935   val Loss: 83.2056   time: 0.28s   best: 83.1026
2023-10-07 21:26:05,497:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:05,577:INFO:  Epoch 89/500:  train Loss: 83.5922   val Loss: 82.3534   time: 0.27s   best: 82.3534
2023-10-07 21:26:05,848:INFO:  Epoch 90/500:  train Loss: 83.6387   val Loss: 82.5567   time: 0.27s   best: 82.3534
2023-10-07 21:26:06,131:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:06,157:INFO:  Epoch 91/500:  train Loss: 83.4094   val Loss: 82.2626   time: 0.28s   best: 82.2626
2023-10-07 21:26:06,425:INFO:  Epoch 92/500:  train Loss: 82.8326   val Loss: 83.2343   time: 0.26s   best: 82.2626
2023-10-07 21:26:06,712:INFO:  Epoch 93/500:  train Loss: 83.4806   val Loss: 83.1417   time: 0.28s   best: 82.2626
2023-10-07 21:26:06,984:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:07,166:INFO:  Epoch 94/500:  train Loss: 83.2827   val Loss: 81.7152   time: 0.27s   best: 81.7152
2023-10-07 21:26:07,445:INFO:  Epoch 95/500:  train Loss: 83.2640   val Loss: 82.5029   time: 0.28s   best: 81.7152
2023-10-07 21:26:07,724:INFO:  Epoch 96/500:  train Loss: 82.5181   val Loss: 85.6096   time: 0.28s   best: 81.7152
2023-10-07 21:26:07,989:INFO:  Epoch 97/500:  train Loss: 86.5265   val Loss: 84.4776   time: 0.26s   best: 81.7152
2023-10-07 21:26:08,290:INFO:  Epoch 98/500:  train Loss: 84.4960   val Loss: 86.1904   time: 0.30s   best: 81.7152
2023-10-07 21:26:08,645:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:08,729:INFO:  Epoch 99/500:  train Loss: 86.2017   val Loss: 81.7136   time: 0.26s   best: 81.7136
2023-10-07 21:26:09,115:INFO:  Epoch 100/500:  train Loss: 81.7520   val Loss: 82.0099   time: 0.37s   best: 81.7136
2023-10-07 21:26:09,411:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:09,496:INFO:  Epoch 101/500:  train Loss: 82.3955   val Loss: 80.9780   time: 0.29s   best: 80.9780
2023-10-07 21:26:09,792:INFO:  Epoch 102/500:  train Loss: 82.2475   val Loss: 81.3314   time: 0.28s   best: 80.9780
2023-10-07 21:26:10,066:INFO:  Epoch 103/500:  train Loss: 82.2167   val Loss: 81.2284   time: 0.26s   best: 80.9780
2023-10-07 21:26:10,363:INFO:  Epoch 104/500:  train Loss: 82.6353   val Loss: 82.5966   time: 0.28s   best: 80.9780
2023-10-07 21:26:10,629:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:10,661:INFO:  Epoch 105/500:  train Loss: 80.9355   val Loss: 80.5620   time: 0.26s   best: 80.5620
2023-10-07 21:26:10,953:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:11,060:INFO:  Epoch 106/500:  train Loss: 81.1033   val Loss: 80.1275   time: 0.29s   best: 80.1275
2023-10-07 21:26:11,348:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:11,435:INFO:  Epoch 107/500:  train Loss: 81.1875   val Loss: 79.9566   time: 0.28s   best: 79.9566
2023-10-07 21:26:11,709:INFO:  Epoch 108/500:  train Loss: 81.6084   val Loss: 85.2508   time: 0.26s   best: 79.9566
2023-10-07 21:26:12,001:INFO:  Epoch 109/500:  train Loss: 81.6371   val Loss: 86.2969   time: 0.28s   best: 79.9566
2023-10-07 21:26:12,298:INFO:  Epoch 110/500:  train Loss: 89.5443   val Loss: 89.3732   time: 0.28s   best: 79.9566
2023-10-07 21:26:12,581:INFO:  Epoch 111/500:  train Loss: 87.4613   val Loss: 84.7108   time: 0.27s   best: 79.9566
2023-10-07 21:26:12,881:INFO:  Epoch 112/500:  train Loss: 86.7938   val Loss: 86.0579   time: 0.29s   best: 79.9566
2023-10-07 21:26:13,157:INFO:  Epoch 113/500:  train Loss: 84.9082   val Loss: 83.3815   time: 0.26s   best: 79.9566
2023-10-07 21:26:13,461:INFO:  Epoch 114/500:  train Loss: 83.1180   val Loss: 82.4104   time: 0.29s   best: 79.9566
2023-10-07 21:26:13,734:INFO:  Epoch 115/500:  train Loss: 82.3110   val Loss: 80.6224   time: 0.26s   best: 79.9566
2023-10-07 21:26:14,018:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:14,122:INFO:  Epoch 116/500:  train Loss: 80.7957   val Loss: 79.8038   time: 0.28s   best: 79.8038
2023-10-07 21:26:14,409:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:14,481:INFO:  Epoch 117/500:  train Loss: 80.1684   val Loss: 79.6935   time: 0.28s   best: 79.6935
2023-10-07 21:26:14,750:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:14,829:INFO:  Epoch 118/500:  train Loss: 79.5960   val Loss: 79.2487   time: 0.26s   best: 79.2487
2023-10-07 21:26:15,125:INFO:  Epoch 119/500:  train Loss: 80.2053   val Loss: 79.6456   time: 0.28s   best: 79.2487
2023-10-07 21:26:15,424:INFO:  Epoch 120/500:  train Loss: 81.2268   val Loss: 80.0676   time: 0.29s   best: 79.2487
2023-10-07 21:26:15,706:INFO:  Epoch 121/500:  train Loss: 80.1586   val Loss: 79.7743   time: 0.27s   best: 79.2487
2023-10-07 21:26:15,996:INFO:  Epoch 122/500:  train Loss: 80.3099   val Loss: 79.5396   time: 0.28s   best: 79.2487
2023-10-07 21:26:16,262:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:16,364:INFO:  Epoch 123/500:  train Loss: 79.6577   val Loss: 79.0150   time: 0.26s   best: 79.0150
2023-10-07 21:26:16,646:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:16,688:INFO:  Epoch 124/500:  train Loss: 79.9775   val Loss: 78.9721   time: 0.28s   best: 78.9721
2023-10-07 21:26:16,981:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:17,369:INFO:  Epoch 125/500:  train Loss: 79.4677   val Loss: 77.9001   time: 0.29s   best: 77.9001
2023-10-07 21:26:17,635:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:17,681:INFO:  Epoch 126/500:  train Loss: 79.6697   val Loss: 77.8742   time: 0.26s   best: 77.8742
2023-10-07 21:26:17,961:INFO:  Epoch 127/500:  train Loss: 79.2054   val Loss: 78.4235   time: 0.27s   best: 77.8742
2023-10-07 21:26:18,246:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:18,282:INFO:  Epoch 128/500:  train Loss: 78.7341   val Loss: 77.0880   time: 0.28s   best: 77.0880
2023-10-07 21:26:18,572:INFO:  Epoch 129/500:  train Loss: 80.2510   val Loss: 81.3608   time: 0.28s   best: 77.0880
2023-10-07 21:26:18,847:INFO:  Epoch 130/500:  train Loss: 82.8177   val Loss: 81.0923   time: 0.26s   best: 77.0880
2023-10-07 21:26:19,142:INFO:  Epoch 131/500:  train Loss: 82.1105   val Loss: 81.0426   time: 0.28s   best: 77.0880
2023-10-07 21:26:19,430:INFO:  Epoch 132/500:  train Loss: 81.1724   val Loss: 79.7574   time: 0.27s   best: 77.0880
2023-10-07 21:26:19,724:INFO:  Epoch 133/500:  train Loss: 79.7704   val Loss: 79.0464   time: 0.28s   best: 77.0880
2023-10-07 21:26:19,998:INFO:  Epoch 134/500:  train Loss: 79.6813   val Loss: 78.3188   time: 0.26s   best: 77.0880
2023-10-07 21:26:20,304:INFO:  Epoch 135/500:  train Loss: 79.0112   val Loss: 77.5120   time: 0.29s   best: 77.0880
2023-10-07 21:26:20,578:INFO:  Epoch 136/500:  train Loss: 78.9465   val Loss: 79.7299   time: 0.26s   best: 77.0880
2023-10-07 21:26:20,871:INFO:  Epoch 137/500:  train Loss: 81.6191   val Loss: 79.4265   time: 0.28s   best: 77.0880
2023-10-07 21:26:21,171:INFO:  Epoch 138/500:  train Loss: 80.3678   val Loss: 79.1586   time: 0.29s   best: 77.0880
2023-10-07 21:26:21,450:INFO:  Epoch 139/500:  train Loss: 80.1776   val Loss: 79.5987   time: 0.27s   best: 77.0880
2023-10-07 21:26:21,753:INFO:  Epoch 140/500:  train Loss: 80.1101   val Loss: 79.2392   time: 0.29s   best: 77.0880
2023-10-07 21:26:22,024:INFO:  Epoch 141/500:  train Loss: 79.3238   val Loss: 78.2241   time: 0.26s   best: 77.0880
2023-10-07 21:26:22,326:INFO:  Epoch 142/500:  train Loss: 78.7054   val Loss: 77.6852   time: 0.29s   best: 77.0880
2023-10-07 21:26:22,605:INFO:  Epoch 143/500:  train Loss: 78.1540   val Loss: 77.1800   time: 0.27s   best: 77.0880
2023-10-07 21:26:22,888:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:23,088:INFO:  Epoch 144/500:  train Loss: 78.1073   val Loss: 76.6078   time: 0.28s   best: 76.6078
2023-10-07 21:26:23,381:INFO:  Epoch 145/500:  train Loss: 79.2195   val Loss: 82.5429   time: 0.28s   best: 76.6078
2023-10-07 21:26:23,652:INFO:  Epoch 146/500:  train Loss: 85.8031   val Loss: 85.0416   time: 0.26s   best: 76.6078
2023-10-07 21:26:23,953:INFO:  Epoch 147/500:  train Loss: 83.1887   val Loss: 80.8397   time: 0.29s   best: 76.6078
2023-10-07 21:26:24,228:INFO:  Epoch 148/500:  train Loss: 84.3871   val Loss: 83.0689   time: 0.26s   best: 76.6078
2023-10-07 21:26:24,527:INFO:  Epoch 149/500:  train Loss: 80.8292   val Loss: 79.3550   time: 0.29s   best: 76.6078
2023-10-07 21:26:24,803:INFO:  Epoch 150/500:  train Loss: 79.8600   val Loss: 78.4345   time: 0.26s   best: 76.6078
2023-10-07 21:26:25,101:INFO:  Epoch 151/500:  train Loss: 78.9817   val Loss: 77.6404   time: 0.29s   best: 76.6078
2023-10-07 21:26:25,405:INFO:  Epoch 152/500:  train Loss: 77.9808   val Loss: 76.6176   time: 0.29s   best: 76.6078
2023-10-07 21:26:25,672:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:25,781:INFO:  Epoch 153/500:  train Loss: 77.5814   val Loss: 76.3591   time: 0.26s   best: 76.3591
2023-10-07 21:26:26,061:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:26,138:INFO:  Epoch 154/500:  train Loss: 77.7111   val Loss: 75.9364   time: 0.27s   best: 75.9364
2023-10-07 21:26:26,434:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:26,610:INFO:  Epoch 155/500:  train Loss: 77.1229   val Loss: 75.6339   time: 0.29s   best: 75.6339
2023-10-07 21:26:26,882:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:26,932:INFO:  Epoch 156/500:  train Loss: 76.2101   val Loss: 75.0909   time: 0.27s   best: 75.0909
2023-10-07 21:26:27,215:INFO:  Epoch 157/500:  train Loss: 76.5728   val Loss: 75.1109   time: 0.27s   best: 75.0909
2023-10-07 21:26:27,500:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:27,569:INFO:  Epoch 158/500:  train Loss: 76.1024   val Loss: 74.6047   time: 0.28s   best: 74.6047
2023-10-07 21:26:27,843:INFO:  Epoch 159/500:  train Loss: 75.8728   val Loss: 75.0101   time: 0.26s   best: 74.6047
2023-10-07 21:26:28,134:INFO:  Epoch 160/500:  train Loss: 76.1685   val Loss: 75.4209   time: 0.28s   best: 74.6047
2023-10-07 21:26:28,418:INFO:  Epoch 161/500:  train Loss: 76.3004   val Loss: 74.9350   time: 0.27s   best: 74.6047
2023-10-07 21:26:28,711:INFO:  Epoch 162/500:  train Loss: 76.0651   val Loss: 76.8643   time: 0.29s   best: 74.6047
2023-10-07 21:26:29,051:INFO:  Epoch 163/500:  train Loss: 79.2388   val Loss: 77.3084   time: 0.33s   best: 74.6047
2023-10-07 21:26:29,528:INFO:  Epoch 164/500:  train Loss: 78.6944   val Loss: 78.3229   time: 0.46s   best: 74.6047
2023-10-07 21:26:29,809:INFO:  Epoch 165/500:  train Loss: 78.0902   val Loss: 76.3951   time: 0.27s   best: 74.6047
2023-10-07 21:26:30,110:INFO:  Epoch 166/500:  train Loss: 76.5722   val Loss: 74.6697   time: 0.29s   best: 74.6047
2023-10-07 21:26:30,388:INFO:  Epoch 167/500:  train Loss: 76.2237   val Loss: 74.7523   time: 0.26s   best: 74.6047
2023-10-07 21:26:30,680:INFO:  Epoch 168/500:  train Loss: 75.7276   val Loss: 75.0119   time: 0.28s   best: 74.6047
2023-10-07 21:26:30,963:INFO:  Epoch 169/500:  train Loss: 75.8058   val Loss: 74.6948   time: 0.27s   best: 74.6047
2023-10-07 21:26:31,246:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:31,407:INFO:  Epoch 170/500:  train Loss: 75.4940   val Loss: 74.2739   time: 0.28s   best: 74.2739
2023-10-07 21:26:31,695:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:31,755:INFO:  Epoch 171/500:  train Loss: 75.3285   val Loss: 74.1117   time: 0.28s   best: 74.1117
2023-10-07 21:26:32,032:INFO:  Epoch 172/500:  train Loss: 75.0545   val Loss: 74.2725   time: 0.26s   best: 74.1117
2023-10-07 21:26:32,320:INFO:  Epoch 173/500:  train Loss: 75.3459   val Loss: 76.0836   time: 0.29s   best: 74.1117
2023-10-07 21:26:32,608:INFO:  Epoch 174/500:  train Loss: 76.6521   val Loss: 76.4564   time: 0.27s   best: 74.1117
2023-10-07 21:26:32,903:INFO:  Epoch 175/500:  train Loss: 78.5828   val Loss: 74.9683   time: 0.28s   best: 74.1117
2023-10-07 21:26:33,196:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:33,292:INFO:  Epoch 176/500:  train Loss: 74.8686   val Loss: 73.9068   time: 0.28s   best: 73.9068
2023-10-07 21:26:33,565:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:33,658:INFO:  Epoch 177/500:  train Loss: 74.7032   val Loss: 73.4538   time: 0.27s   best: 73.4538
2023-10-07 21:26:33,944:INFO:  Epoch 178/500:  train Loss: 75.8824   val Loss: 76.9507   time: 0.27s   best: 73.4538
2023-10-07 21:26:34,223:INFO:  Epoch 179/500:  train Loss: 78.8670   val Loss: 77.0003   time: 0.26s   best: 73.4538
2023-10-07 21:26:34,527:INFO:  Epoch 180/500:  train Loss: 78.0813   val Loss: 78.0149   time: 0.29s   best: 73.4538
2023-10-07 21:26:34,818:INFO:  Epoch 181/500:  train Loss: 77.7752   val Loss: 74.4303   time: 0.28s   best: 73.4538
2023-10-07 21:26:35,089:INFO:  Epoch 182/500:  train Loss: 75.2128   val Loss: 73.6991   time: 0.26s   best: 73.4538
2023-10-07 21:26:35,388:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:35,477:INFO:  Epoch 183/500:  train Loss: 74.0747   val Loss: 72.4505   time: 0.29s   best: 72.4505
2023-10-07 21:26:35,754:INFO:  Epoch 184/500:  train Loss: 76.1179   val Loss: 76.8821   time: 0.26s   best: 72.4505
2023-10-07 21:26:36,048:INFO:  Epoch 185/500:  train Loss: 77.6235   val Loss: 77.4089   time: 0.28s   best: 72.4505
2023-10-07 21:26:36,343:INFO:  Epoch 186/500:  train Loss: 77.5664   val Loss: 75.1541   time: 0.27s   best: 72.4505
2023-10-07 21:26:36,623:INFO:  Epoch 187/500:  train Loss: 75.1188   val Loss: 74.9201   time: 0.27s   best: 72.4505
2023-10-07 21:26:36,922:INFO:  Epoch 188/500:  train Loss: 75.7454   val Loss: 73.8171   time: 0.29s   best: 72.4505
2023-10-07 21:26:37,197:INFO:  Epoch 189/500:  train Loss: 74.6316   val Loss: 73.7501   time: 0.26s   best: 72.4505
2023-10-07 21:26:37,491:INFO:  Epoch 190/500:  train Loss: 74.5822   val Loss: 73.5034   time: 0.28s   best: 72.4505
2023-10-07 21:26:37,770:INFO:  Epoch 191/500:  train Loss: 74.7325   val Loss: 72.6014   time: 0.27s   best: 72.4505
2023-10-07 21:26:38,064:INFO:  Epoch 192/500:  train Loss: 73.6175   val Loss: 73.1075   time: 0.28s   best: 72.4505
2023-10-07 21:26:38,351:INFO:  Epoch 193/500:  train Loss: 74.2447   val Loss: 73.1137   time: 0.27s   best: 72.4505
2023-10-07 21:26:38,637:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:38,669:INFO:  Epoch 194/500:  train Loss: 73.6672   val Loss: 72.3234   time: 0.28s   best: 72.3234
2023-10-07 21:26:38,943:INFO:  Epoch 195/500:  train Loss: 73.7282   val Loss: 75.3999   time: 0.26s   best: 72.3234
2023-10-07 21:26:39,244:INFO:  Epoch 196/500:  train Loss: 76.3649   val Loss: 73.4429   time: 0.29s   best: 72.3234
2023-10-07 21:26:39,577:INFO:  Epoch 197/500:  train Loss: 75.8009   val Loss: 75.1196   time: 0.33s   best: 72.3234
2023-10-07 21:26:39,861:INFO:  Epoch 198/500:  train Loss: 75.1397   val Loss: 74.4697   time: 0.27s   best: 72.3234
2023-10-07 21:26:40,159:INFO:  Epoch 199/500:  train Loss: 74.9150   val Loss: 74.1725   time: 0.29s   best: 72.3234
2023-10-07 21:26:40,476:INFO:  Epoch 200/500:  train Loss: 75.2217   val Loss: 73.3885   time: 0.30s   best: 72.3234
2023-10-07 21:26:40,777:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:40,921:INFO:  Epoch 201/500:  train Loss: 73.8810   val Loss: 72.2463   time: 0.29s   best: 72.2463
2023-10-07 21:26:41,202:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:41,331:INFO:  Epoch 202/500:  train Loss: 73.2018   val Loss: 71.5323   time: 0.28s   best: 71.5323
2023-10-07 21:26:41,622:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:41,666:INFO:  Epoch 203/500:  train Loss: 72.8539   val Loss: 71.1339   time: 0.29s   best: 71.1339
2023-10-07 21:26:41,938:INFO:  Epoch 204/500:  train Loss: 72.8395   val Loss: 71.1794   time: 0.26s   best: 71.1339
2023-10-07 21:26:42,229:INFO:  Epoch 205/500:  train Loss: 72.8672   val Loss: 71.8900   time: 0.28s   best: 71.1339
2023-10-07 21:26:42,509:INFO:  Epoch 206/500:  train Loss: 72.5863   val Loss: 71.8696   time: 0.27s   best: 71.1339
2023-10-07 21:26:42,706:INFO:  Epoch 215/500:  train Loss: 18.8112   val Loss: 24.1246   time: 488.83s   best: 23.0233
2023-10-07 21:26:42,809:INFO:  Epoch 207/500:  train Loss: 73.2355   val Loss: 72.5666   time: 0.29s   best: 71.1339
2023-10-07 21:26:43,084:INFO:  Epoch 208/500:  train Loss: 73.7502   val Loss: 72.4649   time: 0.26s   best: 71.1339
2023-10-07 21:26:43,378:INFO:  Epoch 209/500:  train Loss: 74.0715   val Loss: 71.9300   time: 0.28s   best: 71.1339
2023-10-07 21:26:43,659:INFO:  Epoch 210/500:  train Loss: 72.9092   val Loss: 72.2150   time: 0.27s   best: 71.1339
2023-10-07 21:26:43,943:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:44,083:INFO:  Epoch 211/500:  train Loss: 72.0002   val Loss: 70.9524   time: 0.28s   best: 70.9524
2023-10-07 21:26:44,382:INFO:  Epoch 212/500:  train Loss: 73.1974   val Loss: 71.7665   time: 0.29s   best: 70.9524
2023-10-07 21:26:44,656:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:44,692:INFO:  Epoch 213/500:  train Loss: 72.2562   val Loss: 70.3989   time: 0.27s   best: 70.3989
2023-10-07 21:26:44,982:INFO:  Epoch 214/500:  train Loss: 72.5943   val Loss: 72.8192   time: 0.28s   best: 70.3989
2023-10-07 21:26:45,278:INFO:  Epoch 215/500:  train Loss: 73.7866   val Loss: 72.3875   time: 0.28s   best: 70.3989
2023-10-07 21:26:45,564:INFO:  Epoch 216/500:  train Loss: 73.5391   val Loss: 71.4046   time: 0.26s   best: 70.3989
2023-10-07 21:26:45,861:INFO:  Epoch 217/500:  train Loss: 72.8033   val Loss: 71.5584   time: 0.28s   best: 70.3989
2023-10-07 21:26:46,136:INFO:  Epoch 218/500:  train Loss: 73.4095   val Loss: 70.8845   time: 0.26s   best: 70.3989
2023-10-07 21:26:46,433:INFO:  Epoch 219/500:  train Loss: 71.6779   val Loss: 70.8477   time: 0.28s   best: 70.3989
2023-10-07 21:26:46,709:INFO:  Epoch 220/500:  train Loss: 72.3050   val Loss: 72.8395   time: 0.26s   best: 70.3989
2023-10-07 21:26:47,002:INFO:  Epoch 221/500:  train Loss: 75.5876   val Loss: 72.2752   time: 0.28s   best: 70.3989
2023-10-07 21:26:47,274:INFO:  Epoch 222/500:  train Loss: 74.8300   val Loss: 72.8419   time: 0.27s   best: 70.3989
2023-10-07 21:26:47,591:INFO:  Epoch 223/500:  train Loss: 72.9741   val Loss: 72.5590   time: 0.30s   best: 70.3989
2023-10-07 21:26:47,863:INFO:  Epoch 224/500:  train Loss: 73.0662   val Loss: 71.8727   time: 0.26s   best: 70.3989
2023-10-07 21:26:48,158:INFO:  Epoch 225/500:  train Loss: 73.0629   val Loss: 70.8816   time: 0.28s   best: 70.3989
2023-10-07 21:26:48,455:INFO:  Epoch 226/500:  train Loss: 72.0213   val Loss: 70.9104   time: 0.28s   best: 70.3989
2023-10-07 21:26:48,730:INFO:  Epoch 227/500:  train Loss: 72.1678   val Loss: 70.4429   time: 0.26s   best: 70.3989
2023-10-07 21:26:49,022:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:49,057:INFO:  Epoch 228/500:  train Loss: 71.1740   val Loss: 69.3362   time: 0.29s   best: 69.3362
2023-10-07 21:26:49,326:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:49,456:INFO:  Epoch 229/500:  train Loss: 70.5064   val Loss: 69.2326   time: 0.26s   best: 69.2326
2023-10-07 21:26:49,745:INFO:  Epoch 230/500:  train Loss: 70.9721   val Loss: 70.0851   time: 0.28s   best: 69.2326
2023-10-07 21:26:50,035:INFO:  Epoch 231/500:  train Loss: 71.5321   val Loss: 70.0612   time: 0.28s   best: 69.2326
2023-10-07 21:26:50,311:INFO:  Epoch 232/500:  train Loss: 71.3576   val Loss: 69.6385   time: 0.26s   best: 69.2326
2023-10-07 21:26:50,608:INFO:  Epoch 233/500:  train Loss: 71.4929   val Loss: 69.7603   time: 0.28s   best: 69.2326
2023-10-07 21:26:50,880:INFO:  Epoch 234/500:  train Loss: 71.2379   val Loss: 70.1557   time: 0.26s   best: 69.2326
2023-10-07 21:26:51,179:INFO:  Epoch 235/500:  train Loss: 70.7412   val Loss: 69.4402   time: 0.29s   best: 69.2326
2023-10-07 21:26:51,446:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:51,484:INFO:  Epoch 236/500:  train Loss: 69.9398   val Loss: 68.3787   time: 0.26s   best: 68.3787
2023-10-07 21:26:51,787:INFO:  Epoch 237/500:  train Loss: 70.4916   val Loss: 69.4057   time: 0.29s   best: 68.3787
2023-10-07 21:26:52,057:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:26:52,232:INFO:  Epoch 238/500:  train Loss: 70.8237   val Loss: 68.0963   time: 0.27s   best: 68.0963
2023-10-07 21:26:52,511:INFO:  Epoch 239/500:  train Loss: 73.6783   val Loss: 72.1590   time: 0.27s   best: 68.0963
2023-10-07 21:26:52,799:INFO:  Epoch 240/500:  train Loss: 72.3983   val Loss: 71.9651   time: 0.27s   best: 68.0963
2023-10-07 21:26:53,071:INFO:  Epoch 241/500:  train Loss: 73.2713   val Loss: 73.1533   time: 0.26s   best: 68.0963
2023-10-07 21:26:53,375:INFO:  Epoch 242/500:  train Loss: 73.4453   val Loss: 72.1053   time: 0.29s   best: 68.0963
2023-10-07 21:26:53,675:INFO:  Epoch 243/500:  train Loss: 72.9973   val Loss: 72.5888   time: 0.29s   best: 68.0963
2023-10-07 21:26:53,954:INFO:  Epoch 244/500:  train Loss: 73.1448   val Loss: 70.5209   time: 0.26s   best: 68.0963
2023-10-07 21:26:54,249:INFO:  Epoch 245/500:  train Loss: 71.0271   val Loss: 70.9827   time: 0.28s   best: 68.0963
2023-10-07 21:26:54,529:INFO:  Epoch 246/500:  train Loss: 74.8413   val Loss: 85.4223   time: 0.27s   best: 68.0963
2023-10-07 21:26:54,821:INFO:  Epoch 247/500:  train Loss: 87.1466   val Loss: 84.4977   time: 0.28s   best: 68.0963
2023-10-07 21:26:55,095:INFO:  Epoch 248/500:  train Loss: 84.2838   val Loss: 84.2299   time: 0.26s   best: 68.0963
2023-10-07 21:26:55,389:INFO:  Epoch 249/500:  train Loss: 87.0553   val Loss: 83.0578   time: 0.28s   best: 68.0963
2023-10-07 21:26:55,681:INFO:  Epoch 250/500:  train Loss: 80.7894   val Loss: 79.5257   time: 0.28s   best: 68.0963
2023-10-07 21:26:55,972:INFO:  Epoch 251/500:  train Loss: 79.5836   val Loss: 76.6465   time: 0.28s   best: 68.0963
2023-10-07 21:26:56,268:INFO:  Epoch 252/500:  train Loss: 78.0737   val Loss: 75.6745   time: 0.28s   best: 68.0963
2023-10-07 21:26:56,557:INFO:  Epoch 253/500:  train Loss: 74.4183   val Loss: 71.9574   time: 0.28s   best: 68.0963
2023-10-07 21:26:56,848:INFO:  Epoch 254/500:  train Loss: 72.0948   val Loss: 70.9434   time: 0.28s   best: 68.0963
2023-10-07 21:26:57,122:INFO:  Epoch 255/500:  train Loss: 71.2598   val Loss: 70.2695   time: 0.26s   best: 68.0963
2023-10-07 21:26:57,416:INFO:  Epoch 256/500:  train Loss: 70.4209   val Loss: 69.3994   time: 0.28s   best: 68.0963
2023-10-07 21:26:57,696:INFO:  Epoch 257/500:  train Loss: 70.0151   val Loss: 69.1879   time: 0.27s   best: 68.0963
2023-10-07 21:26:57,998:INFO:  Epoch 258/500:  train Loss: 70.0569   val Loss: 69.3953   time: 0.29s   best: 68.0963
2023-10-07 21:26:58,261:INFO:  Epoch 259/500:  train Loss: 71.1594   val Loss: 68.2769   time: 0.26s   best: 68.0963
2023-10-07 21:26:58,578:INFO:  Epoch 260/500:  train Loss: 72.1087   val Loss: 70.7396   time: 0.30s   best: 68.0963
2023-10-07 21:26:58,855:INFO:  Epoch 261/500:  train Loss: 71.4029   val Loss: 70.9577   time: 0.26s   best: 68.0963
2023-10-07 21:26:59,147:INFO:  Epoch 262/500:  train Loss: 71.5806   val Loss: 68.9674   time: 0.28s   best: 68.0963
2023-10-07 21:26:59,449:INFO:  Epoch 263/500:  train Loss: 70.7239   val Loss: 68.4358   time: 0.29s   best: 68.0963
2023-10-07 21:26:59,835:INFO:  Epoch 264/500:  train Loss: 70.0353   val Loss: 68.3141   time: 0.37s   best: 68.0963
2023-10-07 21:27:00,248:INFO:  Epoch 265/500:  train Loss: 70.0580   val Loss: 68.4445   time: 0.40s   best: 68.0963
2023-10-07 21:27:00,535:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:00,660:INFO:  Epoch 266/500:  train Loss: 69.0224   val Loss: 67.8575   time: 0.28s   best: 67.8575
2023-10-07 21:27:00,930:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:00,988:INFO:  Epoch 267/500:  train Loss: 68.7605   val Loss: 67.7419   time: 0.27s   best: 67.7419
2023-10-07 21:27:01,261:INFO:  Epoch 268/500:  train Loss: 69.6364   val Loss: 68.3215   time: 0.26s   best: 67.7419
2023-10-07 21:27:01,546:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:01,690:INFO:  Epoch 269/500:  train Loss: 68.7610   val Loss: 67.5039   time: 0.28s   best: 67.5039
2023-10-07 21:27:01,954:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:02,010:INFO:  Epoch 270/500:  train Loss: 69.1573   val Loss: 67.3512   time: 0.26s   best: 67.3512
2023-10-07 21:27:02,284:INFO:  Epoch 271/500:  train Loss: 68.4605   val Loss: 67.5912   time: 0.27s   best: 67.3512
2023-10-07 21:27:02,602:INFO:  Epoch 272/500:  train Loss: 68.7521   val Loss: 68.0846   time: 0.30s   best: 67.3512
2023-10-07 21:27:02,869:INFO:  Epoch 273/500:  train Loss: 68.8769   val Loss: 67.6321   time: 0.25s   best: 67.3512
2023-10-07 21:27:03,169:INFO:  Epoch 274/500:  train Loss: 71.2190   val Loss: 70.2855   time: 0.29s   best: 67.3512
2023-10-07 21:27:03,440:INFO:  Epoch 275/500:  train Loss: 70.6629   val Loss: 68.7302   time: 0.26s   best: 67.3512
2023-10-07 21:27:03,735:INFO:  Epoch 276/500:  train Loss: 69.5956   val Loss: 68.1247   time: 0.28s   best: 67.3512
2023-10-07 21:27:04,007:INFO:  Epoch 277/500:  train Loss: 69.3643   val Loss: 69.5763   time: 0.26s   best: 67.3512
2023-10-07 21:27:04,311:INFO:  Epoch 278/500:  train Loss: 71.6345   val Loss: 71.8229   time: 0.29s   best: 67.3512
2023-10-07 21:27:04,590:INFO:  Epoch 279/500:  train Loss: 69.2662   val Loss: 68.5058   time: 0.27s   best: 67.3512
2023-10-07 21:27:04,891:INFO:  Epoch 280/500:  train Loss: 69.6069   val Loss: 67.5018   time: 0.29s   best: 67.3512
2023-10-07 21:27:05,176:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:05,213:INFO:  Epoch 281/500:  train Loss: 69.6858   val Loss: 66.3706   time: 0.28s   best: 66.3706
2023-10-07 21:27:05,493:INFO:  Epoch 282/500:  train Loss: 69.2033   val Loss: 68.1032   time: 0.27s   best: 66.3706
2023-10-07 21:27:05,785:INFO:  Epoch 283/500:  train Loss: 69.8790   val Loss: 66.6723   time: 0.28s   best: 66.3706
2023-10-07 21:27:06,058:INFO:  Epoch 284/500:  train Loss: 69.0318   val Loss: 67.9821   time: 0.26s   best: 66.3706
2023-10-07 21:27:06,369:INFO:  Epoch 285/500:  train Loss: 68.5491   val Loss: 67.3389   time: 0.30s   best: 66.3706
2023-10-07 21:27:06,638:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:06,776:INFO:  Epoch 286/500:  train Loss: 68.9643   val Loss: 65.8403   time: 0.26s   best: 65.8403
2023-10-07 21:27:07,049:INFO:  Epoch 287/500:  train Loss: 67.3327   val Loss: 67.2513   time: 0.26s   best: 65.8403
2023-10-07 21:27:07,347:INFO:  Epoch 288/500:  train Loss: 69.3256   val Loss: 67.4249   time: 0.29s   best: 65.8403
2023-10-07 21:27:07,617:INFO:  Epoch 289/500:  train Loss: 67.6812   val Loss: 66.4807   time: 0.26s   best: 65.8403
2023-10-07 21:27:07,912:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:08,001:INFO:  Epoch 290/500:  train Loss: 67.3662   val Loss: 65.8045   time: 0.29s   best: 65.8045
2023-10-07 21:27:08,299:INFO:  Epoch 291/500:  train Loss: 67.8064   val Loss: 65.8482   time: 0.28s   best: 65.8045
2023-10-07 21:27:08,579:INFO:  Epoch 292/500:  train Loss: 67.2968   val Loss: 66.2141   time: 0.27s   best: 65.8045
2023-10-07 21:27:08,873:INFO:  Epoch 293/500:  train Loss: 66.8505   val Loss: 65.8986   time: 0.28s   best: 65.8045
2023-10-07 21:27:09,138:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:09,207:INFO:  Epoch 294/500:  train Loss: 67.4830   val Loss: 65.7157   time: 0.26s   best: 65.7157
2023-10-07 21:27:09,497:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:09,562:INFO:  Epoch 295/500:  train Loss: 66.4792   val Loss: 64.8846   time: 0.28s   best: 64.8846
2023-10-07 21:27:09,852:INFO:  Epoch 296/500:  train Loss: 67.2592   val Loss: 66.8314   time: 0.28s   best: 64.8846
2023-10-07 21:27:10,126:INFO:  Epoch 297/500:  train Loss: 67.5276   val Loss: 65.9860   time: 0.26s   best: 64.8846
2023-10-07 21:27:10,433:INFO:  Epoch 298/500:  train Loss: 66.9991   val Loss: 66.6112   time: 0.29s   best: 64.8846
2023-10-07 21:27:10,707:INFO:  Epoch 299/500:  train Loss: 72.8162   val Loss: 83.3963   time: 0.26s   best: 64.8846
2023-10-07 21:27:11,045:INFO:  Epoch 300/500:  train Loss: 85.3382   val Loss: 83.1810   time: 0.32s   best: 64.8846
2023-10-07 21:27:11,332:INFO:  Epoch 301/500:  train Loss: 83.6298   val Loss: 79.7453   time: 0.27s   best: 64.8846
2023-10-07 21:27:11,625:INFO:  Epoch 302/500:  train Loss: 81.7829   val Loss: 79.2503   time: 0.28s   best: 64.8846
2023-10-07 21:27:11,906:INFO:  Epoch 303/500:  train Loss: 78.5151   val Loss: 75.0700   time: 0.27s   best: 64.8846
2023-10-07 21:27:12,207:INFO:  Epoch 304/500:  train Loss: 76.2208   val Loss: 73.2178   time: 0.28s   best: 64.8846
2023-10-07 21:27:12,496:INFO:  Epoch 305/500:  train Loss: 75.4457   val Loss: 73.2831   time: 0.29s   best: 64.8846
2023-10-07 21:27:12,773:INFO:  Epoch 306/500:  train Loss: 78.3095   val Loss: 75.8650   time: 0.26s   best: 64.8846
2023-10-07 21:27:13,068:INFO:  Epoch 307/500:  train Loss: 73.4265   val Loss: 71.3944   time: 0.28s   best: 64.8846
2023-10-07 21:27:13,342:INFO:  Epoch 308/500:  train Loss: 72.3842   val Loss: 70.5953   time: 0.26s   best: 64.8846
2023-10-07 21:27:13,646:INFO:  Epoch 309/500:  train Loss: 71.2049   val Loss: 69.4258   time: 0.29s   best: 64.8846
2023-10-07 21:27:13,920:INFO:  Epoch 310/500:  train Loss: 69.9956   val Loss: 68.5569   time: 0.26s   best: 64.8846
2023-10-07 21:27:14,223:INFO:  Epoch 311/500:  train Loss: 69.6003   val Loss: 68.3573   time: 0.28s   best: 64.8846
2023-10-07 21:27:14,502:INFO:  Epoch 312/500:  train Loss: 69.3158   val Loss: 67.6521   time: 0.27s   best: 64.8846
2023-10-07 21:27:14,797:INFO:  Epoch 313/500:  train Loss: 69.1996   val Loss: 68.6773   time: 0.28s   best: 64.8846
2023-10-07 21:27:15,084:INFO:  Epoch 314/500:  train Loss: 68.7341   val Loss: 67.4768   time: 0.27s   best: 64.8846
2023-10-07 21:27:15,372:INFO:  Epoch 315/500:  train Loss: 67.9532   val Loss: 67.0262   time: 0.27s   best: 64.8846
2023-10-07 21:27:15,669:INFO:  Epoch 316/500:  train Loss: 67.7185   val Loss: 66.8480   time: 0.28s   best: 64.8846
2023-10-07 21:27:15,943:INFO:  Epoch 317/500:  train Loss: 68.8823   val Loss: 67.6893   time: 0.26s   best: 64.8846
2023-10-07 21:27:16,242:INFO:  Epoch 318/500:  train Loss: 67.6834   val Loss: 68.2757   time: 0.29s   best: 64.8846
2023-10-07 21:27:16,530:INFO:  Epoch 319/500:  train Loss: 70.6529   val Loss: 68.3227   time: 0.27s   best: 64.8846
2023-10-07 21:27:16,835:INFO:  Epoch 320/500:  train Loss: 71.5144   val Loss: 68.6301   time: 0.29s   best: 64.8846
2023-10-07 21:27:17,099:INFO:  Epoch 321/500:  train Loss: 69.3650   val Loss: 67.1471   time: 0.26s   best: 64.8846
2023-10-07 21:27:17,399:INFO:  Epoch 322/500:  train Loss: 68.6654   val Loss: 66.6026   time: 0.29s   best: 64.8846
2023-10-07 21:27:17,674:INFO:  Epoch 323/500:  train Loss: 70.6349   val Loss: 66.1697   time: 0.26s   best: 64.8846
2023-10-07 21:27:17,973:INFO:  Epoch 324/500:  train Loss: 69.2438   val Loss: 67.0331   time: 0.28s   best: 64.8846
2023-10-07 21:27:18,271:INFO:  Epoch 325/500:  train Loss: 67.5764   val Loss: 66.3447   time: 0.28s   best: 64.8846
2023-10-07 21:27:18,541:INFO:  Epoch 326/500:  train Loss: 67.5392   val Loss: 65.5153   time: 0.26s   best: 64.8846
2023-10-07 21:27:18,851:INFO:  Epoch 327/500:  train Loss: 67.6470   val Loss: 66.6083   time: 0.30s   best: 64.8846
2023-10-07 21:27:19,123:INFO:  Epoch 328/500:  train Loss: 69.2441   val Loss: 70.3293   time: 0.26s   best: 64.8846
2023-10-07 21:27:19,418:INFO:  Epoch 329/500:  train Loss: 68.7858   val Loss: 70.1595   time: 0.28s   best: 64.8846
2023-10-07 21:27:19,700:INFO:  Epoch 330/500:  train Loss: 70.9260   val Loss: 72.9716   time: 0.27s   best: 64.8846
2023-10-07 21:27:19,985:INFO:  Epoch 331/500:  train Loss: 70.9281   val Loss: 68.9160   time: 0.28s   best: 64.8846
2023-10-07 21:27:20,273:INFO:  Epoch 332/500:  train Loss: 68.6237   val Loss: 68.0951   time: 0.27s   best: 64.8846
2023-10-07 21:27:20,572:INFO:  Epoch 333/500:  train Loss: 69.1844   val Loss: 69.1961   time: 0.29s   best: 64.8846
2023-10-07 21:27:20,846:INFO:  Epoch 334/500:  train Loss: 70.5824   val Loss: 73.6003   time: 0.26s   best: 64.8846
2023-10-07 21:27:21,148:INFO:  Epoch 335/500:  train Loss: 72.9629   val Loss: 74.2476   time: 0.29s   best: 64.8846
2023-10-07 21:27:21,433:INFO:  Epoch 336/500:  train Loss: 75.9217   val Loss: 71.4320   time: 0.28s   best: 64.8846
2023-10-07 21:27:21,725:INFO:  Epoch 337/500:  train Loss: 73.4785   val Loss: 70.1751   time: 0.28s   best: 64.8846
2023-10-07 21:27:22,013:INFO:  Epoch 338/500:  train Loss: 71.9289   val Loss: 68.5121   time: 0.27s   best: 64.8846
2023-10-07 21:27:22,293:INFO:  Epoch 339/500:  train Loss: 68.6710   val Loss: 66.8847   time: 0.27s   best: 64.8846
2023-10-07 21:27:22,593:INFO:  Epoch 340/500:  train Loss: 71.7687   val Loss: 74.1757   time: 0.28s   best: 64.8846
2023-10-07 21:27:22,860:INFO:  Epoch 341/500:  train Loss: 76.7763   val Loss: 72.7049   time: 0.26s   best: 64.8846
2023-10-07 21:27:23,160:INFO:  Epoch 342/500:  train Loss: 73.4546   val Loss: 69.8856   time: 0.29s   best: 64.8846
2023-10-07 21:27:23,444:INFO:  Epoch 343/500:  train Loss: 69.4105   val Loss: 67.8858   time: 0.27s   best: 64.8846
2023-10-07 21:27:23,738:INFO:  Epoch 344/500:  train Loss: 67.5562   val Loss: 67.2082   time: 0.28s   best: 64.8846
2023-10-07 21:27:24,020:INFO:  Epoch 345/500:  train Loss: 67.3678   val Loss: 65.4092   time: 0.26s   best: 64.8846
2023-10-07 21:27:24,323:INFO:  Epoch 346/500:  train Loss: 66.6267   val Loss: 66.3731   time: 0.29s   best: 64.8846
2023-10-07 21:27:24,616:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:24,648:INFO:  Epoch 347/500:  train Loss: 67.0695   val Loss: 64.8835   time: 0.29s   best: 64.8835
2023-10-07 21:27:24,920:INFO:  Epoch 348/500:  train Loss: 68.1401   val Loss: 65.8023   time: 0.26s   best: 64.8835
2023-10-07 21:27:25,212:INFO:  Epoch 349/500:  train Loss: 67.6652   val Loss: 65.3205   time: 0.28s   best: 64.8835
2023-10-07 21:27:25,490:INFO:  Epoch 350/500:  train Loss: 67.0532   val Loss: 66.0964   time: 0.26s   best: 64.8835
2023-10-07 21:27:25,792:INFO:  Epoch 351/500:  train Loss: 68.0194   val Loss: 65.6075   time: 0.29s   best: 64.8835
2023-10-07 21:27:26,063:INFO:  Epoch 352/500:  train Loss: 67.7361   val Loss: 67.5007   time: 0.26s   best: 64.8835
2023-10-07 21:27:26,365:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:26,489:INFO:  Epoch 353/500:  train Loss: 68.6010   val Loss: 64.5034   time: 0.29s   best: 64.5034
2023-10-07 21:27:26,777:INFO:  Epoch 354/500:  train Loss: 67.9961   val Loss: 65.0344   time: 0.27s   best: 64.5034
2023-10-07 21:27:27,040:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:27,075:INFO:  Epoch 355/500:  train Loss: 66.3759   val Loss: 63.7677   time: 0.26s   best: 63.7677
2023-10-07 21:27:27,368:INFO:  Epoch 356/500:  train Loss: 65.5909   val Loss: 64.6226   time: 0.28s   best: 63.7677
2023-10-07 21:27:27,646:INFO:  Epoch 357/500:  train Loss: 66.6974   val Loss: 65.4733   time: 0.26s   best: 63.7677
2023-10-07 21:27:27,942:INFO:  Epoch 358/500:  train Loss: 67.5229   val Loss: 66.4846   time: 0.28s   best: 63.7677
2023-10-07 21:27:28,241:INFO:  Epoch 359/500:  train Loss: 67.3070   val Loss: 66.7396   time: 0.29s   best: 63.7677
2023-10-07 21:27:28,527:INFO:  Epoch 360/500:  train Loss: 68.7441   val Loss: 65.4314   time: 0.27s   best: 63.7677
2023-10-07 21:27:28,822:INFO:  Epoch 361/500:  train Loss: 67.4696   val Loss: 64.5248   time: 0.28s   best: 63.7677
2023-10-07 21:27:29,093:INFO:  Epoch 362/500:  train Loss: 67.7887   val Loss: 66.7945   time: 0.26s   best: 63.7677
2023-10-07 21:27:29,386:INFO:  Epoch 363/500:  train Loss: 69.3769   val Loss: 64.9297   time: 0.28s   best: 63.7677
2023-10-07 21:27:29,664:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:29,750:INFO:  Epoch 364/500:  train Loss: 67.2778   val Loss: 63.3062   time: 0.27s   best: 63.3062
2023-10-07 21:27:30,041:INFO:  Epoch 365/500:  train Loss: 67.0292   val Loss: 66.2928   time: 0.28s   best: 63.3062
2023-10-07 21:27:30,365:INFO:  Epoch 366/500:  train Loss: 66.5797   val Loss: 65.7418   time: 0.32s   best: 63.3062
2023-10-07 21:27:30,863:INFO:  Epoch 367/500:  train Loss: 67.8626   val Loss: 64.4427   time: 0.48s   best: 63.3062
2023-10-07 21:27:31,145:INFO:  Epoch 368/500:  train Loss: 69.4098   val Loss: 66.9491   time: 0.27s   best: 63.3062
2023-10-07 21:27:31,438:INFO:  Epoch 369/500:  train Loss: 70.4736   val Loss: 65.8403   time: 0.28s   best: 63.3062
2023-10-07 21:27:31,712:INFO:  Epoch 370/500:  train Loss: 70.3756   val Loss: 70.2498   time: 0.26s   best: 63.3062
2023-10-07 21:27:32,007:INFO:  Epoch 371/500:  train Loss: 70.9286   val Loss: 68.8500   time: 0.28s   best: 63.3062
2023-10-07 21:27:32,276:INFO:  Epoch 372/500:  train Loss: 68.3693   val Loss: 66.5328   time: 0.26s   best: 63.3062
2023-10-07 21:27:32,591:INFO:  Epoch 373/500:  train Loss: 68.5458   val Loss: 67.0512   time: 0.30s   best: 63.3062
2023-10-07 21:27:32,873:INFO:  Epoch 374/500:  train Loss: 67.4808   val Loss: 67.0374   time: 0.27s   best: 63.3062
2023-10-07 21:27:33,166:INFO:  Epoch 375/500:  train Loss: 67.3868   val Loss: 65.4724   time: 0.28s   best: 63.3062
2023-10-07 21:27:33,445:INFO:  Epoch 376/500:  train Loss: 67.0615   val Loss: 65.6168   time: 0.28s   best: 63.3062
2023-10-07 21:27:33,735:INFO:  Epoch 377/500:  train Loss: 65.7318   val Loss: 65.0462   time: 0.28s   best: 63.3062
2023-10-07 21:27:34,029:INFO:  Epoch 378/500:  train Loss: 65.5516   val Loss: 64.7550   time: 0.28s   best: 63.3062
2023-10-07 21:27:34,309:INFO:  Epoch 379/500:  train Loss: 66.0734   val Loss: 64.7092   time: 0.27s   best: 63.3062
2023-10-07 21:27:34,605:INFO:  Epoch 380/500:  train Loss: 65.8938   val Loss: 65.4764   time: 0.28s   best: 63.3062
2023-10-07 21:27:34,892:INFO:  Epoch 381/500:  train Loss: 65.3315   val Loss: 63.5467   time: 0.27s   best: 63.3062
2023-10-07 21:27:35,186:INFO:  Epoch 382/500:  train Loss: 64.5359   val Loss: 63.8958   time: 0.28s   best: 63.3062
2023-10-07 21:27:35,460:INFO:  Epoch 383/500:  train Loss: 64.4105   val Loss: 63.5421   time: 0.26s   best: 63.3062
2023-10-07 21:27:35,761:INFO:  Epoch 384/500:  train Loss: 64.9681   val Loss: 64.1731   time: 0.29s   best: 63.3062
2023-10-07 21:27:36,028:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:36,082:INFO:  Epoch 385/500:  train Loss: 65.1794   val Loss: 63.2909   time: 0.26s   best: 63.2909
2023-10-07 21:27:36,361:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_17ae.pt
2023-10-07 21:27:36,522:INFO:  Epoch 386/500:  train Loss: 64.3159   val Loss: 62.0467   time: 0.27s   best: 62.0467
2023-10-07 21:27:36,810:INFO:  Epoch 387/500:  train Loss: 64.4925   val Loss: 64.0704   time: 0.27s   best: 62.0467
2023-10-07 21:27:37,100:INFO:  Epoch 388/500:  train Loss: 64.6416   val Loss: 63.7401   time: 0.28s   best: 62.0467
2023-10-07 21:27:37,394:INFO:  Epoch 389/500:  train Loss: 64.1554   val Loss: 63.2384   time: 0.28s   best: 62.0467
2023-10-07 21:27:37,689:INFO:  Epoch 390/500:  train Loss: 66.4048   val Loss: 64.8073   time: 0.28s   best: 62.0467
2023-10-07 21:27:37,965:INFO:  Epoch 391/500:  train Loss: 66.3667   val Loss: 65.1229   time: 0.26s   best: 62.0467
2023-10-07 21:27:38,255:INFO:  Epoch 392/500:  train Loss: 64.6772   val Loss: 64.7413   time: 0.28s   best: 62.0467
2023-10-07 21:27:38,539:INFO:  Epoch 393/500:  train Loss: 65.9484   val Loss: 68.6785   time: 0.27s   best: 62.0467
2023-10-07 21:27:38,838:INFO:  Epoch 394/500:  train Loss: 68.2123   val Loss: 68.1572   time: 0.29s   best: 62.0467
2023-10-07 21:27:39,118:INFO:  Epoch 395/500:  train Loss: 64.9824   val Loss: 66.5685   time: 0.27s   best: 62.0467
2023-10-07 21:27:39,416:INFO:  Epoch 396/500:  train Loss: 65.2815   val Loss: 67.5806   time: 0.28s   best: 62.0467
2023-10-07 21:27:39,696:INFO:  Epoch 397/500:  train Loss: 68.6584   val Loss: 68.8835   time: 0.27s   best: 62.0467
2023-10-07 21:27:39,987:INFO:  Epoch 398/500:  train Loss: 67.8567   val Loss: 66.7014   time: 0.28s   best: 62.0467
2023-10-07 21:27:40,283:INFO:  Epoch 399/500:  train Loss: 67.3583   val Loss: 64.1121   time: 0.28s   best: 62.0467
2023-10-07 21:27:40,605:INFO:  Epoch 400/500:  train Loss: 69.0070   val Loss: 65.0963   time: 0.31s   best: 62.0467
2023-10-07 21:27:40,926:INFO:  Epoch 401/500:  train Loss: 68.1603   val Loss: 63.1258   time: 0.31s   best: 62.0467
2023-10-07 21:27:41,197:INFO:  Epoch 402/500:  train Loss: 65.4815   val Loss: 73.2876   time: 0.26s   best: 62.0467
2023-10-07 21:27:41,491:INFO:  Epoch 403/500:  train Loss: 87.6776   val Loss: 97.4892   time: 0.28s   best: 62.0467
2023-10-07 21:27:41,767:INFO:  Epoch 404/500:  train Loss: 97.7918   val Loss: 95.7518   time: 0.26s   best: 62.0467
2023-10-07 21:27:42,059:INFO:  Epoch 405/500:  train Loss: 99.1192   val Loss: 99.3141   time: 0.28s   best: 62.0467
2023-10-07 21:27:42,357:INFO:  Epoch 406/500:  train Loss: 99.5921   val Loss: 99.7969   time: 0.27s   best: 62.0467
2023-10-07 21:27:42,641:INFO:  Epoch 407/500:  train Loss: 99.1451   val Loss: 97.9836   time: 0.27s   best: 62.0467
2023-10-07 21:27:42,938:INFO:  Epoch 408/500:  train Loss: 95.2722   val Loss: 93.7567   time: 0.28s   best: 62.0467
2023-10-07 21:27:43,220:INFO:  Epoch 409/500:  train Loss: 95.9604   val Loss: 95.6151   time: 0.27s   best: 62.0467
2023-10-07 21:27:43,517:INFO:  Epoch 410/500:  train Loss: 96.2332   val Loss: 96.4468   time: 0.28s   best: 62.0467
2023-10-07 21:27:43,789:INFO:  Epoch 411/500:  train Loss: 95.7820   val Loss: 95.0550   time: 0.26s   best: 62.0467
2023-10-07 21:27:44,091:INFO:  Epoch 412/500:  train Loss: 94.5808   val Loss: 94.0596   time: 0.29s   best: 62.0467
2023-10-07 21:27:44,361:INFO:  Epoch 413/500:  train Loss: 94.5907   val Loss: 94.2397   time: 0.26s   best: 62.0467
2023-10-07 21:27:44,661:INFO:  Epoch 414/500:  train Loss: 94.3046   val Loss: 94.0733   time: 0.29s   best: 62.0467
2023-10-07 21:27:44,941:INFO:  Epoch 415/500:  train Loss: 94.2515   val Loss: 94.3679   time: 0.27s   best: 62.0467
2023-10-07 21:27:45,233:INFO:  Epoch 416/500:  train Loss: 94.5417   val Loss: 94.5680   time: 0.28s   best: 62.0467
2023-10-07 21:27:45,541:INFO:  Epoch 417/500:  train Loss: 94.4559   val Loss: 94.2509   time: 0.29s   best: 62.0467
2023-10-07 21:27:45,812:INFO:  Epoch 418/500:  train Loss: 94.0100   val Loss: 93.5358   time: 0.26s   best: 62.0467
2023-10-07 21:27:46,108:INFO:  Epoch 419/500:  train Loss: 93.9603   val Loss: 93.6104   time: 0.28s   best: 62.0467
2023-10-07 21:27:46,384:INFO:  Epoch 420/500:  train Loss: 93.8378   val Loss: 93.8896   time: 0.26s   best: 62.0467
2023-10-07 21:27:46,681:INFO:  Epoch 421/500:  train Loss: 93.6998   val Loss: 93.6527   time: 0.28s   best: 62.0467
2023-10-07 21:27:46,970:INFO:  Epoch 422/500:  train Loss: 93.6089   val Loss: 93.4584   time: 0.28s   best: 62.0467
2023-10-07 21:27:47,259:INFO:  Epoch 423/500:  train Loss: 93.2999   val Loss: 93.2206   time: 0.28s   best: 62.0467
2023-10-07 21:27:47,543:INFO:  Epoch 424/500:  train Loss: 93.1664   val Loss: 92.8420   time: 0.27s   best: 62.0467
2023-10-07 21:27:47,840:INFO:  Epoch 425/500:  train Loss: 92.7614   val Loss: 92.6691   time: 0.28s   best: 62.0467
2023-10-07 21:27:48,113:INFO:  Epoch 426/500:  train Loss: 92.7179   val Loss: 92.4453   time: 0.26s   best: 62.0467
2023-10-07 21:27:48,418:INFO:  Epoch 427/500:  train Loss: 92.3935   val Loss: 92.1635   time: 0.29s   best: 62.0467
2023-10-07 21:27:48,712:INFO:  Epoch 428/500:  train Loss: 92.2850   val Loss: 92.0716   time: 0.29s   best: 62.0467
2023-10-07 21:27:48,994:INFO:  Epoch 429/500:  train Loss: 92.3000   val Loss: 91.5325   time: 0.27s   best: 62.0467
2023-10-07 21:27:49,291:INFO:  Epoch 430/500:  train Loss: 91.4806   val Loss: 90.9228   time: 0.28s   best: 62.0467
2023-10-07 21:27:49,567:INFO:  Epoch 431/500:  train Loss: 90.9303   val Loss: 90.0522   time: 0.26s   best: 62.0467
2023-10-07 21:27:49,859:INFO:  Epoch 432/500:  train Loss: 89.9910   val Loss: 89.1768   time: 0.28s   best: 62.0467
2023-10-07 21:27:50,139:INFO:  Epoch 433/500:  train Loss: 88.9086   val Loss: 88.5949   time: 0.27s   best: 62.0467
2023-10-07 21:27:50,437:INFO:  Epoch 434/500:  train Loss: 89.0170   val Loss: 87.5298   time: 0.28s   best: 62.0467
2023-10-07 21:27:50,712:INFO:  Epoch 435/500:  train Loss: 86.8884   val Loss: 85.2488   time: 0.26s   best: 62.0467
2023-10-07 21:27:51,012:INFO:  Epoch 436/500:  train Loss: 85.3861   val Loss: 85.1303   time: 0.29s   best: 62.0467
2023-10-07 21:27:51,300:INFO:  Epoch 437/500:  train Loss: 87.4859   val Loss: 87.1030   time: 0.26s   best: 62.0467
2023-10-07 21:27:51,592:INFO:  Epoch 438/500:  train Loss: 87.9047   val Loss: 85.8710   time: 0.28s   best: 62.0467
2023-10-07 21:27:51,884:INFO:  Epoch 439/500:  train Loss: 84.9036   val Loss: 85.6154   time: 0.28s   best: 62.0467
2023-10-07 21:27:52,156:INFO:  Epoch 440/500:  train Loss: 89.1688   val Loss: 87.7992   time: 0.26s   best: 62.0467
2023-10-07 21:27:52,456:INFO:  Epoch 441/500:  train Loss: 89.5952   val Loss: 89.0264   time: 0.29s   best: 62.0467
2023-10-07 21:27:52,725:INFO:  Epoch 442/500:  train Loss: 88.0625   val Loss: 86.8202   time: 0.26s   best: 62.0467
2023-10-07 21:27:53,054:INFO:  Epoch 443/500:  train Loss: 88.4239   val Loss: 86.6077   time: 0.30s   best: 62.0467
2023-10-07 21:27:53,338:INFO:  Epoch 444/500:  train Loss: 85.2509   val Loss: 84.0714   time: 0.27s   best: 62.0467
2023-10-07 21:27:53,630:INFO:  Epoch 445/500:  train Loss: 84.9985   val Loss: 82.7205   time: 0.28s   best: 62.0467
2023-10-07 21:27:53,908:INFO:  Epoch 446/500:  train Loss: 84.4476   val Loss: 84.3923   time: 0.26s   best: 62.0467
2023-10-07 21:27:54,200:INFO:  Epoch 447/500:  train Loss: 84.3852   val Loss: 82.1736   time: 0.28s   best: 62.0467
2023-10-07 21:27:54,500:INFO:  Epoch 448/500:  train Loss: 82.9130   val Loss: 82.6244   time: 0.28s   best: 62.0467
2023-10-07 21:27:54,773:INFO:  Epoch 449/500:  train Loss: 82.7019   val Loss: 81.7841   time: 0.26s   best: 62.0467
2023-10-07 21:27:55,076:INFO:  Epoch 450/500:  train Loss: 82.9026   val Loss: 81.9504   time: 0.29s   best: 62.0467
2023-10-07 21:27:55,357:INFO:  Epoch 451/500:  train Loss: 82.9671   val Loss: 82.7023   time: 0.27s   best: 62.0467
2023-10-07 21:27:55,657:INFO:  Epoch 452/500:  train Loss: 83.0570   val Loss: 82.7736   time: 0.29s   best: 62.0467
2023-10-07 21:27:55,931:INFO:  Epoch 453/500:  train Loss: 81.8048   val Loss: 82.1473   time: 0.26s   best: 62.0467
2023-10-07 21:27:56,270:INFO:  Epoch 454/500:  train Loss: 82.3336   val Loss: 81.6426   time: 0.33s   best: 62.0467
2023-10-07 21:27:56,548:INFO:  Epoch 455/500:  train Loss: 82.4688   val Loss: 81.6093   time: 0.27s   best: 62.0467
2023-10-07 21:27:56,844:INFO:  Epoch 456/500:  train Loss: 82.5863   val Loss: 81.4707   time: 0.28s   best: 62.0467
2023-10-07 21:27:57,125:INFO:  Epoch 457/500:  train Loss: 81.8426   val Loss: 81.1760   time: 0.27s   best: 62.0467
2023-10-07 21:27:57,419:INFO:  Epoch 458/500:  train Loss: 83.4192   val Loss: 83.6371   time: 0.28s   best: 62.0467
2023-10-07 21:27:57,723:INFO:  Epoch 459/500:  train Loss: 83.8762   val Loss: 83.0284   time: 0.29s   best: 62.0467
2023-10-07 21:27:57,991:INFO:  Epoch 460/500:  train Loss: 82.9176   val Loss: 81.9496   time: 0.26s   best: 62.0467
2023-10-07 21:27:58,298:INFO:  Epoch 461/500:  train Loss: 81.4042   val Loss: 80.6222   time: 0.29s   best: 62.0467
2023-10-07 21:27:58,578:INFO:  Epoch 462/500:  train Loss: 82.9007   val Loss: 81.1463   time: 0.27s   best: 62.0467
2023-10-07 21:27:58,871:INFO:  Epoch 463/500:  train Loss: 81.6339   val Loss: 80.7190   time: 0.28s   best: 62.0467
2023-10-07 21:27:59,151:INFO:  Epoch 464/500:  train Loss: 81.0825   val Loss: 80.6108   time: 0.27s   best: 62.0467
2023-10-07 21:27:59,443:INFO:  Epoch 465/500:  train Loss: 80.7600   val Loss: 79.5450   time: 0.28s   best: 62.0467
2023-10-07 21:27:59,723:INFO:  Epoch 466/500:  train Loss: 79.5803   val Loss: 83.6772   time: 0.27s   best: 62.0467
2023-10-07 21:28:00,014:INFO:  Epoch 467/500:  train Loss: 80.3850   val Loss: 79.8316   time: 0.29s   best: 62.0467
2023-10-07 21:28:00,294:INFO:  Epoch 468/500:  train Loss: 79.8992   val Loss: 77.4980   time: 0.27s   best: 62.0467
2023-10-07 21:28:00,601:INFO:  Epoch 469/500:  train Loss: 80.8642   val Loss: 80.2271   time: 0.29s   best: 62.0467
2023-10-07 21:28:00,899:INFO:  Epoch 470/500:  train Loss: 80.7712   val Loss: 79.1118   time: 0.28s   best: 62.0467
2023-10-07 21:28:01,378:INFO:  Epoch 471/500:  train Loss: 80.2163   val Loss: 79.0890   time: 0.47s   best: 62.0467
2023-10-07 21:28:01,703:INFO:  Epoch 472/500:  train Loss: 79.7052   val Loss: 77.1431   time: 0.31s   best: 62.0467
2023-10-07 21:28:01,991:INFO:  Epoch 473/500:  train Loss: 78.9407   val Loss: 77.6769   time: 0.28s   best: 62.0467
2023-10-07 21:28:02,270:INFO:  Epoch 474/500:  train Loss: 78.9041   val Loss: 84.7209   time: 0.27s   best: 62.0467
2023-10-07 21:28:02,573:INFO:  Epoch 475/500:  train Loss: 86.7708   val Loss: 85.3939   time: 0.29s   best: 62.0467
2023-10-07 21:28:02,860:INFO:  Epoch 476/500:  train Loss: 87.4842   val Loss: 82.7405   time: 0.27s   best: 62.0467
2023-10-07 21:28:03,154:INFO:  Epoch 477/500:  train Loss: 80.6350   val Loss: 77.8765   time: 0.28s   best: 62.0467
2023-10-07 21:28:03,435:INFO:  Epoch 478/500:  train Loss: 82.4296   val Loss: 79.7959   time: 0.26s   best: 62.0467
2023-10-07 21:28:03,740:INFO:  Epoch 479/500:  train Loss: 81.5147   val Loss: 78.3995   time: 0.29s   best: 62.0467
2023-10-07 21:28:04,033:INFO:  Epoch 480/500:  train Loss: 78.2643   val Loss: 76.4652   time: 0.28s   best: 62.0467
2023-10-07 21:28:04,308:INFO:  Epoch 481/500:  train Loss: 77.0930   val Loss: 74.8740   time: 0.26s   best: 62.0467
2023-10-07 21:28:04,608:INFO:  Epoch 482/500:  train Loss: 75.6913   val Loss: 76.7682   time: 0.29s   best: 62.0467
2023-10-07 21:28:04,882:INFO:  Epoch 483/500:  train Loss: 76.4516   val Loss: 73.4869   time: 0.26s   best: 62.0467
2023-10-07 21:28:05,183:INFO:  Epoch 484/500:  train Loss: 77.5585   val Loss: 77.1910   time: 0.29s   best: 62.0467
2023-10-07 21:28:05,461:INFO:  Epoch 485/500:  train Loss: 79.4584   val Loss: 78.8608   time: 0.26s   best: 62.0467
2023-10-07 21:28:05,764:INFO:  Epoch 486/500:  train Loss: 79.1234   val Loss: 76.8941   time: 0.29s   best: 62.0467
2023-10-07 21:28:06,040:INFO:  Epoch 487/500:  train Loss: 77.9600   val Loss: 77.4946   time: 0.26s   best: 62.0467
2023-10-07 21:28:06,335:INFO:  Epoch 488/500:  train Loss: 76.7383   val Loss: 73.8333   time: 0.28s   best: 62.0467
2023-10-07 21:28:06,636:INFO:  Epoch 489/500:  train Loss: 74.4772   val Loss: 72.3058   time: 0.29s   best: 62.0467
2023-10-07 21:28:06,917:INFO:  Epoch 490/500:  train Loss: 73.3027   val Loss: 72.0499   time: 0.27s   best: 62.0467
2023-10-07 21:28:07,206:INFO:  Epoch 491/500:  train Loss: 75.1710   val Loss: 75.8138   time: 0.28s   best: 62.0467
2023-10-07 21:28:07,489:INFO:  Epoch 492/500:  train Loss: 74.1749   val Loss: 74.8116   time: 0.27s   best: 62.0467
2023-10-07 21:28:07,789:INFO:  Epoch 493/500:  train Loss: 76.8087   val Loss: 74.5257   time: 0.29s   best: 62.0467
2023-10-07 21:28:08,065:INFO:  Epoch 494/500:  train Loss: 76.6256   val Loss: 75.3456   time: 0.26s   best: 62.0467
2023-10-07 21:28:08,360:INFO:  Epoch 495/500:  train Loss: 73.8111   val Loss: 73.6011   time: 0.28s   best: 62.0467
2023-10-07 21:28:08,638:INFO:  Epoch 496/500:  train Loss: 73.8944   val Loss: 76.8591   time: 0.26s   best: 62.0467
2023-10-07 21:28:08,934:INFO:  Epoch 497/500:  train Loss: 77.1245   val Loss: 76.4225   time: 0.28s   best: 62.0467
2023-10-07 21:28:09,231:INFO:  Epoch 498/500:  train Loss: 75.1397   val Loss: 73.5034   time: 0.27s   best: 62.0467
2023-10-07 21:28:09,509:INFO:  Epoch 499/500:  train Loss: 74.8785   val Loss: 69.7843   time: 0.27s   best: 62.0467
2023-10-07 21:28:09,852:INFO:  Epoch 500/500:  train Loss: 73.7416   val Loss: 72.9430   time: 0.33s   best: 62.0467
2023-10-07 21:28:09,852:INFO:  -----> Training complete in 2m 38s   best validation loss: 62.0467
 
2023-10-07 21:34:51,532:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-07 21:34:51,816:INFO:  Epoch 216/500:  train Loss: 18.9225   val Loss: 22.7835   time: 488.75s   best: 22.7835
2023-10-07 21:42:47,200:INFO:  Epoch 217/500:  train Loss: 18.5864   val Loss: 23.7996   time: 475.37s   best: 22.7835
2023-10-07 21:50:50,621:INFO:  Epoch 218/500:  train Loss: 18.6366   val Loss: 26.8986   time: 483.41s   best: 22.7835
2023-10-07 21:58:44,413:INFO:  Epoch 219/500:  train Loss: 18.9765   val Loss: 37.7523   time: 473.77s   best: 22.7835
2023-10-07 22:06:55,228:INFO:  Epoch 220/500:  train Loss: 18.9265   val Loss: 24.9152   time: 490.78s   best: 22.7835
2023-10-07 22:15:07,404:INFO:  Epoch 221/500:  train Loss: 18.6414   val Loss: 23.3967   time: 492.16s   best: 22.7835
2023-10-07 22:23:23,046:INFO:  Epoch 222/500:  train Loss: 18.7459   val Loss: 23.5537   time: 495.60s   best: 22.7835
2023-10-07 22:31:48,218:INFO:  Epoch 223/500:  train Loss: 18.6341   val Loss: 23.2083   time: 505.14s   best: 22.7835
2023-10-07 22:39:57,411:INFO:  Epoch 224/500:  train Loss: 18.7827   val Loss: 23.9219   time: 489.09s   best: 22.7835
2023-10-07 22:48:02,744:INFO:  Epoch 225/500:  train Loss: 18.6407   val Loss: 23.1675   time: 485.31s   best: 22.7835
2023-10-07 22:56:10,854:INFO:  Epoch 226/500:  train Loss: 18.5319   val Loss: 22.9996   time: 488.10s   best: 22.7835
2023-10-07 23:04:20,439:INFO:  Epoch 227/500:  train Loss: 18.4684   val Loss: 22.9347   time: 489.54s   best: 22.7835
2023-10-07 23:12:39,889:INFO:  Epoch 228/500:  train Loss: 18.4575   val Loss: 23.4882   time: 499.42s   best: 22.7835
2023-10-07 23:20:55,030:INFO:  Epoch 229/500:  train Loss: 18.4982   val Loss: 23.4300   time: 495.13s   best: 22.7835
2023-10-07 23:29:20,851:INFO:  Epoch 230/500:  train Loss: 18.5909   val Loss: 23.4063   time: 505.79s   best: 22.7835
2023-10-07 23:37:55,316:INFO:  Epoch 231/500:  train Loss: 18.7899   val Loss: 23.5850   time: 514.43s   best: 22.7835
2023-10-07 23:46:25,214:INFO:  Epoch 232/500:  train Loss: 18.5257   val Loss: 23.4818   time: 509.86s   best: 22.7835
2023-10-07 23:54:46,515:INFO:  Epoch 233/500:  train Loss: 18.5750   val Loss: 24.2706   time: 501.25s   best: 22.7835
2023-10-08 00:03:11,874:INFO:  Epoch 234/500:  train Loss: 18.6802   val Loss: 23.5428   time: 505.32s   best: 22.7835
2023-10-08 00:11:39,870:INFO:  Epoch 235/500:  train Loss: 18.5640   val Loss: 23.2312   time: 507.97s   best: 22.7835
2023-10-08 00:19:58,671:INFO:  Epoch 236/500:  train Loss: 18.4791   val Loss: 23.2376   time: 498.79s   best: 22.7835
2023-10-08 00:28:11,408:INFO:  Epoch 237/500:  train Loss: 18.4816   val Loss: 23.3434   time: 492.69s   best: 22.7835
2023-10-08 00:36:31,262:INFO:  Epoch 238/500:  train Loss: 18.5202   val Loss: 23.1466   time: 499.81s   best: 22.7835
2023-10-08 00:44:47,148:INFO:  Epoch 239/500:  train Loss: 18.4137   val Loss: 23.2318   time: 495.87s   best: 22.7835
2023-10-08 00:53:00,872:INFO:  Epoch 240/500:  train Loss: 18.4997   val Loss: 24.9932   time: 493.70s   best: 22.7835
2023-10-08 01:01:21,044:INFO:  Epoch 241/500:  train Loss: 18.4581   val Loss: 23.2385   time: 500.13s   best: 22.7835
2023-10-08 01:09:31,668:INFO:  Epoch 242/500:  train Loss: 18.8326   val Loss: 23.6259   time: 490.59s   best: 22.7835
2023-10-08 01:17:42,995:INFO:  Epoch 243/500:  train Loss: 18.5402   val Loss: 23.6805   time: 491.32s   best: 22.7835
2023-10-08 01:26:00,797:INFO:  Epoch 244/500:  train Loss: 18.3704   val Loss: 23.0893   time: 497.79s   best: 22.7835
2023-10-08 01:34:04,695:INFO:  Epoch 245/500:  train Loss: 18.3904   val Loss: 24.3328   time: 483.86s   best: 22.7835
2023-10-08 01:42:17,567:INFO:  Epoch 246/500:  train Loss: 18.4996   val Loss: 25.9486   time: 492.86s   best: 22.7835
2023-10-08 01:50:25,790:INFO:  Epoch 247/500:  train Loss: 18.5036   val Loss: 24.9975   time: 488.17s   best: 22.7835
2023-10-08 01:58:37,458:INFO:  Epoch 248/500:  train Loss: 18.3767   val Loss: 22.9624   time: 491.64s   best: 22.7835
2023-10-08 02:06:47,428:INFO:  Epoch 249/500:  train Loss: 18.3284   val Loss: 23.1659   time: 489.94s   best: 22.7835
2023-10-08 02:15:03,488:INFO:  Epoch 250/500:  train Loss: 18.3639   val Loss: 23.4537   time: 496.03s   best: 22.7835
2023-10-08 02:23:14,289:INFO:  Epoch 251/500:  train Loss: 18.4075   val Loss: 23.3519   time: 490.75s   best: 22.7835
2023-10-08 02:31:20,396:INFO:  Epoch 252/500:  train Loss: 18.3522   val Loss: 23.9096   time: 486.06s   best: 22.7835
2023-10-08 02:39:21,377:INFO:  Epoch 253/500:  train Loss: 18.3817   val Loss: 23.0707   time: 480.95s   best: 22.7835
2023-10-08 02:47:26,675:INFO:  Epoch 254/500:  train Loss: 18.5976   val Loss: 23.6117   time: 485.24s   best: 22.7835
2023-10-08 02:55:25,463:INFO:  Epoch 255/500:  train Loss: 18.4783   val Loss: 23.4665   time: 478.73s   best: 22.7835
2023-10-08 03:03:43,453:INFO:  Epoch 256/500:  train Loss: 18.5483   val Loss: 23.8004   time: 497.95s   best: 22.7835
2023-10-08 03:11:54,167:INFO:  Epoch 257/500:  train Loss: 18.2819   val Loss: 23.5026   time: 490.69s   best: 22.7835
2023-10-08 03:19:55,667:INFO:  Epoch 258/500:  train Loss: 18.5532   val Loss: 25.1500   time: 481.49s   best: 22.7835
2023-10-08 03:27:59,383:INFO:  Epoch 259/500:  train Loss: 18.4012   val Loss: 24.0424   time: 483.68s   best: 22.7835
2023-10-08 03:35:58,130:INFO:  Epoch 260/500:  train Loss: 18.3383   val Loss: 23.6870   time: 478.73s   best: 22.7835
2023-10-08 03:43:59,243:INFO:  Epoch 261/500:  train Loss: 18.4992   val Loss: 23.5195   time: 481.06s   best: 22.7835
2023-10-08 03:52:10,557:INFO:  Epoch 262/500:  train Loss: 18.2558   val Loss: 23.5566   time: 491.25s   best: 22.7835
2023-10-08 04:00:09,546:INFO:  Epoch 263/500:  train Loss: 18.1988   val Loss: 23.5488   time: 478.95s   best: 22.7835
2023-10-08 04:08:14,842:INFO:  Epoch 264/500:  train Loss: 18.2949   val Loss: 23.5416   time: 485.27s   best: 22.7835
2023-10-08 04:16:18,808:INFO:  Epoch 265/500:  train Loss: 18.2777   val Loss: 23.9663   time: 483.94s   best: 22.7835
2023-10-08 04:24:34,821:INFO:  Epoch 266/500:  train Loss: 18.1905   val Loss: 23.5209   time: 495.98s   best: 22.7835
2023-10-08 04:32:41,532:INFO:  Epoch 267/500:  train Loss: 18.2332   val Loss: 23.4215   time: 486.69s   best: 22.7835
2023-10-08 04:40:50,406:INFO:  Epoch 268/500:  train Loss: 18.2837   val Loss: 23.3711   time: 488.84s   best: 22.7835
2023-10-08 04:49:03,312:INFO:  Epoch 269/500:  train Loss: 18.1380   val Loss: 23.6664   time: 492.88s   best: 22.7835
2023-10-08 04:57:06,290:INFO:  Epoch 270/500:  train Loss: 18.1404   val Loss: 29.1731   time: 482.95s   best: 22.7835
2023-10-08 05:05:14,289:INFO:  Epoch 271/500:  train Loss: 18.1680   val Loss: 23.3326   time: 487.94s   best: 22.7835
2023-10-08 05:13:19,604:INFO:  Epoch 272/500:  train Loss: 18.1561   val Loss: 23.4543   time: 485.28s   best: 22.7835
2023-10-08 05:21:19,141:INFO:  Epoch 273/500:  train Loss: 18.1970   val Loss: 22.8342   time: 479.51s   best: 22.7835
2023-10-08 05:29:26,468:INFO:  Epoch 274/500:  train Loss: 18.1265   val Loss: 22.9434   time: 487.30s   best: 22.7835
2023-10-08 05:37:15,391:INFO:  Epoch 275/500:  train Loss: 18.1878   val Loss: 23.2045   time: 468.90s   best: 22.7835
2023-10-08 05:45:12,292:INFO:  Epoch 276/500:  train Loss: 18.3061   val Loss: 23.8545   time: 476.88s   best: 22.7835
2023-10-08 05:53:09,574:INFO:  Epoch 277/500:  train Loss: 18.1794   val Loss: 25.8957   time: 477.25s   best: 22.7835
2023-10-08 06:00:57,749:INFO:  Epoch 278/500:  train Loss: 18.1703   val Loss: 23.4985   time: 468.14s   best: 22.7835
2023-10-08 06:09:02,069:INFO:  Epoch 279/500:  train Loss: 18.1458   val Loss: 23.0286   time: 484.29s   best: 22.7835
2023-10-08 06:16:57,787:INFO:  Epoch 280/500:  train Loss: 18.2954   val Loss: 23.4083   time: 475.70s   best: 22.7835
2023-10-08 06:24:56,380:INFO:  Epoch 281/500:  train Loss: 18.3653   val Loss: 24.3047   time: 478.57s   best: 22.7835
2023-10-08 06:33:11,697:INFO:  Epoch 282/500:  train Loss: 18.0533   val Loss: 23.6864   time: 495.30s   best: 22.7835
2023-10-08 06:41:18,351:INFO:  Epoch 283/500:  train Loss: 18.1633   val Loss: 23.3677   time: 486.62s   best: 22.7835
2023-10-08 06:49:18,518:INFO:  Epoch 284/500:  train Loss: 18.1298   val Loss: 23.1626   time: 480.15s   best: 22.7835
2023-10-08 06:57:20,989:INFO:  Epoch 285/500:  train Loss: 18.1812   val Loss: 23.4220   time: 482.41s   best: 22.7835
2023-10-08 07:05:31,606:INFO:  Epoch 286/500:  train Loss: 18.2171   val Loss: 23.2800   time: 490.59s   best: 22.7835
2023-10-08 07:13:23,877:INFO:  Epoch 287/500:  train Loss: 18.1326   val Loss: 23.6548   time: 472.24s   best: 22.7835
2023-10-08 07:21:23,092:INFO:  Epoch 288/500:  train Loss: 18.0443   val Loss: 23.2448   time: 479.21s   best: 22.7835
2023-10-08 07:29:06,768:INFO:  Epoch 289/500:  train Loss: 18.0282   val Loss: 23.1957   time: 463.65s   best: 22.7835
2023-10-08 07:36:59,896:INFO:  Epoch 290/500:  train Loss: 18.0471   val Loss: 25.1650   time: 473.10s   best: 22.7835
2023-10-08 07:44:52,951:INFO:  Epoch 291/500:  train Loss: 18.0235   val Loss: 23.3761   time: 473.04s   best: 22.7835
2023-10-08 07:53:11,029:INFO:  Epoch 292/500:  train Loss: 18.0625   val Loss: 23.0554   time: 498.05s   best: 22.7835
2023-10-08 08:01:25,237:INFO:  Epoch 293/500:  train Loss: 18.2780   val Loss: 23.7348   time: 494.19s   best: 22.7835
2023-10-08 08:09:33,730:INFO:  Epoch 294/500:  train Loss: 18.0151   val Loss: 23.4918   time: 488.47s   best: 22.7835
2023-10-08 08:17:38,446:INFO:  Epoch 295/500:  train Loss: 18.0225   val Loss: 23.1694   time: 484.70s   best: 22.7835
2023-10-08 08:25:37,369:INFO:  Epoch 296/500:  train Loss: 17.9873   val Loss: 23.5050   time: 478.89s   best: 22.7835
2023-10-08 08:33:40,182:INFO:  Epoch 297/500:  train Loss: 18.0053   val Loss: 23.1587   time: 482.78s   best: 22.7835
2023-10-08 08:41:34,924:INFO:  Epoch 298/500:  train Loss: 17.9338   val Loss: 23.4120   time: 474.71s   best: 22.7835
2023-10-08 08:49:39,003:INFO:  Epoch 299/500:  train Loss: 18.2638   val Loss: 23.5335   time: 484.03s   best: 22.7835
2023-10-08 08:57:31,795:INFO:  Epoch 300/500:  train Loss: 17.8972   val Loss: 23.5605   time: 472.76s   best: 22.7835
2023-10-08 09:05:34,893:INFO:  Epoch 301/500:  train Loss: 18.1061   val Loss: 23.7272   time: 483.07s   best: 22.7835
2023-10-08 09:13:24,857:INFO:  Epoch 302/500:  train Loss: 17.9922   val Loss: 22.9641   time: 469.90s   best: 22.7835
2023-10-08 09:21:25,233:INFO:  Epoch 303/500:  train Loss: 18.0308   val Loss: 23.3732   time: 480.36s   best: 22.7835
2023-10-08 09:29:28,360:INFO:  Epoch 304/500:  train Loss: 18.0087   val Loss: 23.4941   time: 483.11s   best: 22.7835
2023-10-08 09:37:28,456:INFO:  Epoch 305/500:  train Loss: 18.0856   val Loss: 23.2954   time: 480.06s   best: 22.7835
2023-10-08 09:45:23,356:INFO:  Epoch 306/500:  train Loss: 18.3214   val Loss: 23.1323   time: 474.87s   best: 22.7835
2023-10-08 09:53:18,785:INFO:  Epoch 307/500:  train Loss: 18.1127   val Loss: 23.2731   time: 475.40s   best: 22.7835
2023-10-08 09:56:41,281:INFO:  Starting experiment lstm autoencoder debug
2023-10-08 09:56:41,294:INFO:  Defining the model
2023-10-08 09:56:41,342:INFO:  Reading the dataset
2023-10-08 09:57:02,805:INFO:  Starting experiment lstm autoencoder debug
2023-10-08 09:57:02,806:INFO:  Defining the model
2023-10-08 09:57:02,850:INFO:  Reading the dataset
2023-10-08 09:57:35,286:INFO:  Starting experiment lstm autoencoder debug
2023-10-08 09:57:35,286:INFO:  Defining the model
2023-10-08 09:57:35,330:INFO:  Reading the dataset
2023-10-08 09:57:43,095:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:43,124:INFO:  Epoch 1/500:  train Loss: 99.2820   val Loss: 97.2127   time: 2.09s   best: 97.2127
2023-10-08 09:57:43,429:INFO:  Epoch 2/500:  train Loss: 98.8403   val Loss: 100.0758   time: 0.30s   best: 97.2127
2023-10-08 09:57:43,736:INFO:  Epoch 3/500:  train Loss: 99.3664   val Loss: 100.0475   time: 0.30s   best: 97.2127
2023-10-08 09:57:44,030:INFO:  Epoch 4/500:  train Loss: 98.9470   val Loss: 97.9293   time: 0.29s   best: 97.2127
2023-10-08 09:57:44,332:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:44,426:INFO:  Epoch 5/500:  train Loss: 97.7169   val Loss: 94.6194   time: 0.30s   best: 94.6194
2023-10-08 09:57:44,730:INFO:  Epoch 6/500:  train Loss: 97.5818   val Loss: 95.1822   time: 0.30s   best: 94.6194
2023-10-08 09:57:45,020:INFO:  Epoch 7/500:  train Loss: 97.7828   val Loss: 97.8244   time: 0.29s   best: 94.6194
2023-10-08 09:57:45,335:INFO:  Epoch 8/500:  train Loss: 97.6695   val Loss: 97.4203   time: 0.31s   best: 94.6194
2023-10-08 09:57:45,626:INFO:  Epoch 9/500:  train Loss: 97.1299   val Loss: 96.3104   time: 0.29s   best: 94.6194
2023-10-08 09:57:45,929:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:45,954:INFO:  Epoch 10/500:  train Loss: 95.7810   val Loss: 93.3522   time: 0.30s   best: 93.3522
2023-10-08 09:57:46,263:INFO:  Epoch 11/500:  train Loss: 94.9568   val Loss: 94.7174   time: 0.30s   best: 93.3522
2023-10-08 09:57:46,548:INFO:  Epoch 12/500:  train Loss: 94.4018   val Loss: 94.6660   time: 0.28s   best: 93.3522
2023-10-08 09:57:46,853:INFO:  Epoch 13/500:  train Loss: 93.9665   val Loss: 94.5561   time: 0.30s   best: 93.3522
2023-10-08 09:57:47,156:INFO:  Epoch 14/500:  train Loss: 93.9341   val Loss: 94.4166   time: 0.30s   best: 93.3522
2023-10-08 09:57:47,461:INFO:  Epoch 15/500:  train Loss: 94.3168   val Loss: 94.2551   time: 0.30s   best: 93.3522
2023-10-08 09:57:47,766:INFO:  Epoch 16/500:  train Loss: 93.5461   val Loss: 94.0104   time: 0.30s   best: 93.3522
2023-10-08 09:57:48,054:INFO:  Epoch 17/500:  train Loss: 93.1355   val Loss: 94.0846   time: 0.28s   best: 93.3522
2023-10-08 09:57:48,361:INFO:  Epoch 18/500:  train Loss: 93.7542   val Loss: 94.2479   time: 0.30s   best: 93.3522
2023-10-08 09:57:48,651:INFO:  Epoch 19/500:  train Loss: 93.7186   val Loss: 94.3804   time: 0.29s   best: 93.3522
2023-10-08 09:57:48,952:INFO:  Epoch 20/500:  train Loss: 94.1684   val Loss: 94.4682   time: 0.30s   best: 93.3522
2023-10-08 09:57:49,252:INFO:  Epoch 21/500:  train Loss: 94.2417   val Loss: 94.5073   time: 0.29s   best: 93.3522
2023-10-08 09:57:49,562:INFO:  Epoch 22/500:  train Loss: 94.7018   val Loss: 94.4994   time: 0.31s   best: 93.3522
2023-10-08 09:57:49,866:INFO:  Epoch 23/500:  train Loss: 94.3683   val Loss: 94.3877   time: 0.30s   best: 93.3522
2023-10-08 09:57:50,161:INFO:  Epoch 24/500:  train Loss: 93.9571   val Loss: 94.1932   time: 0.29s   best: 93.3522
2023-10-08 09:57:50,467:INFO:  Epoch 25/500:  train Loss: 93.7292   val Loss: 93.9713   time: 0.30s   best: 93.3522
2023-10-08 09:57:50,750:INFO:  Epoch 26/500:  train Loss: 93.5230   val Loss: 93.7197   time: 0.28s   best: 93.3522
2023-10-08 09:57:51,069:INFO:  Epoch 27/500:  train Loss: 92.9181   val Loss: 93.8243   time: 0.31s   best: 93.3522
2023-10-08 09:57:51,355:INFO:  Epoch 28/500:  train Loss: 93.3243   val Loss: 93.9228   time: 0.28s   best: 93.3522
2023-10-08 09:57:51,666:INFO:  Epoch 29/500:  train Loss: 93.6471   val Loss: 94.0319   time: 0.31s   best: 93.3522
2023-10-08 09:57:51,975:INFO:  Epoch 30/500:  train Loss: 93.8049   val Loss: 94.0983   time: 0.30s   best: 93.3522
2023-10-08 09:57:52,269:INFO:  Epoch 31/500:  train Loss: 94.0451   val Loss: 94.0742   time: 0.29s   best: 93.3522
2023-10-08 09:57:52,563:INFO:  Epoch 32/500:  train Loss: 93.8598   val Loss: 94.0279   time: 0.29s   best: 93.3522
2023-10-08 09:57:52,816:INFO:  Epoch 33/500:  train Loss: 94.0803   val Loss: 93.8879   time: 0.25s   best: 93.3522
2023-10-08 09:57:53,130:INFO:  Epoch 34/500:  train Loss: 93.9503   val Loss: 93.5668   time: 0.31s   best: 93.3522
2023-10-08 09:57:53,421:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:53,484:INFO:  Epoch 35/500:  train Loss: 93.1305   val Loss: 93.2094   time: 0.29s   best: 93.2094
2023-10-08 09:57:53,775:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:54,062:INFO:  Epoch 36/500:  train Loss: 92.8341   val Loss: 92.8577   time: 0.28s   best: 92.8577
2023-10-08 09:57:54,354:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:54,707:INFO:  Epoch 37/500:  train Loss: 92.8704   val Loss: 92.5816   time: 0.29s   best: 92.5816
2023-10-08 09:57:55,005:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:55,064:INFO:  Epoch 38/500:  train Loss: 92.5051   val Loss: 92.2143   time: 0.28s   best: 92.2143
2023-10-08 09:57:55,362:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:55,388:INFO:  Epoch 39/500:  train Loss: 92.2446   val Loss: 91.6847   time: 0.29s   best: 91.6847
2023-10-08 09:57:55,695:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:55,783:INFO:  Epoch 40/500:  train Loss: 91.4861   val Loss: 91.1335   time: 0.30s   best: 91.1335
2023-10-08 09:57:56,090:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:56,125:INFO:  Epoch 41/500:  train Loss: 90.8999   val Loss: 90.6090   time: 0.30s   best: 90.6090
2023-10-08 09:57:56,415:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:56,446:INFO:  Epoch 42/500:  train Loss: 90.3232   val Loss: 90.1347   time: 0.29s   best: 90.1347
2023-10-08 09:57:56,747:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:56,869:INFO:  Epoch 43/500:  train Loss: 90.4456   val Loss: 89.8786   time: 0.30s   best: 89.8786
2023-10-08 09:57:57,182:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:57,209:INFO:  Epoch 44/500:  train Loss: 90.1283   val Loss: 89.4463   time: 0.31s   best: 89.4463
2023-10-08 09:57:57,499:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:57,531:INFO:  Epoch 45/500:  train Loss: 89.7307   val Loss: 88.7773   time: 0.29s   best: 88.7773
2023-10-08 09:57:57,841:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:58,067:INFO:  Epoch 46/500:  train Loss: 88.9238   val Loss: 87.9462   time: 0.30s   best: 87.9462
2023-10-08 09:57:58,368:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:58,401:INFO:  Epoch 47/500:  train Loss: 88.1181   val Loss: 87.4205   time: 0.29s   best: 87.4205
2023-10-08 09:57:58,705:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:57:58,749:INFO:  Epoch 48/500:  train Loss: 87.6470   val Loss: 86.9784   time: 0.30s   best: 86.9784
2023-10-08 09:57:59,041:INFO:  Epoch 49/500:  train Loss: 87.5918   val Loss: 87.5534   time: 0.29s   best: 86.9784
2023-10-08 09:57:59,350:INFO:  Epoch 50/500:  train Loss: 88.2939   val Loss: 87.1761   time: 0.30s   best: 86.9784
2023-10-08 09:57:59,662:INFO:  Epoch 51/500:  train Loss: 87.6320   val Loss: 88.3986   time: 0.31s   best: 86.9784
2023-10-08 09:57:59,950:INFO:  Epoch 52/500:  train Loss: 88.9350   val Loss: 87.9826   time: 0.28s   best: 86.9784
2023-10-08 09:58:00,260:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:00,288:INFO:  Epoch 53/500:  train Loss: 88.2932   val Loss: 86.9567   time: 0.30s   best: 86.9567
2023-10-08 09:58:00,575:INFO:  Epoch 54/500:  train Loss: 87.2546   val Loss: 86.9615   time: 0.28s   best: 86.9567
2023-10-08 09:58:00,874:INFO:  Epoch 55/500:  train Loss: 87.8762   val Loss: 87.4443   time: 0.29s   best: 86.9567
2023-10-08 09:58:01,181:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:01,239:INFO:  Epoch 56/500:  train Loss: 87.5717   val Loss: 86.4019   time: 0.30s   best: 86.4019
2023-10-08 09:58:01,529:INFO:  Epoch 57/500:  train Loss: 86.9767   val Loss: 86.5412   time: 0.29s   best: 86.4019
2023-10-08 09:58:01,834:INFO:  Epoch 58/500:  train Loss: 87.4868   val Loss: 86.6257   time: 0.30s   best: 86.4019
2023-10-08 09:58:02,129:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:02,184:INFO:  Epoch 59/500:  train Loss: 86.5468   val Loss: 86.3358   time: 0.29s   best: 86.3358
2023-10-08 09:58:02,486:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:02,728:INFO:  Epoch 60/500:  train Loss: 87.0386   val Loss: 86.1482   time: 0.30s   best: 86.1482
2023-10-08 09:58:03,027:INFO:  Epoch 61/500:  train Loss: 86.6759   val Loss: 86.4249   time: 0.29s   best: 86.1482
2023-10-08 09:58:03,339:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:03,390:INFO:  Epoch 62/500:  train Loss: 86.8100   val Loss: 85.3562   time: 0.31s   best: 85.3562
2023-10-08 09:58:03,686:INFO:  Epoch 63/500:  train Loss: 86.8953   val Loss: 86.1619   time: 0.29s   best: 85.3562
2023-10-08 09:58:03,992:INFO:  Epoch 64/500:  train Loss: 86.4652   val Loss: 85.8955   time: 0.30s   best: 85.3562
2023-10-08 09:58:04,284:INFO:  Epoch 65/500:  train Loss: 86.5231   val Loss: 85.4842   time: 0.29s   best: 85.3562
2023-10-08 09:58:04,591:INFO:  Epoch 66/500:  train Loss: 85.9143   val Loss: 85.5894   time: 0.30s   best: 85.3562
2023-10-08 09:58:04,892:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:05,215:INFO:  Epoch 67/500:  train Loss: 86.2902   val Loss: 84.8709   time: 0.29s   best: 84.8709
2023-10-08 09:58:05,515:INFO:  Epoch 68/500:  train Loss: 86.1654   val Loss: 85.2574   time: 0.30s   best: 84.8709
2023-10-08 09:58:05,803:INFO:  Epoch 69/500:  train Loss: 86.4934   val Loss: 86.8186   time: 0.28s   best: 84.8709
2023-10-08 09:58:06,113:INFO:  Epoch 70/500:  train Loss: 86.9027   val Loss: 85.4306   time: 0.31s   best: 84.8709
2023-10-08 09:58:06,419:INFO:  Epoch 71/500:  train Loss: 86.4050   val Loss: 84.9296   time: 0.30s   best: 84.8709
2023-10-08 09:58:06,706:INFO:  Epoch 72/500:  train Loss: 85.9678   val Loss: 85.8483   time: 0.28s   best: 84.8709
2023-10-08 09:58:07,016:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:07,045:INFO:  Epoch 73/500:  train Loss: 85.7672   val Loss: 84.5343   time: 0.30s   best: 84.5343
2023-10-08 09:58:07,339:INFO:  Epoch 74/500:  train Loss: 86.2326   val Loss: 84.8522   time: 0.29s   best: 84.5343
2023-10-08 09:58:07,647:INFO:  Epoch 75/500:  train Loss: 85.0195   val Loss: 85.0091   time: 0.30s   best: 84.5343
2023-10-08 09:58:07,956:INFO:  Epoch 76/500:  train Loss: 85.9302   val Loss: 84.6483   time: 0.30s   best: 84.5343
2023-10-08 09:58:08,250:INFO:  Epoch 77/500:  train Loss: 84.0464   val Loss: 85.3439   time: 0.29s   best: 84.5343
2023-10-08 09:58:08,560:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:08,648:INFO:  Epoch 78/500:  train Loss: 87.0353   val Loss: 83.9795   time: 0.30s   best: 83.9795
2023-10-08 09:58:08,938:INFO:  Epoch 79/500:  train Loss: 85.7933   val Loss: 85.8755   time: 0.29s   best: 83.9795
2023-10-08 09:58:09,385:INFO:  Epoch 80/500:  train Loss: 86.2144   val Loss: 84.5742   time: 0.45s   best: 83.9795
2023-10-08 09:58:09,790:INFO:  Epoch 81/500:  train Loss: 85.5766   val Loss: 84.1715   time: 0.40s   best: 83.9795
2023-10-08 09:58:10,094:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:10,120:INFO:  Epoch 82/500:  train Loss: 83.9509   val Loss: 82.9485   time: 0.30s   best: 82.9485
2023-10-08 09:58:10,413:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:10,743:INFO:  Epoch 83/500:  train Loss: 83.2450   val Loss: 82.1841   time: 0.29s   best: 82.1841
2023-10-08 09:58:11,045:INFO:  Epoch 84/500:  train Loss: 84.0921   val Loss: 82.2319   time: 0.30s   best: 82.1841
2023-10-08 09:58:11,334:INFO:  Epoch 85/500:  train Loss: 85.4548   val Loss: 87.7364   time: 0.29s   best: 82.1841
2023-10-08 09:58:11,642:INFO:  Epoch 86/500:  train Loss: 87.6047   val Loss: 85.0862   time: 0.30s   best: 82.1841
2023-10-08 09:58:11,943:INFO:  Epoch 87/500:  train Loss: 85.0347   val Loss: 83.7997   time: 0.30s   best: 82.1841
2023-10-08 09:58:12,251:INFO:  Epoch 88/500:  train Loss: 84.9258   val Loss: 83.6120   time: 0.30s   best: 82.1841
2023-10-08 09:58:12,537:INFO:  Epoch 89/500:  train Loss: 83.9731   val Loss: 83.6875   time: 0.28s   best: 82.1841
2023-10-08 09:58:12,843:INFO:  Epoch 90/500:  train Loss: 84.3314   val Loss: 83.0028   time: 0.30s   best: 82.1841
2023-10-08 09:58:13,153:INFO:  Epoch 91/500:  train Loss: 83.4551   val Loss: 82.5894   time: 0.31s   best: 82.1841
2023-10-08 09:58:13,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:13,507:INFO:  Epoch 92/500:  train Loss: 83.1916   val Loss: 81.9601   time: 0.28s   best: 81.9601
2023-10-08 09:58:13,869:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:13,895:INFO:  Epoch 93/500:  train Loss: 82.2556   val Loss: 81.6446   time: 0.36s   best: 81.6446
2023-10-08 09:58:14,187:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:14,246:INFO:  Epoch 94/500:  train Loss: 82.3669   val Loss: 81.3585   time: 0.29s   best: 81.3585
2023-10-08 09:58:14,532:INFO:  Epoch 95/500:  train Loss: 82.6579   val Loss: 81.4709   time: 0.28s   best: 81.3585
2023-10-08 09:58:14,832:INFO:  Epoch 96/500:  train Loss: 83.7633   val Loss: 84.4439   time: 0.29s   best: 81.3585
2023-10-08 09:58:15,126:INFO:  Epoch 97/500:  train Loss: 84.8442   val Loss: 82.5678   time: 0.29s   best: 81.3585
2023-10-08 09:58:15,428:INFO:  Epoch 98/500:  train Loss: 83.2859   val Loss: 82.5191   time: 0.30s   best: 81.3585
2023-10-08 09:58:15,723:INFO:  Epoch 99/500:  train Loss: 83.0349   val Loss: 81.8822   time: 0.29s   best: 81.3585
2023-10-08 09:58:16,087:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:16,201:INFO:  Epoch 100/500:  train Loss: 82.2652   val Loss: 81.0948   time: 0.36s   best: 81.0948
2023-10-08 09:58:16,518:INFO:  Epoch 101/500:  train Loss: 82.0487   val Loss: 81.9042   time: 0.30s   best: 81.0948
2023-10-08 09:58:16,827:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:16,870:INFO:  Epoch 102/500:  train Loss: 82.0057   val Loss: 80.7132   time: 0.30s   best: 80.7132
2023-10-08 09:58:17,169:INFO:  Epoch 103/500:  train Loss: 82.4680   val Loss: 80.8818   time: 0.29s   best: 80.7132
2023-10-08 09:58:17,477:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:17,514:INFO:  Epoch 104/500:  train Loss: 81.6397   val Loss: 80.5291   time: 0.30s   best: 80.5291
2023-10-08 09:58:17,915:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:18,265:INFO:  Epoch 105/500:  train Loss: 81.5884   val Loss: 80.2604   time: 0.29s   best: 80.2604
2023-10-08 09:58:18,558:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:18,605:INFO:  Epoch 106/500:  train Loss: 81.4338   val Loss: 80.0914   time: 0.29s   best: 80.0914
2023-10-08 09:58:18,907:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:18,949:INFO:  Epoch 107/500:  train Loss: 81.6621   val Loss: 79.6318   time: 0.30s   best: 79.6318
2023-10-08 09:58:19,256:INFO:  Epoch 108/500:  train Loss: 81.0996   val Loss: 80.0673   time: 0.29s   best: 79.6318
2023-10-08 09:58:19,561:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:19,619:INFO:  Epoch 109/500:  train Loss: 80.9996   val Loss: 79.6015   time: 0.30s   best: 79.6015
2023-10-08 09:58:19,929:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:19,966:INFO:  Epoch 110/500:  train Loss: 80.7166   val Loss: 79.4143   time: 0.30s   best: 79.4143
2023-10-08 09:58:20,260:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:20,300:INFO:  Epoch 111/500:  train Loss: 80.2295   val Loss: 78.7942   time: 0.29s   best: 78.7942
2023-10-08 09:58:20,612:INFO:  Epoch 112/500:  train Loss: 80.1911   val Loss: 79.2673   time: 0.30s   best: 78.7942
2023-10-08 09:58:20,912:INFO:  Epoch 113/500:  train Loss: 81.2671   val Loss: 80.5291   time: 0.29s   best: 78.7942
2023-10-08 09:58:21,230:INFO:  Epoch 114/500:  train Loss: 80.9525   val Loss: 79.2244   time: 0.30s   best: 78.7942
2023-10-08 09:58:21,549:INFO:  Epoch 115/500:  train Loss: 79.9351   val Loss: 79.6554   time: 0.30s   best: 78.7942
2023-10-08 09:58:21,851:INFO:  Epoch 116/500:  train Loss: 80.3172   val Loss: 80.1479   time: 0.29s   best: 78.7942
2023-10-08 09:58:22,156:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:22,209:INFO:  Epoch 117/500:  train Loss: 80.8676   val Loss: 78.3969   time: 0.30s   best: 78.3969
2023-10-08 09:58:22,509:INFO:  Epoch 118/500:  train Loss: 80.4817   val Loss: 78.6178   time: 0.29s   best: 78.3969
2023-10-08 09:58:22,808:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:22,844:INFO:  Epoch 119/500:  train Loss: 79.6836   val Loss: 78.0598   time: 0.29s   best: 78.0598
2023-10-08 09:58:23,162:INFO:  Epoch 120/500:  train Loss: 79.1143   val Loss: 78.8230   time: 0.30s   best: 78.0598
2023-10-08 09:58:23,467:INFO:  Epoch 121/500:  train Loss: 79.5373   val Loss: 78.0609   time: 0.29s   best: 78.0598
2023-10-08 09:58:23,775:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:23,816:INFO:  Epoch 122/500:  train Loss: 79.0526   val Loss: 77.7974   time: 0.30s   best: 77.7974
2023-10-08 09:58:24,122:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:24,167:INFO:  Epoch 123/500:  train Loss: 78.9659   val Loss: 77.7132   time: 0.30s   best: 77.7132
2023-10-08 09:58:24,466:INFO:  Epoch 124/500:  train Loss: 78.7170   val Loss: 78.1348   time: 0.28s   best: 77.7132
2023-10-08 09:58:24,766:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:24,808:INFO:  Epoch 125/500:  train Loss: 79.7096   val Loss: 77.2997   time: 0.30s   best: 77.2997
2023-10-08 09:58:25,104:INFO:  Epoch 126/500:  train Loss: 78.9067   val Loss: 78.4767   time: 0.29s   best: 77.2997
2023-10-08 09:58:25,421:INFO:  Epoch 127/500:  train Loss: 82.7465   val Loss: 82.5687   time: 0.30s   best: 77.2997
2023-10-08 09:58:25,751:INFO:  Epoch 128/500:  train Loss: 81.6817   val Loss: 80.3576   time: 0.32s   best: 77.2997
2023-10-08 09:58:26,039:INFO:  Epoch 129/500:  train Loss: 82.3678   val Loss: 80.0437   time: 0.29s   best: 77.2997
2023-10-08 09:58:26,362:INFO:  Epoch 130/500:  train Loss: 80.7193   val Loss: 79.6786   time: 0.31s   best: 77.2997
2023-10-08 09:58:26,660:INFO:  Epoch 131/500:  train Loss: 79.9066   val Loss: 78.3881   time: 0.28s   best: 77.2997
2023-10-08 09:58:26,974:INFO:  Epoch 132/500:  train Loss: 79.8641   val Loss: 77.8708   time: 0.30s   best: 77.2997
2023-10-08 09:58:27,293:INFO:  Epoch 133/500:  train Loss: 78.5629   val Loss: 77.9279   time: 0.30s   best: 77.2997
2023-10-08 09:58:27,585:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:27,620:INFO:  Epoch 134/500:  train Loss: 78.3191   val Loss: 76.6890   time: 0.29s   best: 76.6890
2023-10-08 09:58:27,935:INFO:  Epoch 135/500:  train Loss: 78.6602   val Loss: 77.1222   time: 0.30s   best: 76.6890
2023-10-08 09:58:28,253:INFO:  Epoch 136/500:  train Loss: 78.2203   val Loss: 77.4983   time: 0.29s   best: 76.6890
2023-10-08 09:58:28,552:INFO:  Epoch 137/500:  train Loss: 78.2367   val Loss: 77.4781   time: 0.29s   best: 76.6890
2023-10-08 09:58:28,863:INFO:  Epoch 138/500:  train Loss: 78.6315   val Loss: 76.9435   time: 0.30s   best: 76.6890
2023-10-08 09:58:29,163:INFO:  Epoch 139/500:  train Loss: 78.6113   val Loss: 77.3529   time: 0.29s   best: 76.6890
2023-10-08 09:58:29,465:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:29,523:INFO:  Epoch 140/500:  train Loss: 77.8190   val Loss: 76.4497   time: 0.30s   best: 76.4497
2023-10-08 09:58:29,848:INFO:  Epoch 141/500:  train Loss: 77.5194   val Loss: 76.5779   time: 0.31s   best: 76.4497
2023-10-08 09:58:30,137:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:30,172:INFO:  Epoch 142/500:  train Loss: 77.2595   val Loss: 76.2519   time: 0.28s   best: 76.2519
2023-10-08 09:58:30,478:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:30,726:INFO:  Epoch 143/500:  train Loss: 77.5635   val Loss: 75.7104   time: 0.30s   best: 75.7104
2023-10-08 09:58:31,032:INFO:  Epoch 144/500:  train Loss: 77.5966   val Loss: 76.6613   time: 0.29s   best: 75.7104
2023-10-08 09:58:31,336:INFO:  Epoch 145/500:  train Loss: 81.0093   val Loss: 82.9538   time: 0.29s   best: 75.7104
2023-10-08 09:58:31,658:INFO:  Epoch 146/500:  train Loss: 82.2892   val Loss: 79.6810   time: 0.31s   best: 75.7104
2023-10-08 09:58:31,975:INFO:  Epoch 147/500:  train Loss: 81.8190   val Loss: 81.9438   time: 0.30s   best: 75.7104
2023-10-08 09:58:32,282:INFO:  Epoch 148/500:  train Loss: 81.6216   val Loss: 79.3483   time: 0.29s   best: 75.7104
2023-10-08 09:58:32,599:INFO:  Epoch 149/500:  train Loss: 79.8648   val Loss: 78.2631   time: 0.30s   best: 75.7104
2023-10-08 09:58:32,895:INFO:  Epoch 150/500:  train Loss: 78.5443   val Loss: 76.3873   time: 0.28s   best: 75.7104
2023-10-08 09:58:33,211:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:33,255:INFO:  Epoch 151/500:  train Loss: 77.7955   val Loss: 75.4857   time: 0.31s   best: 75.4857
2023-10-08 09:58:33,562:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:33,614:INFO:  Epoch 152/500:  train Loss: 77.0864   val Loss: 75.0469   time: 0.30s   best: 75.0469
2023-10-08 09:58:33,911:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:34,093:INFO:  Epoch 153/500:  train Loss: 76.3575   val Loss: 74.8333   time: 0.29s   best: 74.8333
2023-10-08 09:58:34,385:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:34,421:INFO:  Epoch 154/500:  train Loss: 76.2055   val Loss: 74.4714   time: 0.29s   best: 74.4714
2023-10-08 09:58:34,720:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:34,837:INFO:  Epoch 155/500:  train Loss: 75.5821   val Loss: 74.1351   time: 0.29s   best: 74.1351
2023-10-08 09:58:35,154:INFO:  Epoch 156/500:  train Loss: 75.7726   val Loss: 74.1846   time: 0.30s   best: 74.1351
2023-10-08 09:58:35,450:INFO:  Epoch 157/500:  train Loss: 75.5336   val Loss: 74.8027   time: 0.28s   best: 74.1351
2023-10-08 09:58:35,768:INFO:  Epoch 158/500:  train Loss: 75.5821   val Loss: 74.3487   time: 0.30s   best: 74.1351
2023-10-08 09:58:36,079:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:36,127:INFO:  Epoch 159/500:  train Loss: 75.5581   val Loss: 73.9850   time: 0.30s   best: 73.9850
2023-10-08 09:58:36,418:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:36,458:INFO:  Epoch 160/500:  train Loss: 76.0156   val Loss: 73.4954   time: 0.29s   best: 73.4954
2023-10-08 09:58:36,768:INFO:  Epoch 161/500:  train Loss: 75.2738   val Loss: 74.5213   time: 0.30s   best: 73.4954
2023-10-08 09:58:37,087:INFO:  Epoch 162/500:  train Loss: 75.5239   val Loss: 74.2772   time: 0.30s   best: 73.4954
2023-10-08 09:58:37,384:INFO:  Epoch 163/500:  train Loss: 75.5606   val Loss: 74.3289   time: 0.28s   best: 73.4954
2023-10-08 09:58:37,692:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:38,067:INFO:  Epoch 164/500:  train Loss: 75.9111   val Loss: 73.4827   time: 0.30s   best: 73.4827
2023-10-08 09:58:38,371:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:38,406:INFO:  Epoch 165/500:  train Loss: 75.0567   val Loss: 73.3118   time: 0.30s   best: 73.3118
2023-10-08 09:58:38,717:INFO:  Epoch 166/500:  train Loss: 76.0042   val Loss: 73.7940   time: 0.30s   best: 73.3118
2023-10-08 09:58:39,019:INFO:  Epoch 167/500:  train Loss: 74.7909   val Loss: 73.9244   time: 0.28s   best: 73.3118
2023-10-08 09:58:39,337:INFO:  Epoch 168/500:  train Loss: 75.6293   val Loss: 73.3233   time: 0.30s   best: 73.3118
2023-10-08 09:58:39,653:INFO:  Epoch 169/500:  train Loss: 74.6848   val Loss: 73.3176   time: 0.28s   best: 73.3118
2023-10-08 09:58:40,092:INFO:  Epoch 170/500:  train Loss: 74.7985   val Loss: 73.7699   time: 0.43s   best: 73.3118
2023-10-08 09:58:40,506:INFO:  Epoch 171/500:  train Loss: 74.7725   val Loss: 73.4665   time: 0.40s   best: 73.3118
2023-10-08 09:58:40,806:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:40,858:INFO:  Epoch 172/500:  train Loss: 74.5729   val Loss: 72.1883   time: 0.29s   best: 72.1883
2023-10-08 09:58:41,161:INFO:  Epoch 173/500:  train Loss: 74.5544   val Loss: 73.3320   time: 0.30s   best: 72.1883
2023-10-08 09:58:41,475:INFO:  Epoch 174/500:  train Loss: 74.4804   val Loss: 73.5140   time: 0.30s   best: 72.1883
2023-10-08 09:58:41,792:INFO:  Epoch 175/500:  train Loss: 74.6529   val Loss: 73.2507   time: 0.30s   best: 72.1883
2023-10-08 09:58:42,081:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:42,125:INFO:  Epoch 176/500:  train Loss: 73.8108   val Loss: 72.0216   time: 0.28s   best: 72.0216
2023-10-08 09:58:42,442:INFO:  Epoch 177/500:  train Loss: 73.7924   val Loss: 72.4578   time: 0.30s   best: 72.0216
2023-10-08 09:58:42,756:INFO:  Epoch 178/500:  train Loss: 74.0491   val Loss: 72.6560   time: 0.30s   best: 72.0216
2023-10-08 09:58:43,054:INFO:  Epoch 179/500:  train Loss: 73.7325   val Loss: 72.5151   time: 0.28s   best: 72.0216
2023-10-08 09:58:43,377:INFO:  Epoch 180/500:  train Loss: 73.3519   val Loss: 72.3220   time: 0.31s   best: 72.0216
2023-10-08 09:58:43,679:INFO:  Epoch 181/500:  train Loss: 73.7778   val Loss: 72.2386   time: 0.29s   best: 72.0216
2023-10-08 09:58:43,989:INFO:  Epoch 182/500:  train Loss: 73.5430   val Loss: 72.1341   time: 0.30s   best: 72.0216
2023-10-08 09:58:44,307:INFO:  Epoch 183/500:  train Loss: 74.1708   val Loss: 72.3201   time: 0.30s   best: 72.0216
2023-10-08 09:58:44,616:INFO:  Epoch 184/500:  train Loss: 73.7336   val Loss: 72.4830   time: 0.29s   best: 72.0216
2023-10-08 09:58:44,925:INFO:  Epoch 185/500:  train Loss: 74.2703   val Loss: 73.4586   time: 0.29s   best: 72.0216
2023-10-08 09:58:45,232:INFO:  Epoch 186/500:  train Loss: 73.6645   val Loss: 72.0746   time: 0.29s   best: 72.0216
2023-10-08 09:58:45,537:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:45,578:INFO:  Epoch 187/500:  train Loss: 72.2204   val Loss: 70.4762   time: 0.30s   best: 70.4762
2023-10-08 09:58:45,892:INFO:  Epoch 188/500:  train Loss: 74.6325   val Loss: 73.2344   time: 0.30s   best: 70.4762
2023-10-08 09:58:46,193:INFO:  Epoch 189/500:  train Loss: 74.5608   val Loss: 74.1576   time: 0.29s   best: 70.4762
2023-10-08 09:58:46,510:INFO:  Epoch 190/500:  train Loss: 74.9377   val Loss: 73.4069   time: 0.30s   best: 70.4762
2023-10-08 09:58:46,809:INFO:  Epoch 191/500:  train Loss: 73.3532   val Loss: 71.1076   time: 0.28s   best: 70.4762
2023-10-08 09:58:47,119:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:47,155:INFO:  Epoch 192/500:  train Loss: 72.3847   val Loss: 70.4271   time: 0.30s   best: 70.4271
2023-10-08 09:58:47,469:INFO:  Epoch 193/500:  train Loss: 72.4558   val Loss: 70.8361   time: 0.30s   best: 70.4271
2023-10-08 09:58:47,774:INFO:  Epoch 194/500:  train Loss: 73.0980   val Loss: 72.0546   time: 0.29s   best: 70.4271
2023-10-08 09:58:48,084:INFO:  Epoch 195/500:  train Loss: 76.1630   val Loss: 74.5200   time: 0.30s   best: 70.4271
2023-10-08 09:58:48,396:INFO:  Epoch 196/500:  train Loss: 74.2840   val Loss: 73.5738   time: 0.30s   best: 70.4271
2023-10-08 09:58:48,711:INFO:  Epoch 197/500:  train Loss: 74.3876   val Loss: 70.5785   time: 0.30s   best: 70.4271
2023-10-08 09:58:49,016:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:49,162:INFO:  Epoch 198/500:  train Loss: 71.8284   val Loss: 70.3656   time: 0.30s   best: 70.3656
2023-10-08 09:58:49,459:INFO:  Epoch 199/500:  train Loss: 72.4884   val Loss: 71.4734   time: 0.28s   best: 70.3656
2023-10-08 09:58:49,823:INFO:  Epoch 200/500:  train Loss: 72.0904   val Loss: 72.3451   time: 0.35s   best: 70.3656
2023-10-08 09:58:50,148:INFO:  Epoch 201/500:  train Loss: 73.5388   val Loss: 71.3508   time: 0.31s   best: 70.3656
2023-10-08 09:58:50,453:INFO:  Epoch 202/500:  train Loss: 72.1975   val Loss: 71.0504   time: 0.29s   best: 70.3656
2023-10-08 09:58:50,765:INFO:  Epoch 203/500:  train Loss: 72.7093   val Loss: 72.5605   time: 0.30s   best: 70.3656
2023-10-08 09:58:51,079:INFO:  Epoch 204/500:  train Loss: 73.1763   val Loss: 71.7471   time: 0.30s   best: 70.3656
2023-10-08 09:58:51,381:INFO:  Epoch 205/500:  train Loss: 72.6606   val Loss: 71.1184   time: 0.29s   best: 70.3656
2023-10-08 09:58:51,693:INFO:  Epoch 206/500:  train Loss: 72.0249   val Loss: 70.8390   time: 0.30s   best: 70.3656
2023-10-08 09:58:51,990:INFO:  Epoch 207/500:  train Loss: 72.7348   val Loss: 71.9022   time: 0.28s   best: 70.3656
2023-10-08 09:58:52,309:INFO:  Epoch 208/500:  train Loss: 72.3665   val Loss: 71.2897   time: 0.30s   best: 70.3656
2023-10-08 09:58:52,617:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:52,653:INFO:  Epoch 209/500:  train Loss: 72.5128   val Loss: 70.2171   time: 0.30s   best: 70.2171
2023-10-08 09:58:52,948:INFO:  Epoch 210/500:  train Loss: 71.6238   val Loss: 70.9610   time: 0.28s   best: 70.2171
2023-10-08 09:58:53,268:INFO:  Epoch 211/500:  train Loss: 72.0473   val Loss: 71.6814   time: 0.31s   best: 70.2171
2023-10-08 09:58:53,555:INFO:  Epoch 212/500:  train Loss: 72.5159   val Loss: 70.5220   time: 0.28s   best: 70.2171
2023-10-08 09:58:53,869:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:58:54,115:INFO:  Epoch 213/500:  train Loss: 71.3611   val Loss: 69.4468   time: 0.31s   best: 69.4468
2023-10-08 09:58:54,435:INFO:  Epoch 214/500:  train Loss: 71.0390   val Loss: 70.4768   time: 0.31s   best: 69.4468
2023-10-08 09:58:54,745:INFO:  Epoch 215/500:  train Loss: 71.7055   val Loss: 70.4694   time: 0.30s   best: 69.4468
2023-10-08 09:58:55,040:INFO:  Epoch 216/500:  train Loss: 71.7740   val Loss: 69.9533   time: 0.28s   best: 69.4468
2023-10-08 09:58:55,358:INFO:  Epoch 217/500:  train Loss: 75.6874   val Loss: 72.8037   time: 0.30s   best: 69.4468
2023-10-08 09:58:55,691:INFO:  Epoch 218/500:  train Loss: 72.6240   val Loss: 71.3888   time: 0.32s   best: 69.4468
2023-10-08 09:58:55,999:INFO:  Epoch 219/500:  train Loss: 72.2186   val Loss: 70.4874   time: 0.30s   best: 69.4468
2023-10-08 09:58:56,305:INFO:  Epoch 220/500:  train Loss: 71.7721   val Loss: 70.5804   time: 0.30s   best: 69.4468
2023-10-08 09:58:56,621:INFO:  Epoch 221/500:  train Loss: 74.2103   val Loss: 73.6914   time: 0.30s   best: 69.4468
2023-10-08 09:58:56,941:INFO:  Epoch 222/500:  train Loss: 76.2150   val Loss: 76.2673   time: 0.31s   best: 69.4468
2023-10-08 09:58:57,259:INFO:  Epoch 223/500:  train Loss: 76.8096   val Loss: 74.1110   time: 0.29s   best: 69.4468
2023-10-08 09:58:57,560:INFO:  Epoch 224/500:  train Loss: 74.1319   val Loss: 72.6708   time: 0.29s   best: 69.4468
2023-10-08 09:58:57,872:INFO:  Epoch 225/500:  train Loss: 74.2168   val Loss: 73.9249   time: 0.30s   best: 69.4468
2023-10-08 09:58:58,171:INFO:  Epoch 226/500:  train Loss: 75.4869   val Loss: 74.6672   time: 0.28s   best: 69.4468
2023-10-08 09:58:58,497:INFO:  Epoch 227/500:  train Loss: 74.9561   val Loss: 73.5885   time: 0.31s   best: 69.4468
2023-10-08 09:58:58,797:INFO:  Epoch 228/500:  train Loss: 73.8583   val Loss: 72.2275   time: 0.29s   best: 69.4468
2023-10-08 09:58:59,113:INFO:  Epoch 229/500:  train Loss: 73.5221   val Loss: 72.1685   time: 0.30s   best: 69.4468
2023-10-08 09:58:59,427:INFO:  Epoch 230/500:  train Loss: 72.2588   val Loss: 71.3969   time: 0.30s   best: 69.4468
2023-10-08 09:58:59,725:INFO:  Epoch 231/500:  train Loss: 72.8530   val Loss: 70.6904   time: 0.28s   best: 69.4468
2023-10-08 09:59:00,039:INFO:  Epoch 232/500:  train Loss: 72.8631   val Loss: 70.1567   time: 0.30s   best: 69.4468
2023-10-08 09:59:00,339:INFO:  Epoch 233/500:  train Loss: 72.1468   val Loss: 69.7079   time: 0.28s   best: 69.4468
2023-10-08 09:59:00,659:INFO:  Epoch 234/500:  train Loss: 71.9464   val Loss: 71.7373   time: 0.31s   best: 69.4468
2023-10-08 09:59:00,973:INFO:  Epoch 235/500:  train Loss: 71.8740   val Loss: 71.3475   time: 0.30s   best: 69.4468
2023-10-08 09:59:01,267:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:01,330:INFO:  Epoch 236/500:  train Loss: 71.8665   val Loss: 69.3606   time: 0.29s   best: 69.3606
2023-10-08 09:59:01,634:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:01,676:INFO:  Epoch 237/500:  train Loss: 71.0015   val Loss: 68.3103   time: 0.30s   best: 68.3103
2023-10-08 09:59:01,988:INFO:  Epoch 238/500:  train Loss: 71.2359   val Loss: 69.8913   time: 0.30s   best: 68.3103
2023-10-08 09:59:02,287:INFO:  Epoch 239/500:  train Loss: 71.7677   val Loss: 69.9285   time: 0.28s   best: 68.3103
2023-10-08 09:59:02,599:INFO:  Epoch 240/500:  train Loss: 71.6279   val Loss: 68.7883   time: 0.31s   best: 68.3103
2023-10-08 09:59:02,905:INFO:  Epoch 241/500:  train Loss: 70.6040   val Loss: 68.3982   time: 0.29s   best: 68.3103
2023-10-08 09:59:03,214:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:03,288:INFO:  Epoch 242/500:  train Loss: 69.5133   val Loss: 68.2232   time: 0.30s   best: 68.2232
2023-10-08 09:59:03,600:INFO:  Epoch 243/500:  train Loss: 69.7257   val Loss: 68.5983   time: 0.30s   best: 68.2232
2023-10-08 09:59:03,889:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:03,923:INFO:  Epoch 244/500:  train Loss: 69.9965   val Loss: 67.6415   time: 0.28s   best: 67.6415
2023-10-08 09:59:04,237:INFO:  Epoch 245/500:  train Loss: 70.2679   val Loss: 69.5930   time: 0.30s   best: 67.6415
2023-10-08 09:59:04,559:INFO:  Epoch 246/500:  train Loss: 70.5790   val Loss: 69.3648   time: 0.31s   best: 67.6415
2023-10-08 09:59:04,858:INFO:  Epoch 247/500:  train Loss: 70.6320   val Loss: 68.3166   time: 0.28s   best: 67.6415
2023-10-08 09:59:05,170:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:06,158:INFO:  Epoch 248/500:  train Loss: 70.5499   val Loss: 67.5833   time: 0.30s   best: 67.5833
2023-10-08 09:59:06,469:INFO:  Epoch 249/500:  train Loss: 75.2706   val Loss: 72.8914   time: 0.29s   best: 67.5833
2023-10-08 09:59:06,778:INFO:  Epoch 250/500:  train Loss: 74.3056   val Loss: 72.9016   time: 0.30s   best: 67.5833
2023-10-08 09:59:07,076:INFO:  Epoch 251/500:  train Loss: 71.4983   val Loss: 69.8482   time: 0.30s   best: 67.5833
2023-10-08 09:59:07,396:INFO:  Epoch 252/500:  train Loss: 70.1187   val Loss: 69.3160   time: 0.31s   best: 67.5833
2023-10-08 09:59:07,712:INFO:  Epoch 253/500:  train Loss: 70.2205   val Loss: 69.1030   time: 0.30s   best: 67.5833
2023-10-08 09:59:08,008:INFO:  Epoch 254/500:  train Loss: 69.4149   val Loss: 68.0300   time: 0.28s   best: 67.5833
2023-10-08 09:59:08,329:INFO:  Epoch 255/500:  train Loss: 71.6792   val Loss: 71.1738   time: 0.31s   best: 67.5833
2023-10-08 09:59:08,646:INFO:  Epoch 256/500:  train Loss: 71.8691   val Loss: 71.5917   time: 0.29s   best: 67.5833
2023-10-08 09:59:08,946:INFO:  Epoch 257/500:  train Loss: 72.7242   val Loss: 71.3875   time: 0.29s   best: 67.5833
2023-10-08 09:59:09,269:INFO:  Epoch 258/500:  train Loss: 71.7472   val Loss: 69.3464   time: 0.31s   best: 67.5833
2023-10-08 09:59:09,570:INFO:  Epoch 259/500:  train Loss: 70.5547   val Loss: 69.0157   time: 0.29s   best: 67.5833
2023-10-08 09:59:09,882:INFO:  Epoch 260/500:  train Loss: 71.9174   val Loss: 69.7006   time: 0.30s   best: 67.5833
2023-10-08 09:59:10,180:INFO:  Epoch 261/500:  train Loss: 71.6346   val Loss: 68.8465   time: 0.28s   best: 67.5833
2023-10-08 09:59:10,538:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:10,652:INFO:  Epoch 262/500:  train Loss: 70.6817   val Loss: 67.2075   time: 0.34s   best: 67.2075
2023-10-08 09:59:11,063:INFO:  Epoch 263/500:  train Loss: 70.6814   val Loss: 67.8096   time: 0.40s   best: 67.2075
2023-10-08 09:59:11,380:INFO:  Epoch 264/500:  train Loss: 69.7794   val Loss: 68.9036   time: 0.30s   best: 67.2075
2023-10-08 09:59:11,681:INFO:  Epoch 265/500:  train Loss: 70.0548   val Loss: 68.9972   time: 0.29s   best: 67.2075
2023-10-08 09:59:12,003:INFO:  Epoch 266/500:  train Loss: 69.8935   val Loss: 68.6032   time: 0.31s   best: 67.2075
2023-10-08 09:59:12,319:INFO:  Epoch 267/500:  train Loss: 70.7874   val Loss: 69.1253   time: 0.30s   best: 67.2075
2023-10-08 09:59:12,616:INFO:  Epoch 268/500:  train Loss: 69.5983   val Loss: 69.1594   time: 0.28s   best: 67.2075
2023-10-08 09:59:12,935:INFO:  Epoch 269/500:  train Loss: 71.7920   val Loss: 68.9032   time: 0.30s   best: 67.2075
2023-10-08 09:59:13,240:INFO:  Epoch 270/500:  train Loss: 71.9547   val Loss: 68.6071   time: 0.29s   best: 67.2075
2023-10-08 09:59:13,555:INFO:  Epoch 271/500:  train Loss: 72.9240   val Loss: 70.2756   time: 0.30s   best: 67.2075
2023-10-08 09:59:13,856:INFO:  Epoch 272/500:  train Loss: 71.2849   val Loss: 68.8836   time: 0.30s   best: 67.2075
2023-10-08 09:59:14,168:INFO:  Epoch 273/500:  train Loss: 71.1253   val Loss: 68.8636   time: 0.30s   best: 67.2075
2023-10-08 09:59:14,484:INFO:  Epoch 274/500:  train Loss: 70.1408   val Loss: 67.2284   time: 0.30s   best: 67.2075
2023-10-08 09:59:14,771:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:14,806:INFO:  Epoch 275/500:  train Loss: 68.5912   val Loss: 67.1040   time: 0.28s   best: 67.1040
2023-10-08 09:59:15,118:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:15,178:INFO:  Epoch 276/500:  train Loss: 68.2478   val Loss: 66.5675   time: 0.31s   best: 66.5675
2023-10-08 09:59:15,486:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:15,523:INFO:  Epoch 277/500:  train Loss: 67.8711   val Loss: 65.6170   time: 0.30s   best: 65.6170
2023-10-08 09:59:15,821:INFO:  Epoch 278/500:  train Loss: 68.9296   val Loss: 66.7340   time: 0.28s   best: 65.6170
2023-10-08 09:59:16,133:INFO:  Epoch 279/500:  train Loss: 69.6497   val Loss: 67.1834   time: 0.30s   best: 65.6170
2023-10-08 09:59:16,436:INFO:  Epoch 280/500:  train Loss: 68.1167   val Loss: 65.7902   time: 0.29s   best: 65.6170
2023-10-08 09:59:16,747:INFO:  Epoch 281/500:  train Loss: 70.8866   val Loss: 68.3563   time: 0.30s   best: 65.6170
2023-10-08 09:59:17,067:INFO:  Epoch 282/500:  train Loss: 70.7532   val Loss: 68.6521   time: 0.30s   best: 65.6170
2023-10-08 09:59:17,373:INFO:  Epoch 283/500:  train Loss: 69.5525   val Loss: 67.0008   time: 0.29s   best: 65.6170
2023-10-08 09:59:17,682:INFO:  Epoch 284/500:  train Loss: 69.9363   val Loss: 66.9140   time: 0.30s   best: 65.6170
2023-10-08 09:59:17,989:INFO:  Epoch 285/500:  train Loss: 67.8277   val Loss: 65.9256   time: 0.29s   best: 65.6170
2023-10-08 09:59:18,307:INFO:  Epoch 286/500:  train Loss: 66.7503   val Loss: 66.3699   time: 0.30s   best: 65.6170
2023-10-08 09:59:18,613:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:18,750:INFO:  Epoch 287/500:  train Loss: 67.0452   val Loss: 65.4546   time: 0.30s   best: 65.4546
2023-10-08 09:59:19,060:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:19,102:INFO:  Epoch 288/500:  train Loss: 66.8277   val Loss: 64.9066   time: 0.30s   best: 64.9066
2023-10-08 09:59:19,405:INFO:  Epoch 289/500:  train Loss: 66.5048   val Loss: 66.8449   time: 0.29s   best: 64.9066
2023-10-08 09:59:19,718:INFO:  Epoch 290/500:  train Loss: 67.2910   val Loss: 64.9611   time: 0.30s   best: 64.9066
2023-10-08 09:59:20,016:INFO:  Epoch 291/500:  train Loss: 66.5738   val Loss: 65.0446   time: 0.28s   best: 64.9066
2023-10-08 09:59:20,333:INFO:  Epoch 292/500:  train Loss: 67.0782   val Loss: 65.6651   time: 0.30s   best: 64.9066
2023-10-08 09:59:20,646:INFO:  Epoch 293/500:  train Loss: 67.6410   val Loss: 65.9435   time: 0.30s   best: 64.9066
2023-10-08 09:59:20,951:INFO:  Epoch 294/500:  train Loss: 68.5976   val Loss: 68.7994   time: 0.28s   best: 64.9066
2023-10-08 09:59:21,272:INFO:  Epoch 295/500:  train Loss: 68.9275   val Loss: 66.6023   time: 0.31s   best: 64.9066
2023-10-08 09:59:21,569:INFO:  Epoch 296/500:  train Loss: 74.9146   val Loss: 73.1928   time: 0.28s   best: 64.9066
2023-10-08 09:59:21,884:INFO:  Epoch 297/500:  train Loss: 76.0779   val Loss: 75.7827   time: 0.30s   best: 64.9066
2023-10-08 09:59:22,198:INFO:  Epoch 298/500:  train Loss: 76.2513   val Loss: 74.8612   time: 0.30s   best: 64.9066
2023-10-08 09:59:22,500:INFO:  Epoch 299/500:  train Loss: 75.6963   val Loss: 74.0835   time: 0.29s   best: 64.9066
2023-10-08 09:59:22,860:INFO:  Epoch 300/500:  train Loss: 74.8880   val Loss: 74.0452   time: 0.34s   best: 64.9066
2023-10-08 09:59:23,197:INFO:  Epoch 301/500:  train Loss: 74.4987   val Loss: 72.4307   time: 0.31s   best: 64.9066
2023-10-08 09:59:23,497:INFO:  Epoch 302/500:  train Loss: 72.9014   val Loss: 70.8974   time: 0.29s   best: 64.9066
2023-10-08 09:59:23,810:INFO:  Epoch 303/500:  train Loss: 71.6796   val Loss: 70.7280   time: 0.30s   best: 64.9066
2023-10-08 09:59:24,105:INFO:  Epoch 304/500:  train Loss: 71.1570   val Loss: 70.6893   time: 0.28s   best: 64.9066
2023-10-08 09:59:24,422:INFO:  Epoch 305/500:  train Loss: 72.8200   val Loss: 70.6516   time: 0.30s   best: 64.9066
2023-10-08 09:59:24,718:INFO:  Epoch 306/500:  train Loss: 71.8312   val Loss: 71.1987   time: 0.28s   best: 64.9066
2023-10-08 09:59:25,039:INFO:  Epoch 307/500:  train Loss: 72.0600   val Loss: 70.6848   time: 0.31s   best: 64.9066
2023-10-08 09:59:25,357:INFO:  Epoch 308/500:  train Loss: 71.5675   val Loss: 70.5953   time: 0.30s   best: 64.9066
2023-10-08 09:59:25,653:INFO:  Epoch 309/500:  train Loss: 72.3678   val Loss: 69.8114   time: 0.28s   best: 64.9066
2023-10-08 09:59:25,967:INFO:  Epoch 310/500:  train Loss: 71.5980   val Loss: 71.2739   time: 0.30s   best: 64.9066
2023-10-08 09:59:26,263:INFO:  Epoch 311/500:  train Loss: 72.8398   val Loss: 70.5935   time: 0.29s   best: 64.9066
2023-10-08 09:59:26,588:INFO:  Epoch 312/500:  train Loss: 74.9437   val Loss: 72.3648   time: 0.31s   best: 64.9066
2023-10-08 09:59:26,902:INFO:  Epoch 313/500:  train Loss: 73.5379   val Loss: 69.8069   time: 0.30s   best: 64.9066
2023-10-08 09:59:27,209:INFO:  Epoch 314/500:  train Loss: 72.6836   val Loss: 68.6933   time: 0.29s   best: 64.9066
2023-10-08 09:59:27,525:INFO:  Epoch 315/500:  train Loss: 72.5892   val Loss: 69.5349   time: 0.30s   best: 64.9066
2023-10-08 09:59:27,825:INFO:  Epoch 316/500:  train Loss: 75.6365   val Loss: 73.8094   time: 0.29s   best: 64.9066
2023-10-08 09:59:28,139:INFO:  Epoch 317/500:  train Loss: 73.3616   val Loss: 74.5970   time: 0.30s   best: 64.9066
2023-10-08 09:59:28,460:INFO:  Epoch 318/500:  train Loss: 73.6107   val Loss: 74.0926   time: 0.31s   best: 64.9066
2023-10-08 09:59:28,759:INFO:  Epoch 319/500:  train Loss: 72.5829   val Loss: 71.3886   time: 0.28s   best: 64.9066
2023-10-08 09:59:29,078:INFO:  Epoch 320/500:  train Loss: 71.8291   val Loss: 68.7359   time: 0.30s   best: 64.9066
2023-10-08 09:59:29,381:INFO:  Epoch 321/500:  train Loss: 69.7090   val Loss: 66.8165   time: 0.29s   best: 64.9066
2023-10-08 09:59:29,696:INFO:  Epoch 322/500:  train Loss: 68.8652   val Loss: 67.8539   time: 0.30s   best: 64.9066
2023-10-08 09:59:30,010:INFO:  Epoch 323/500:  train Loss: 68.0988   val Loss: 67.1665   time: 0.30s   best: 64.9066
2023-10-08 09:59:30,306:INFO:  Epoch 324/500:  train Loss: 68.8932   val Loss: 66.6387   time: 0.28s   best: 64.9066
2023-10-08 09:59:30,621:INFO:  Epoch 325/500:  train Loss: 69.0030   val Loss: 66.2412   time: 0.30s   best: 64.9066
2023-10-08 09:59:30,919:INFO:  Epoch 326/500:  train Loss: 68.3312   val Loss: 66.8491   time: 0.28s   best: 64.9066
2023-10-08 09:59:31,244:INFO:  Epoch 327/500:  train Loss: 68.2017   val Loss: 66.1790   time: 0.31s   best: 64.9066
2023-10-08 09:59:31,560:INFO:  Epoch 328/500:  train Loss: 68.6095   val Loss: 66.2723   time: 0.30s   best: 64.9066
2023-10-08 09:59:31,859:INFO:  Epoch 329/500:  train Loss: 68.9771   val Loss: 66.0336   time: 0.28s   best: 64.9066
2023-10-08 09:59:32,173:INFO:  Epoch 330/500:  train Loss: 68.8342   val Loss: 65.6012   time: 0.30s   best: 64.9066
2023-10-08 09:59:32,464:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:32,499:INFO:  Epoch 331/500:  train Loss: 68.1993   val Loss: 64.1767   time: 0.29s   best: 64.1767
2023-10-08 09:59:32,813:INFO:  Epoch 332/500:  train Loss: 68.8103   val Loss: 67.0768   time: 0.30s   best: 64.1767
2023-10-08 09:59:33,137:INFO:  Epoch 333/500:  train Loss: 69.4646   val Loss: 66.8292   time: 0.31s   best: 64.1767
2023-10-08 09:59:33,438:INFO:  Epoch 334/500:  train Loss: 67.6571   val Loss: 65.4166   time: 0.29s   best: 64.1767
2023-10-08 09:59:33,750:INFO:  Epoch 335/500:  train Loss: 69.4848   val Loss: 66.5292   time: 0.30s   best: 64.1767
2023-10-08 09:59:34,047:INFO:  Epoch 336/500:  train Loss: 68.2363   val Loss: 67.1719   time: 0.28s   best: 64.1767
2023-10-08 09:59:34,363:INFO:  Epoch 337/500:  train Loss: 67.6883   val Loss: 65.5755   time: 0.30s   best: 64.1767
2023-10-08 09:59:34,677:INFO:  Epoch 338/500:  train Loss: 68.3219   val Loss: 65.1606   time: 0.30s   best: 64.1767
2023-10-08 09:59:34,972:INFO:  Epoch 339/500:  train Loss: 69.3420   val Loss: 67.2093   time: 0.28s   best: 64.1767
2023-10-08 09:59:35,297:INFO:  Epoch 340/500:  train Loss: 77.4389   val Loss: 77.1536   time: 0.31s   best: 64.1767
2023-10-08 09:59:35,594:INFO:  Epoch 341/500:  train Loss: 77.6702   val Loss: 72.5225   time: 0.28s   best: 64.1767
2023-10-08 09:59:35,909:INFO:  Epoch 342/500:  train Loss: 73.1237   val Loss: 72.6242   time: 0.30s   best: 64.1767
2023-10-08 09:59:36,200:INFO:  Epoch 343/500:  train Loss: 72.6697   val Loss: 72.4427   time: 0.28s   best: 64.1767
2023-10-08 09:59:36,525:INFO:  Epoch 344/500:  train Loss: 73.7046   val Loss: 69.5668   time: 0.31s   best: 64.1767
2023-10-08 09:59:36,839:INFO:  Epoch 345/500:  train Loss: 69.7080   val Loss: 67.7695   time: 0.30s   best: 64.1767
2023-10-08 09:59:37,144:INFO:  Epoch 346/500:  train Loss: 67.7394   val Loss: 66.3325   time: 0.29s   best: 64.1767
2023-10-08 09:59:37,462:INFO:  Epoch 347/500:  train Loss: 66.6244   val Loss: 65.1090   time: 0.30s   best: 64.1767
2023-10-08 09:59:37,764:INFO:  Epoch 348/500:  train Loss: 67.8471   val Loss: 67.2726   time: 0.29s   best: 64.1767
2023-10-08 09:59:38,078:INFO:  Epoch 349/500:  train Loss: 69.1169   val Loss: 67.6121   time: 0.30s   best: 64.1767
2023-10-08 09:59:38,395:INFO:  Epoch 350/500:  train Loss: 67.3165   val Loss: 65.0281   time: 0.30s   best: 64.1767
2023-10-08 09:59:38,692:INFO:  Epoch 351/500:  train Loss: 67.7135   val Loss: 67.8686   time: 0.28s   best: 64.1767
2023-10-08 09:59:39,006:INFO:  Epoch 352/500:  train Loss: 77.6438   val Loss: 78.8660   time: 0.30s   best: 64.1767
2023-10-08 09:59:39,318:INFO:  Epoch 353/500:  train Loss: 80.0466   val Loss: 82.1695   time: 0.30s   best: 64.1767
2023-10-08 09:59:39,634:INFO:  Epoch 354/500:  train Loss: 82.1526   val Loss: 78.7826   time: 0.30s   best: 64.1767
2023-10-08 09:59:39,948:INFO:  Epoch 355/500:  train Loss: 78.7512   val Loss: 76.7192   time: 0.30s   best: 64.1767
2023-10-08 09:59:40,247:INFO:  Epoch 356/500:  train Loss: 76.4494   val Loss: 74.6534   time: 0.28s   best: 64.1767
2023-10-08 09:59:40,566:INFO:  Epoch 357/500:  train Loss: 74.4658   val Loss: 71.6878   time: 0.30s   best: 64.1767
2023-10-08 09:59:40,864:INFO:  Epoch 358/500:  train Loss: 71.6780   val Loss: 70.0659   time: 0.28s   best: 64.1767
2023-10-08 09:59:41,318:INFO:  Epoch 359/500:  train Loss: 71.0203   val Loss: 68.6225   time: 0.45s   best: 64.1767
2023-10-08 09:59:41,747:INFO:  Epoch 360/500:  train Loss: 69.5757   val Loss: 66.9594   time: 0.41s   best: 64.1767
2023-10-08 09:59:42,057:INFO:  Epoch 361/500:  train Loss: 67.4822   val Loss: 66.6569   time: 0.30s   best: 64.1767
2023-10-08 09:59:42,356:INFO:  Epoch 362/500:  train Loss: 67.6297   val Loss: 65.5318   time: 0.28s   best: 64.1767
2023-10-08 09:59:42,671:INFO:  Epoch 363/500:  train Loss: 68.0318   val Loss: 67.2035   time: 0.30s   best: 64.1767
2023-10-08 09:59:42,967:INFO:  Epoch 364/500:  train Loss: 69.5685   val Loss: 68.1415   time: 0.28s   best: 64.1767
2023-10-08 09:59:43,286:INFO:  Epoch 365/500:  train Loss: 69.4589   val Loss: 66.9513   time: 0.29s   best: 64.1767
2023-10-08 09:59:43,607:INFO:  Epoch 366/500:  train Loss: 68.7354   val Loss: 66.8818   time: 0.31s   best: 64.1767
2023-10-08 09:59:43,910:INFO:  Epoch 367/500:  train Loss: 68.5289   val Loss: 67.0242   time: 0.29s   best: 64.1767
2023-10-08 09:59:44,224:INFO:  Epoch 368/500:  train Loss: 69.2080   val Loss: 66.0063   time: 0.30s   best: 64.1767
2023-10-08 09:59:44,524:INFO:  Epoch 369/500:  train Loss: 68.2823   val Loss: 66.2628   time: 0.29s   best: 64.1767
2023-10-08 09:59:44,839:INFO:  Epoch 370/500:  train Loss: 68.0581   val Loss: 65.3361   time: 0.30s   best: 64.1767
2023-10-08 09:59:45,153:INFO:  Epoch 371/500:  train Loss: 69.1151   val Loss: 64.3236   time: 0.30s   best: 64.1767
2023-10-08 09:59:45,455:INFO:  Epoch 372/500:  train Loss: 68.4522   val Loss: 65.3590   time: 0.29s   best: 64.1767
2023-10-08 09:59:45,782:INFO:  Epoch 373/500:  train Loss: 67.6138   val Loss: 64.2224   time: 0.31s   best: 64.1767
2023-10-08 09:59:46,079:INFO:  Epoch 374/500:  train Loss: 66.6318   val Loss: 65.3584   time: 0.28s   best: 64.1767
2023-10-08 09:59:46,396:INFO:  Epoch 375/500:  train Loss: 66.8338   val Loss: 65.2442   time: 0.30s   best: 64.1767
2023-10-08 09:59:46,707:INFO:  Epoch 376/500:  train Loss: 67.1528   val Loss: 65.2546   time: 0.30s   best: 64.1767
2023-10-08 09:59:47,004:INFO:  Epoch 377/500:  train Loss: 66.1593   val Loss: 64.6187   time: 0.28s   best: 64.1767
2023-10-08 09:59:47,323:INFO:  Epoch 378/500:  train Loss: 66.7926   val Loss: 64.6366   time: 0.30s   best: 64.1767
2023-10-08 09:59:47,619:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:47,659:INFO:  Epoch 379/500:  train Loss: 65.8152   val Loss: 63.9371   time: 0.29s   best: 63.9371
2023-10-08 09:59:47,971:INFO:  Epoch 380/500:  train Loss: 65.6706   val Loss: 64.0270   time: 0.30s   best: 63.9371
2023-10-08 09:59:48,277:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:48,334:INFO:  Epoch 381/500:  train Loss: 65.3756   val Loss: 63.2753   time: 0.30s   best: 63.2753
2023-10-08 09:59:48,637:INFO:  Epoch 382/500:  train Loss: 64.4159   val Loss: 63.5391   time: 0.29s   best: 63.2753
2023-10-08 09:59:48,950:INFO:  Epoch 383/500:  train Loss: 65.2945   val Loss: 64.6112   time: 0.30s   best: 63.2753
2023-10-08 09:59:49,258:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:49,294:INFO:  Epoch 384/500:  train Loss: 64.6276   val Loss: 62.1555   time: 0.30s   best: 62.1555
2023-10-08 09:59:49,594:INFO:  Epoch 385/500:  train Loss: 65.2311   val Loss: 66.2421   time: 0.29s   best: 62.1555
2023-10-08 09:59:49,916:INFO:  Epoch 386/500:  train Loss: 67.7572   val Loss: 66.2756   time: 0.31s   best: 62.1555
2023-10-08 09:59:50,214:INFO:  Epoch 387/500:  train Loss: 67.1960   val Loss: 65.4493   time: 0.28s   best: 62.1555
2023-10-08 09:59:50,532:INFO:  Epoch 388/500:  train Loss: 67.3585   val Loss: 67.1464   time: 0.30s   best: 62.1555
2023-10-08 09:59:50,848:INFO:  Epoch 389/500:  train Loss: 66.3987   val Loss: 63.2516   time: 0.30s   best: 62.1555
2023-10-08 09:59:51,147:INFO:  Epoch 390/500:  train Loss: 66.0005   val Loss: 64.2150   time: 0.28s   best: 62.1555
2023-10-08 09:59:51,467:INFO:  Epoch 391/500:  train Loss: 65.7858   val Loss: 64.9053   time: 0.31s   best: 62.1555
2023-10-08 09:59:51,770:INFO:  Epoch 392/500:  train Loss: 66.2036   val Loss: 62.6051   time: 0.29s   best: 62.1555
2023-10-08 09:59:52,084:INFO:  Epoch 393/500:  train Loss: 66.7108   val Loss: 65.7590   time: 0.30s   best: 62.1555
2023-10-08 09:59:52,402:INFO:  Epoch 394/500:  train Loss: 73.9884   val Loss: 70.4935   time: 0.30s   best: 62.1555
2023-10-08 09:59:52,700:INFO:  Epoch 395/500:  train Loss: 71.0583   val Loss: 68.8766   time: 0.28s   best: 62.1555
2023-10-08 09:59:53,012:INFO:  Epoch 396/500:  train Loss: 67.4894   val Loss: 65.6946   time: 0.30s   best: 62.1555
2023-10-08 09:59:53,314:INFO:  Epoch 397/500:  train Loss: 66.6704   val Loss: 63.5714   time: 0.29s   best: 62.1555
2023-10-08 09:59:53,627:INFO:  Epoch 398/500:  train Loss: 65.4282   val Loss: 65.6982   time: 0.29s   best: 62.1555
2023-10-08 09:59:53,944:INFO:  Epoch 399/500:  train Loss: 65.5761   val Loss: 62.4348   time: 0.30s   best: 62.1555
2023-10-08 09:59:54,292:INFO:  Epoch 400/500:  train Loss: 64.8139   val Loss: 62.9343   time: 0.33s   best: 62.1555
2023-10-08 09:59:54,621:INFO:  Epoch 401/500:  train Loss: 65.3054   val Loss: 62.5598   time: 0.31s   best: 62.1555
2023-10-08 09:59:54,917:INFO:  Epoch 402/500:  train Loss: 64.1774   val Loss: 64.6134   time: 0.28s   best: 62.1555
2023-10-08 09:59:55,235:INFO:  Epoch 403/500:  train Loss: 64.8223   val Loss: 64.8127   time: 0.30s   best: 62.1555
2023-10-08 09:59:55,552:INFO:  Epoch 404/500:  train Loss: 65.4866   val Loss: 63.3248   time: 0.30s   best: 62.1555
2023-10-08 09:59:55,859:INFO:  Epoch 405/500:  train Loss: 66.0738   val Loss: 62.6396   time: 0.29s   best: 62.1555
2023-10-08 09:59:56,168:INFO:  Epoch 406/500:  train Loss: 66.2380   val Loss: 62.6967   time: 0.30s   best: 62.1555
2023-10-08 09:59:56,479:INFO:  Epoch 407/500:  train Loss: 65.4907   val Loss: 62.8868   time: 0.30s   best: 62.1555
2023-10-08 09:59:56,791:INFO:  Epoch 408/500:  train Loss: 65.6350   val Loss: 63.0848   time: 0.30s   best: 62.1555
2023-10-08 09:59:57,104:INFO:  Epoch 409/500:  train Loss: 65.0849   val Loss: 64.8522   time: 0.30s   best: 62.1555
2023-10-08 09:59:57,413:INFO:  Epoch 410/500:  train Loss: 66.2613   val Loss: 62.3211   time: 0.29s   best: 62.1555
2023-10-08 09:59:57,734:INFO:  Epoch 411/500:  train Loss: 66.2635   val Loss: 62.9350   time: 0.31s   best: 62.1555
2023-10-08 09:59:58,033:INFO:  Epoch 412/500:  train Loss: 65.3455   val Loss: 62.6079   time: 0.28s   best: 62.1555
2023-10-08 09:59:58,347:INFO:  Epoch 413/500:  train Loss: 65.5268   val Loss: 62.5162   time: 0.30s   best: 62.1555
2023-10-08 09:59:58,657:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:58,772:INFO:  Epoch 414/500:  train Loss: 65.2589   val Loss: 61.9312   time: 0.30s   best: 61.9312
2023-10-08 09:59:59,062:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:59,103:INFO:  Epoch 415/500:  train Loss: 64.0249   val Loss: 61.3072   time: 0.28s   best: 61.3072
2023-10-08 09:59:59,418:INFO:  Epoch 416/500:  train Loss: 64.0067   val Loss: 61.3783   time: 0.30s   best: 61.3072
2023-10-08 09:59:59,731:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 09:59:59,768:INFO:  Epoch 417/500:  train Loss: 63.7218   val Loss: 61.1839   time: 0.31s   best: 61.1839
2023-10-08 10:00:00,064:INFO:  Epoch 418/500:  train Loss: 63.9492   val Loss: 61.8013   time: 0.28s   best: 61.1839
2023-10-08 10:00:00,379:INFO:  Epoch 419/500:  train Loss: 63.3650   val Loss: 61.3707   time: 0.30s   best: 61.1839
2023-10-08 10:00:00,677:INFO:  Epoch 420/500:  train Loss: 63.8720   val Loss: 63.5434   time: 0.28s   best: 61.1839
2023-10-08 10:00:00,991:INFO:  Epoch 421/500:  train Loss: 65.5963   val Loss: 63.6382   time: 0.30s   best: 61.1839
2023-10-08 10:00:01,308:INFO:  Epoch 422/500:  train Loss: 64.9186   val Loss: 63.6084   time: 0.30s   best: 61.1839
2023-10-08 10:00:01,599:INFO:  Epoch 423/500:  train Loss: 64.3922   val Loss: 61.2302   time: 0.28s   best: 61.1839
2023-10-08 10:00:01,929:INFO:  Epoch 424/500:  train Loss: 62.8851   val Loss: 63.1987   time: 0.31s   best: 61.1839
2023-10-08 10:00:02,215:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 10:00:02,315:INFO:  Epoch 425/500:  train Loss: 63.5396   val Loss: 61.1763   time: 0.28s   best: 61.1763
2023-10-08 10:00:02,608:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 10:00:02,644:INFO:  Epoch 426/500:  train Loss: 63.6219   val Loss: 61.0407   time: 0.29s   best: 61.0407
2023-10-08 10:00:02,942:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 10:00:03,015:INFO:  Epoch 427/500:  train Loss: 62.5375   val Loss: 60.8815   time: 0.29s   best: 60.8815
2023-10-08 10:00:03,330:INFO:  Epoch 428/500:  train Loss: 63.3247   val Loss: 60.8834   time: 0.30s   best: 60.8815
2023-10-08 10:00:03,621:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 10:00:03,682:INFO:  Epoch 429/500:  train Loss: 63.8767   val Loss: 59.9013   time: 0.29s   best: 59.9013
2023-10-08 10:00:03,994:INFO:  Epoch 430/500:  train Loss: 64.7869   val Loss: 61.5184   time: 0.30s   best: 59.9013
2023-10-08 10:00:04,290:INFO:  Epoch 431/500:  train Loss: 64.2066   val Loss: 61.0631   time: 0.28s   best: 59.9013
2023-10-08 10:00:04,607:INFO:  Epoch 432/500:  train Loss: 65.1535   val Loss: 61.6362   time: 0.30s   best: 59.9013
2023-10-08 10:00:04,919:INFO:  Epoch 433/500:  train Loss: 63.4024   val Loss: 60.6221   time: 0.30s   best: 59.9013
2023-10-08 10:00:05,215:INFO:  Epoch 434/500:  train Loss: 63.2413   val Loss: 62.4092   time: 0.28s   best: 59.9013
2023-10-08 10:00:05,537:INFO:  Epoch 435/500:  train Loss: 64.5026   val Loss: 61.9877   time: 0.31s   best: 59.9013
2023-10-08 10:00:05,844:INFO:  Epoch 436/500:  train Loss: 70.2134   val Loss: 69.8035   time: 0.29s   best: 59.9013
2023-10-08 10:00:06,157:INFO:  Epoch 437/500:  train Loss: 68.7986   val Loss: 64.9581   time: 0.30s   best: 59.9013
2023-10-08 10:00:06,471:INFO:  Epoch 438/500:  train Loss: 64.7169   val Loss: 63.8602   time: 0.30s   best: 59.9013
2023-10-08 10:00:06,768:INFO:  Epoch 439/500:  train Loss: 65.7406   val Loss: 62.2847   time: 0.28s   best: 59.9013
2023-10-08 10:00:07,080:INFO:  Epoch 440/500:  train Loss: 64.9842   val Loss: 64.6182   time: 0.30s   best: 59.9013
2023-10-08 10:00:07,382:INFO:  Epoch 441/500:  train Loss: 65.0398   val Loss: 62.3406   time: 0.29s   best: 59.9013
2023-10-08 10:00:07,714:INFO:  Epoch 442/500:  train Loss: 62.4718   val Loss: 60.0036   time: 0.32s   best: 59.9013
2023-10-08 10:00:08,025:INFO:  Epoch 443/500:  train Loss: 61.7008   val Loss: 60.7861   time: 0.30s   best: 59.9013
2023-10-08 10:00:08,324:INFO:  Epoch 444/500:  train Loss: 65.4330   val Loss: 62.4780   time: 0.28s   best: 59.9013
2023-10-08 10:00:08,639:INFO:  Epoch 445/500:  train Loss: 68.5766   val Loss: 70.6493   time: 0.30s   best: 59.9013
2023-10-08 10:00:08,939:INFO:  Epoch 446/500:  train Loss: 71.9439   val Loss: 69.1240   time: 0.28s   best: 59.9013
2023-10-08 10:00:09,261:INFO:  Epoch 447/500:  train Loss: 66.9354   val Loss: 63.3125   time: 0.31s   best: 59.9013
2023-10-08 10:00:09,576:INFO:  Epoch 448/500:  train Loss: 63.4010   val Loss: 61.7285   time: 0.30s   best: 59.9013
2023-10-08 10:00:09,884:INFO:  Epoch 449/500:  train Loss: 62.9005   val Loss: 60.7379   time: 0.29s   best: 59.9013
2023-10-08 10:00:10,196:INFO:  Epoch 450/500:  train Loss: 62.2164   val Loss: 63.0239   time: 0.30s   best: 59.9013
2023-10-08 10:00:10,499:INFO:  Epoch 451/500:  train Loss: 63.4493   val Loss: 60.7398   time: 0.29s   best: 59.9013
2023-10-08 10:00:10,812:INFO:  Epoch 452/500:  train Loss: 62.3842   val Loss: 60.8401   time: 0.30s   best: 59.9013
2023-10-08 10:00:11,120:INFO:  Epoch 453/500:  train Loss: 62.2564   val Loss: 62.9454   time: 0.30s   best: 59.9013
2023-10-08 10:00:11,434:INFO:  Epoch 454/500:  train Loss: 62.6584   val Loss: 60.4570   time: 0.30s   best: 59.9013
2023-10-08 10:00:11,816:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 10:00:11,861:INFO:  Epoch 455/500:  train Loss: 61.2334   val Loss: 59.5206   time: 0.37s   best: 59.5206
2023-10-08 10:00:12,319:INFO:  Epoch 456/500:  train Loss: 62.3579   val Loss: 60.6304   time: 0.44s   best: 59.5206
2023-10-08 10:00:12,637:INFO:  Epoch 457/500:  train Loss: 63.1695   val Loss: 59.7539   time: 0.30s   best: 59.5206
2023-10-08 10:00:12,934:INFO:  Epoch 458/500:  train Loss: 62.8194   val Loss: 60.2024   time: 0.28s   best: 59.5206
2023-10-08 10:00:13,251:INFO:  Epoch 459/500:  train Loss: 61.5681   val Loss: 61.0060   time: 0.30s   best: 59.5206
2023-10-08 10:00:13,542:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 10:00:13,660:INFO:  Epoch 460/500:  train Loss: 60.9577   val Loss: 59.0572   time: 0.29s   best: 59.0572
2023-10-08 10:00:13,959:INFO:  Epoch 461/500:  train Loss: 61.1303   val Loss: 61.2164   time: 0.28s   best: 59.0572
2023-10-08 10:00:14,278:INFO:  Epoch 462/500:  train Loss: 61.1920   val Loss: 63.0936   time: 0.30s   best: 59.0572
2023-10-08 10:00:14,578:INFO:  Epoch 463/500:  train Loss: 63.6492   val Loss: 62.2045   time: 0.29s   best: 59.0572
2023-10-08 10:00:14,892:INFO:  Epoch 464/500:  train Loss: 63.1275   val Loss: 60.7882   time: 0.30s   best: 59.0572
2023-10-08 10:00:15,206:INFO:  Epoch 465/500:  train Loss: 61.3023   val Loss: 59.7649   time: 0.29s   best: 59.0572
2023-10-08 10:00:15,512:INFO:  Epoch 466/500:  train Loss: 62.0375   val Loss: 60.2288   time: 0.29s   best: 59.0572
2023-10-08 10:00:15,826:INFO:  Epoch 467/500:  train Loss: 61.9920   val Loss: 61.0844   time: 0.30s   best: 59.0572
2023-10-08 10:00:16,123:INFO:  Epoch 468/500:  train Loss: 63.2140   val Loss: 60.2151   time: 0.28s   best: 59.0572
2023-10-08 10:00:16,446:INFO:  Epoch 469/500:  train Loss: 61.3645   val Loss: 60.4771   time: 0.31s   best: 59.0572
2023-10-08 10:00:16,744:INFO:  Epoch 470/500:  train Loss: 61.2953   val Loss: 60.2569   time: 0.28s   best: 59.0572
2023-10-08 10:00:17,058:INFO:  Epoch 471/500:  train Loss: 62.9890   val Loss: 59.7076   time: 0.30s   best: 59.0572
2023-10-08 10:00:17,374:INFO:  Epoch 472/500:  train Loss: 62.3454   val Loss: 60.2822   time: 0.30s   best: 59.0572
2023-10-08 10:00:17,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug_6033.pt
2023-10-08 10:00:17,718:INFO:  Epoch 473/500:  train Loss: 64.0059   val Loss: 58.4357   time: 0.29s   best: 58.4357
2023-10-08 10:00:18,030:INFO:  Epoch 474/500:  train Loss: 62.2101   val Loss: 60.2703   time: 0.30s   best: 58.4357
2023-10-08 10:00:18,352:INFO:  Epoch 475/500:  train Loss: 64.8434   val Loss: 59.3985   time: 0.31s   best: 58.4357
2023-10-08 10:00:18,658:INFO:  Epoch 476/500:  train Loss: 66.9924   val Loss: 68.8339   time: 0.29s   best: 58.4357
2023-10-08 10:00:18,971:INFO:  Epoch 477/500:  train Loss: 69.9106   val Loss: 67.6543   time: 0.30s   best: 58.4357
2023-10-08 10:00:19,263:INFO:  Epoch 478/500:  train Loss: 67.0397   val Loss: 65.7316   time: 0.29s   best: 58.4357
2023-10-08 10:00:19,583:INFO:  Epoch 479/500:  train Loss: 66.9274   val Loss: 64.7341   time: 0.31s   best: 58.4357
2023-10-08 10:00:19,890:INFO:  Epoch 480/500:  train Loss: 67.6832   val Loss: 66.0242   time: 0.29s   best: 58.4357
2023-10-08 10:00:20,200:INFO:  Epoch 481/500:  train Loss: 67.2525   val Loss: 69.7870   time: 0.30s   best: 58.4357
2023-10-08 10:00:20,525:INFO:  Epoch 482/500:  train Loss: 71.5790   val Loss: 70.9470   time: 0.31s   best: 58.4357
2023-10-08 10:00:20,822:INFO:  Epoch 483/500:  train Loss: 70.8120   val Loss: 67.3854   time: 0.28s   best: 58.4357
2023-10-08 10:00:21,137:INFO:  Epoch 484/500:  train Loss: 66.9827   val Loss: 64.9959   time: 0.30s   best: 58.4357
2023-10-08 10:00:21,438:INFO:  Epoch 485/500:  train Loss: 65.4804   val Loss: 64.5095   time: 0.29s   best: 58.4357
2023-10-08 10:00:21,755:INFO:  Epoch 486/500:  train Loss: 69.3453   val Loss: 66.0969   time: 0.30s   best: 58.4357
2023-10-08 10:00:22,066:INFO:  Epoch 487/500:  train Loss: 75.6943   val Loss: 77.0482   time: 0.30s   best: 58.4357
2023-10-08 10:00:22,375:INFO:  Epoch 488/500:  train Loss: 75.5227   val Loss: 72.2603   time: 0.29s   best: 58.4357
2023-10-08 10:00:22,691:INFO:  Epoch 489/500:  train Loss: 69.5362   val Loss: 65.9219   time: 0.30s   best: 58.4357
2023-10-08 10:00:22,991:INFO:  Epoch 490/500:  train Loss: 65.4068   val Loss: 62.9807   time: 0.29s   best: 58.4357
2023-10-08 10:00:23,309:INFO:  Epoch 491/500:  train Loss: 64.9312   val Loss: 64.9779   time: 0.30s   best: 58.4357
2023-10-08 10:00:23,624:INFO:  Epoch 492/500:  train Loss: 65.5791   val Loss: 64.4725   time: 0.30s   best: 58.4357
2023-10-08 10:00:23,924:INFO:  Epoch 493/500:  train Loss: 67.2917   val Loss: 62.3865   time: 0.29s   best: 58.4357
2023-10-08 10:00:24,238:INFO:  Epoch 494/500:  train Loss: 65.3680   val Loss: 65.4112   time: 0.30s   best: 58.4357
2023-10-08 10:00:24,549:INFO:  Epoch 495/500:  train Loss: 65.2455   val Loss: 64.7392   time: 0.30s   best: 58.4357
2023-10-08 10:00:24,863:INFO:  Epoch 496/500:  train Loss: 65.0704   val Loss: 62.9716   time: 0.30s   best: 58.4357
2023-10-08 10:00:25,179:INFO:  Epoch 497/500:  train Loss: 63.6904   val Loss: 62.2489   time: 0.30s   best: 58.4357
2023-10-08 10:00:25,483:INFO:  Epoch 498/500:  train Loss: 61.6675   val Loss: 59.4083   time: 0.29s   best: 58.4357
2023-10-08 10:00:25,797:INFO:  Epoch 499/500:  train Loss: 61.0589   val Loss: 60.6627   time: 0.30s   best: 58.4357
2023-10-08 10:00:26,153:INFO:  Epoch 500/500:  train Loss: 60.9748   val Loss: 60.6230   time: 0.34s   best: 58.4357
2023-10-08 10:00:26,154:INFO:  -----> Training complete in 2m 45s   best validation loss: 58.4357
 
2023-10-08 10:01:18,165:INFO:  Epoch 308/500:  train Loss: 17.9565   val Loss: 24.3467   time: 479.34s   best: 22.7835
2023-10-08 10:09:25,212:INFO:  Epoch 309/500:  train Loss: 17.9063   val Loss: 23.7864   time: 487.03s   best: 22.7835
2023-10-08 10:17:39,048:INFO:  Epoch 310/500:  train Loss: 17.9791   val Loss: 25.0892   time: 493.78s   best: 22.7835
2023-10-08 10:25:47,741:INFO:  Epoch 311/500:  train Loss: 17.9335   val Loss: 22.9998   time: 488.67s   best: 22.7835
2023-10-08 10:33:52,893:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-08 10:33:53,063:INFO:  Epoch 312/500:  train Loss: 18.0580   val Loss: 22.7288   time: 485.03s   best: 22.7288
2023-10-08 10:41:53,238:INFO:  Epoch 313/500:  train Loss: 18.2308   val Loss: 28.2426   time: 480.14s   best: 22.7288
2023-10-08 10:50:05,797:INFO:  Epoch 314/500:  train Loss: 18.2263   val Loss: 23.8462   time: 492.52s   best: 22.7288
2023-10-08 10:58:14,861:INFO:  Epoch 315/500:  train Loss: 18.0107   val Loss: 23.6650   time: 489.01s   best: 22.7288
2023-10-08 11:06:28,838:INFO:  Epoch 316/500:  train Loss: 17.9906   val Loss: 23.2255   time: 493.96s   best: 22.7288
2023-10-08 11:14:39,639:INFO:  Epoch 317/500:  train Loss: 17.8062   val Loss: 22.9141   time: 490.78s   best: 22.7288
2023-10-08 11:22:36,770:INFO:  Epoch 318/500:  train Loss: 17.9356   val Loss: 23.6435   time: 477.10s   best: 22.7288
2023-10-08 11:30:57,100:INFO:  Epoch 319/500:  train Loss: 17.9828   val Loss: 23.6793   time: 500.30s   best: 22.7288
2023-10-08 11:39:27,982:INFO:  Epoch 320/500:  train Loss: 17.8985   val Loss: 24.1050   time: 510.87s   best: 22.7288
2023-10-08 11:47:55,285:INFO:  Epoch 321/500:  train Loss: 18.0742   val Loss: 29.9546   time: 507.27s   best: 22.7288
2023-10-08 11:56:01,602:INFO:  Epoch 322/500:  train Loss: 18.0907   val Loss: 24.2595   time: 486.29s   best: 22.7288
2023-10-08 12:03:52,543:INFO:  Epoch 323/500:  train Loss: 17.9201   val Loss: 23.5981   time: 470.91s   best: 22.7288
2023-10-08 12:11:41,944:INFO:  Epoch 324/500:  train Loss: 18.2353   val Loss: 23.5174   time: 469.37s   best: 22.7288
2023-10-08 12:19:24,685:INFO:  Epoch 325/500:  train Loss: 17.8584   val Loss: 23.3161   time: 462.72s   best: 22.7288
2023-10-08 12:27:11,893:INFO:  Epoch 326/500:  train Loss: 18.0515   val Loss: 23.0592   time: 467.18s   best: 22.7288
2023-10-08 12:34:58,308:INFO:  Epoch 327/500:  train Loss: 17.9019   val Loss: 22.9849   time: 466.38s   best: 22.7288
2023-10-08 12:43:13,778:INFO:  Epoch 328/500:  train Loss: 17.8608   val Loss: 23.5660   time: 495.43s   best: 22.7288
2023-10-08 12:51:48,385:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-08 12:51:48,779:INFO:  Epoch 329/500:  train Loss: 18.1587   val Loss: 22.6943   time: 514.50s   best: 22.6943
2023-10-08 13:00:20,437:INFO:  Epoch 330/500:  train Loss: 17.8590   val Loss: 23.2294   time: 511.65s   best: 22.6943
2023-10-08 13:08:55,841:INFO:  Epoch 331/500:  train Loss: 17.8808   val Loss: 23.5268   time: 515.17s   best: 22.6943
2023-10-08 13:17:22,508:INFO:  Epoch 332/500:  train Loss: 17.9334   val Loss: 23.8664   time: 506.64s   best: 22.6943
2023-10-08 13:26:08,216:INFO:  Epoch 333/500:  train Loss: 17.8058   val Loss: 23.2063   time: 525.46s   best: 22.6943
2023-10-08 13:34:06,728:INFO:  Epoch 334/500:  train Loss: 18.0499   val Loss: 23.5788   time: 478.49s   best: 22.6943
2023-10-08 13:42:13,713:INFO:  Epoch 335/500:  train Loss: 17.7714   val Loss: 23.1604   time: 486.96s   best: 22.6943
2023-10-08 13:50:43,570:INFO:  Epoch 336/500:  train Loss: 17.8654   val Loss: 23.3233   time: 509.84s   best: 22.6943
2023-10-08 13:58:43,635:INFO:  Epoch 337/500:  train Loss: 17.7857   val Loss: 23.1980   time: 480.02s   best: 22.6943
2023-10-08 14:06:41,110:INFO:  Epoch 338/500:  train Loss: 17.7307   val Loss: 23.1657   time: 477.43s   best: 22.6943
2023-10-08 14:14:58,086:INFO:  Epoch 339/500:  train Loss: 17.7876   val Loss: 23.2057   time: 496.93s   best: 22.6943
2023-10-08 14:25:22,486:INFO:  Epoch 340/500:  train Loss: 17.8672   val Loss: 23.6282   time: 624.36s   best: 22.6943
2023-10-08 14:37:03,906:INFO:  Epoch 341/500:  train Loss: 17.7497   val Loss: 23.7003   time: 701.37s   best: 22.6943
2023-10-08 14:50:25,074:INFO:  Epoch 342/500:  train Loss: 17.8105   val Loss: 23.2153   time: 801.13s   best: 22.6943
2023-10-08 15:02:43,588:INFO:  Epoch 343/500:  train Loss: 17.8611   val Loss: 23.0888   time: 738.49s   best: 22.6943
2023-10-08 15:14:48,759:INFO:  Epoch 344/500:  train Loss: 17.9141   val Loss: 23.5189   time: 725.13s   best: 22.6943
2023-10-08 15:27:10,011:INFO:  Epoch 345/500:  train Loss: 17.6794   val Loss: 23.6442   time: 741.05s   best: 22.6943
2023-10-08 15:39:42,802:INFO:  Epoch 346/500:  train Loss: 17.7932   val Loss: 23.6856   time: 752.74s   best: 22.6943
2023-10-08 15:51:57,308:INFO:  Epoch 347/500:  train Loss: 17.9104   val Loss: 24.1284   time: 734.47s   best: 22.6943
2023-10-08 16:04:11,559:INFO:  Epoch 348/500:  train Loss: 17.7338   val Loss: 23.6407   time: 734.20s   best: 22.6943
2023-10-08 16:15:45,929:INFO:  Epoch 349/500:  train Loss: 17.8303   val Loss: 22.9164   time: 694.26s   best: 22.6943
2023-10-08 16:27:10,020:INFO:  Epoch 350/500:  train Loss: 17.7242   val Loss: 23.4343   time: 684.06s   best: 22.6943
2023-10-08 16:38:35,927:INFO:  Epoch 351/500:  train Loss: 18.0628   val Loss: 23.2084   time: 685.86s   best: 22.6943
2023-10-08 16:50:34,854:INFO:  Epoch 352/500:  train Loss: 17.6607   val Loss: 23.4867   time: 718.89s   best: 22.6943
2023-10-08 17:01:27,972:INFO:  Epoch 353/500:  train Loss: 17.7581   val Loss: 23.3205   time: 653.10s   best: 22.6943
2023-10-08 17:11:53,677:INFO:  Epoch 354/500:  train Loss: 17.7369   val Loss: 23.4220   time: 625.67s   best: 22.6943
2023-10-08 17:15:44,120:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-08 17:15:44,128:INFO:  Defining the model
2023-10-08 17:15:44,377:INFO:  Reading the dataset
2023-10-08 17:22:29,625:INFO:  Epoch 355/500:  train Loss: 17.7220   val Loss: 23.5814   time: 635.91s   best: 22.6943
2023-10-08 17:32:26,895:INFO:  Epoch 356/500:  train Loss: 17.6744   val Loss: 23.3061   time: 597.25s   best: 22.6943
2023-10-08 17:42:25,207:INFO:  Epoch 357/500:  train Loss: 17.6177   val Loss: 23.7563   time: 598.23s   best: 22.6943
2023-10-08 17:52:21,090:INFO:  Epoch 358/500:  train Loss: 17.6366   val Loss: 23.0465   time: 595.85s   best: 22.6943
2023-10-08 18:02:54,548:INFO:  Epoch 359/500:  train Loss: 17.7293   val Loss: 23.3532   time: 633.44s   best: 22.6943
2023-10-08 18:13:10,750:INFO:  Epoch 360/500:  train Loss: 17.7783   val Loss: 23.1930   time: 616.14s   best: 22.6943
2023-10-08 18:19:12,422:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-08 18:19:12,432:INFO:  Defining the model
2023-10-08 18:19:12,983:INFO:  Reading the dataset
2023-10-08 18:23:41,394:INFO:  Epoch 361/500:  train Loss: 17.6974   val Loss: 23.5500   time: 630.61s   best: 22.6943
2023-10-08 18:34:03,652:INFO:  Epoch 362/500:  train Loss: 17.7118   val Loss: 23.1924   time: 622.18s   best: 22.6943
2023-10-08 18:44:47,562:INFO:  Epoch 363/500:  train Loss: 18.0605   val Loss: 23.2161   time: 643.82s   best: 22.6943
2023-10-08 18:55:33,339:INFO:  Epoch 364/500:  train Loss: 17.9538   val Loss: 23.4449   time: 645.53s   best: 22.6943
2023-10-08 19:05:47,009:INFO:  Epoch 365/500:  train Loss: 17.7148   val Loss: 23.7382   time: 613.60s   best: 22.6943
2023-10-08 19:16:45,856:INFO:  Epoch 366/500:  train Loss: 17.6132   val Loss: 23.5691   time: 658.82s   best: 22.6943
2023-10-08 19:28:11,514:INFO:  Epoch 367/500:  train Loss: 17.7066   val Loss: 23.8355   time: 685.63s   best: 22.6943
2023-10-08 19:39:14,201:INFO:  Epoch 368/500:  train Loss: 17.6742   val Loss: 23.4178   time: 662.65s   best: 22.6943
2023-10-08 19:50:16,846:INFO:  Epoch 369/500:  train Loss: 17.6508   val Loss: 28.4104   time: 662.58s   best: 22.6943
2023-10-08 20:00:58,070:INFO:  Epoch 370/500:  train Loss: 17.7402   val Loss: 23.3329   time: 641.19s   best: 22.6943
2023-10-08 20:11:42,863:INFO:  Epoch 371/500:  train Loss: 17.6947   val Loss: 23.7144   time: 644.73s   best: 22.6943
2023-10-08 20:21:48,415:INFO:  Epoch 372/500:  train Loss: 17.8469   val Loss: 24.4778   time: 605.48s   best: 22.6943
2023-10-08 20:31:48,435:INFO:  Epoch 373/500:  train Loss: 17.6808   val Loss: 23.6364   time: 599.96s   best: 22.6943
2023-10-08 20:41:54,880:INFO:  Epoch 374/500:  train Loss: 17.5891   val Loss: 22.9201   time: 606.41s   best: 22.6943
2023-10-08 20:52:35,471:INFO:  Epoch 375/500:  train Loss: 17.8717   val Loss: 23.7009   time: 640.55s   best: 22.6943
2023-10-08 21:00:35,417:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-08 21:00:35,425:INFO:  Defining the model
2023-10-08 21:00:35,660:INFO:  Reading the dataset
2023-10-08 21:02:40,645:INFO:  Epoch 376/500:  train Loss: 17.6143   val Loss: 23.7040   time: 605.10s   best: 22.6943
2023-10-08 21:12:56,660:INFO:  Epoch 377/500:  train Loss: 17.6610   val Loss: 24.3663   time: 615.96s   best: 22.6943
2023-10-08 21:22:46,556:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-08 21:22:46,562:INFO:  Defining the model
2023-10-08 21:22:46,751:INFO:  Reading the dataset
2023-10-08 21:22:51,858:INFO:  Epoch 378/500:  train Loss: 17.7020   val Loss: 23.4758   time: 594.37s   best: 22.6943
2023-10-08 21:34:00,616:INFO:  Epoch 379/500:  train Loss: 17.8613   val Loss: 23.1073   time: 668.74s   best: 22.6943
2023-10-08 21:44:09,923:INFO:  Epoch 380/500:  train Loss: 17.6341   val Loss: 23.4160   time: 609.23s   best: 22.6943
2023-10-08 21:54:15,327:INFO:  Epoch 381/500:  train Loss: 17.5792   val Loss: 24.1231   time: 605.36s   best: 22.6943
2023-10-08 22:04:05,122:INFO:  Epoch 382/500:  train Loss: 17.6238   val Loss: 23.0475   time: 589.74s   best: 22.6943
2023-10-08 22:14:03,595:INFO:  Epoch 383/500:  train Loss: 17.7957   val Loss: 25.3900   time: 598.44s   best: 22.6943
2023-10-08 22:24:20,382:INFO:  Epoch 384/500:  train Loss: 17.6706   val Loss: 23.3108   time: 616.75s   best: 22.6943
2023-10-08 22:34:50,551:INFO:  Epoch 385/500:  train Loss: 17.5544   val Loss: 23.3144   time: 630.12s   best: 22.6943
2023-10-08 22:44:54,571:INFO:  Epoch 386/500:  train Loss: 17.6019   val Loss: 23.0233   time: 603.97s   best: 22.6943
2023-10-08 22:55:17,962:INFO:  Epoch 387/500:  train Loss: 17.5658   val Loss: 22.9996   time: 623.20s   best: 22.6943
2023-10-08 23:05:23,029:INFO:  Epoch 388/500:  train Loss: 17.6447   val Loss: 23.6450   time: 605.00s   best: 22.6943
2023-10-08 23:15:32,690:INFO:  Epoch 389/500:  train Loss: 17.4686   val Loss: 23.4915   time: 609.61s   best: 22.6943
2023-10-08 23:25:26,052:INFO:  Epoch 390/500:  train Loss: 17.6564   val Loss: 23.7756   time: 593.31s   best: 22.6943
2023-10-08 23:36:00,236:INFO:  Epoch 391/500:  train Loss: 17.5055   val Loss: 23.2758   time: 634.10s   best: 22.6943
2023-10-08 23:46:12,373:INFO:  Epoch 392/500:  train Loss: 17.5537   val Loss: 22.9052   time: 612.11s   best: 22.6943
2023-10-08 23:56:07,871:INFO:  Epoch 393/500:  train Loss: 17.6418   val Loss: 23.2082   time: 595.45s   best: 22.6943
2023-10-09 00:05:56,453:INFO:  Epoch 394/500:  train Loss: 17.6222   val Loss: 23.2281   time: 588.55s   best: 22.6943
2023-10-09 00:15:29,769:INFO:  Epoch 395/500:  train Loss: 17.5965   val Loss: 23.6378   time: 573.30s   best: 22.6943
2023-10-09 00:25:29,725:INFO:  Epoch 396/500:  train Loss: 17.8236   val Loss: 23.6314   time: 599.91s   best: 22.6943
2023-10-09 00:35:29,991:INFO:  Epoch 397/500:  train Loss: 17.7696   val Loss: 23.2277   time: 600.20s   best: 22.6943
2023-10-09 00:46:12,066:INFO:  Epoch 398/500:  train Loss: 17.4982   val Loss: 22.8563   time: 642.02s   best: 22.6943
2023-10-09 00:56:23,460:INFO:  Epoch 399/500:  train Loss: 17.5291   val Loss: 22.7358   time: 611.36s   best: 22.6943
2023-10-09 01:06:15,046:INFO:  Epoch 400/500:  train Loss: 17.8388   val Loss: 23.3819   time: 591.57s   best: 22.6943
2023-10-09 01:15:45,802:INFO:  Epoch 401/500:  train Loss: 17.4720   val Loss: 23.7763   time: 570.72s   best: 22.6943
2023-10-09 01:25:44,925:INFO:  Epoch 402/500:  train Loss: 17.4895   val Loss: 23.1630   time: 599.08s   best: 22.6943
2023-10-09 01:35:40,086:INFO:  Epoch 403/500:  train Loss: 17.4856   val Loss: 23.2119   time: 595.10s   best: 22.6943
2023-10-09 01:45:26,738:INFO:  Epoch 404/500:  train Loss: 17.4149   val Loss: 23.3862   time: 586.62s   best: 22.6943
2023-10-09 01:55:14,770:INFO:  Epoch 405/500:  train Loss: 17.5294   val Loss: 24.5729   time: 588.00s   best: 22.6943
2023-10-09 02:06:24,210:INFO:  Epoch 406/500:  train Loss: 17.5868   val Loss: 23.4045   time: 669.40s   best: 22.6943
2023-10-09 02:16:37,599:INFO:  Epoch 407/500:  train Loss: 17.6014   val Loss: 23.2680   time: 613.32s   best: 22.6943
2023-10-09 02:26:39,880:INFO:  Epoch 408/500:  train Loss: 17.5515   val Loss: 23.0828   time: 601.98s   best: 22.6943
2023-10-09 02:36:54,012:INFO:  Epoch 409/500:  train Loss: 17.4433   val Loss: 23.2047   time: 614.12s   best: 22.6943
2023-10-09 02:47:29,939:INFO:  Epoch 410/500:  train Loss: 17.6396   val Loss: 23.4174   time: 635.89s   best: 22.6943
2023-10-09 02:58:24,411:INFO:  Epoch 411/500:  train Loss: 17.4289   val Loss: 23.4583   time: 654.39s   best: 22.6943
2023-10-09 03:08:45,922:INFO:  Epoch 412/500:  train Loss: 17.4574   val Loss: 23.3132   time: 621.47s   best: 22.6943
2023-10-09 03:18:45,810:INFO:  Epoch 413/500:  train Loss: 17.5933   val Loss: 23.3927   time: 599.85s   best: 22.6943
2023-10-09 03:29:45,668:INFO:  Epoch 414/500:  train Loss: 17.5178   val Loss: 23.9735   time: 659.83s   best: 22.6943
2023-10-09 03:40:11,600:INFO:  Epoch 415/500:  train Loss: 17.4748   val Loss: 23.3808   time: 625.89s   best: 22.6943
2023-10-09 03:50:18,224:INFO:  Epoch 416/500:  train Loss: 17.4149   val Loss: 22.9125   time: 606.56s   best: 22.6943
2023-10-09 04:00:37,762:INFO:  Epoch 417/500:  train Loss: 17.5027   val Loss: 26.0306   time: 619.50s   best: 22.6943
2023-10-09 04:11:43,742:INFO:  Epoch 418/500:  train Loss: 17.5198   val Loss: 23.1778   time: 665.95s   best: 22.6943
2023-10-09 04:22:30,952:INFO:  Epoch 419/500:  train Loss: 17.4490   val Loss: 23.4006   time: 647.16s   best: 22.6943
2023-10-09 04:33:50,808:INFO:  Epoch 420/500:  train Loss: 17.5172   val Loss: 23.1350   time: 679.83s   best: 22.6943
2023-10-09 04:44:37,600:INFO:  Epoch 421/500:  train Loss: 17.6645   val Loss: 23.0544   time: 646.69s   best: 22.6943
2023-10-09 04:55:23,741:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (+ hidden vector)_e5f4.pt
2023-10-09 04:55:24,686:INFO:  Epoch 422/500:  train Loss: 17.5946   val Loss: 22.6032   time: 645.83s   best: 22.6032
2023-10-09 05:06:11,868:INFO:  Epoch 423/500:  train Loss: 17.4300   val Loss: 23.1640   time: 647.18s   best: 22.6032
2023-10-09 05:17:32,708:INFO:  Epoch 424/500:  train Loss: 17.4569   val Loss: 23.4632   time: 680.62s   best: 22.6032
2023-10-09 05:28:20,567:INFO:  Epoch 425/500:  train Loss: 17.4947   val Loss: 23.3620   time: 647.81s   best: 22.6032
2023-10-09 05:38:52,333:INFO:  Epoch 426/500:  train Loss: 17.3915   val Loss: 23.6603   time: 631.45s   best: 22.6032
2023-10-09 05:49:49,404:INFO:  Epoch 427/500:  train Loss: 17.3990   val Loss: 23.0812   time: 657.04s   best: 22.6032
2023-10-09 06:01:26,973:INFO:  Epoch 428/500:  train Loss: 17.5850   val Loss: 23.5100   time: 697.53s   best: 22.6032
2023-10-09 06:12:34,605:INFO:  Epoch 429/500:  train Loss: 17.4866   val Loss: 22.6769   time: 667.59s   best: 22.6032
2023-10-09 06:23:03,841:INFO:  Epoch 430/500:  train Loss: 17.6210   val Loss: 23.2224   time: 629.21s   best: 22.6032
2023-10-09 06:34:16,602:INFO:  Epoch 431/500:  train Loss: 18.0046   val Loss: 23.5578   time: 672.70s   best: 22.6032
2023-10-09 06:45:13,365:INFO:  Epoch 432/500:  train Loss: 17.4686   val Loss: 23.4344   time: 656.71s   best: 22.6032
2023-10-09 06:56:19,669:INFO:  Epoch 433/500:  train Loss: 17.6689   val Loss: 24.2526   time: 666.27s   best: 22.6032
2023-10-09 07:06:59,955:INFO:  Epoch 434/500:  train Loss: 17.6577   val Loss: 23.3589   time: 640.25s   best: 22.6032
2023-10-09 07:17:50,381:INFO:  Epoch 435/500:  train Loss: 17.6231   val Loss: 22.8784   time: 650.36s   best: 22.6032
2023-10-09 07:28:04,580:INFO:  Epoch 436/500:  train Loss: 17.4942   val Loss: 22.7690   time: 614.15s   best: 22.6032
2023-10-09 07:39:01,400:INFO:  Epoch 437/500:  train Loss: 17.5135   val Loss: 23.3247   time: 656.31s   best: 22.6032
2023-10-09 07:50:08,760:INFO:  Epoch 438/500:  train Loss: 17.6740   val Loss: 22.7782   time: 667.31s   best: 22.6032
2023-10-09 08:01:13,541:INFO:  Epoch 439/500:  train Loss: 17.4067   val Loss: 23.3046   time: 664.74s   best: 22.6032
2023-10-09 08:12:18,636:INFO:  Epoch 440/500:  train Loss: 17.6115   val Loss: 24.6021   time: 665.06s   best: 22.6032
2023-10-09 08:23:16,519:INFO:  Epoch 441/500:  train Loss: 17.4411   val Loss: 24.3526   time: 657.85s   best: 22.6032
2023-10-09 08:34:17,449:INFO:  Epoch 442/500:  train Loss: 17.4957   val Loss: 23.6923   time: 660.87s   best: 22.6032
2023-10-09 08:45:06,041:INFO:  Epoch 443/500:  train Loss: 17.4088   val Loss: 23.5145   time: 648.56s   best: 22.6032
2023-10-09 08:56:21,390:INFO:  Epoch 444/500:  train Loss: 17.4696   val Loss: 22.8932   time: 675.07s   best: 22.6032
2023-10-09 09:01:38,043:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-09 09:01:38,052:INFO:  Defining the model
2023-10-09 09:01:39,112:INFO:  Reading the dataset
2023-10-09 09:07:35,899:INFO:  Epoch 445/500:  train Loss: 17.5671   val Loss: 23.2681   time: 674.43s   best: 22.6032
2023-10-09 09:18:29,287:INFO:  Epoch 446/500:  train Loss: 17.4687   val Loss: 23.8176   time: 653.35s   best: 22.6032
2023-10-09 09:29:20,731:INFO:  Epoch 447/500:  train Loss: 17.3826   val Loss: 23.3325   time: 651.43s   best: 22.6032
2023-10-09 09:40:52,795:INFO:  Epoch 448/500:  train Loss: 17.2644   val Loss: 22.9299   time: 692.04s   best: 22.6032
2023-10-09 09:52:31,870:INFO:  Epoch 449/500:  train Loss: 17.5955   val Loss: 22.8192   time: 699.04s   best: 22.6032
2023-10-09 10:03:59,435:INFO:  Epoch 450/500:  train Loss: 17.3325   val Loss: 23.6807   time: 687.52s   best: 22.6032
2023-10-09 10:13:11,745:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-09 10:13:11,758:INFO:  Defining the model
2023-10-09 10:13:12,087:INFO:  Reading the dataset
2023-10-09 10:16:10,398:INFO:  Epoch 451/500:  train Loss: 17.8383   val Loss: 23.1609   time: 730.93s   best: 22.6032
2023-10-09 10:28:22,264:INFO:  Epoch 452/500:  train Loss: 17.5159   val Loss: 23.2124   time: 731.83s   best: 22.6032
2023-10-09 10:39:23,946:INFO:  Epoch 453/500:  train Loss: 17.3864   val Loss: 23.0752   time: 661.63s   best: 22.6032
2023-10-09 10:50:27,877:INFO:  Epoch 454/500:  train Loss: 17.4401   val Loss: 24.2207   time: 663.90s   best: 22.6032
2023-10-09 11:01:59,788:INFO:  Epoch 455/500:  train Loss: 17.2932   val Loss: 23.8337   time: 691.82s   best: 22.6032
2023-10-09 11:07:15,033:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-09 11:07:15,046:INFO:  Defining the model
2023-10-09 11:07:15,390:INFO:  Reading the dataset
2023-10-09 11:14:22,202:INFO:  Epoch 456/500:  train Loss: 17.3586   val Loss: 23.7745   time: 742.37s   best: 22.6032
2023-10-09 11:26:34,559:INFO:  Epoch 457/500:  train Loss: 17.4736   val Loss: 26.0285   time: 732.24s   best: 22.6032
2023-10-09 11:37:50,367:INFO:  Epoch 458/500:  train Loss: 17.4502   val Loss: 23.3283   time: 675.78s   best: 22.6032
2023-10-09 11:49:04,583:INFO:  Epoch 459/500:  train Loss: 17.3282   val Loss: 23.0829   time: 674.17s   best: 22.6032
2023-10-09 12:00:24,728:INFO:  Epoch 460/500:  train Loss: 17.4323   val Loss: 23.6051   time: 680.10s   best: 22.6032
2023-10-09 12:12:38,529:INFO:  Epoch 461/500:  train Loss: 17.4954   val Loss: 22.9790   time: 733.74s   best: 22.6032
2023-10-09 12:24:10,249:INFO:  Epoch 462/500:  train Loss: 17.3361   val Loss: 23.1187   time: 691.66s   best: 22.6032
2023-10-09 12:29:20,893:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-09 12:29:20,911:INFO:  Defining the model
2023-10-09 12:29:21,123:INFO:  Reading the dataset
2023-10-09 15:11:48,429:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-09 15:11:48,457:INFO:  Defining the model
2023-10-09 15:11:49,033:INFO:  Reading the dataset
2023-10-09 16:05:46,146:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-09 16:05:46,154:INFO:  Defining the model
2023-10-09 16:05:46,662:INFO:  Reading the dataset
2023-10-09 17:12:14,538:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-09 17:12:14,549:INFO:  Defining the model
2023-10-09 17:12:14,929:INFO:  Reading the dataset
2023-10-09 17:14:33,380:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-09 17:14:33,380:INFO:  Defining the model
2023-10-09 17:14:33,455:INFO:  Reading the dataset
2023-10-09 17:14:52,190:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:14:52,613:INFO:  Epoch 1/500:  train Loss: 98.9477   val Loss: 95.9821   time: 5.60s   best: 95.9821
2023-10-09 17:14:52,810:INFO:  Epoch 2/500:  train Loss: 98.1830   val Loss: 98.2710   time: 0.20s   best: 95.9821
2023-10-09 17:14:53,004:INFO:  Epoch 3/500:  train Loss: 98.4051   val Loss: 96.1242   time: 0.19s   best: 95.9821
2023-10-09 17:14:53,236:INFO:  Epoch 4/500:  train Loss: 98.2577   val Loss: 96.5249   time: 0.23s   best: 95.9821
2023-10-09 17:14:53,430:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:14:53,722:INFO:  Epoch 5/500:  train Loss: 97.8363   val Loss: 95.7889   time: 0.19s   best: 95.7889
2023-10-09 17:14:53,913:INFO:  Epoch 6/500:  train Loss: 97.9019   val Loss: 99.1355   time: 0.19s   best: 95.7889
2023-10-09 17:14:54,111:INFO:  Epoch 7/500:  train Loss: 98.4055   val Loss: 98.8181   time: 0.20s   best: 95.7889
2023-10-09 17:14:54,304:INFO:  Epoch 8/500:  train Loss: 97.8110   val Loss: 97.4867   time: 0.19s   best: 95.7889
2023-10-09 17:14:54,496:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:14:54,510:INFO:  Epoch 9/500:  train Loss: 97.2894   val Loss: 92.9272   time: 0.19s   best: 92.9272
2023-10-09 17:14:54,706:INFO:  Epoch 10/500:  train Loss: 95.9191   val Loss: 93.0643   time: 0.20s   best: 92.9272
2023-10-09 17:14:54,895:INFO:  Epoch 11/500:  train Loss: 94.9655   val Loss: 95.1070   time: 0.19s   best: 92.9272
2023-10-09 17:14:55,084:INFO:  Epoch 12/500:  train Loss: 94.9483   val Loss: 95.6756   time: 0.19s   best: 92.9272
2023-10-09 17:14:55,301:INFO:  Epoch 13/500:  train Loss: 95.0819   val Loss: 95.3471   time: 0.22s   best: 92.9272
2023-10-09 17:14:55,492:INFO:  Epoch 14/500:  train Loss: 95.2035   val Loss: 95.0046   time: 0.19s   best: 92.9272
2023-10-09 17:14:55,689:INFO:  Epoch 15/500:  train Loss: 94.8641   val Loss: 94.7807   time: 0.20s   best: 92.9272
2023-10-09 17:14:55,892:INFO:  Epoch 16/500:  train Loss: 94.1294   val Loss: 94.6251   time: 0.20s   best: 92.9272
2023-10-09 17:14:56,082:INFO:  Epoch 17/500:  train Loss: 93.9047   val Loss: 94.2719   time: 0.19s   best: 92.9272
2023-10-09 17:14:56,280:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:14:56,367:INFO:  Epoch 18/500:  train Loss: 93.7057   val Loss: 90.8449   time: 0.19s   best: 90.8449
2023-10-09 17:14:56,558:INFO:  Epoch 19/500:  train Loss: 94.0474   val Loss: 93.1156   time: 0.19s   best: 90.8449
2023-10-09 17:14:56,756:INFO:  Epoch 20/500:  train Loss: 93.8905   val Loss: 94.4178   time: 0.20s   best: 90.8449
2023-10-09 17:14:56,947:INFO:  Epoch 21/500:  train Loss: 94.2383   val Loss: 94.6172   time: 0.19s   best: 90.8449
2023-10-09 17:14:57,140:INFO:  Epoch 22/500:  train Loss: 94.3769   val Loss: 94.5570   time: 0.19s   best: 90.8449
2023-10-09 17:14:57,337:INFO:  Epoch 23/500:  train Loss: 94.5814   val Loss: 94.3897   time: 0.20s   best: 90.8449
2023-10-09 17:14:57,527:INFO:  Epoch 24/500:  train Loss: 94.0828   val Loss: 94.1656   time: 0.19s   best: 90.8449
2023-10-09 17:14:57,724:INFO:  Epoch 25/500:  train Loss: 93.9084   val Loss: 94.0642   time: 0.20s   best: 90.8449
2023-10-09 17:14:57,914:INFO:  Epoch 26/500:  train Loss: 93.5179   val Loss: 94.1651   time: 0.19s   best: 90.8449
2023-10-09 17:14:58,105:INFO:  Epoch 27/500:  train Loss: 94.1631   val Loss: 94.1300   time: 0.19s   best: 90.8449
2023-10-09 17:14:58,301:INFO:  Epoch 28/500:  train Loss: 93.8931   val Loss: 94.0106   time: 0.20s   best: 90.8449
2023-10-09 17:14:58,491:INFO:  Epoch 29/500:  train Loss: 93.9325   val Loss: 93.9081   time: 0.19s   best: 90.8449
2023-10-09 17:14:58,683:INFO:  Epoch 30/500:  train Loss: 93.7923   val Loss: 93.8894   time: 0.19s   best: 90.8449
2023-10-09 17:14:58,877:INFO:  Epoch 31/500:  train Loss: 94.0111   val Loss: 93.9443   time: 0.19s   best: 90.8449
2023-10-09 17:14:59,067:INFO:  Epoch 32/500:  train Loss: 93.8100   val Loss: 93.9032   time: 0.19s   best: 90.8449
2023-10-09 17:14:59,264:INFO:  Epoch 33/500:  train Loss: 93.4446   val Loss: 93.8037   time: 0.20s   best: 90.8449
2023-10-09 17:14:59,454:INFO:  Epoch 34/500:  train Loss: 93.8914   val Loss: 93.8248   time: 0.19s   best: 90.8449
2023-10-09 17:14:59,645:INFO:  Epoch 35/500:  train Loss: 93.6500   val Loss: 93.6527   time: 0.19s   best: 90.8449
2023-10-09 17:14:59,842:INFO:  Epoch 36/500:  train Loss: 93.3993   val Loss: 93.4227   time: 0.20s   best: 90.8449
2023-10-09 17:15:00,032:INFO:  Epoch 37/500:  train Loss: 93.0138   val Loss: 93.1422   time: 0.19s   best: 90.8449
2023-10-09 17:15:00,221:INFO:  Epoch 38/500:  train Loss: 93.0731   val Loss: 92.6237   time: 0.19s   best: 90.8449
2023-10-09 17:15:00,416:INFO:  Epoch 39/500:  train Loss: 92.2000   val Loss: 91.9987   time: 0.19s   best: 90.8449
2023-10-09 17:15:00,605:INFO:  Epoch 40/500:  train Loss: 91.5872   val Loss: 91.5185   time: 0.19s   best: 90.8449
2023-10-09 17:15:00,803:INFO:  Epoch 41/500:  train Loss: 91.3714   val Loss: 91.4923   time: 0.20s   best: 90.8449
2023-10-09 17:15:00,993:INFO:  Epoch 42/500:  train Loss: 91.2961   val Loss: 91.3228   time: 0.19s   best: 90.8449
2023-10-09 17:15:01,183:INFO:  Epoch 43/500:  train Loss: 91.0632   val Loss: 90.9141   time: 0.19s   best: 90.8449
2023-10-09 17:15:01,377:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:01,820:INFO:  Epoch 44/500:  train Loss: 91.0618   val Loss: 90.5312   time: 0.19s   best: 90.5312
2023-10-09 17:15:02,013:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:02,027:INFO:  Epoch 45/500:  train Loss: 90.7976   val Loss: 89.9248   time: 0.19s   best: 89.9248
2023-10-09 17:15:02,219:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:02,346:INFO:  Epoch 46/500:  train Loss: 90.2399   val Loss: 88.7988   time: 0.19s   best: 88.7988
2023-10-09 17:15:02,541:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:02,562:INFO:  Epoch 47/500:  train Loss: 88.9143   val Loss: 88.4531   time: 0.19s   best: 88.4531
2023-10-09 17:15:02,765:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:06,342:INFO:  Epoch 48/500:  train Loss: 88.4754   val Loss: 87.5617   time: 0.19s   best: 87.5617
2023-10-09 17:15:06,540:INFO:  Epoch 49/500:  train Loss: 88.1506   val Loss: 87.8325   time: 0.20s   best: 87.5617
2023-10-09 17:15:06,731:INFO:  Epoch 50/500:  train Loss: 87.5339   val Loss: 88.4236   time: 0.19s   best: 87.5617
2023-10-09 17:15:06,928:INFO:  Epoch 51/500:  train Loss: 89.3100   val Loss: 89.1221   time: 0.20s   best: 87.5617
2023-10-09 17:15:07,122:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:07,136:INFO:  Epoch 52/500:  train Loss: 88.6308   val Loss: 87.2436   time: 0.19s   best: 87.2436
2023-10-09 17:15:07,335:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:07,612:INFO:  Epoch 53/500:  train Loss: 87.8014   val Loss: 87.0027   time: 0.20s   best: 87.0027
2023-10-09 17:15:07,805:INFO:  Epoch 54/500:  train Loss: 87.0682   val Loss: 87.4556   time: 0.19s   best: 87.0027
2023-10-09 17:15:08,007:INFO:  Epoch 55/500:  train Loss: 88.2519   val Loss: 88.2050   time: 0.20s   best: 87.0027
2023-10-09 17:15:08,200:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:08,214:INFO:  Epoch 56/500:  train Loss: 87.8298   val Loss: 86.5621   time: 0.19s   best: 86.5621
2023-10-09 17:15:08,415:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:08,429:INFO:  Epoch 57/500:  train Loss: 86.9491   val Loss: 86.2368   time: 0.20s   best: 86.2368
2023-10-09 17:15:08,621:INFO:  Epoch 58/500:  train Loss: 86.6545   val Loss: 86.9567   time: 0.19s   best: 86.2368
2023-10-09 17:15:08,816:INFO:  Epoch 59/500:  train Loss: 87.5734   val Loss: 87.2394   time: 0.19s   best: 86.2368
2023-10-09 17:15:09,010:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:09,531:INFO:  Epoch 60/500:  train Loss: 87.2951   val Loss: 86.1834   time: 0.19s   best: 86.1834
2023-10-09 17:15:09,724:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:09,785:INFO:  Epoch 61/500:  train Loss: 86.3556   val Loss: 86.0102   time: 0.19s   best: 86.0102
2023-10-09 17:15:10,003:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:10,018:INFO:  Epoch 62/500:  train Loss: 86.1335   val Loss: 85.8333   time: 0.21s   best: 85.8333
2023-10-09 17:15:10,212:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:10,806:INFO:  Epoch 63/500:  train Loss: 85.9744   val Loss: 85.7761   time: 0.19s   best: 85.7761
2023-10-09 17:15:10,996:INFO:  Epoch 64/500:  train Loss: 86.6407   val Loss: 86.7647   time: 0.19s   best: 85.7761
2023-10-09 17:15:11,186:INFO:  Epoch 65/500:  train Loss: 87.6982   val Loss: 87.4200   time: 0.19s   best: 85.7761
2023-10-09 17:15:11,387:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:11,414:INFO:  Epoch 66/500:  train Loss: 87.1923   val Loss: 85.5502   time: 0.20s   best: 85.5502
2023-10-09 17:15:11,605:INFO:  Epoch 67/500:  train Loss: 86.7540   val Loss: 85.5920   time: 0.19s   best: 85.5502
2023-10-09 17:15:11,800:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:12,033:INFO:  Epoch 68/500:  train Loss: 85.7379   val Loss: 84.9997   time: 0.19s   best: 84.9997
2023-10-09 17:15:12,227:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:12,241:INFO:  Epoch 69/500:  train Loss: 85.2877   val Loss: 84.8357   time: 0.19s   best: 84.8357
2023-10-09 17:15:12,438:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:12,451:INFO:  Epoch 70/500:  train Loss: 85.3132   val Loss: 84.5828   time: 0.19s   best: 84.5828
2023-10-09 17:15:12,644:INFO:  Epoch 71/500:  train Loss: 85.2819   val Loss: 84.8514   time: 0.19s   best: 84.5828
2023-10-09 17:15:12,841:INFO:  Epoch 72/500:  train Loss: 84.8329   val Loss: 85.4311   time: 0.20s   best: 84.5828
2023-10-09 17:15:13,037:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:13,105:INFO:  Epoch 73/500:  train Loss: 85.3666   val Loss: 84.5297   time: 0.19s   best: 84.5297
2023-10-09 17:15:13,299:INFO:  Epoch 74/500:  train Loss: 85.5540   val Loss: 85.0428   time: 0.19s   best: 84.5297
2023-10-09 17:15:13,500:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:13,682:INFO:  Epoch 75/500:  train Loss: 85.6297   val Loss: 84.3559   time: 0.20s   best: 84.3559
2023-10-09 17:15:13,881:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:13,907:INFO:  Epoch 76/500:  train Loss: 85.5270   val Loss: 83.8279   time: 0.19s   best: 83.8279
2023-10-09 17:15:14,113:INFO:  Epoch 77/500:  train Loss: 84.8324   val Loss: 84.4894   time: 0.20s   best: 83.8279
2023-10-09 17:15:14,305:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:14,555:INFO:  Epoch 78/500:  train Loss: 85.2154   val Loss: 83.3539   time: 0.19s   best: 83.3539
2023-10-09 17:15:14,749:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:16,104:INFO:  Epoch 79/500:  train Loss: 83.5369   val Loss: 83.1608   time: 0.19s   best: 83.1608
2023-10-09 17:15:16,297:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:17,324:INFO:  Epoch 80/500:  train Loss: 83.7463   val Loss: 83.1268   time: 0.19s   best: 83.1268
2023-10-09 17:15:17,522:INFO:  Epoch 81/500:  train Loss: 83.5394   val Loss: 83.2744   time: 0.20s   best: 83.1268
2023-10-09 17:15:17,717:INFO:  Epoch 82/500:  train Loss: 83.5804   val Loss: 83.1817   time: 0.19s   best: 83.1268
2023-10-09 17:15:17,913:INFO:  Epoch 83/500:  train Loss: 83.8059   val Loss: 83.3031   time: 0.20s   best: 83.1268
2023-10-09 17:15:18,118:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:18,133:INFO:  Epoch 84/500:  train Loss: 84.2423   val Loss: 83.0647   time: 0.20s   best: 83.0647
2023-10-09 17:15:18,323:INFO:  Epoch 85/500:  train Loss: 83.9216   val Loss: 83.6422   time: 0.19s   best: 83.0647
2023-10-09 17:15:18,520:INFO:  Epoch 86/500:  train Loss: 83.7724   val Loss: 83.9652   time: 0.20s   best: 83.0647
2023-10-09 17:15:18,712:INFO:  Epoch 87/500:  train Loss: 84.3010   val Loss: 83.0960   time: 0.19s   best: 83.0647
2023-10-09 17:15:18,908:INFO:  Epoch 88/500:  train Loss: 84.0693   val Loss: 84.3432   time: 0.20s   best: 83.0647
2023-10-09 17:15:19,103:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:20,366:INFO:  Epoch 89/500:  train Loss: 84.6818   val Loss: 82.6163   time: 0.19s   best: 82.6163
2023-10-09 17:15:20,560:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:20,576:INFO:  Epoch 90/500:  train Loss: 83.7226   val Loss: 82.3726   time: 0.19s   best: 82.3726
2023-10-09 17:15:20,772:INFO:  Epoch 91/500:  train Loss: 83.1213   val Loss: 82.5323   time: 0.19s   best: 82.3726
2023-10-09 17:15:20,968:INFO:  Epoch 92/500:  train Loss: 82.7970   val Loss: 83.2306   time: 0.20s   best: 82.3726
2023-10-09 17:15:21,217:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:21,595:INFO:  Epoch 93/500:  train Loss: 84.0539   val Loss: 81.7098   time: 0.19s   best: 81.7098
2023-10-09 17:15:21,786:INFO:  Epoch 94/500:  train Loss: 83.2756   val Loss: 82.1679   time: 0.19s   best: 81.7098
2023-10-09 17:15:21,982:INFO:  Epoch 95/500:  train Loss: 82.0754   val Loss: 83.9443   time: 0.20s   best: 81.7098
2023-10-09 17:15:22,186:INFO:  Epoch 96/500:  train Loss: 84.8145   val Loss: 81.7102   time: 0.20s   best: 81.7098
2023-10-09 17:15:22,379:INFO:  Epoch 97/500:  train Loss: 87.0993   val Loss: 92.5117   time: 0.19s   best: 81.7098
2023-10-09 17:15:22,572:INFO:  Epoch 98/500:  train Loss: 93.1358   val Loss: 92.5091   time: 0.19s   best: 81.7098
2023-10-09 17:15:22,763:INFO:  Epoch 99/500:  train Loss: 91.6298   val Loss: 88.6367   time: 0.19s   best: 81.7098
2023-10-09 17:15:23,142:INFO:  Epoch 100/500:  train Loss: 87.9859   val Loss: 86.4945   time: 0.38s   best: 81.7098
2023-10-09 17:15:23,338:INFO:  Epoch 101/500:  train Loss: 87.3636   val Loss: 87.1995   time: 0.19s   best: 81.7098
2023-10-09 17:15:23,534:INFO:  Epoch 102/500:  train Loss: 88.0981   val Loss: 85.4657   time: 0.20s   best: 81.7098
2023-10-09 17:15:23,729:INFO:  Epoch 103/500:  train Loss: 84.6614   val Loss: 83.2208   time: 0.19s   best: 81.7098
2023-10-09 17:15:23,922:INFO:  Epoch 104/500:  train Loss: 83.4960   val Loss: 82.7754   time: 0.19s   best: 81.7098
2023-10-09 17:15:24,134:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:24,332:INFO:  Epoch 105/500:  train Loss: 82.9359   val Loss: 81.5806   time: 0.21s   best: 81.5806
2023-10-09 17:15:24,527:INFO:  Epoch 106/500:  train Loss: 82.0760   val Loss: 82.0625   time: 0.19s   best: 81.5806
2023-10-09 17:15:24,721:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:24,735:INFO:  Epoch 107/500:  train Loss: 81.8737   val Loss: 81.1249   time: 0.19s   best: 81.1249
2023-10-09 17:15:24,930:INFO:  Epoch 108/500:  train Loss: 82.2726   val Loss: 81.6001   time: 0.19s   best: 81.1249
2023-10-09 17:15:25,127:INFO:  Epoch 109/500:  train Loss: 81.7800   val Loss: 81.2871   time: 0.19s   best: 81.1249
2023-10-09 17:15:25,320:INFO:  Epoch 110/500:  train Loss: 81.9169   val Loss: 81.1457   time: 0.19s   best: 81.1249
2023-10-09 17:15:25,522:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:25,536:INFO:  Epoch 111/500:  train Loss: 81.7878   val Loss: 80.6673   time: 0.20s   best: 80.6673
2023-10-09 17:15:25,736:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:25,751:INFO:  Epoch 112/500:  train Loss: 81.2095   val Loss: 80.5745   time: 0.19s   best: 80.5745
2023-10-09 17:15:25,952:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:25,966:INFO:  Epoch 113/500:  train Loss: 81.4758   val Loss: 80.2893   time: 0.20s   best: 80.2893
2023-10-09 17:15:26,168:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:26,356:INFO:  Epoch 114/500:  train Loss: 81.3149   val Loss: 80.1506   time: 0.20s   best: 80.1506
2023-10-09 17:15:26,550:INFO:  Epoch 115/500:  train Loss: 82.6026   val Loss: 80.2326   time: 0.19s   best: 80.1506
2023-10-09 17:15:26,745:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:26,760:INFO:  Epoch 116/500:  train Loss: 81.0458   val Loss: 79.8914   time: 0.19s   best: 79.8914
2023-10-09 17:15:26,954:INFO:  Epoch 117/500:  train Loss: 80.9182   val Loss: 80.2605   time: 0.19s   best: 79.8914
2023-10-09 17:15:27,153:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:27,193:INFO:  Epoch 118/500:  train Loss: 81.1040   val Loss: 79.6517   time: 0.20s   best: 79.6517
2023-10-09 17:15:27,389:INFO:  Epoch 119/500:  train Loss: 80.8155   val Loss: 79.8701   time: 0.20s   best: 79.6517
2023-10-09 17:15:27,590:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:27,677:INFO:  Epoch 120/500:  train Loss: 81.0837   val Loss: 79.3705   time: 0.20s   best: 79.3705
2023-10-09 17:15:27,871:INFO:  Epoch 121/500:  train Loss: 81.1375   val Loss: 80.5905   time: 0.19s   best: 79.3705
2023-10-09 17:15:28,073:INFO:  Epoch 122/500:  train Loss: 81.3378   val Loss: 80.5287   time: 0.20s   best: 79.3705
2023-10-09 17:15:28,273:INFO:  Epoch 123/500:  train Loss: 81.4983   val Loss: 79.4370   time: 0.20s   best: 79.3705
2023-10-09 17:15:28,466:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:28,481:INFO:  Epoch 124/500:  train Loss: 80.3618   val Loss: 78.9953   time: 0.19s   best: 78.9953
2023-10-09 17:15:28,678:INFO:  Epoch 125/500:  train Loss: 80.0845   val Loss: 79.0132   time: 0.20s   best: 78.9953
2023-10-09 17:15:28,868:INFO:  Epoch 126/500:  train Loss: 80.6388   val Loss: 79.8227   time: 0.19s   best: 78.9953
2023-10-09 17:15:29,064:INFO:  Epoch 127/500:  train Loss: 80.5445   val Loss: 79.0956   time: 0.20s   best: 78.9953
2023-10-09 17:15:29,254:INFO:  Epoch 128/500:  train Loss: 80.0215   val Loss: 79.3629   time: 0.19s   best: 78.9953
2023-10-09 17:15:29,447:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:30,180:INFO:  Epoch 129/500:  train Loss: 79.6966   val Loss: 78.7896   time: 0.19s   best: 78.7896
2023-10-09 17:15:30,369:INFO:  Epoch 130/500:  train Loss: 80.1200   val Loss: 78.8318   time: 0.19s   best: 78.7896
2023-10-09 17:15:30,567:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:31,171:INFO:  Epoch 131/500:  train Loss: 79.9885   val Loss: 78.7502   time: 0.19s   best: 78.7502
2023-10-09 17:15:31,364:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:31,378:INFO:  Epoch 132/500:  train Loss: 79.3235   val Loss: 78.0504   time: 0.19s   best: 78.0504
2023-10-09 17:15:31,575:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:31,599:INFO:  Epoch 133/500:  train Loss: 79.7073   val Loss: 78.0152   time: 0.19s   best: 78.0152
2023-10-09 17:15:31,798:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:31,814:INFO:  Epoch 134/500:  train Loss: 78.9625   val Loss: 77.5485   time: 0.20s   best: 77.5485
2023-10-09 17:15:32,014:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:32,126:INFO:  Epoch 135/500:  train Loss: 78.5285   val Loss: 77.4013   time: 0.20s   best: 77.4013
2023-10-09 17:15:32,333:INFO:  Epoch 136/500:  train Loss: 79.3552   val Loss: 78.5797   time: 0.21s   best: 77.4013
2023-10-09 17:15:32,533:INFO:  Epoch 137/500:  train Loss: 80.4028   val Loss: 78.6112   time: 0.20s   best: 77.4013
2023-10-09 17:15:32,730:INFO:  Epoch 138/500:  train Loss: 79.0429   val Loss: 78.6736   time: 0.20s   best: 77.4013
2023-10-09 17:15:32,919:INFO:  Epoch 139/500:  train Loss: 80.1210   val Loss: 78.5516   time: 0.19s   best: 77.4013
2023-10-09 17:15:33,117:INFO:  Epoch 140/500:  train Loss: 81.8629   val Loss: 82.5679   time: 0.19s   best: 77.4013
2023-10-09 17:15:33,309:INFO:  Epoch 141/500:  train Loss: 81.5994   val Loss: 79.7772   time: 0.19s   best: 77.4013
2023-10-09 17:15:33,501:INFO:  Epoch 142/500:  train Loss: 80.9445   val Loss: 80.3155   time: 0.19s   best: 77.4013
2023-10-09 17:15:33,700:INFO:  Epoch 143/500:  train Loss: 79.6818   val Loss: 78.9656   time: 0.20s   best: 77.4013
2023-10-09 17:15:33,892:INFO:  Epoch 144/500:  train Loss: 78.9577   val Loss: 78.3040   time: 0.19s   best: 77.4013
2023-10-09 17:15:34,091:INFO:  Epoch 145/500:  train Loss: 82.9647   val Loss: 77.9163   time: 0.20s   best: 77.4013
2023-10-09 17:15:34,283:INFO:  Epoch 146/500:  train Loss: 86.3275   val Loss: 94.4557   time: 0.19s   best: 77.4013
2023-10-09 17:15:34,475:INFO:  Epoch 147/500:  train Loss: 95.0717   val Loss: 94.2004   time: 0.19s   best: 77.4013
2023-10-09 17:15:34,673:INFO:  Epoch 148/500:  train Loss: 92.6979   val Loss: 89.3681   time: 0.19s   best: 77.4013
2023-10-09 17:15:34,864:INFO:  Epoch 149/500:  train Loss: 88.3415   val Loss: 86.6572   time: 0.19s   best: 77.4013
2023-10-09 17:15:35,056:INFO:  Epoch 150/500:  train Loss: 87.5668   val Loss: 86.5029   time: 0.19s   best: 77.4013
2023-10-09 17:15:35,254:INFO:  Epoch 151/500:  train Loss: 86.3073   val Loss: 85.3484   time: 0.19s   best: 77.4013
2023-10-09 17:15:35,446:INFO:  Epoch 152/500:  train Loss: 85.1250   val Loss: 84.4403   time: 0.19s   best: 77.4013
2023-10-09 17:15:35,643:INFO:  Epoch 153/500:  train Loss: 84.7017   val Loss: 83.6203   time: 0.19s   best: 77.4013
2023-10-09 17:15:35,835:INFO:  Epoch 154/500:  train Loss: 83.9647   val Loss: 82.3423   time: 0.19s   best: 77.4013
2023-10-09 17:15:36,027:INFO:  Epoch 155/500:  train Loss: 82.5492   val Loss: 81.3658   time: 0.19s   best: 77.4013
2023-10-09 17:15:36,232:INFO:  Epoch 156/500:  train Loss: 81.6416   val Loss: 80.2582   time: 0.20s   best: 77.4013
2023-10-09 17:15:36,424:INFO:  Epoch 157/500:  train Loss: 80.6366   val Loss: 79.3758   time: 0.19s   best: 77.4013
2023-10-09 17:15:36,622:INFO:  Epoch 158/500:  train Loss: 79.6855   val Loss: 78.1524   time: 0.19s   best: 77.4013
2023-10-09 17:15:36,818:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:37,071:INFO:  Epoch 159/500:  train Loss: 78.4176   val Loss: 77.2022   time: 0.19s   best: 77.2022
2023-10-09 17:15:37,267:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:37,289:INFO:  Epoch 160/500:  train Loss: 77.8330   val Loss: 76.7204   time: 0.19s   best: 76.7204
2023-10-09 17:15:37,482:INFO:  Epoch 161/500:  train Loss: 78.2654   val Loss: 76.9573   time: 0.19s   best: 76.7204
2023-10-09 17:15:37,677:INFO:  Epoch 162/500:  train Loss: 78.5591   val Loss: 77.6037   time: 0.19s   best: 76.7204
2023-10-09 17:15:37,866:INFO:  Epoch 163/500:  train Loss: 78.3210   val Loss: 77.4462   time: 0.19s   best: 76.7204
2023-10-09 17:15:38,055:INFO:  Epoch 164/500:  train Loss: 77.8645   val Loss: 76.7477   time: 0.19s   best: 76.7204
2023-10-09 17:15:38,268:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:38,519:INFO:  Epoch 165/500:  train Loss: 77.3338   val Loss: 76.0118   time: 0.21s   best: 76.0118
2023-10-09 17:15:38,715:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:38,945:INFO:  Epoch 166/500:  train Loss: 77.6872   val Loss: 75.8633   time: 0.19s   best: 75.8633
2023-10-09 17:15:39,138:INFO:  Epoch 167/500:  train Loss: 77.6887   val Loss: 77.0522   time: 0.19s   best: 75.8633
2023-10-09 17:15:39,334:INFO:  Epoch 168/500:  train Loss: 77.6870   val Loss: 77.3437   time: 0.20s   best: 75.8633
2023-10-09 17:15:39,523:INFO:  Epoch 169/500:  train Loss: 80.4723   val Loss: 76.7685   time: 0.19s   best: 75.8633
2023-10-09 17:15:39,718:INFO:  Epoch 170/500:  train Loss: 79.5978   val Loss: 80.1094   time: 0.19s   best: 75.8633
2023-10-09 17:15:39,908:INFO:  Epoch 171/500:  train Loss: 79.4473   val Loss: 77.0980   time: 0.19s   best: 75.8633
2023-10-09 17:15:40,103:INFO:  Epoch 172/500:  train Loss: 78.3535   val Loss: 77.7571   time: 0.20s   best: 75.8633
2023-10-09 17:15:40,328:INFO:  Epoch 173/500:  train Loss: 78.9592   val Loss: 77.3683   time: 0.22s   best: 75.8633
2023-10-09 17:15:40,518:INFO:  Epoch 174/500:  train Loss: 78.0374   val Loss: 76.3460   time: 0.19s   best: 75.8633
2023-10-09 17:15:40,716:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:41,027:INFO:  Epoch 175/500:  train Loss: 77.1601   val Loss: 75.7642   time: 0.20s   best: 75.7642
2023-10-09 17:15:41,224:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:41,239:INFO:  Epoch 176/500:  train Loss: 76.7326   val Loss: 75.2883   time: 0.19s   best: 75.2883
2023-10-09 17:15:41,430:INFO:  Epoch 177/500:  train Loss: 76.7264   val Loss: 76.3495   time: 0.19s   best: 75.2883
2023-10-09 17:15:41,623:INFO:  Epoch 178/500:  train Loss: 77.0920   val Loss: 76.6948   time: 0.19s   best: 75.2883
2023-10-09 17:15:41,820:INFO:  Epoch 179/500:  train Loss: 76.9591   val Loss: 75.6677   time: 0.20s   best: 75.2883
2023-10-09 17:15:42,013:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:42,233:INFO:  Epoch 180/500:  train Loss: 76.3491   val Loss: 75.1261   time: 0.19s   best: 75.1261
2023-10-09 17:15:42,423:INFO:  Epoch 181/500:  train Loss: 78.0833   val Loss: 75.3505   time: 0.19s   best: 75.1261
2023-10-09 17:15:42,619:INFO:  Epoch 182/500:  train Loss: 76.1233   val Loss: 75.3335   time: 0.19s   best: 75.1261
2023-10-09 17:15:42,817:INFO:  Epoch 183/500:  train Loss: 76.3541   val Loss: 75.1628   time: 0.19s   best: 75.1261
2023-10-09 17:15:43,012:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:43,049:INFO:  Epoch 184/500:  train Loss: 75.7194   val Loss: 74.7199   time: 0.19s   best: 74.7199
2023-10-09 17:15:43,253:INFO:  Epoch 185/500:  train Loss: 76.0683   val Loss: 75.0801   time: 0.20s   best: 74.7199
2023-10-09 17:15:43,448:INFO:  Epoch 186/500:  train Loss: 76.4499   val Loss: 76.0805   time: 0.19s   best: 74.7199
2023-10-09 17:15:43,648:INFO:  Epoch 187/500:  train Loss: 77.1125   val Loss: 75.8740   time: 0.20s   best: 74.7199
2023-10-09 17:15:43,845:INFO:  Epoch 188/500:  train Loss: 76.2463   val Loss: 75.2598   time: 0.20s   best: 74.7199
2023-10-09 17:15:44,039:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:44,054:INFO:  Epoch 189/500:  train Loss: 75.7608   val Loss: 73.9280   time: 0.19s   best: 73.9280
2023-10-09 17:15:44,250:INFO:  Epoch 190/500:  train Loss: 76.9845   val Loss: 75.2212   time: 0.20s   best: 73.9280
2023-10-09 17:15:44,440:INFO:  Epoch 191/500:  train Loss: 77.3905   val Loss: 75.3697   time: 0.19s   best: 73.9280
2023-10-09 17:15:44,631:INFO:  Epoch 192/500:  train Loss: 81.9413   val Loss: 86.4499   time: 0.19s   best: 73.9280
2023-10-09 17:15:44,823:INFO:  Epoch 193/500:  train Loss: 85.2384   val Loss: 81.3847   time: 0.19s   best: 73.9280
2023-10-09 17:15:45,014:INFO:  Epoch 194/500:  train Loss: 82.0495   val Loss: 81.4225   time: 0.19s   best: 73.9280
2023-10-09 17:15:45,211:INFO:  Epoch 195/500:  train Loss: 81.5446   val Loss: 79.5887   time: 0.20s   best: 73.9280
2023-10-09 17:15:45,402:INFO:  Epoch 196/500:  train Loss: 79.5203   val Loss: 78.0509   time: 0.19s   best: 73.9280
2023-10-09 17:15:45,592:INFO:  Epoch 197/500:  train Loss: 78.4005   val Loss: 76.1717   time: 0.19s   best: 73.9280
2023-10-09 17:15:45,788:INFO:  Epoch 198/500:  train Loss: 77.1290   val Loss: 75.8290   time: 0.20s   best: 73.9280
2023-10-09 17:15:45,978:INFO:  Epoch 199/500:  train Loss: 76.4533   val Loss: 74.6702   time: 0.19s   best: 73.9280
2023-10-09 17:15:46,197:INFO:  Epoch 200/500:  train Loss: 75.7972   val Loss: 74.7007   time: 0.22s   best: 73.9280
2023-10-09 17:15:46,407:INFO:  Epoch 201/500:  train Loss: 75.6782   val Loss: 74.7534   time: 0.21s   best: 73.9280
2023-10-09 17:15:46,597:INFO:  Epoch 202/500:  train Loss: 76.0495   val Loss: 76.2879   time: 0.19s   best: 73.9280
2023-10-09 17:15:46,794:INFO:  Epoch 203/500:  train Loss: 76.7511   val Loss: 74.4378   time: 0.20s   best: 73.9280
2023-10-09 17:15:46,984:INFO:  Epoch 204/500:  train Loss: 75.1226   val Loss: 74.7020   time: 0.19s   best: 73.9280
2023-10-09 17:15:47,174:INFO:  Epoch 205/500:  train Loss: 75.8228   val Loss: 74.6599   time: 0.19s   best: 73.9280
2023-10-09 17:15:47,369:INFO:  Epoch 206/500:  train Loss: 75.0318   val Loss: 73.9324   time: 0.19s   best: 73.9280
2023-10-09 17:15:47,562:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:47,759:INFO:  Epoch 207/500:  train Loss: 75.4984   val Loss: 73.8619   time: 0.19s   best: 73.8619
2023-10-09 17:15:47,953:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:48,154:INFO:  Epoch 208/500:  train Loss: 74.3475   val Loss: 73.4270   time: 0.19s   best: 73.4270
2023-10-09 17:15:48,345:INFO:  Epoch 209/500:  train Loss: 75.1341   val Loss: 73.6054   time: 0.19s   best: 73.4270
2023-10-09 17:15:48,546:INFO:  Epoch 210/500:  train Loss: 74.6648   val Loss: 73.5606   time: 0.20s   best: 73.4270
2023-10-09 17:15:48,740:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:49,838:INFO:  Epoch 211/500:  train Loss: 73.9094   val Loss: 73.2729   time: 0.19s   best: 73.2729
2023-10-09 17:15:50,028:INFO:  Epoch 212/500:  train Loss: 74.1382   val Loss: 73.6240   time: 0.19s   best: 73.2729
2023-10-09 17:15:50,221:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:50,307:INFO:  Epoch 213/500:  train Loss: 74.1352   val Loss: 73.0845   time: 0.19s   best: 73.0845
2023-10-09 17:15:50,501:INFO:  Epoch 214/500:  train Loss: 74.5053   val Loss: 73.3420   time: 0.19s   best: 73.0845
2023-10-09 17:15:50,699:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:51,401:INFO:  Epoch 215/500:  train Loss: 74.5752   val Loss: 72.9762   time: 0.19s   best: 72.9762
2023-10-09 17:15:51,594:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:51,889:INFO:  Epoch 216/500:  train Loss: 74.6494   val Loss: 72.2654   time: 0.19s   best: 72.2654
2023-10-09 17:15:52,079:INFO:  Epoch 217/500:  train Loss: 74.3286   val Loss: 73.2770   time: 0.19s   best: 72.2654
2023-10-09 17:15:52,278:INFO:  Epoch 218/500:  train Loss: 74.3904   val Loss: 73.3410   time: 0.20s   best: 72.2654
2023-10-09 17:15:52,485:INFO:  Epoch 219/500:  train Loss: 73.3623   val Loss: 72.3097   time: 0.21s   best: 72.2654
2023-10-09 17:15:52,676:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:52,714:INFO:  Epoch 220/500:  train Loss: 73.3243   val Loss: 71.9477   time: 0.19s   best: 71.9477
2023-10-09 17:15:52,913:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:53,196:INFO:  Epoch 221/500:  train Loss: 73.2210   val Loss: 71.5686   time: 0.20s   best: 71.5686
2023-10-09 17:15:53,389:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:53,410:INFO:  Epoch 222/500:  train Loss: 72.6322   val Loss: 71.4844   time: 0.19s   best: 71.4844
2023-10-09 17:15:53,605:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:53,979:INFO:  Epoch 223/500:  train Loss: 73.7911   val Loss: 71.4223   time: 0.19s   best: 71.4223
2023-10-09 17:15:54,170:INFO:  Epoch 224/500:  train Loss: 73.8114   val Loss: 71.9808   time: 0.19s   best: 71.4223
2023-10-09 17:15:54,365:INFO:  Epoch 225/500:  train Loss: 73.3452   val Loss: 72.9452   time: 0.19s   best: 71.4223
2023-10-09 17:15:54,558:INFO:  Epoch 226/500:  train Loss: 73.7191   val Loss: 71.6589   time: 0.19s   best: 71.4223
2023-10-09 17:15:54,753:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:54,768:INFO:  Epoch 227/500:  train Loss: 72.3557   val Loss: 71.3024   time: 0.19s   best: 71.3024
2023-10-09 17:15:54,964:INFO:  Epoch 228/500:  train Loss: 72.4519   val Loss: 71.6661   time: 0.19s   best: 71.3024
2023-10-09 17:15:55,154:INFO:  Epoch 229/500:  train Loss: 72.7698   val Loss: 71.4944   time: 0.19s   best: 71.3024
2023-10-09 17:15:55,355:INFO:  Epoch 230/500:  train Loss: 72.9174   val Loss: 72.5807   time: 0.20s   best: 71.3024
2023-10-09 17:15:55,549:INFO:  Epoch 231/500:  train Loss: 74.9387   val Loss: 72.5224   time: 0.19s   best: 71.3024
2023-10-09 17:15:55,742:INFO:  Epoch 232/500:  train Loss: 74.1202   val Loss: 72.8958   time: 0.19s   best: 71.3024
2023-10-09 17:15:55,937:INFO:  Epoch 233/500:  train Loss: 73.2847   val Loss: 71.7704   time: 0.20s   best: 71.3024
2023-10-09 17:15:56,130:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:56,145:INFO:  Epoch 234/500:  train Loss: 72.6066   val Loss: 70.6447   time: 0.19s   best: 70.6447
2023-10-09 17:15:56,341:INFO:  Epoch 235/500:  train Loss: 72.2142   val Loss: 72.4750   time: 0.20s   best: 70.6447
2023-10-09 17:15:56,532:INFO:  Epoch 236/500:  train Loss: 73.5795   val Loss: 72.0997   time: 0.19s   best: 70.6447
2023-10-09 17:15:56,723:INFO:  Epoch 237/500:  train Loss: 72.8283   val Loss: 71.0612   time: 0.19s   best: 70.6447
2023-10-09 17:15:56,919:INFO:  Epoch 238/500:  train Loss: 72.1806   val Loss: 71.4257   time: 0.20s   best: 70.6447
2023-10-09 17:15:57,111:INFO:  Epoch 239/500:  train Loss: 73.7069   val Loss: 71.8772   time: 0.19s   best: 70.6447
2023-10-09 17:15:57,300:INFO:  Epoch 240/500:  train Loss: 72.7302   val Loss: 71.4905   time: 0.19s   best: 70.6447
2023-10-09 17:15:57,497:INFO:  Epoch 241/500:  train Loss: 72.1283   val Loss: 70.6955   time: 0.20s   best: 70.6447
2023-10-09 17:15:57,689:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:57,985:INFO:  Epoch 242/500:  train Loss: 72.3511   val Loss: 70.3365   time: 0.19s   best: 70.3365
2023-10-09 17:15:58,179:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:15:58,454:INFO:  Epoch 243/500:  train Loss: 71.1057   val Loss: 69.7404   time: 0.19s   best: 69.7404
2023-10-09 17:15:58,643:INFO:  Epoch 244/500:  train Loss: 72.1069   val Loss: 71.0022   time: 0.19s   best: 69.7404
2023-10-09 17:15:58,839:INFO:  Epoch 245/500:  train Loss: 71.5884   val Loss: 70.1359   time: 0.19s   best: 69.7404
2023-10-09 17:15:59,038:INFO:  Epoch 246/500:  train Loss: 71.8744   val Loss: 70.4626   time: 0.20s   best: 69.7404
2023-10-09 17:15:59,227:INFO:  Epoch 247/500:  train Loss: 73.5306   val Loss: 72.2590   time: 0.19s   best: 69.7404
2023-10-09 17:15:59,422:INFO:  Epoch 248/500:  train Loss: 84.3986   val Loss: 90.6318   time: 0.19s   best: 69.7404
2023-10-09 17:15:59,615:INFO:  Epoch 249/500:  train Loss: 88.1563   val Loss: 82.2981   time: 0.19s   best: 69.7404
2023-10-09 17:15:59,807:INFO:  Epoch 250/500:  train Loss: 83.1999   val Loss: 81.9758   time: 0.19s   best: 69.7404
2023-10-09 17:16:00,003:INFO:  Epoch 251/500:  train Loss: 80.9772   val Loss: 79.6445   time: 0.20s   best: 69.7404
2023-10-09 17:16:00,192:INFO:  Epoch 252/500:  train Loss: 79.1421   val Loss: 76.5422   time: 0.19s   best: 69.7404
2023-10-09 17:16:00,387:INFO:  Epoch 253/500:  train Loss: 76.5639   val Loss: 74.1743   time: 0.19s   best: 69.7404
2023-10-09 17:16:00,592:INFO:  Epoch 254/500:  train Loss: 75.0446   val Loss: 73.3754   time: 0.20s   best: 69.7404
2023-10-09 17:16:00,782:INFO:  Epoch 255/500:  train Loss: 73.7023   val Loss: 71.5995   time: 0.19s   best: 69.7404
2023-10-09 17:16:00,973:INFO:  Epoch 256/500:  train Loss: 72.7318   val Loss: 70.5774   time: 0.19s   best: 69.7404
2023-10-09 17:16:01,162:INFO:  Epoch 257/500:  train Loss: 71.8666   val Loss: 70.6150   time: 0.19s   best: 69.7404
2023-10-09 17:16:01,351:INFO:  Epoch 258/500:  train Loss: 71.8089   val Loss: 70.0554   time: 0.19s   best: 69.7404
2023-10-09 17:16:01,543:INFO:  Epoch 259/500:  train Loss: 71.4159   val Loss: 70.1435   time: 0.19s   best: 69.7404
2023-10-09 17:16:01,732:INFO:  Epoch 260/500:  train Loss: 71.7162   val Loss: 69.7857   time: 0.19s   best: 69.7404
2023-10-09 17:16:01,928:INFO:  Epoch 261/500:  train Loss: 73.2543   val Loss: 70.0495   time: 0.20s   best: 69.7404
2023-10-09 17:16:02,133:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:02,149:INFO:  Epoch 262/500:  train Loss: 70.3891   val Loss: 69.6419   time: 0.20s   best: 69.6419
2023-10-09 17:16:02,343:INFO:  Epoch 263/500:  train Loss: 70.6694   val Loss: 70.4643   time: 0.19s   best: 69.6419
2023-10-09 17:16:02,556:INFO:  Epoch 264/500:  train Loss: 71.9866   val Loss: 72.3211   time: 0.21s   best: 69.6419
2023-10-09 17:16:02,750:INFO:  Epoch 265/500:  train Loss: 74.5219   val Loss: 70.7997   time: 0.19s   best: 69.6419
2023-10-09 17:16:02,952:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:03,258:INFO:  Epoch 266/500:  train Loss: 71.5411   val Loss: 69.1739   time: 0.20s   best: 69.1739
2023-10-09 17:16:03,450:INFO:  Epoch 267/500:  train Loss: 72.0239   val Loss: 69.5506   time: 0.19s   best: 69.1739
2023-10-09 17:16:03,647:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:03,767:INFO:  Epoch 268/500:  train Loss: 72.4119   val Loss: 68.7805   time: 0.19s   best: 68.7805
2023-10-09 17:16:03,969:INFO:  Epoch 269/500:  train Loss: 71.7985   val Loss: 69.4719   time: 0.20s   best: 68.7805
2023-10-09 17:16:04,167:INFO:  Epoch 270/500:  train Loss: 71.7188   val Loss: 70.2646   time: 0.19s   best: 68.7805
2023-10-09 17:16:04,370:INFO:  Epoch 271/500:  train Loss: 71.2903   val Loss: 69.7089   time: 0.20s   best: 68.7805
2023-10-09 17:16:04,590:INFO:  Epoch 272/500:  train Loss: 70.1105   val Loss: 70.2136   time: 0.21s   best: 68.7805
2023-10-09 17:16:04,789:INFO:  Epoch 273/500:  train Loss: 70.5358   val Loss: 69.0340   time: 0.19s   best: 68.7805
2023-10-09 17:16:04,995:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:06,417:INFO:  Epoch 274/500:  train Loss: 70.1809   val Loss: 68.7783   time: 0.20s   best: 68.7783
2023-10-09 17:16:06,623:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:06,637:INFO:  Epoch 275/500:  train Loss: 69.4487   val Loss: 68.5758   time: 0.20s   best: 68.5758
2023-10-09 17:16:06,830:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:06,844:INFO:  Epoch 276/500:  train Loss: 68.6347   val Loss: 67.3223   time: 0.19s   best: 67.3223
2023-10-09 17:16:07,040:INFO:  Epoch 277/500:  train Loss: 72.1127   val Loss: 71.0694   time: 0.19s   best: 67.3223
2023-10-09 17:16:07,248:INFO:  Epoch 278/500:  train Loss: 74.4434   val Loss: 73.3393   time: 0.20s   best: 67.3223
2023-10-09 17:16:07,445:INFO:  Epoch 279/500:  train Loss: 73.5322   val Loss: 70.8745   time: 0.20s   best: 67.3223
2023-10-09 17:16:07,635:INFO:  Epoch 280/500:  train Loss: 72.5936   val Loss: 71.4283   time: 0.19s   best: 67.3223
2023-10-09 17:16:07,825:INFO:  Epoch 281/500:  train Loss: 72.8252   val Loss: 70.4656   time: 0.19s   best: 67.3223
2023-10-09 17:16:08,022:INFO:  Epoch 282/500:  train Loss: 70.6504   val Loss: 68.6126   time: 0.20s   best: 67.3223
2023-10-09 17:16:08,211:INFO:  Epoch 283/500:  train Loss: 69.8477   val Loss: 69.3532   time: 0.19s   best: 67.3223
2023-10-09 17:16:08,401:INFO:  Epoch 284/500:  train Loss: 71.0748   val Loss: 69.2474   time: 0.19s   best: 67.3223
2023-10-09 17:16:08,598:INFO:  Epoch 285/500:  train Loss: 72.5426   val Loss: 67.8390   time: 0.20s   best: 67.3223
2023-10-09 17:16:08,802:INFO:  Epoch 286/500:  train Loss: 70.6742   val Loss: 70.1236   time: 0.20s   best: 67.3223
2023-10-09 17:16:08,999:INFO:  Epoch 287/500:  train Loss: 71.7905   val Loss: 69.8998   time: 0.20s   best: 67.3223
2023-10-09 17:16:09,189:INFO:  Epoch 288/500:  train Loss: 71.8141   val Loss: 68.6074   time: 0.19s   best: 67.3223
2023-10-09 17:16:09,382:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:10,406:INFO:  Epoch 289/500:  train Loss: 69.4876   val Loss: 67.0739   time: 0.19s   best: 67.0739
2023-10-09 17:16:10,599:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:10,613:INFO:  Epoch 290/500:  train Loss: 68.9755   val Loss: 66.6215   time: 0.19s   best: 66.6215
2023-10-09 17:16:10,805:INFO:  Epoch 291/500:  train Loss: 70.1273   val Loss: 68.5167   time: 0.19s   best: 66.6215
2023-10-09 17:16:11,005:INFO:  Epoch 292/500:  train Loss: 70.6218   val Loss: 67.3569   time: 0.20s   best: 66.6215
2023-10-09 17:16:11,200:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:11,220:INFO:  Epoch 293/500:  train Loss: 69.0156   val Loss: 66.4836   time: 0.19s   best: 66.4836
2023-10-09 17:16:11,413:INFO:  Epoch 294/500:  train Loss: 67.7824   val Loss: 67.2391   time: 0.19s   best: 66.4836
2023-10-09 17:16:11,610:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:11,746:INFO:  Epoch 295/500:  train Loss: 67.5293   val Loss: 66.1341   time: 0.19s   best: 66.1341
2023-10-09 17:16:11,937:INFO:  Epoch 296/500:  train Loss: 69.0166   val Loss: 68.8490   time: 0.19s   best: 66.1341
2023-10-09 17:16:12,137:INFO:  Epoch 297/500:  train Loss: 71.4621   val Loss: 70.5060   time: 0.20s   best: 66.1341
2023-10-09 17:16:12,329:INFO:  Epoch 298/500:  train Loss: 70.5314   val Loss: 68.4147   time: 0.19s   best: 66.1341
2023-10-09 17:16:12,528:INFO:  Epoch 299/500:  train Loss: 69.8210   val Loss: 67.4941   time: 0.20s   best: 66.1341
2023-10-09 17:16:12,761:INFO:  Epoch 300/500:  train Loss: 69.3124   val Loss: 67.9012   time: 0.23s   best: 66.1341
2023-10-09 17:16:12,957:INFO:  Epoch 301/500:  train Loss: 70.1304   val Loss: 67.3163   time: 0.20s   best: 66.1341
2023-10-09 17:16:13,157:INFO:  Epoch 302/500:  train Loss: 69.3663   val Loss: 68.2197   time: 0.20s   best: 66.1341
2023-10-09 17:16:13,347:INFO:  Epoch 303/500:  train Loss: 68.5297   val Loss: 66.1602   time: 0.19s   best: 66.1341
2023-10-09 17:16:13,546:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:13,562:INFO:  Epoch 304/500:  train Loss: 68.4841   val Loss: 65.8373   time: 0.20s   best: 65.8373
2023-10-09 17:16:13,758:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:13,772:INFO:  Epoch 305/500:  train Loss: 67.3221   val Loss: 65.2409   time: 0.19s   best: 65.2409
2023-10-09 17:16:13,962:INFO:  Epoch 306/500:  train Loss: 67.5422   val Loss: 65.3097   time: 0.19s   best: 65.2409
2023-10-09 17:16:14,158:INFO:  Epoch 307/500:  train Loss: 69.3980   val Loss: 67.3588   time: 0.20s   best: 65.2409
2023-10-09 17:16:14,351:INFO:  Epoch 308/500:  train Loss: 70.5184   val Loss: 68.1218   time: 0.19s   best: 65.2409
2023-10-09 17:16:14,548:INFO:  Epoch 309/500:  train Loss: 72.9694   val Loss: 72.2269   time: 0.20s   best: 65.2409
2023-10-09 17:16:14,754:INFO:  Epoch 310/500:  train Loss: 72.0740   val Loss: 70.8043   time: 0.21s   best: 65.2409
2023-10-09 17:16:14,943:INFO:  Epoch 311/500:  train Loss: 70.8007   val Loss: 70.6006   time: 0.19s   best: 65.2409
2023-10-09 17:16:15,137:INFO:  Epoch 312/500:  train Loss: 71.0888   val Loss: 70.4310   time: 0.19s   best: 65.2409
2023-10-09 17:16:15,327:INFO:  Epoch 313/500:  train Loss: 70.7182   val Loss: 69.0553   time: 0.19s   best: 65.2409
2023-10-09 17:16:15,517:INFO:  Epoch 314/500:  train Loss: 69.1925   val Loss: 68.2396   time: 0.19s   best: 65.2409
2023-10-09 17:16:15,713:INFO:  Epoch 315/500:  train Loss: 69.0934   val Loss: 67.6101   time: 0.20s   best: 65.2409
2023-10-09 17:16:15,903:INFO:  Epoch 316/500:  train Loss: 68.8575   val Loss: 66.0842   time: 0.19s   best: 65.2409
2023-10-09 17:16:16,099:INFO:  Epoch 317/500:  train Loss: 67.3499   val Loss: 66.6417   time: 0.20s   best: 65.2409
2023-10-09 17:16:16,289:INFO:  Epoch 318/500:  train Loss: 68.1447   val Loss: 67.3164   time: 0.19s   best: 65.2409
2023-10-09 17:16:16,479:INFO:  Epoch 319/500:  train Loss: 68.2517   val Loss: 67.0647   time: 0.19s   best: 65.2409
2023-10-09 17:16:16,674:INFO:  Epoch 320/500:  train Loss: 68.7621   val Loss: 67.9131   time: 0.20s   best: 65.2409
2023-10-09 17:16:16,865:INFO:  Epoch 321/500:  train Loss: 68.7096   val Loss: 66.4302   time: 0.19s   best: 65.2409
2023-10-09 17:16:17,062:INFO:  Epoch 322/500:  train Loss: 69.7481   val Loss: 66.0440   time: 0.20s   best: 65.2409
2023-10-09 17:16:17,253:INFO:  Epoch 323/500:  train Loss: 69.3785   val Loss: 66.1849   time: 0.19s   best: 65.2409
2023-10-09 17:16:17,443:INFO:  Epoch 324/500:  train Loss: 69.6584   val Loss: 66.3400   time: 0.19s   best: 65.2409
2023-10-09 17:16:17,639:INFO:  Epoch 325/500:  train Loss: 72.8279   val Loss: 68.9856   time: 0.20s   best: 65.2409
2023-10-09 17:16:17,829:INFO:  Epoch 326/500:  train Loss: 69.2120   val Loss: 66.9921   time: 0.19s   best: 65.2409
2023-10-09 17:16:18,018:INFO:  Epoch 327/500:  train Loss: 68.5391   val Loss: 67.9219   time: 0.19s   best: 65.2409
2023-10-09 17:16:18,216:INFO:  Epoch 328/500:  train Loss: 68.9166   val Loss: 65.8354   time: 0.20s   best: 65.2409
2023-10-09 17:16:18,405:INFO:  Epoch 329/500:  train Loss: 67.9638   val Loss: 66.9323   time: 0.19s   best: 65.2409
2023-10-09 17:16:18,601:INFO:  Epoch 330/500:  train Loss: 68.7825   val Loss: 65.7745   time: 0.20s   best: 65.2409
2023-10-09 17:16:18,806:INFO:  Epoch 331/500:  train Loss: 70.2335   val Loss: 67.5596   time: 0.20s   best: 65.2409
2023-10-09 17:16:18,996:INFO:  Epoch 332/500:  train Loss: 68.4845   val Loss: 67.9407   time: 0.19s   best: 65.2409
2023-10-09 17:16:19,190:INFO:  Epoch 333/500:  train Loss: 68.1876   val Loss: 65.5014   time: 0.19s   best: 65.2409
2023-10-09 17:16:19,389:INFO:  Epoch 334/500:  train Loss: 66.1134   val Loss: 66.0405   time: 0.20s   best: 65.2409
2023-10-09 17:16:19,586:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:19,608:INFO:  Epoch 335/500:  train Loss: 66.9415   val Loss: 64.7550   time: 0.19s   best: 64.7550
2023-10-09 17:16:19,803:INFO:  Epoch 336/500:  train Loss: 66.0822   val Loss: 65.0177   time: 0.19s   best: 64.7550
2023-10-09 17:16:20,000:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:20,596:INFO:  Epoch 337/500:  train Loss: 65.8025   val Loss: 64.4750   time: 0.19s   best: 64.4750
2023-10-09 17:16:20,800:INFO:  Epoch 338/500:  train Loss: 66.2336   val Loss: 65.8000   time: 0.20s   best: 64.4750
2023-10-09 17:16:20,991:INFO:  Epoch 339/500:  train Loss: 66.8229   val Loss: 64.5076   time: 0.19s   best: 64.4750
2023-10-09 17:16:21,189:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:21,300:INFO:  Epoch 340/500:  train Loss: 66.3361   val Loss: 64.0736   time: 0.19s   best: 64.0736
2023-10-09 17:16:21,496:INFO:  Epoch 341/500:  train Loss: 65.5144   val Loss: 64.4612   time: 0.19s   best: 64.0736
2023-10-09 17:16:21,701:INFO:  Epoch 342/500:  train Loss: 66.5576   val Loss: 66.1939   time: 0.20s   best: 64.0736
2023-10-09 17:16:21,893:INFO:  Epoch 343/500:  train Loss: 67.3652   val Loss: 66.4601   time: 0.19s   best: 64.0736
2023-10-09 17:16:22,084:INFO:  Epoch 344/500:  train Loss: 68.2571   val Loss: 67.0709   time: 0.19s   best: 64.0736
2023-10-09 17:16:22,288:INFO:  Epoch 345/500:  train Loss: 70.0056   val Loss: 69.6400   time: 0.20s   best: 64.0736
2023-10-09 17:16:22,478:INFO:  Epoch 346/500:  train Loss: 73.1381   val Loss: 66.1251   time: 0.19s   best: 64.0736
2023-10-09 17:16:22,674:INFO:  Epoch 347/500:  train Loss: 70.4543   val Loss: 65.3427   time: 0.20s   best: 64.0736
2023-10-09 17:16:22,864:INFO:  Epoch 348/500:  train Loss: 70.0174   val Loss: 66.3535   time: 0.19s   best: 64.0736
2023-10-09 17:16:23,054:INFO:  Epoch 349/500:  train Loss: 67.8891   val Loss: 67.1377   time: 0.19s   best: 64.0736
2023-10-09 17:16:23,250:INFO:  Epoch 350/500:  train Loss: 67.8753   val Loss: 66.1938   time: 0.20s   best: 64.0736
2023-10-09 17:16:23,439:INFO:  Epoch 351/500:  train Loss: 66.6932   val Loss: 65.5813   time: 0.19s   best: 64.0736
2023-10-09 17:16:23,633:INFO:  Epoch 352/500:  train Loss: 67.1097   val Loss: 65.7133   time: 0.19s   best: 64.0736
2023-10-09 17:16:23,829:INFO:  Epoch 353/500:  train Loss: 67.3083   val Loss: 65.7904   time: 0.20s   best: 64.0736
2023-10-09 17:16:24,018:INFO:  Epoch 354/500:  train Loss: 68.5284   val Loss: 65.9014   time: 0.19s   best: 64.0736
2023-10-09 17:16:24,214:INFO:  Epoch 355/500:  train Loss: 68.7569   val Loss: 65.1962   time: 0.19s   best: 64.0736
2023-10-09 17:16:24,404:INFO:  Epoch 356/500:  train Loss: 68.2547   val Loss: 67.7870   time: 0.19s   best: 64.0736
2023-10-09 17:16:24,594:INFO:  Epoch 357/500:  train Loss: 70.2832   val Loss: 67.6342   time: 0.19s   best: 64.0736
2023-10-09 17:16:24,786:INFO:  Epoch 358/500:  train Loss: 71.5460   val Loss: 67.3698   time: 0.19s   best: 64.0736
2023-10-09 17:16:24,976:INFO:  Epoch 359/500:  train Loss: 72.1688   val Loss: 65.1939   time: 0.19s   best: 64.0736
2023-10-09 17:16:25,173:INFO:  Epoch 360/500:  train Loss: 69.4557   val Loss: 68.2178   time: 0.20s   best: 64.0736
2023-10-09 17:16:25,374:INFO:  Epoch 361/500:  train Loss: 67.7276   val Loss: 66.7133   time: 0.20s   best: 64.0736
2023-10-09 17:16:25,564:INFO:  Epoch 362/500:  train Loss: 67.3002   val Loss: 64.7256   time: 0.19s   best: 64.0736
2023-10-09 17:16:25,761:INFO:  Epoch 363/500:  train Loss: 66.3258   val Loss: 64.2573   time: 0.20s   best: 64.0736
2023-10-09 17:16:25,952:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:26,317:INFO:  Epoch 364/500:  train Loss: 65.4139   val Loss: 63.8927   time: 0.19s   best: 63.8927
2023-10-09 17:16:26,507:INFO:  Epoch 365/500:  train Loss: 64.7263   val Loss: 64.5810   time: 0.19s   best: 63.8927
2023-10-09 17:16:26,705:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:26,719:INFO:  Epoch 366/500:  train Loss: 65.9442   val Loss: 62.5536   time: 0.19s   best: 62.5536
2023-10-09 17:16:26,912:INFO:  Epoch 367/500:  train Loss: 65.7696   val Loss: 67.4298   time: 0.19s   best: 62.5536
2023-10-09 17:16:27,104:INFO:  Epoch 368/500:  train Loss: 68.4814   val Loss: 64.6799   time: 0.19s   best: 62.5536
2023-10-09 17:16:27,302:INFO:  Epoch 369/500:  train Loss: 66.6379   val Loss: 66.1792   time: 0.19s   best: 62.5536
2023-10-09 17:16:27,495:INFO:  Epoch 370/500:  train Loss: 67.8946   val Loss: 65.0146   time: 0.19s   best: 62.5536
2023-10-09 17:16:27,701:INFO:  Epoch 371/500:  train Loss: 66.1407   val Loss: 64.9710   time: 0.20s   best: 62.5536
2023-10-09 17:16:27,893:INFO:  Epoch 372/500:  train Loss: 66.1255   val Loss: 64.0837   time: 0.19s   best: 62.5536
2023-10-09 17:16:28,086:INFO:  Epoch 373/500:  train Loss: 66.8991   val Loss: 64.5181   time: 0.19s   best: 62.5536
2023-10-09 17:16:28,287:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_feca.pt
2023-10-09 17:16:28,368:INFO:  Epoch 374/500:  train Loss: 66.3138   val Loss: 62.3628   time: 0.20s   best: 62.3628
2023-10-09 17:16:28,565:INFO:  Epoch 375/500:  train Loss: 66.6098   val Loss: 62.4723   time: 0.19s   best: 62.3628
2023-10-09 17:16:28,771:INFO:  Epoch 376/500:  train Loss: 75.6052   val Loss: 69.5277   time: 0.20s   best: 62.3628
2023-10-09 17:16:28,978:INFO:  Epoch 377/500:  train Loss: 83.5645   val Loss: 94.6003   time: 0.20s   best: 62.3628
2023-10-09 17:16:29,174:INFO:  Epoch 378/500:  train Loss: 96.4425   val Loss: 97.6878   time: 0.19s   best: 62.3628
2023-10-09 17:16:29,369:INFO:  Epoch 379/500:  train Loss: 97.4080   val Loss: 96.2362   time: 0.19s   best: 62.3628
2023-10-09 17:16:29,561:INFO:  Epoch 380/500:  train Loss: 95.9569   val Loss: 94.5092   time: 0.19s   best: 62.3628
2023-10-09 17:16:29,755:INFO:  Epoch 381/500:  train Loss: 94.5997   val Loss: 93.7147   time: 0.19s   best: 62.3628
2023-10-09 17:16:29,944:INFO:  Epoch 382/500:  train Loss: 93.8916   val Loss: 93.4787   time: 0.19s   best: 62.3628
2023-10-09 17:16:30,135:INFO:  Epoch 383/500:  train Loss: 93.5194   val Loss: 93.5222   time: 0.19s   best: 62.3628
2023-10-09 17:16:30,333:INFO:  Epoch 384/500:  train Loss: 93.8125   val Loss: 93.8325   time: 0.20s   best: 62.3628
2023-10-09 17:16:30,522:INFO:  Epoch 385/500:  train Loss: 93.9807   val Loss: 93.7536   time: 0.19s   best: 62.3628
2023-10-09 17:16:30,711:INFO:  Epoch 386/500:  train Loss: 93.7088   val Loss: 92.5091   time: 0.19s   best: 62.3628
2023-10-09 17:16:30,919:INFO:  Epoch 387/500:  train Loss: 92.6691   val Loss: 91.9448   time: 0.21s   best: 62.3628
2023-10-09 17:16:31,108:INFO:  Epoch 388/500:  train Loss: 92.0644   val Loss: 91.8160   time: 0.19s   best: 62.3628
2023-10-09 17:16:31,301:INFO:  Epoch 389/500:  train Loss: 91.8654   val Loss: 91.7584   time: 0.19s   best: 62.3628
2023-10-09 17:16:31,490:INFO:  Epoch 390/500:  train Loss: 91.8698   val Loss: 91.5059   time: 0.19s   best: 62.3628
2023-10-09 17:16:31,679:INFO:  Epoch 391/500:  train Loss: 91.1477   val Loss: 90.3206   time: 0.19s   best: 62.3628
2023-10-09 17:16:31,874:INFO:  Epoch 392/500:  train Loss: 90.1364   val Loss: 89.0757   time: 0.19s   best: 62.3628
2023-10-09 17:16:32,062:INFO:  Epoch 393/500:  train Loss: 88.7173   val Loss: 87.4093   time: 0.19s   best: 62.3628
2023-10-09 17:16:32,251:INFO:  Epoch 394/500:  train Loss: 86.9397   val Loss: 85.2819   time: 0.19s   best: 62.3628
2023-10-09 17:16:32,446:INFO:  Epoch 395/500:  train Loss: 84.5665   val Loss: 81.9588   time: 0.19s   best: 62.3628
2023-10-09 17:16:32,635:INFO:  Epoch 396/500:  train Loss: 81.3427   val Loss: 78.9584   time: 0.19s   best: 62.3628
2023-10-09 17:16:32,829:INFO:  Epoch 397/500:  train Loss: 78.4557   val Loss: 76.0862   time: 0.19s   best: 62.3628
2023-10-09 17:16:33,018:INFO:  Epoch 398/500:  train Loss: 76.0428   val Loss: 73.7785   time: 0.19s   best: 62.3628
2023-10-09 17:16:33,206:INFO:  Epoch 399/500:  train Loss: 74.1258   val Loss: 72.5063   time: 0.19s   best: 62.3628
2023-10-09 17:16:33,423:INFO:  Epoch 400/500:  train Loss: 75.7773   val Loss: 71.7783   time: 0.22s   best: 62.3628
2023-10-09 17:16:33,611:INFO:  Epoch 401/500:  train Loss: 74.7876   val Loss: 70.7890   time: 0.19s   best: 62.3628
2023-10-09 17:16:33,806:INFO:  Epoch 402/500:  train Loss: 72.5679   val Loss: 71.0044   time: 0.19s   best: 62.3628
2023-10-09 17:16:33,995:INFO:  Epoch 403/500:  train Loss: 71.3182   val Loss: 70.7342   time: 0.19s   best: 62.3628
2023-10-09 17:16:34,184:INFO:  Epoch 404/500:  train Loss: 71.0184   val Loss: 69.3931   time: 0.19s   best: 62.3628
2023-10-09 17:16:34,379:INFO:  Epoch 405/500:  train Loss: 70.2934   val Loss: 68.7192   time: 0.19s   best: 62.3628
2023-10-09 17:16:34,568:INFO:  Epoch 406/500:  train Loss: 69.5311   val Loss: 68.1835   time: 0.19s   best: 62.3628
2023-10-09 17:16:34,756:INFO:  Epoch 407/500:  train Loss: 68.9701   val Loss: 67.3385   time: 0.19s   best: 62.3628
2023-10-09 17:16:34,968:INFO:  Epoch 408/500:  train Loss: 68.3883   val Loss: 69.3406   time: 0.21s   best: 62.3628
2023-10-09 17:16:35,156:INFO:  Epoch 409/500:  train Loss: 70.8552   val Loss: 70.7758   time: 0.19s   best: 62.3628
2023-10-09 17:16:35,351:INFO:  Epoch 410/500:  train Loss: 72.1164   val Loss: 69.2720   time: 0.19s   best: 62.3628
2023-10-09 17:16:35,539:INFO:  Epoch 411/500:  train Loss: 70.6277   val Loss: 69.4973   time: 0.19s   best: 62.3628
2023-10-09 17:16:35,728:INFO:  Epoch 412/500:  train Loss: 70.2130   val Loss: 67.9449   time: 0.19s   best: 62.3628
2023-10-09 17:16:35,923:INFO:  Epoch 413/500:  train Loss: 69.4658   val Loss: 69.1118   time: 0.19s   best: 62.3628
2023-10-09 17:16:36,112:INFO:  Epoch 414/500:  train Loss: 70.2298   val Loss: 67.4156   time: 0.19s   best: 62.3628
2023-10-09 17:16:36,300:INFO:  Epoch 415/500:  train Loss: 69.1043   val Loss: 67.6742   time: 0.19s   best: 62.3628
2023-10-09 17:16:36,493:INFO:  Epoch 416/500:  train Loss: 70.1398   val Loss: 72.5277   time: 0.19s   best: 62.3628
2023-10-09 17:16:36,682:INFO:  Epoch 417/500:  train Loss: 71.5797   val Loss: 69.3411   time: 0.19s   best: 62.3628
2023-10-09 17:16:36,877:INFO:  Epoch 418/500:  train Loss: 69.7031   val Loss: 68.4610   time: 0.19s   best: 62.3628
2023-10-09 17:16:37,066:INFO:  Epoch 419/500:  train Loss: 69.5882   val Loss: 67.2087   time: 0.19s   best: 62.3628
2023-10-09 17:16:37,254:INFO:  Epoch 420/500:  train Loss: 68.3331   val Loss: 67.7902   time: 0.19s   best: 62.3628
2023-10-09 17:16:37,449:INFO:  Epoch 421/500:  train Loss: 69.5482   val Loss: 64.9306   time: 0.19s   best: 62.3628
2023-10-09 17:16:37,638:INFO:  Epoch 422/500:  train Loss: 67.2816   val Loss: 64.6011   time: 0.19s   best: 62.3628
2023-10-09 17:16:37,827:INFO:  Epoch 423/500:  train Loss: 67.9340   val Loss: 64.4258   time: 0.19s   best: 62.3628
2023-10-09 17:16:38,028:INFO:  Epoch 424/500:  train Loss: 67.3564   val Loss: 65.9862   time: 0.20s   best: 62.3628
2023-10-09 17:16:38,217:INFO:  Epoch 425/500:  train Loss: 68.9689   val Loss: 66.1226   time: 0.19s   best: 62.3628
2023-10-09 17:16:38,411:INFO:  Epoch 426/500:  train Loss: 69.3970   val Loss: 64.4515   time: 0.19s   best: 62.3628
2023-10-09 17:16:38,600:INFO:  Epoch 427/500:  train Loss: 69.5264   val Loss: 64.0993   time: 0.19s   best: 62.3628
2023-10-09 17:16:38,789:INFO:  Epoch 428/500:  train Loss: 71.4734   val Loss: 67.9699   time: 0.19s   best: 62.3628
2023-10-09 17:16:38,994:INFO:  Epoch 429/500:  train Loss: 69.1636   val Loss: 63.6061   time: 0.20s   best: 62.3628
2023-10-09 17:16:39,182:INFO:  Epoch 430/500:  train Loss: 73.1464   val Loss: 74.4327   time: 0.19s   best: 62.3628
2023-10-09 17:16:39,380:INFO:  Epoch 431/500:  train Loss: 75.4650   val Loss: 74.9930   time: 0.20s   best: 62.3628
2023-10-09 17:16:39,569:INFO:  Epoch 432/500:  train Loss: 74.4769   val Loss: 71.5947   time: 0.19s   best: 62.3628
2023-10-09 17:16:39,758:INFO:  Epoch 433/500:  train Loss: 71.7868   val Loss: 69.5826   time: 0.19s   best: 62.3628
2023-10-09 17:16:39,953:INFO:  Epoch 434/500:  train Loss: 70.3544   val Loss: 68.5531   time: 0.19s   best: 62.3628
2023-10-09 17:16:40,142:INFO:  Epoch 435/500:  train Loss: 68.9116   val Loss: 67.9097   time: 0.19s   best: 62.3628
2023-10-09 17:16:40,330:INFO:  Epoch 436/500:  train Loss: 70.3416   val Loss: 67.9888   time: 0.19s   best: 62.3628
2023-10-09 17:16:40,526:INFO:  Epoch 437/500:  train Loss: 70.2040   val Loss: 66.7754   time: 0.19s   best: 62.3628
2023-10-09 17:16:40,714:INFO:  Epoch 438/500:  train Loss: 68.6356   val Loss: 67.0757   time: 0.19s   best: 62.3628
2023-10-09 17:16:41,355:INFO:  Epoch 439/500:  train Loss: 68.2559   val Loss: 66.3672   time: 0.19s   best: 62.3628
2023-10-09 17:16:41,548:INFO:  Epoch 440/500:  train Loss: 69.6698   val Loss: 66.9690   time: 0.19s   best: 62.3628
2023-10-09 17:16:41,736:INFO:  Epoch 441/500:  train Loss: 68.5430   val Loss: 65.9973   time: 0.19s   best: 62.3628
2023-10-09 17:16:41,941:INFO:  Epoch 442/500:  train Loss: 69.0733   val Loss: 66.3297   time: 0.20s   best: 62.3628
2023-10-09 17:16:42,134:INFO:  Epoch 443/500:  train Loss: 69.8093   val Loss: 66.0079   time: 0.19s   best: 62.3628
2023-10-09 17:16:42,323:INFO:  Epoch 444/500:  train Loss: 68.6091   val Loss: 66.0012   time: 0.19s   best: 62.3628
2023-10-09 17:16:42,518:INFO:  Epoch 445/500:  train Loss: 70.0485   val Loss: 65.7114   time: 0.19s   best: 62.3628
2023-10-09 17:16:42,707:INFO:  Epoch 446/500:  train Loss: 69.3136   val Loss: 68.0296   time: 0.19s   best: 62.3628
2023-10-09 17:16:42,897:INFO:  Epoch 447/500:  train Loss: 69.5570   val Loss: 67.0659   time: 0.19s   best: 62.3628
2023-10-09 17:16:43,113:INFO:  Epoch 448/500:  train Loss: 67.7464   val Loss: 65.2548   time: 0.22s   best: 62.3628
2023-10-09 17:16:43,301:INFO:  Epoch 449/500:  train Loss: 68.3283   val Loss: 66.4008   time: 0.19s   best: 62.3628
2023-10-09 17:16:43,496:INFO:  Epoch 450/500:  train Loss: 67.9462   val Loss: 65.6725   time: 0.19s   best: 62.3628
2023-10-09 17:16:43,685:INFO:  Epoch 451/500:  train Loss: 67.6070   val Loss: 66.4212   time: 0.19s   best: 62.3628
2023-10-09 17:16:43,874:INFO:  Epoch 452/500:  train Loss: 70.7595   val Loss: 70.8342   time: 0.19s   best: 62.3628
2023-10-09 17:16:44,069:INFO:  Epoch 453/500:  train Loss: 70.0968   val Loss: 67.7142   time: 0.19s   best: 62.3628
2023-10-09 17:16:44,258:INFO:  Epoch 454/500:  train Loss: 68.3427   val Loss: 66.1859   time: 0.19s   best: 62.3628
2023-10-09 17:16:44,454:INFO:  Epoch 455/500:  train Loss: 67.3605   val Loss: 63.8888   time: 0.20s   best: 62.3628
2023-10-09 17:16:44,642:INFO:  Epoch 456/500:  train Loss: 65.6602   val Loss: 64.6441   time: 0.19s   best: 62.3628
2023-10-09 17:16:44,832:INFO:  Epoch 457/500:  train Loss: 67.5341   val Loss: 64.4374   time: 0.19s   best: 62.3628
2023-10-09 17:16:45,027:INFO:  Epoch 458/500:  train Loss: 67.2309   val Loss: 63.5253   time: 0.19s   best: 62.3628
2023-10-09 17:16:45,231:INFO:  Epoch 459/500:  train Loss: 66.2997   val Loss: 67.0969   time: 0.20s   best: 62.3628
2023-10-09 17:16:45,420:INFO:  Epoch 460/500:  train Loss: 67.9912   val Loss: 66.3566   time: 0.19s   best: 62.3628
2023-10-09 17:16:45,613:INFO:  Epoch 461/500:  train Loss: 68.1532   val Loss: 67.0001   time: 0.19s   best: 62.3628
2023-10-09 17:16:45,801:INFO:  Epoch 462/500:  train Loss: 68.8819   val Loss: 64.7362   time: 0.19s   best: 62.3628
2023-10-09 17:16:45,996:INFO:  Epoch 463/500:  train Loss: 67.3245   val Loss: 65.0807   time: 0.19s   best: 62.3628
2023-10-09 17:16:46,185:INFO:  Epoch 464/500:  train Loss: 67.6420   val Loss: 64.0977   time: 0.19s   best: 62.3628
2023-10-09 17:16:46,374:INFO:  Epoch 465/500:  train Loss: 68.2531   val Loss: 64.6792   time: 0.19s   best: 62.3628
2023-10-09 17:16:46,566:INFO:  Epoch 466/500:  train Loss: 69.2739   val Loss: 67.8187   time: 0.19s   best: 62.3628
2023-10-09 17:16:46,755:INFO:  Epoch 467/500:  train Loss: 69.9167   val Loss: 65.9250   time: 0.19s   best: 62.3628
2023-10-09 17:16:46,944:INFO:  Epoch 468/500:  train Loss: 67.1341   val Loss: 65.7224   time: 0.19s   best: 62.3628
2023-10-09 17:16:47,153:INFO:  Epoch 469/500:  train Loss: 72.2805   val Loss: 73.3337   time: 0.21s   best: 62.3628
2023-10-09 17:16:47,341:INFO:  Epoch 470/500:  train Loss: 75.7978   val Loss: 74.4031   time: 0.19s   best: 62.3628
2023-10-09 17:16:47,535:INFO:  Epoch 471/500:  train Loss: 74.6630   val Loss: 72.3789   time: 0.19s   best: 62.3628
2023-10-09 17:16:47,723:INFO:  Epoch 472/500:  train Loss: 72.2058   val Loss: 72.6062   time: 0.19s   best: 62.3628
2023-10-09 17:16:47,912:INFO:  Epoch 473/500:  train Loss: 73.3179   val Loss: 71.7946   time: 0.19s   best: 62.3628
2023-10-09 17:16:48,107:INFO:  Epoch 474/500:  train Loss: 72.0000   val Loss: 69.3079   time: 0.19s   best: 62.3628
2023-10-09 17:16:48,296:INFO:  Epoch 475/500:  train Loss: 71.2435   val Loss: 69.6442   time: 0.19s   best: 62.3628
2023-10-09 17:16:48,487:INFO:  Epoch 476/500:  train Loss: 69.7565   val Loss: 67.5459   time: 0.19s   best: 62.3628
2023-10-09 17:16:48,683:INFO:  Epoch 477/500:  train Loss: 70.7583   val Loss: 68.5295   time: 0.19s   best: 62.3628
2023-10-09 17:16:48,871:INFO:  Epoch 478/500:  train Loss: 70.6082   val Loss: 70.3588   time: 0.19s   best: 62.3628
2023-10-09 17:16:49,066:INFO:  Epoch 479/500:  train Loss: 72.6246   val Loss: 69.9839   time: 0.19s   best: 62.3628
2023-10-09 17:16:49,271:INFO:  Epoch 480/500:  train Loss: 73.7162   val Loss: 69.4389   time: 0.20s   best: 62.3628
2023-10-09 17:16:49,459:INFO:  Epoch 481/500:  train Loss: 71.5773   val Loss: 69.3619   time: 0.19s   best: 62.3628
2023-10-09 17:16:49,655:INFO:  Epoch 482/500:  train Loss: 71.3618   val Loss: 68.5848   time: 0.20s   best: 62.3628
2023-10-09 17:16:49,844:INFO:  Epoch 483/500:  train Loss: 70.4064   val Loss: 67.4776   time: 0.19s   best: 62.3628
2023-10-09 17:16:50,039:INFO:  Epoch 484/500:  train Loss: 68.5888   val Loss: 67.0046   time: 0.19s   best: 62.3628
2023-10-09 17:16:50,228:INFO:  Epoch 485/500:  train Loss: 70.4160   val Loss: 69.3866   time: 0.19s   best: 62.3628
2023-10-09 17:16:50,417:INFO:  Epoch 486/500:  train Loss: 70.9080   val Loss: 69.0909   time: 0.19s   best: 62.3628
2023-10-09 17:16:50,612:INFO:  Epoch 487/500:  train Loss: 73.4306   val Loss: 71.8719   time: 0.19s   best: 62.3628
2023-10-09 17:16:50,801:INFO:  Epoch 488/500:  train Loss: 72.3402   val Loss: 70.6974   time: 0.19s   best: 62.3628
2023-10-09 17:16:50,990:INFO:  Epoch 489/500:  train Loss: 70.6206   val Loss: 65.9083   time: 0.19s   best: 62.3628
2023-10-09 17:16:51,200:INFO:  Epoch 490/500:  train Loss: 67.1286   val Loss: 67.4912   time: 0.21s   best: 62.3628
2023-10-09 17:16:51,389:INFO:  Epoch 491/500:  train Loss: 68.6538   val Loss: 70.9447   time: 0.19s   best: 62.3628
2023-10-09 17:16:51,584:INFO:  Epoch 492/500:  train Loss: 77.8899   val Loss: 77.9388   time: 0.19s   best: 62.3628
2023-10-09 17:16:51,773:INFO:  Epoch 493/500:  train Loss: 78.7187   val Loss: 78.1687   time: 0.19s   best: 62.3628
2023-10-09 17:16:51,962:INFO:  Epoch 494/500:  train Loss: 76.9623   val Loss: 73.9952   time: 0.19s   best: 62.3628
2023-10-09 17:16:52,157:INFO:  Epoch 495/500:  train Loss: 72.8003   val Loss: 69.5127   time: 0.19s   best: 62.3628
2023-10-09 17:16:52,346:INFO:  Epoch 496/500:  train Loss: 69.6634   val Loss: 71.2823   time: 0.19s   best: 62.3628
2023-10-09 17:16:52,535:INFO:  Epoch 497/500:  train Loss: 71.5770   val Loss: 70.5603   time: 0.19s   best: 62.3628
2023-10-09 17:16:52,728:INFO:  Epoch 498/500:  train Loss: 72.1957   val Loss: 70.4004   time: 0.19s   best: 62.3628
2023-10-09 17:16:52,917:INFO:  Epoch 499/500:  train Loss: 71.2069   val Loss: 68.3416   time: 0.19s   best: 62.3628
2023-10-09 17:16:53,139:INFO:  Epoch 500/500:  train Loss: 69.7471   val Loss: 67.7351   time: 0.22s   best: 62.3628
2023-10-09 17:16:53,139:INFO:  -----> Training complete in 2m 7s   best validation loss: 62.3628
 
2023-10-09 17:21:35,960:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-09 17:21:35,982:INFO:  Defining the model
2023-10-09 17:21:36,052:INFO:  Reading the dataset
2023-10-09 19:46:10,212:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-09 19:46:10,224:INFO:  Defining the model
2023-10-09 19:46:11,070:INFO:  Reading the dataset
2023-10-10 09:39:50,803:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-10 09:39:50,825:INFO:  Defining the model
2023-10-10 09:39:51,493:INFO:  Reading the dataset
2023-10-10 10:45:12,821:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-10 10:45:12,834:INFO:  Defining the model
2023-10-10 10:45:13,582:INFO:  Reading the dataset
2023-10-11 16:14:13,122:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-11 16:14:13,294:INFO:  Defining the model
2023-10-11 16:14:14,405:INFO:  Reading the dataset
2023-10-11 18:16:15,114:INFO:  Starting experiment lstm autoencoder (perm dataset + 0.3 dropout)
2023-10-11 18:16:15,127:INFO:  Defining the model
2023-10-11 18:16:15,184:INFO:  Reading the dataset
2023-10-11 18:58:33,130:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 18:58:33,304:INFO:  Epoch 1/500:  train Loss: 74.4150   val Loss: 67.7800   time: 433.36s   best: 67.7800
2023-10-11 19:05:46,236:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 19:05:46,376:INFO:  Epoch 2/500:  train Loss: 62.4082   val Loss: 59.0774   time: 432.91s   best: 59.0774
2023-10-11 19:12:59,491:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 19:12:59,683:INFO:  Epoch 3/500:  train Loss: 55.7203   val Loss: 53.1439   time: 433.09s   best: 53.1439
2023-10-11 19:20:11,721:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 19:20:12,009:INFO:  Epoch 4/500:  train Loss: 50.1330   val Loss: 48.4398   time: 432.03s   best: 48.4398
2023-10-11 19:27:26,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 19:27:26,476:INFO:  Epoch 5/500:  train Loss: 45.9380   val Loss: 45.5906   time: 434.43s   best: 45.5906
2023-10-11 19:34:41,138:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 19:34:41,480:INFO:  Epoch 6/500:  train Loss: 43.0557   val Loss: 43.0461   time: 434.63s   best: 43.0461
2023-10-11 19:41:55,431:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 19:41:55,713:INFO:  Epoch 7/500:  train Loss: 40.8181   val Loss: 41.2174   time: 433.95s   best: 41.2174
2023-10-11 19:49:06,039:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 19:49:06,071:INFO:  Epoch 8/500:  train Loss: 38.8348   val Loss: 40.1054   time: 430.32s   best: 40.1054
2023-10-11 19:56:19,933:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 19:56:20,109:INFO:  Epoch 9/500:  train Loss: 37.5231   val Loss: 38.1320   time: 433.86s   best: 38.1320
2023-10-11 20:03:33,515:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 20:03:33,552:INFO:  Epoch 10/500:  train Loss: 36.2052   val Loss: 36.9667   time: 433.39s   best: 36.9667
2023-10-11 20:10:47,766:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 20:10:48,476:INFO:  Epoch 11/500:  train Loss: 35.3538   val Loss: 36.1368   time: 434.21s   best: 36.1368
2023-10-11 20:18:02,704:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 20:18:04,059:INFO:  Epoch 12/500:  train Loss: 34.4470   val Loss: 34.9464   time: 434.22s   best: 34.9464
2023-10-11 20:25:17,339:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 20:25:17,500:INFO:  Epoch 13/500:  train Loss: 33.7284   val Loss: 34.3043   time: 433.28s   best: 34.3043
2023-10-11 20:32:27,840:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 20:32:27,882:INFO:  Epoch 14/500:  train Loss: 33.1875   val Loss: 33.8077   time: 430.31s   best: 33.8077
2023-10-11 20:39:42,828:INFO:  Epoch 15/500:  train Loss: 32.5421   val Loss: 34.8855   time: 434.95s   best: 33.8077
2023-10-11 20:46:56,413:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 20:46:57,538:INFO:  Epoch 16/500:  train Loss: 32.1978   val Loss: 33.2095   time: 433.56s   best: 33.2095
2023-10-11 20:54:07,412:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 20:54:07,479:INFO:  Epoch 17/500:  train Loss: 31.9082   val Loss: 32.8330   time: 429.87s   best: 32.8330
2023-10-11 21:01:19,994:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 21:01:20,012:INFO:  Epoch 18/500:  train Loss: 31.4658   val Loss: 32.3583   time: 432.49s   best: 32.3583
2023-10-11 21:08:34,112:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 21:08:34,188:INFO:  Epoch 19/500:  train Loss: 30.8703   val Loss: 31.7013   time: 434.10s   best: 31.7013
2023-10-11 21:15:44,569:INFO:  Epoch 20/500:  train Loss: 30.7386   val Loss: 32.0542   time: 430.37s   best: 31.7013
2023-10-11 21:22:57,431:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 21:22:57,903:INFO:  Epoch 21/500:  train Loss: 30.2491   val Loss: 31.3709   time: 432.83s   best: 31.3709
2023-10-11 21:30:08,181:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 21:30:08,297:INFO:  Epoch 22/500:  train Loss: 29.9115   val Loss: 31.3229   time: 430.27s   best: 31.3229
2023-10-11 21:37:21,719:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 21:37:21,931:INFO:  Epoch 23/500:  train Loss: 29.6749   val Loss: 30.7083   time: 433.41s   best: 30.7083
2023-10-11 21:44:35,312:INFO:  Epoch 24/500:  train Loss: 29.3674   val Loss: 31.0045   time: 433.38s   best: 30.7083
2023-10-11 21:51:45,914:INFO:  Epoch 25/500:  train Loss: 29.1252   val Loss: 30.8151   time: 430.58s   best: 30.7083
2023-10-11 21:58:59,954:INFO:  Epoch 26/500:  train Loss: 28.8955   val Loss: 30.8783   time: 434.01s   best: 30.7083
2023-10-11 22:06:12,063:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 22:06:12,885:INFO:  Epoch 27/500:  train Loss: 28.7945   val Loss: 30.6778   time: 432.10s   best: 30.6778
2023-10-11 22:13:22,927:INFO:  Epoch 28/500:  train Loss: 28.6181   val Loss: 30.8630   time: 430.04s   best: 30.6778
2023-10-11 22:20:37,163:INFO:  Epoch 29/500:  train Loss: 28.3926   val Loss: 30.7356   time: 434.21s   best: 30.6778
2023-10-11 22:27:47,255:INFO:  Epoch 30/500:  train Loss: 28.3190   val Loss: 31.0323   time: 430.07s   best: 30.6778
2023-10-11 22:34:58,148:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 22:34:58,180:INFO:  Epoch 31/500:  train Loss: 28.0442   val Loss: 30.0727   time: 430.87s   best: 30.0727
2023-10-11 22:42:11,364:INFO:  Epoch 32/500:  train Loss: 27.8648   val Loss: 30.3267   time: 433.18s   best: 30.0727
2023-10-11 22:49:25,588:INFO:  Epoch 33/500:  train Loss: 27.6764   val Loss: 30.1107   time: 434.21s   best: 30.0727
2023-10-11 22:56:37,808:INFO:  Epoch 34/500:  train Loss: 27.5003   val Loss: 30.2312   time: 432.20s   best: 30.0727
2023-10-11 23:03:52,269:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 23:03:52,302:INFO:  Epoch 35/500:  train Loss: 27.3831   val Loss: 29.6419   time: 434.43s   best: 29.6419
2023-10-11 23:11:07,243:INFO:  Epoch 36/500:  train Loss: 27.2477   val Loss: 29.7718   time: 434.94s   best: 29.6419
2023-10-11 23:18:18,198:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 23:18:18,300:INFO:  Epoch 37/500:  train Loss: 27.0981   val Loss: 29.4811   time: 430.93s   best: 29.4811
2023-10-11 23:25:28,603:INFO:  Epoch 38/500:  train Loss: 27.2255   val Loss: 29.6585   time: 430.28s   best: 29.4811
2023-10-11 23:32:39,095:INFO:  Epoch 39/500:  train Loss: 26.7753   val Loss: 29.5348   time: 430.48s   best: 29.4811
2023-10-11 23:39:53,488:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 23:39:53,645:INFO:  Epoch 40/500:  train Loss: 26.8853   val Loss: 29.3426   time: 434.38s   best: 29.3426
2023-10-11 23:47:04,539:INFO:  Epoch 41/500:  train Loss: 26.8574   val Loss: 29.3593   time: 430.87s   best: 29.3426
2023-10-11 23:54:17,926:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-11 23:54:19,109:INFO:  Epoch 42/500:  train Loss: 26.5133   val Loss: 28.9859   time: 433.23s   best: 28.9859
2023-10-12 00:01:29,381:INFO:  Epoch 43/500:  train Loss: 26.5615   val Loss: 29.4371   time: 430.27s   best: 28.9859
2023-10-12 00:08:40,398:INFO:  Epoch 44/500:  train Loss: 26.3351   val Loss: 29.2493   time: 430.99s   best: 28.9859
2023-10-12 00:15:54,481:INFO:  Epoch 45/500:  train Loss: 26.2818   val Loss: 29.1896   time: 434.08s   best: 28.9859
2023-10-12 00:23:05,173:INFO:  Epoch 46/500:  train Loss: 26.0346   val Loss: 29.6746   time: 430.67s   best: 28.9859
2023-10-12 00:30:19,571:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 00:30:19,635:INFO:  Epoch 47/500:  train Loss: 26.1218   val Loss: 28.9732   time: 434.37s   best: 28.9732
2023-10-12 00:37:29,381:INFO:  Epoch 48/500:  train Loss: 25.9418   val Loss: 29.0663   time: 429.72s   best: 28.9732
2023-10-12 00:44:43,736:INFO:  Epoch 49/500:  train Loss: 25.8044   val Loss: 30.3105   time: 434.33s   best: 28.9732
2023-10-12 00:51:54,800:INFO:  Epoch 50/500:  train Loss: 25.8548   val Loss: 29.0623   time: 431.04s   best: 28.9732
2023-10-12 00:59:06,133:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 00:59:06,500:INFO:  Epoch 51/500:  train Loss: 25.6306   val Loss: 28.8039   time: 431.28s   best: 28.8039
2023-10-12 01:06:21,896:INFO:  Epoch 52/500:  train Loss: 25.6186   val Loss: 28.9045   time: 435.40s   best: 28.8039
2023-10-12 01:13:36,117:INFO:  Epoch 53/500:  train Loss: 25.6575   val Loss: 29.1465   time: 434.21s   best: 28.8039
2023-10-12 01:20:48,290:INFO:  Epoch 54/500:  train Loss: 25.6200   val Loss: 29.3794   time: 432.14s   best: 28.8039
2023-10-12 01:27:57,660:INFO:  Epoch 55/500:  train Loss: 25.5064   val Loss: 28.9964   time: 429.34s   best: 28.8039
2023-10-12 01:35:08,654:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 01:35:08,884:INFO:  Epoch 56/500:  train Loss: 25.3261   val Loss: 28.4877   time: 430.98s   best: 28.4877
2023-10-12 01:42:22,906:INFO:  Epoch 57/500:  train Loss: 25.1937   val Loss: 28.5781   time: 434.02s   best: 28.4877
2023-10-12 01:49:36,870:INFO:  Epoch 58/500:  train Loss: 25.0504   val Loss: 28.5031   time: 433.94s   best: 28.4877
2023-10-12 01:56:50,877:INFO:  Epoch 59/500:  train Loss: 25.0406   val Loss: 29.0289   time: 433.99s   best: 28.4877
2023-10-12 02:04:04,878:INFO:  Epoch 60/500:  train Loss: 25.0032   val Loss: 28.8758   time: 433.98s   best: 28.4877
2023-10-12 02:11:18,739:INFO:  Epoch 61/500:  train Loss: 24.9605   val Loss: 28.8309   time: 433.84s   best: 28.4877
2023-10-12 02:18:33,722:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 02:18:33,801:INFO:  Epoch 62/500:  train Loss: 25.0995   val Loss: 28.2754   time: 434.95s   best: 28.2754
2023-10-12 02:25:45,768:INFO:  Epoch 63/500:  train Loss: 24.8098   val Loss: 30.3507   time: 431.95s   best: 28.2754
2023-10-12 02:32:59,642:INFO:  Epoch 64/500:  train Loss: 24.7095   val Loss: 28.4970   time: 433.82s   best: 28.2754
2023-10-12 02:40:10,140:INFO:  Epoch 65/500:  train Loss: 24.8038   val Loss: 28.4631   time: 430.44s   best: 28.2754
2023-10-12 02:47:21,189:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 02:47:21,220:INFO:  Epoch 66/500:  train Loss: 24.6722   val Loss: 28.1794   time: 431.03s   best: 28.1794
2023-10-12 02:54:36,640:INFO:  Epoch 67/500:  train Loss: 24.5666   val Loss: 28.2580   time: 435.41s   best: 28.1794
2023-10-12 03:01:46,868:INFO:  Epoch 68/500:  train Loss: 24.7424   val Loss: 28.3181   time: 430.20s   best: 28.1794
2023-10-12 03:08:57,652:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 03:08:57,736:INFO:  Epoch 69/500:  train Loss: 24.5697   val Loss: 28.1104   time: 430.75s   best: 28.1104
2023-10-12 03:16:07,975:INFO:  Epoch 70/500:  train Loss: 24.4510   val Loss: 34.9285   time: 430.23s   best: 28.1104
2023-10-12 03:23:22,486:INFO:  Epoch 71/500:  train Loss: 24.5556   val Loss: 28.3477   time: 434.50s   best: 28.1104
2023-10-12 03:30:36,507:INFO:  Epoch 72/500:  train Loss: 24.3961   val Loss: 28.1221   time: 433.99s   best: 28.1104
2023-10-12 03:37:46,904:INFO:  Epoch 73/500:  train Loss: 24.3234   val Loss: 28.5834   time: 430.37s   best: 28.1104
2023-10-12 03:44:58,028:INFO:  Epoch 74/500:  train Loss: 24.1939   val Loss: 28.3116   time: 431.10s   best: 28.1104
2023-10-12 03:52:09,625:INFO:  Epoch 75/500:  train Loss: 24.2247   val Loss: 29.1369   time: 431.58s   best: 28.1104
2023-10-12 03:59:22,643:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 03:59:23,003:INFO:  Epoch 76/500:  train Loss: 24.1080   val Loss: 28.0575   time: 432.96s   best: 28.0575
2023-10-12 04:06:33,797:INFO:  Epoch 77/500:  train Loss: 24.0702   val Loss: 28.0674   time: 430.79s   best: 28.0575
2023-10-12 04:13:44,669:INFO:  Epoch 78/500:  train Loss: 24.2156   val Loss: 28.5536   time: 430.85s   best: 28.0575
2023-10-12 04:20:57,852:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 04:20:58,024:INFO:  Epoch 79/500:  train Loss: 23.9921   val Loss: 28.0323   time: 433.16s   best: 28.0323
2023-10-12 04:28:08,443:INFO:  Epoch 80/500:  train Loss: 24.0488   val Loss: 28.2611   time: 430.40s   best: 28.0323
2023-10-12 04:35:18,992:INFO:  Epoch 81/500:  train Loss: 24.0192   val Loss: 28.4271   time: 430.54s   best: 28.0323
2023-10-12 04:42:29,166:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 04:42:29,196:INFO:  Epoch 82/500:  train Loss: 24.0356   val Loss: 27.8773   time: 430.15s   best: 27.8773
2023-10-12 04:49:44,516:INFO:  Epoch 83/500:  train Loss: 23.7714   val Loss: 28.7175   time: 435.32s   best: 27.8773
2023-10-12 04:56:54,632:INFO:  Epoch 84/500:  train Loss: 23.7099   val Loss: 27.9702   time: 430.09s   best: 27.8773
2023-10-12 05:04:09,412:INFO:  Epoch 85/500:  train Loss: 23.7620   val Loss: 27.9116   time: 434.75s   best: 27.8773
2023-10-12 05:11:20,591:INFO:  Epoch 86/500:  train Loss: 23.6567   val Loss: 28.1424   time: 431.17s   best: 27.8773
2023-10-12 05:18:31,863:INFO:  Epoch 87/500:  train Loss: 23.5506   val Loss: 28.9967   time: 431.24s   best: 27.8773
2023-10-12 05:25:41,812:INFO:  Epoch 88/500:  train Loss: 23.5853   val Loss: 28.0628   time: 429.87s   best: 27.8773
2023-10-12 05:32:52,542:INFO:  Epoch 89/500:  train Loss: 23.6506   val Loss: 28.0330   time: 430.69s   best: 27.8773
2023-10-12 05:40:02,137:INFO:  Epoch 90/500:  train Loss: 23.4620   val Loss: 35.7863   time: 429.57s   best: 27.8773
2023-10-12 05:47:14,077:INFO:  Epoch 91/500:  train Loss: 23.6842   val Loss: 27.9961   time: 431.92s   best: 27.8773
2023-10-12 05:54:24,411:INFO:  Epoch 92/500:  train Loss: 23.4159   val Loss: 27.8916   time: 430.31s   best: 27.8773
2023-10-12 06:01:38,514:INFO:  Epoch 93/500:  train Loss: 23.4498   val Loss: 27.9655   time: 433.98s   best: 27.8773
2023-10-12 06:08:53,393:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 06:08:53,454:INFO:  Epoch 94/500:  train Loss: 23.3866   val Loss: 27.7161   time: 434.84s   best: 27.7161
2023-10-12 06:16:03,878:INFO:  Epoch 95/500:  train Loss: 23.3366   val Loss: 28.0140   time: 430.41s   best: 27.7161
2023-10-12 06:23:14,421:INFO:  Epoch 96/500:  train Loss: 23.6045   val Loss: 27.9785   time: 430.51s   best: 27.7161
2023-10-12 06:30:27,592:INFO:  Epoch 97/500:  train Loss: 23.3164   val Loss: 30.5186   time: 433.15s   best: 27.7161
2023-10-12 06:37:37,379:INFO:  Epoch 98/500:  train Loss: 23.2696   val Loss: 27.8164   time: 429.76s   best: 27.7161
2023-10-12 06:44:47,778:INFO:  Epoch 99/500:  train Loss: 23.2400   val Loss: 27.8119   time: 430.36s   best: 27.7161
2023-10-12 06:52:01,323:INFO:  Epoch 100/500:  train Loss: 23.1867   val Loss: 27.9034   time: 433.53s   best: 27.7161
2023-10-12 06:59:11,394:INFO:  Epoch 101/500:  train Loss: 23.4044   val Loss: 28.0772   time: 430.05s   best: 27.7161
2023-10-12 07:06:26,399:INFO:  Epoch 102/500:  train Loss: 23.1072   val Loss: 27.8124   time: 434.98s   best: 27.7161
2023-10-12 07:13:37,129:INFO:  Epoch 103/500:  train Loss: 23.4846   val Loss: 27.8619   time: 430.70s   best: 27.7161
2023-10-12 07:20:49,273:INFO:  Epoch 104/500:  train Loss: 23.2153   val Loss: 28.0067   time: 432.12s   best: 27.7161
2023-10-12 07:28:04,180:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 07:28:05,610:INFO:  Epoch 105/500:  train Loss: 23.0209   val Loss: 27.6459   time: 434.83s   best: 27.6459
2023-10-12 07:35:20,424:INFO:  Epoch 106/500:  train Loss: 23.0291   val Loss: 27.8576   time: 434.81s   best: 27.6459
2023-10-12 07:42:31,270:INFO:  Epoch 107/500:  train Loss: 23.1762   val Loss: 27.8645   time: 430.81s   best: 27.6459
2023-10-12 07:49:45,739:INFO:  Epoch 108/500:  train Loss: 23.4640   val Loss: 31.1481   time: 434.44s   best: 27.6459
2023-10-12 07:57:00,184:INFO:  Epoch 109/500:  train Loss: 23.1220   val Loss: 28.2655   time: 434.42s   best: 27.6459
2023-10-12 08:04:10,936:INFO:  Epoch 110/500:  train Loss: 22.9210   val Loss: 28.6090   time: 430.71s   best: 27.6459
2023-10-12 08:11:25,485:INFO:  Epoch 111/500:  train Loss: 23.0120   val Loss: 28.7622   time: 434.53s   best: 27.6459
2023-10-12 08:18:41,321:INFO:  Epoch 112/500:  train Loss: 23.0562   val Loss: 28.0687   time: 435.81s   best: 27.6459
2023-10-12 08:25:55,633:INFO:  Epoch 113/500:  train Loss: 22.8956   val Loss: 28.1229   time: 434.30s   best: 27.6459
2023-10-12 08:33:10,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-12 08:33:10,716:INFO:  Epoch 114/500:  train Loss: 22.8995   val Loss: 27.4866   time: 434.99s   best: 27.4866
2023-10-12 08:40:24,760:INFO:  Epoch 115/500:  train Loss: 22.9633   val Loss: 28.3054   time: 434.02s   best: 27.4866
2023-10-12 08:47:34,729:INFO:  Epoch 116/500:  train Loss: 22.7966   val Loss: 27.8952   time: 429.95s   best: 27.4866
2023-10-12 08:54:48,342:INFO:  Epoch 117/500:  train Loss: 22.8424   val Loss: 27.8279   time: 433.59s   best: 27.4866
2023-10-12 09:02:01,990:INFO:  Epoch 118/500:  train Loss: 22.7287   val Loss: 27.6902   time: 433.57s   best: 27.4866
2023-10-12 09:09:15,771:INFO:  Epoch 119/500:  train Loss: 22.7789   val Loss: 29.1821   time: 433.75s   best: 27.4866
2023-10-12 09:16:25,541:INFO:  Epoch 120/500:  train Loss: 22.7937   val Loss: 28.0658   time: 429.74s   best: 27.4866
2023-10-12 09:23:39,375:INFO:  Epoch 121/500:  train Loss: 22.6673   val Loss: 28.8112   time: 433.81s   best: 27.4866
2023-10-12 09:30:49,888:INFO:  Epoch 122/500:  train Loss: 22.6254   val Loss: 27.8693   time: 430.47s   best: 27.4866
2023-10-12 09:38:00,520:INFO:  Epoch 123/500:  train Loss: 22.7074   val Loss: 28.5941   time: 430.61s   best: 27.4866
2023-10-12 09:45:10,441:INFO:  Epoch 124/500:  train Loss: 22.7907   val Loss: 30.5492   time: 429.90s   best: 27.4866
2023-10-12 09:52:24,273:INFO:  Epoch 125/500:  train Loss: 22.7720   val Loss: 29.3886   time: 433.81s   best: 27.4866
2023-10-12 09:59:37,983:INFO:  Epoch 126/500:  train Loss: 22.5736   val Loss: 27.7378   time: 433.68s   best: 27.4866
2023-10-12 10:06:48,313:INFO:  Epoch 127/500:  train Loss: 22.7035   val Loss: 28.4181   time: 430.32s   best: 27.4866
2023-10-12 10:14:02,529:INFO:  Epoch 128/500:  train Loss: 22.5980   val Loss: 28.4584   time: 434.19s   best: 27.4866
2023-10-12 10:21:17,571:INFO:  Epoch 129/500:  train Loss: 22.5791   val Loss: 27.7654   time: 434.97s   best: 27.4866
2023-10-12 10:28:27,373:INFO:  Epoch 130/500:  train Loss: 22.5216   val Loss: 27.6918   time: 429.78s   best: 27.4866
2023-10-12 10:35:37,213:INFO:  Epoch 131/500:  train Loss: 22.4803   val Loss: 28.6733   time: 429.82s   best: 27.4866
2023-10-12 10:42:51,388:INFO:  Epoch 132/500:  train Loss: 22.4035   val Loss: 28.0597   time: 434.15s   best: 27.4866
2023-10-12 10:50:01,666:INFO:  Epoch 133/500:  train Loss: 22.5200   val Loss: 30.0673   time: 430.25s   best: 27.4866
2023-10-12 10:57:17,601:INFO:  Epoch 134/500:  train Loss: 22.5544   val Loss: 27.8809   time: 435.07s   best: 27.4866
2023-10-12 11:04:27,494:INFO:  Epoch 135/500:  train Loss: 22.3639   val Loss: 28.5211   time: 429.87s   best: 27.4866
2023-10-12 11:11:40,095:INFO:  Epoch 136/500:  train Loss: 22.4445   val Loss: 28.6988   time: 432.57s   best: 27.4866
2023-10-12 11:18:55,052:INFO:  Epoch 137/500:  train Loss: 22.4226   val Loss: 28.0492   time: 434.59s   best: 27.4866
2023-10-12 11:26:09,507:INFO:  Epoch 138/500:  train Loss: 22.3018   val Loss: 28.0695   time: 434.42s   best: 27.4866
2023-10-12 11:33:19,001:INFO:  Epoch 139/500:  train Loss: 22.2946   val Loss: 32.7573   time: 429.49s   best: 27.4866
2023-10-12 11:40:31,819:INFO:  Epoch 140/500:  train Loss: 22.3190   val Loss: 27.6630   time: 432.81s   best: 27.4866
2023-10-12 11:47:43,503:INFO:  Epoch 141/500:  train Loss: 22.3671   val Loss: 28.6909   time: 431.59s   best: 27.4866
2023-10-12 11:54:53,930:INFO:  Epoch 142/500:  train Loss: 22.3089   val Loss: 28.3248   time: 430.40s   best: 27.4866
2023-10-12 12:02:06,663:INFO:  Epoch 143/500:  train Loss: 22.2076   val Loss: 28.0286   time: 432.70s   best: 27.4866
2023-10-12 12:09:18,013:INFO:  Epoch 144/500:  train Loss: 22.2552   val Loss: 38.8843   time: 431.34s   best: 27.4866
2023-10-12 12:16:33,072:INFO:  Epoch 145/500:  train Loss: 22.3355   val Loss: 28.5671   time: 435.04s   best: 27.4866
2023-10-12 12:23:46,904:INFO:  Epoch 146/500:  train Loss: 22.2266   val Loss: 27.6824   time: 433.80s   best: 27.4866
2023-10-12 12:31:00,897:INFO:  Epoch 147/500:  train Loss: 22.3548   val Loss: 27.8783   time: 433.97s   best: 27.4866
2023-10-12 12:38:12,335:INFO:  Epoch 148/500:  train Loss: 22.2566   val Loss: 28.2071   time: 431.43s   best: 27.4866
2023-10-12 12:45:22,256:INFO:  Epoch 149/500:  train Loss: 22.4570   val Loss: 28.0644   time: 429.90s   best: 27.4866
2023-10-12 12:52:32,789:INFO:  Epoch 150/500:  train Loss: 22.1327   val Loss: 27.8317   time: 430.51s   best: 27.4866
2023-10-12 12:59:47,570:INFO:  Epoch 151/500:  train Loss: 22.1675   val Loss: 28.4323   time: 434.77s   best: 27.4866
2023-10-12 13:06:58,415:INFO:  Epoch 152/500:  train Loss: 22.1128   val Loss: 28.7059   time: 430.78s   best: 27.4866
2023-10-12 13:14:09,258:INFO:  Epoch 153/500:  train Loss: 22.7239   val Loss: 30.8462   time: 430.81s   best: 27.4866
2023-10-12 13:21:20,646:INFO:  Epoch 154/500:  train Loss: 22.2457   val Loss: 28.2855   time: 431.36s   best: 27.4866
2023-10-12 13:28:31,282:INFO:  Epoch 155/500:  train Loss: 22.4697   val Loss: 28.2418   time: 430.56s   best: 27.4866
2023-10-12 13:35:46,125:INFO:  Epoch 156/500:  train Loss: 21.9562   val Loss: 27.7399   time: 434.83s   best: 27.4866
2023-10-12 13:42:59,009:INFO:  Epoch 157/500:  train Loss: 22.1089   val Loss: 28.4806   time: 432.88s   best: 27.4866
2023-10-12 13:50:09,137:INFO:  Epoch 158/500:  train Loss: 22.1533   val Loss: 27.6177   time: 430.11s   best: 27.4866
2023-10-12 13:57:19,382:INFO:  Epoch 159/500:  train Loss: 22.1428   val Loss: 28.3627   time: 430.21s   best: 27.4866
2023-10-12 14:04:29,859:INFO:  Epoch 160/500:  train Loss: 22.0576   val Loss: 28.4475   time: 430.46s   best: 27.4866
2023-10-12 14:11:44,625:INFO:  Epoch 161/500:  train Loss: 21.9647   val Loss: 28.0336   time: 434.75s   best: 27.4866
2023-10-12 14:18:57,519:INFO:  Epoch 162/500:  train Loss: 21.9314   val Loss: 28.5526   time: 432.88s   best: 27.4866
2023-10-12 14:26:07,090:INFO:  Epoch 163/500:  train Loss: 22.2625   val Loss: 31.0673   time: 429.55s   best: 27.4866
2023-10-12 14:33:17,339:INFO:  Epoch 164/500:  train Loss: 21.9982   val Loss: 28.0301   time: 430.23s   best: 27.4866
2023-10-12 14:40:29,497:INFO:  Epoch 165/500:  train Loss: 21.9638   val Loss: 30.5931   time: 432.13s   best: 27.4866
2023-10-12 14:47:43,897:INFO:  Epoch 166/500:  train Loss: 21.8512   val Loss: 27.9449   time: 434.37s   best: 27.4866
2023-10-12 14:54:57,214:INFO:  Epoch 167/500:  train Loss: 21.8833   val Loss: 28.6237   time: 433.30s   best: 27.4866
2023-10-12 15:02:07,095:INFO:  Epoch 168/500:  train Loss: 21.7949   val Loss: 27.8699   time: 429.84s   best: 27.4866
2023-10-12 15:09:16,368:INFO:  Epoch 169/500:  train Loss: 21.9389   val Loss: 28.0863   time: 429.24s   best: 27.4866
2023-10-12 15:16:29,420:INFO:  Epoch 170/500:  train Loss: 21.8218   val Loss: 28.0793   time: 433.02s   best: 27.4866
2023-10-12 15:23:41,977:INFO:  Epoch 171/500:  train Loss: 22.0620   val Loss: 27.7628   time: 432.47s   best: 27.4866
2023-10-12 15:30:56,994:INFO:  Epoch 172/500:  train Loss: 21.8850   val Loss: 28.1741   time: 435.01s   best: 27.4866
2023-10-12 15:38:10,242:INFO:  Epoch 173/500:  train Loss: 21.8172   val Loss: 28.5202   time: 433.19s   best: 27.4866
2023-10-12 15:45:22,628:INFO:  Epoch 174/500:  train Loss: 22.1619   val Loss: 28.1704   time: 432.37s   best: 27.4866
2023-10-12 15:52:33,568:INFO:  Epoch 175/500:  train Loss: 21.8810   val Loss: 29.2497   time: 430.92s   best: 27.4866
2023-10-12 15:59:45,497:INFO:  Epoch 176/500:  train Loss: 22.0276   val Loss: 28.0176   time: 431.90s   best: 27.4866
2023-10-12 16:06:59,148:INFO:  Epoch 177/500:  train Loss: 21.6625   val Loss: 27.9276   time: 433.63s   best: 27.4866
2023-10-12 16:14:09,809:INFO:  Epoch 178/500:  train Loss: 22.0104   val Loss: 28.1185   time: 430.63s   best: 27.4866
2023-10-12 16:21:19,569:INFO:  Epoch 179/500:  train Loss: 21.9212   val Loss: 27.6925   time: 429.74s   best: 27.4866
2023-10-12 16:28:29,734:INFO:  Epoch 180/500:  train Loss: 21.5942   val Loss: 28.0874   time: 430.16s   best: 27.4866
2023-10-12 16:35:39,075:INFO:  Epoch 181/500:  train Loss: 21.6402   val Loss: 28.3371   time: 429.33s   best: 27.4866
2023-10-12 16:42:50,107:INFO:  Epoch 182/500:  train Loss: 21.6876   val Loss: 28.1046   time: 431.01s   best: 27.4866
2023-10-12 16:50:03,594:INFO:  Epoch 183/500:  train Loss: 21.6632   val Loss: 28.1632   time: 433.46s   best: 27.4866
2023-10-12 16:57:13,486:INFO:  Epoch 184/500:  train Loss: 21.6743   val Loss: 28.5903   time: 429.87s   best: 27.4866
2023-10-12 17:04:23,444:INFO:  Epoch 185/500:  train Loss: 21.8960   val Loss: 28.0844   time: 429.77s   best: 27.4866
2023-10-12 17:11:34,001:INFO:  Epoch 186/500:  train Loss: 21.7421   val Loss: 28.0341   time: 430.55s   best: 27.4866
2023-10-12 17:18:44,214:INFO:  Epoch 187/500:  train Loss: 21.9112   val Loss: 28.1341   time: 430.17s   best: 27.4866
2023-10-12 17:25:58,119:INFO:  Epoch 188/500:  train Loss: 21.7774   val Loss: 29.0586   time: 433.90s   best: 27.4866
2023-10-12 17:33:08,339:INFO:  Epoch 189/500:  train Loss: 21.6614   val Loss: 28.4828   time: 430.20s   best: 27.4866
2023-10-12 17:40:21,946:INFO:  Epoch 190/500:  train Loss: 21.6182   val Loss: 28.9537   time: 433.58s   best: 27.4866
2023-10-12 17:47:35,013:INFO:  Epoch 191/500:  train Loss: 21.6209   val Loss: 28.5451   time: 433.06s   best: 27.4866
2023-10-12 17:54:49,616:INFO:  Epoch 192/500:  train Loss: 21.5763   val Loss: 28.2537   time: 434.57s   best: 27.4866
2023-10-12 18:02:03,332:INFO:  Epoch 193/500:  train Loss: 21.5329   val Loss: 28.3317   time: 433.69s   best: 27.4866
2023-10-12 18:09:13,210:INFO:  Epoch 194/500:  train Loss: 21.5525   val Loss: 28.3833   time: 429.87s   best: 27.4866
2023-10-12 18:16:27,927:INFO:  Epoch 195/500:  train Loss: 21.5111   val Loss: 27.8496   time: 434.69s   best: 27.4866
2023-10-12 18:23:40,774:INFO:  Epoch 196/500:  train Loss: 21.5214   val Loss: 28.0117   time: 432.61s   best: 27.4866
2023-10-12 18:30:52,255:INFO:  Epoch 197/500:  train Loss: 21.7132   val Loss: 27.9564   time: 431.47s   best: 27.4866
2023-10-12 18:38:03,803:INFO:  Epoch 198/500:  train Loss: 21.5137   val Loss: 27.9924   time: 431.52s   best: 27.4866
2023-10-12 18:45:13,467:INFO:  Epoch 199/500:  train Loss: 21.4758   val Loss: 28.1493   time: 429.65s   best: 27.4866
2023-10-12 18:52:24,928:INFO:  Epoch 200/500:  train Loss: 21.6882   val Loss: 28.2960   time: 431.45s   best: 27.4866
2023-10-12 18:59:38,195:INFO:  Epoch 201/500:  train Loss: 21.5430   val Loss: 28.8030   time: 433.20s   best: 27.4866
2023-10-12 19:06:51,785:INFO:  Epoch 202/500:  train Loss: 21.5905   val Loss: 28.0643   time: 433.55s   best: 27.4866
2023-10-12 19:14:04,499:INFO:  Epoch 203/500:  train Loss: 21.4485   val Loss: 29.4223   time: 432.68s   best: 27.4866
2023-10-12 19:21:17,569:INFO:  Epoch 204/500:  train Loss: 21.4735   val Loss: 30.6570   time: 433.04s   best: 27.4866
2023-10-12 19:28:27,308:INFO:  Epoch 205/500:  train Loss: 21.5637   val Loss: 27.9357   time: 429.70s   best: 27.4866
2023-10-12 19:35:37,799:INFO:  Epoch 206/500:  train Loss: 21.7787   val Loss: 30.0956   time: 430.47s   best: 27.4866
2023-10-12 19:42:47,967:INFO:  Epoch 207/500:  train Loss: 21.4125   val Loss: 28.1424   time: 430.15s   best: 27.4866
2023-10-12 19:49:59,863:INFO:  Epoch 208/500:  train Loss: 21.4010   val Loss: 28.6161   time: 431.89s   best: 27.4866
2023-10-12 19:57:12,511:INFO:  Epoch 209/500:  train Loss: 21.3776   val Loss: 28.1018   time: 432.43s   best: 27.4866
2023-10-12 20:04:22,554:INFO:  Epoch 210/500:  train Loss: 21.4997   val Loss: 28.1969   time: 430.02s   best: 27.4866
2023-10-12 20:11:35,483:INFO:  Epoch 211/500:  train Loss: 21.4475   val Loss: 28.5186   time: 432.90s   best: 27.4866
2023-10-12 20:18:49,500:INFO:  Epoch 212/500:  train Loss: 21.6768   val Loss: 28.0444   time: 433.95s   best: 27.4866
2023-10-12 20:26:02,988:INFO:  Epoch 213/500:  train Loss: 21.5021   val Loss: 28.1258   time: 433.47s   best: 27.4866
2023-10-12 20:33:13,101:INFO:  Epoch 214/500:  train Loss: 21.3887   val Loss: 27.9283   time: 430.09s   best: 27.4866
2023-10-12 20:40:27,188:INFO:  Epoch 215/500:  train Loss: 21.8472   val Loss: 27.8111   time: 434.06s   best: 27.4866
2023-10-12 20:47:40,683:INFO:  Epoch 216/500:  train Loss: 21.6227   val Loss: 28.4014   time: 433.49s   best: 27.4866
2023-10-12 20:54:54,617:INFO:  Epoch 217/500:  train Loss: 21.4106   val Loss: 28.2289   time: 433.92s   best: 27.4866
2023-10-12 21:02:09,651:INFO:  Epoch 218/500:  train Loss: 21.2630   val Loss: 27.9084   time: 435.00s   best: 27.4866
2023-10-12 21:09:20,491:INFO:  Epoch 219/500:  train Loss: 21.3953   val Loss: 28.6280   time: 430.82s   best: 27.4866
2023-10-12 21:16:34,429:INFO:  Epoch 220/500:  train Loss: 21.2977   val Loss: 28.3745   time: 433.92s   best: 27.4866
2023-10-12 21:23:44,079:INFO:  Epoch 221/500:  train Loss: 21.7402   val Loss: 28.3732   time: 429.63s   best: 27.4866
2023-10-12 21:30:53,186:INFO:  Epoch 222/500:  train Loss: 21.5728   val Loss: 27.8907   time: 429.10s   best: 27.4866
2023-10-12 21:38:04,981:INFO:  Epoch 223/500:  train Loss: 21.2554   val Loss: 28.1842   time: 431.78s   best: 27.4866
2023-10-12 21:45:17,613:INFO:  Epoch 224/500:  train Loss: 21.2827   val Loss: 28.0857   time: 432.62s   best: 27.4866
2023-10-12 21:52:30,003:INFO:  Epoch 225/500:  train Loss: 21.4685   val Loss: 28.5649   time: 432.38s   best: 27.4866
2023-10-12 21:59:39,731:INFO:  Epoch 226/500:  train Loss: 21.5787   val Loss: 28.0373   time: 429.71s   best: 27.4866
2023-10-12 22:06:49,799:INFO:  Epoch 227/500:  train Loss: 21.1541   val Loss: 27.9906   time: 430.03s   best: 27.4866
2023-10-12 22:13:59,642:INFO:  Epoch 228/500:  train Loss: 21.2175   val Loss: 28.5274   time: 429.80s   best: 27.4866
2023-10-12 22:21:12,752:INFO:  Epoch 229/500:  train Loss: 21.2638   val Loss: 28.3809   time: 433.09s   best: 27.4866
2023-10-12 22:28:23,920:INFO:  Epoch 230/500:  train Loss: 21.2106   val Loss: 28.8849   time: 431.14s   best: 27.4866
2023-10-12 22:35:32,969:INFO:  Epoch 231/500:  train Loss: 21.3624   val Loss: 28.3429   time: 429.03s   best: 27.4866
2023-10-12 22:42:44,882:INFO:  Epoch 232/500:  train Loss: 21.6619   val Loss: 29.9060   time: 431.89s   best: 27.4866
2023-10-12 22:49:58,296:INFO:  Epoch 233/500:  train Loss: 21.2775   val Loss: 28.2280   time: 433.01s   best: 27.4866
2023-10-12 22:57:09,833:INFO:  Epoch 234/500:  train Loss: 21.2156   val Loss: 28.9840   time: 431.49s   best: 27.4866
2023-10-12 23:04:19,935:INFO:  Epoch 235/500:  train Loss: 21.1247   val Loss: 28.3014   time: 430.09s   best: 27.4866
2023-10-12 23:11:32,697:INFO:  Epoch 236/500:  train Loss: 21.1817   val Loss: 28.2935   time: 432.73s   best: 27.4866
2023-10-12 23:18:45,952:INFO:  Epoch 237/500:  train Loss: 21.0895   val Loss: 28.1652   time: 433.23s   best: 27.4866
2023-10-12 23:25:59,931:INFO:  Epoch 238/500:  train Loss: 21.1995   val Loss: 28.5026   time: 433.94s   best: 27.4866
2023-10-12 23:33:09,403:INFO:  Epoch 239/500:  train Loss: 21.2520   val Loss: 28.8460   time: 429.44s   best: 27.4866
2023-10-12 23:40:19,322:INFO:  Epoch 240/500:  train Loss: 21.1296   val Loss: 28.2892   time: 429.90s   best: 27.4866
2023-10-12 23:47:29,267:INFO:  Epoch 241/500:  train Loss: 21.1695   val Loss: 27.8263   time: 429.92s   best: 27.4866
2023-10-12 23:54:42,768:INFO:  Epoch 242/500:  train Loss: 21.1747   val Loss: 28.2433   time: 433.49s   best: 27.4866
2023-10-13 00:01:55,076:INFO:  Epoch 243/500:  train Loss: 21.1514   val Loss: 27.9268   time: 432.28s   best: 27.4866
2023-10-13 00:09:07,879:INFO:  Epoch 244/500:  train Loss: 21.1076   val Loss: 28.6349   time: 432.79s   best: 27.4866
2023-10-13 00:16:22,986:INFO:  Epoch 245/500:  train Loss: 21.1729   val Loss: 28.5799   time: 435.08s   best: 27.4866
2023-10-13 00:23:32,729:INFO:  Epoch 246/500:  train Loss: 21.1080   val Loss: 27.9971   time: 429.71s   best: 27.4866
2023-10-13 00:30:46,329:INFO:  Epoch 247/500:  train Loss: 21.0035   val Loss: 27.8193   time: 433.57s   best: 27.4866
2023-10-13 00:38:00,508:INFO:  Epoch 248/500:  train Loss: 21.1142   val Loss: 28.5753   time: 434.14s   best: 27.4866
2023-10-13 00:45:14,138:INFO:  Epoch 249/500:  train Loss: 21.0910   val Loss: 28.4675   time: 433.61s   best: 27.4866
2023-10-13 00:52:25,594:INFO:  Epoch 250/500:  train Loss: 21.0001   val Loss: 28.3994   time: 431.44s   best: 27.4866
2023-10-13 00:59:39,443:INFO:  Epoch 251/500:  train Loss: 21.4101   val Loss: 27.9514   time: 433.83s   best: 27.4866
2023-10-13 01:06:50,917:INFO:  Epoch 252/500:  train Loss: 21.1270   val Loss: 29.1720   time: 431.45s   best: 27.4866
2023-10-13 01:14:04,048:INFO:  Epoch 253/500:  train Loss: 21.0075   val Loss: 28.1488   time: 433.11s   best: 27.4866
2023-10-13 01:21:13,656:INFO:  Epoch 254/500:  train Loss: 21.3596   val Loss: 28.0132   time: 429.58s   best: 27.4866
2023-10-13 01:28:26,145:INFO:  Epoch 255/500:  train Loss: 21.0633   val Loss: 28.2545   time: 432.22s   best: 27.4866
2023-10-13 01:35:36,025:INFO:  Epoch 256/500:  train Loss: 21.0072   val Loss: 28.1932   time: 429.86s   best: 27.4866
2023-10-13 01:42:45,347:INFO:  Epoch 257/500:  train Loss: 21.3193   val Loss: 28.3684   time: 429.28s   best: 27.4866
2023-10-13 01:49:59,313:INFO:  Epoch 258/500:  train Loss: 21.0423   val Loss: 28.0460   time: 433.94s   best: 27.4866
2023-10-13 01:57:10,606:INFO:  Epoch 259/500:  train Loss: 20.9393   val Loss: 28.7009   time: 431.27s   best: 27.4866
2023-10-13 02:04:23,851:INFO:  Epoch 260/500:  train Loss: 21.0385   val Loss: 28.2403   time: 433.24s   best: 27.4866
2023-10-13 02:11:32,945:INFO:  Epoch 261/500:  train Loss: 21.1645   val Loss: 29.3631   time: 429.08s   best: 27.4866
2023-10-13 02:18:44,891:INFO:  Epoch 262/500:  train Loss: 21.0041   val Loss: 28.5736   time: 431.90s   best: 27.4866
2023-10-13 02:25:59,199:INFO:  Epoch 263/500:  train Loss: 21.1943   val Loss: 28.4620   time: 434.26s   best: 27.4866
2023-10-13 02:33:14,246:INFO:  Epoch 264/500:  train Loss: 20.9079   val Loss: 28.1883   time: 435.04s   best: 27.4866
2023-10-13 02:40:27,805:INFO:  Epoch 265/500:  train Loss: 21.0408   val Loss: 29.0424   time: 433.53s   best: 27.4866
2023-10-13 02:47:39,539:INFO:  Epoch 266/500:  train Loss: 21.0124   val Loss: 28.8266   time: 431.69s   best: 27.4866
2023-10-13 02:54:51,703:INFO:  Epoch 267/500:  train Loss: 21.1526   val Loss: 28.4109   time: 432.14s   best: 27.4866
2023-10-13 03:02:03,393:INFO:  Epoch 268/500:  train Loss: 21.0298   val Loss: 28.1790   time: 431.68s   best: 27.4866
2023-10-13 03:09:17,142:INFO:  Epoch 269/500:  train Loss: 20.9306   val Loss: 28.1659   time: 433.71s   best: 27.4866
2023-10-13 03:16:28,311:INFO:  Epoch 270/500:  train Loss: 20.9692   val Loss: 28.5061   time: 431.15s   best: 27.4866
2023-10-13 03:23:40,323:INFO:  Epoch 271/500:  train Loss: 20.8552   val Loss: 28.2475   time: 431.98s   best: 27.4866
2023-10-13 03:30:53,157:INFO:  Epoch 272/500:  train Loss: 21.3670   val Loss: 29.9000   time: 432.79s   best: 27.4866
2023-10-13 03:38:09,529:INFO:  Epoch 273/500:  train Loss: 20.9013   val Loss: 28.0426   time: 436.35s   best: 27.4866
2023-10-13 03:45:23,995:INFO:  Epoch 274/500:  train Loss: 21.0278   val Loss: 27.9450   time: 434.42s   best: 27.4866
2023-10-13 03:52:39,400:INFO:  Epoch 275/500:  train Loss: 20.8914   val Loss: 28.0954   time: 435.39s   best: 27.4866
2023-10-13 03:59:55,399:INFO:  Epoch 276/500:  train Loss: 20.8087   val Loss: 28.1575   time: 435.92s   best: 27.4866
2023-10-13 04:07:06,815:INFO:  Epoch 277/500:  train Loss: 20.8901   val Loss: 28.5542   time: 431.38s   best: 27.4866
2023-10-13 04:14:21,905:INFO:  Epoch 278/500:  train Loss: 20.9068   val Loss: 29.5342   time: 435.07s   best: 27.4866
2023-10-13 04:21:33,708:INFO:  Epoch 279/500:  train Loss: 20.8822   val Loss: 29.5066   time: 431.77s   best: 27.4866
2023-10-13 04:28:45,724:INFO:  Epoch 280/500:  train Loss: 20.8362   val Loss: 28.3685   time: 431.96s   best: 27.4866
2023-10-13 04:36:00,652:INFO:  Epoch 281/500:  train Loss: 20.9389   val Loss: 28.3894   time: 434.89s   best: 27.4866
2023-10-13 04:43:13,198:INFO:  Epoch 282/500:  train Loss: 20.7952   val Loss: 28.4215   time: 432.52s   best: 27.4866
2023-10-13 04:50:29,969:INFO:  Epoch 283/500:  train Loss: 20.9839   val Loss: 28.3310   time: 436.76s   best: 27.4866
2023-10-13 04:57:44,717:INFO:  Epoch 284/500:  train Loss: 20.8583   val Loss: 28.6141   time: 434.72s   best: 27.4866
2023-10-13 05:05:00,538:INFO:  Epoch 285/500:  train Loss: 20.8453   val Loss: 28.2109   time: 435.79s   best: 27.4866
2023-10-13 05:12:15,162:INFO:  Epoch 286/500:  train Loss: 20.8132   val Loss: 29.7522   time: 434.55s   best: 27.4866
2023-10-13 05:19:31,748:INFO:  Epoch 287/500:  train Loss: 20.8380   val Loss: 28.3074   time: 436.58s   best: 27.4866
2023-10-13 05:26:44,504:INFO:  Epoch 288/500:  train Loss: 20.9993   val Loss: 29.1713   time: 432.74s   best: 27.4866
2023-10-13 06:06:11,993:INFO:  Epoch 289/500:  train Loss: 20.8673   val Loss: 28.2496   time: 2367.47s   best: 27.4866
2023-10-13 06:15:42,607:INFO:  Epoch 290/500:  train Loss: 21.1684   val Loss: 28.5473   time: 570.60s   best: 27.4866
2023-10-13 06:24:58,394:INFO:  Epoch 291/500:  train Loss: 20.8361   val Loss: 28.2118   time: 555.76s   best: 27.4866
2023-10-13 06:33:03,597:INFO:  Epoch 292/500:  train Loss: 20.7955   val Loss: 28.3903   time: 485.18s   best: 27.4866
2023-10-13 06:40:32,932:INFO:  Epoch 293/500:  train Loss: 20.9285   val Loss: 28.3027   time: 449.30s   best: 27.4866
2023-10-13 06:47:47,272:INFO:  Epoch 294/500:  train Loss: 20.7700   val Loss: 28.5820   time: 434.21s   best: 27.4866
2023-10-13 06:55:05,561:INFO:  Epoch 295/500:  train Loss: 20.8609   val Loss: 28.4195   time: 438.28s   best: 27.4866
2023-10-13 07:02:23,570:INFO:  Epoch 296/500:  train Loss: 20.8383   val Loss: 28.2178   time: 436.92s   best: 27.4866
2023-10-13 07:09:39,877:INFO:  Epoch 297/500:  train Loss: 20.7782   val Loss: 30.2737   time: 436.26s   best: 27.4866
2023-10-13 07:16:57,848:INFO:  Epoch 298/500:  train Loss: 20.8030   val Loss: 28.4493   time: 437.81s   best: 27.4866
2023-10-13 07:24:14,579:INFO:  Epoch 299/500:  train Loss: 20.8106   val Loss: 28.0542   time: 436.72s   best: 27.4866
2023-10-13 07:31:30,503:INFO:  Epoch 300/500:  train Loss: 20.8485   val Loss: 28.8081   time: 435.73s   best: 27.4866
2023-10-13 07:38:51,717:INFO:  Epoch 301/500:  train Loss: 20.8235   val Loss: 28.4402   time: 441.20s   best: 27.4866
2023-10-13 07:46:08,567:INFO:  Epoch 302/500:  train Loss: 20.8400   val Loss: 27.7969   time: 436.81s   best: 27.4866
2023-10-13 07:53:23,560:INFO:  Epoch 303/500:  train Loss: 21.0526   val Loss: 28.6256   time: 434.97s   best: 27.4866
2023-10-13 08:00:38,902:INFO:  Epoch 304/500:  train Loss: 20.5436   val Loss: 32.6230   time: 435.16s   best: 27.4866
2023-10-13 08:07:57,581:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 08:07:57,638:INFO:  Epoch 305/500:  train Loss: 20.3740   val Loss: 27.2222   time: 438.63s   best: 27.2222
2023-10-13 08:15:15,096:INFO:  Epoch 306/500:  train Loss: 20.4411   val Loss: 28.8231   time: 437.46s   best: 27.2222
2023-10-13 08:22:32,469:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 08:22:33,637:INFO:  Epoch 307/500:  train Loss: 20.0487   val Loss: 25.8367   time: 437.35s   best: 25.8367
2023-10-13 08:29:48,692:INFO:  Epoch 308/500:  train Loss: 19.7488   val Loss: 25.9944   time: 435.05s   best: 25.8367
2023-10-13 08:37:07,550:INFO:  Epoch 309/500:  train Loss: 19.6573   val Loss: 30.7034   time: 438.85s   best: 25.8367
2023-10-13 08:44:21,738:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 08:44:21,794:INFO:  Epoch 310/500:  train Loss: 19.5368   val Loss: 25.6940   time: 434.16s   best: 25.6940
2023-10-13 08:51:39,456:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 08:51:39,684:INFO:  Epoch 311/500:  train Loss: 19.3863   val Loss: 25.0668   time: 437.65s   best: 25.0668
2023-10-13 08:58:56,223:INFO:  Epoch 312/500:  train Loss: 19.3891   val Loss: 25.0856   time: 436.54s   best: 25.0668
2023-10-13 09:06:10,929:INFO:  Epoch 313/500:  train Loss: 19.3270   val Loss: 25.7055   time: 434.69s   best: 25.0668
2023-10-13 09:13:26,080:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 09:13:26,113:INFO:  Epoch 314/500:  train Loss: 19.6723   val Loss: 24.9297   time: 435.12s   best: 24.9297
2023-10-13 09:20:40,917:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 09:20:40,966:INFO:  Epoch 315/500:  train Loss: 19.2929   val Loss: 24.8656   time: 434.77s   best: 24.8656
2023-10-13 09:27:57,283:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 09:27:57,503:INFO:  Epoch 316/500:  train Loss: 19.1860   val Loss: 24.7813   time: 436.31s   best: 24.7813
2023-10-13 09:35:16,035:INFO:  Epoch 317/500:  train Loss: 19.2084   val Loss: 24.8106   time: 438.53s   best: 24.7813
2023-10-13 09:42:33,140:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 09:42:33,168:INFO:  Epoch 318/500:  train Loss: 19.1903   val Loss: 24.4642   time: 437.05s   best: 24.4642
2023-10-13 09:49:49,208:INFO:  Epoch 319/500:  train Loss: 19.7061   val Loss: 24.6813   time: 436.04s   best: 24.4642
2023-10-13 09:57:07,769:INFO:  Epoch 320/500:  train Loss: 19.2611   val Loss: 25.6767   time: 438.54s   best: 24.4642
2023-10-13 10:04:23,702:INFO:  Epoch 321/500:  train Loss: 19.1041   val Loss: 24.8431   time: 435.91s   best: 24.4642
2023-10-13 10:11:42,987:INFO:  Epoch 322/500:  train Loss: 19.1275   val Loss: 24.6768   time: 439.27s   best: 24.4642
2023-10-13 10:18:55,927:INFO:  Epoch 323/500:  train Loss: 19.2680   val Loss: 24.6394   time: 432.90s   best: 24.4642
2023-10-13 10:26:09,827:INFO:  Epoch 324/500:  train Loss: 19.1325   val Loss: 24.4651   time: 433.87s   best: 24.4642
2023-10-13 10:33:24,219:INFO:  Epoch 325/500:  train Loss: 19.2089   val Loss: 24.5769   time: 434.36s   best: 24.4642
2023-10-13 10:40:43,446:INFO:  Epoch 326/500:  train Loss: 19.1546   val Loss: 24.6013   time: 439.20s   best: 24.4642
2023-10-13 10:48:00,032:INFO:  Epoch 327/500:  train Loss: 19.0192   val Loss: 24.5435   time: 436.56s   best: 24.4642
2023-10-13 10:55:14,227:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 10:55:14,267:INFO:  Epoch 328/500:  train Loss: 19.1406   val Loss: 24.4400   time: 434.15s   best: 24.4400
2023-10-13 11:02:31,549:INFO:  Epoch 329/500:  train Loss: 18.9747   val Loss: 25.0758   time: 437.27s   best: 24.4400
2023-10-13 11:09:49,740:INFO:  Epoch 330/500:  train Loss: 19.0887   val Loss: 24.5136   time: 438.18s   best: 24.4400
2023-10-13 11:17:02,443:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 11:17:02,491:INFO:  Epoch 331/500:  train Loss: 18.9329   val Loss: 24.2323   time: 432.69s   best: 24.2323
2023-10-13 11:24:20,451:INFO:  Epoch 332/500:  train Loss: 18.8116   val Loss: 24.7726   time: 437.96s   best: 24.2323
2023-10-13 11:31:34,596:INFO:  Epoch 333/500:  train Loss: 19.1542   val Loss: 24.6711   time: 434.11s   best: 24.2323
2023-10-13 11:38:50,144:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 11:38:50,168:INFO:  Epoch 334/500:  train Loss: 18.9940   val Loss: 24.1748   time: 435.51s   best: 24.1748
2023-10-13 11:46:05,770:INFO:  Epoch 335/500:  train Loss: 19.0269   val Loss: 24.6693   time: 435.59s   best: 24.1748
2023-10-13 11:53:23,751:INFO:  Epoch 336/500:  train Loss: 18.8643   val Loss: 25.7343   time: 437.95s   best: 24.1748
2023-10-13 12:00:39,640:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 12:00:39,674:INFO:  Epoch 337/500:  train Loss: 18.7809   val Loss: 23.7510   time: 435.88s   best: 23.7510
2023-10-13 12:07:54,304:INFO:  Epoch 338/500:  train Loss: 19.0475   val Loss: 24.2442   time: 434.62s   best: 23.7510
2023-10-13 12:15:08,365:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 12:15:09,470:INFO:  Epoch 339/500:  train Loss: 18.9391   val Loss: 23.7435   time: 434.03s   best: 23.7435
2023-10-13 12:22:28,600:INFO:  Epoch 340/500:  train Loss: 18.8236   val Loss: 27.7187   time: 439.13s   best: 23.7435
2023-10-13 12:29:45,649:INFO:  Epoch 341/500:  train Loss: 18.7300   val Loss: 24.2092   time: 437.03s   best: 23.7435
2023-10-13 12:37:04,401:INFO:  Epoch 342/500:  train Loss: 18.6794   val Loss: 24.9282   time: 438.72s   best: 23.7435
2023-10-13 12:44:19,955:INFO:  Epoch 343/500:  train Loss: 18.8978   val Loss: 26.5936   time: 435.47s   best: 23.7435
2023-10-13 12:51:34,841:INFO:  Epoch 344/500:  train Loss: 18.7092   val Loss: 23.9608   time: 434.85s   best: 23.7435
2023-10-13 12:58:53,193:INFO:  Epoch 345/500:  train Loss: 18.7845   val Loss: 24.0490   time: 438.30s   best: 23.7435
2023-10-13 13:06:07,861:INFO:  Epoch 346/500:  train Loss: 18.6347   val Loss: 24.1080   time: 434.64s   best: 23.7435
2023-10-13 13:13:26,470:INFO:  Epoch 347/500:  train Loss: 18.8115   val Loss: 23.9607   time: 438.59s   best: 23.7435
2023-10-13 13:20:44,881:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 13:20:44,912:INFO:  Epoch 348/500:  train Loss: 18.5989   val Loss: 23.6543   time: 438.40s   best: 23.6543
2023-10-13 13:27:59,413:INFO:  Epoch 349/500:  train Loss: 18.5993   val Loss: 24.1115   time: 434.49s   best: 23.6543
2023-10-13 13:35:16,272:INFO:  Epoch 350/500:  train Loss: 18.6237   val Loss: 23.8568   time: 436.83s   best: 23.6543
2023-10-13 13:42:31,175:INFO:  Epoch 351/500:  train Loss: 18.5218   val Loss: 23.9545   time: 434.88s   best: 23.6543
2023-10-13 13:49:46,270:INFO:  Epoch 352/500:  train Loss: 18.6802   val Loss: 24.8020   time: 435.07s   best: 23.6543
2023-10-13 13:57:00,757:INFO:  Epoch 353/500:  train Loss: 18.5861   val Loss: 24.0413   time: 434.47s   best: 23.6543
2023-10-13 14:04:16,602:INFO:  Epoch 354/500:  train Loss: 18.5134   val Loss: 23.7370   time: 435.79s   best: 23.6543
2023-10-13 14:11:31,012:INFO:  Epoch 355/500:  train Loss: 18.4460   val Loss: 24.0440   time: 434.40s   best: 23.6543
2023-10-13 14:18:46,486:INFO:  Epoch 356/500:  train Loss: 18.7458   val Loss: 26.3468   time: 435.45s   best: 23.6543
2023-10-13 14:26:02,553:INFO:  Epoch 357/500:  train Loss: 18.6140   val Loss: 25.7612   time: 436.04s   best: 23.6543
2023-10-13 14:33:20,952:INFO:  Epoch 358/500:  train Loss: 18.4995   val Loss: 24.4247   time: 438.37s   best: 23.6543
2023-10-13 14:40:36,964:INFO:  Epoch 359/500:  train Loss: 18.4854   val Loss: 23.9063   time: 435.98s   best: 23.6543
2023-10-13 14:47:52,475:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 14:47:52,521:INFO:  Epoch 360/500:  train Loss: 18.5582   val Loss: 23.4794   time: 435.47s   best: 23.4794
2023-10-13 14:55:06,754:INFO:  Epoch 361/500:  train Loss: 18.4176   val Loss: 24.0629   time: 434.22s   best: 23.4794
2023-10-13 15:02:22,965:INFO:  Epoch 362/500:  train Loss: 18.5095   val Loss: 24.4269   time: 436.19s   best: 23.4794
2023-10-13 15:09:37,435:INFO:  Epoch 363/500:  train Loss: 18.4349   val Loss: 24.0988   time: 434.44s   best: 23.4794
2023-10-13 15:16:53,074:INFO:  Epoch 364/500:  train Loss: 18.5293   val Loss: 24.1440   time: 435.62s   best: 23.4794
2023-10-13 15:24:10,129:INFO:  Epoch 365/500:  train Loss: 18.4104   val Loss: 23.8495   time: 437.04s   best: 23.4794
2023-10-13 15:31:26,203:INFO:  Epoch 366/500:  train Loss: 18.7213   val Loss: 33.4736   time: 436.06s   best: 23.4794
2023-10-13 15:38:40,478:INFO:  Epoch 367/500:  train Loss: 18.5772   val Loss: 23.7874   time: 434.27s   best: 23.4794
2023-10-13 15:45:55,524:INFO:  Epoch 368/500:  train Loss: 18.5121   val Loss: 24.1726   time: 435.04s   best: 23.4794
2023-10-13 15:53:12,557:INFO:  Epoch 369/500:  train Loss: 18.5235   val Loss: 24.2037   time: 437.02s   best: 23.4794
2023-10-13 16:00:31,341:INFO:  Epoch 370/500:  train Loss: 18.3498   val Loss: 24.0201   time: 438.76s   best: 23.4794
2023-10-13 16:07:49,797:INFO:  Epoch 371/500:  train Loss: 18.3105   val Loss: 24.0044   time: 438.44s   best: 23.4794
2023-10-13 16:15:08,835:INFO:  Epoch 372/500:  train Loss: 18.3027   val Loss: 24.1666   time: 439.03s   best: 23.4794
2023-10-13 16:22:22,765:INFO:  Epoch 373/500:  train Loss: 18.3800   val Loss: 23.5685   time: 433.89s   best: 23.4794
2023-10-13 16:29:39,994:INFO:  Epoch 374/500:  train Loss: 18.7501   val Loss: 23.6925   time: 437.20s   best: 23.4794
2023-10-13 16:36:56,823:INFO:  Epoch 375/500:  train Loss: 18.4294   val Loss: 24.9816   time: 436.82s   best: 23.4794
2023-10-13 16:44:13,143:INFO:  Epoch 376/500:  train Loss: 18.3040   val Loss: 23.6714   time: 436.29s   best: 23.4794
2023-10-13 16:51:29,509:INFO:  Epoch 377/500:  train Loss: 18.3205   val Loss: 24.0931   time: 436.35s   best: 23.4794
2023-10-13 16:58:47,869:INFO:  Epoch 378/500:  train Loss: 18.2927   val Loss: 24.1123   time: 438.32s   best: 23.4794
2023-10-13 17:06:05,607:INFO:  Epoch 379/500:  train Loss: 18.2691   val Loss: 23.8086   time: 437.73s   best: 23.4794
2023-10-13 17:13:23,745:INFO:  Epoch 380/500:  train Loss: 18.3500   val Loss: 23.7205   time: 438.11s   best: 23.4794
2023-10-13 17:20:41,652:INFO:  Epoch 381/500:  train Loss: 18.2601   val Loss: 23.7937   time: 437.88s   best: 23.4794
2023-10-13 17:27:56,001:INFO:  Epoch 382/500:  train Loss: 18.3236   val Loss: 23.7828   time: 434.34s   best: 23.4794
2023-10-13 17:35:14,637:INFO:  Epoch 383/500:  train Loss: 18.3052   val Loss: 23.5014   time: 438.61s   best: 23.4794
2023-10-13 17:42:28,839:INFO:  Epoch 384/500:  train Loss: 18.1936   val Loss: 24.8320   time: 434.18s   best: 23.4794
2023-10-13 17:49:43,723:INFO:  Epoch 385/500:  train Loss: 18.2491   val Loss: 23.9940   time: 434.87s   best: 23.4794
2023-10-13 17:57:02,638:INFO:  Epoch 386/500:  train Loss: 18.4255   val Loss: 23.8590   time: 438.90s   best: 23.4794
2023-10-13 18:04:17,707:INFO:  Epoch 387/500:  train Loss: 18.1905   val Loss: 28.0956   time: 435.04s   best: 23.4794
2023-10-13 18:11:35,824:INFO:  Epoch 388/500:  train Loss: 18.3279   val Loss: 23.7700   time: 438.08s   best: 23.4794
2023-10-13 18:18:54,361:INFO:  Epoch 389/500:  train Loss: 18.1954   val Loss: 26.2950   time: 438.51s   best: 23.4794
2023-10-13 18:26:08,953:INFO:  Epoch 390/500:  train Loss: 18.2179   val Loss: 24.1768   time: 434.54s   best: 23.4794
2023-10-13 18:33:26,909:INFO:  Epoch 391/500:  train Loss: 18.2283   val Loss: 23.9766   time: 437.92s   best: 23.4794
2023-10-13 18:40:46,026:INFO:  Epoch 392/500:  train Loss: 18.1064   val Loss: 24.4008   time: 439.08s   best: 23.4794
2023-10-13 18:48:03,062:INFO:  Epoch 393/500:  train Loss: 18.1624   val Loss: 23.5876   time: 437.01s   best: 23.4794
2023-10-13 18:55:18,332:INFO:  Epoch 394/500:  train Loss: 18.0793   val Loss: 24.0715   time: 435.24s   best: 23.4794
2023-10-13 19:02:33,275:INFO:  Epoch 395/500:  train Loss: 18.6234   val Loss: 24.0066   time: 434.91s   best: 23.4794
2023-10-13 19:09:49,820:INFO:  Epoch 396/500:  train Loss: 18.3213   val Loss: 26.2369   time: 436.52s   best: 23.4794
2023-10-13 19:17:07,349:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 19:17:07,863:INFO:  Epoch 397/500:  train Loss: 18.2484   val Loss: 23.2811   time: 437.49s   best: 23.2811
2023-10-13 19:24:27,945:INFO:  Epoch 398/500:  train Loss: 18.1401   val Loss: 23.3440   time: 440.08s   best: 23.2811
2023-10-13 19:31:44,642:INFO:  Epoch 399/500:  train Loss: 18.1785   val Loss: 24.2086   time: 436.67s   best: 23.2811
2023-10-13 19:38:59,871:INFO:  Epoch 400/500:  train Loss: 18.2773   val Loss: 23.8175   time: 435.15s   best: 23.2811
2023-10-13 19:46:17,677:INFO:  Epoch 401/500:  train Loss: 18.4589   val Loss: 24.5797   time: 437.77s   best: 23.2811
2023-10-13 19:53:37,066:INFO:  Epoch 402/500:  train Loss: 18.1410   val Loss: 23.6241   time: 439.35s   best: 23.2811
2023-10-13 20:00:52,044:INFO:  Epoch 403/500:  train Loss: 18.0159   val Loss: 23.6105   time: 434.95s   best: 23.2811
2023-10-13 20:08:09,168:INFO:  Epoch 404/500:  train Loss: 18.1647   val Loss: 24.3691   time: 437.11s   best: 23.2811
2023-10-13 20:15:23,208:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 20:15:23,243:INFO:  Epoch 405/500:  train Loss: 18.2356   val Loss: 23.2208   time: 433.99s   best: 23.2208
2023-10-13 20:22:38,211:INFO:  Epoch 406/500:  train Loss: 18.1050   val Loss: 24.2621   time: 434.96s   best: 23.2208
2023-10-13 20:29:57,314:INFO:  Epoch 407/500:  train Loss: 18.0545   val Loss: 23.8654   time: 438.99s   best: 23.2208
2023-10-13 20:37:15,946:INFO:  Epoch 408/500:  train Loss: 18.1031   val Loss: 23.4204   time: 438.62s   best: 23.2208
2023-10-13 20:44:33,982:INFO:  Epoch 409/500:  train Loss: 18.0693   val Loss: 24.1018   time: 438.00s   best: 23.2208
2023-10-13 20:51:53,901:INFO:  Epoch 410/500:  train Loss: 18.0281   val Loss: 24.3267   time: 439.90s   best: 23.2208
2023-10-13 20:59:12,625:INFO:  Epoch 411/500:  train Loss: 18.2067   val Loss: 23.7756   time: 438.71s   best: 23.2208
2023-10-13 21:06:30,354:INFO:  Epoch 412/500:  train Loss: 18.0410   val Loss: 23.6235   time: 437.72s   best: 23.2208
2023-10-13 21:13:48,505:INFO:  Epoch 413/500:  train Loss: 18.1189   val Loss: 23.7990   time: 438.14s   best: 23.2208
2023-10-13 21:21:06,414:INFO:  Epoch 414/500:  train Loss: 18.1206   val Loss: 23.2623   time: 437.88s   best: 23.2208
2023-10-13 21:28:21,089:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (perm dataset + 0.3 dropout)_cb2b.pt
2023-10-13 21:28:21,147:INFO:  Epoch 415/500:  train Loss: 18.1231   val Loss: 23.0780   time: 434.65s   best: 23.0780
2023-10-13 21:35:36,593:INFO:  Epoch 416/500:  train Loss: 18.6155   val Loss: 24.2762   time: 435.43s   best: 23.0780
2023-10-13 21:42:54,061:INFO:  Epoch 417/500:  train Loss: 18.1390   val Loss: 23.9826   time: 437.44s   best: 23.0780
2023-10-13 21:50:13,062:INFO:  Epoch 418/500:  train Loss: 18.0692   val Loss: 23.8927   time: 438.97s   best: 23.0780
2023-10-13 21:57:27,826:INFO:  Epoch 419/500:  train Loss: 18.0187   val Loss: 27.8528   time: 434.70s   best: 23.0780
2023-10-13 22:04:45,543:INFO:  Epoch 420/500:  train Loss: 18.3874   val Loss: 23.2528   time: 437.68s   best: 23.0780
2023-10-13 22:12:04,368:INFO:  Epoch 421/500:  train Loss: 18.0131   val Loss: 23.3526   time: 438.82s   best: 23.0780
2023-10-13 22:19:22,999:INFO:  Epoch 422/500:  train Loss: 18.1163   val Loss: 23.5466   time: 438.61s   best: 23.0780
2023-10-13 22:26:38,227:INFO:  Epoch 423/500:  train Loss: 17.9831   val Loss: 24.0415   time: 435.20s   best: 23.0780
2023-10-13 22:33:53,506:INFO:  Epoch 424/500:  train Loss: 18.0816   val Loss: 28.5020   time: 435.00s   best: 23.0780
2023-10-13 22:41:09,433:INFO:  Epoch 425/500:  train Loss: 18.2744   val Loss: 24.1821   time: 435.89s   best: 23.0780
2023-10-13 22:48:26,349:INFO:  Epoch 426/500:  train Loss: 18.0558   val Loss: 23.4843   time: 436.87s   best: 23.0780
2023-10-13 22:55:40,896:INFO:  Epoch 427/500:  train Loss: 17.9328   val Loss: 23.2759   time: 434.41s   best: 23.0780
2023-10-13 23:02:59,601:INFO:  Epoch 428/500:  train Loss: 17.9547   val Loss: 24.2502   time: 438.70s   best: 23.0780
2023-10-13 23:10:17,642:INFO:  Epoch 429/500:  train Loss: 18.0430   val Loss: 23.6420   time: 438.01s   best: 23.0780
2023-10-13 23:17:35,836:INFO:  Epoch 430/500:  train Loss: 17.9871   val Loss: 23.4611   time: 438.16s   best: 23.0780
2023-10-13 23:24:55,047:INFO:  Epoch 431/500:  train Loss: 18.0080   val Loss: 23.6987   time: 439.19s   best: 23.0780
2023-10-13 23:32:11,688:INFO:  Epoch 432/500:  train Loss: 17.8968   val Loss: 23.8945   time: 436.62s   best: 23.0780
2023-10-13 23:39:29,931:INFO:  Epoch 433/500:  train Loss: 18.1027   val Loss: 23.3585   time: 438.22s   best: 23.0780
2023-10-13 23:46:48,725:INFO:  Epoch 434/500:  train Loss: 18.0813   val Loss: 23.9573   time: 438.75s   best: 23.0780
2023-10-13 23:54:06,730:INFO:  Epoch 435/500:  train Loss: 17.8777   val Loss: 23.6944   time: 437.98s   best: 23.0780
2023-10-14 00:01:23,221:INFO:  Epoch 436/500:  train Loss: 17.9017   val Loss: 23.9967   time: 436.47s   best: 23.0780
2023-10-14 00:08:38,849:INFO:  Epoch 437/500:  train Loss: 18.0840   val Loss: 24.5747   time: 435.61s   best: 23.0780
2023-10-14 00:15:56,292:INFO:  Epoch 438/500:  train Loss: 17.9244   val Loss: 23.8138   time: 437.43s   best: 23.0780
2023-10-14 00:23:13,838:INFO:  Epoch 439/500:  train Loss: 17.8620   val Loss: 24.0768   time: 437.53s   best: 23.0780
2023-10-14 00:30:29,048:INFO:  Epoch 440/500:  train Loss: 17.9436   val Loss: 23.8505   time: 435.16s   best: 23.0780
2023-10-14 00:37:46,758:INFO:  Epoch 441/500:  train Loss: 17.8741   val Loss: 23.8510   time: 437.69s   best: 23.0780
2023-10-14 00:45:04,233:INFO:  Epoch 442/500:  train Loss: 17.8892   val Loss: 23.4658   time: 437.44s   best: 23.0780
2023-10-14 00:52:21,535:INFO:  Epoch 443/500:  train Loss: 18.0741   val Loss: 24.1405   time: 437.27s   best: 23.0780
2023-10-14 00:59:40,698:INFO:  Epoch 444/500:  train Loss: 17.8196   val Loss: 23.5009   time: 439.12s   best: 23.0780
2023-10-14 01:06:55,768:INFO:  Epoch 445/500:  train Loss: 17.8544   val Loss: 23.5809   time: 435.03s   best: 23.0780
2023-10-14 01:14:10,469:INFO:  Epoch 446/500:  train Loss: 17.8197   val Loss: 24.2402   time: 434.66s   best: 23.0780
2023-10-14 01:21:30,810:INFO:  Epoch 447/500:  train Loss: 17.8173   val Loss: 23.8257   time: 440.30s   best: 23.0780
2023-10-14 01:28:50,109:INFO:  Epoch 448/500:  train Loss: 17.9191   val Loss: 24.1272   time: 439.26s   best: 23.0780
2023-10-14 01:36:08,621:INFO:  Epoch 449/500:  train Loss: 17.8256   val Loss: 23.6466   time: 438.45s   best: 23.0780
2023-10-14 01:43:27,187:INFO:  Epoch 450/500:  train Loss: 17.7684   val Loss: 24.4413   time: 438.55s   best: 23.0780
2023-10-14 01:50:46,997:INFO:  Epoch 451/500:  train Loss: 17.6974   val Loss: 24.0529   time: 439.79s   best: 23.0780
2023-10-14 01:58:05,711:INFO:  Epoch 452/500:  train Loss: 17.7382   val Loss: 23.9353   time: 438.69s   best: 23.0780
2023-10-14 02:05:19,625:INFO:  Epoch 453/500:  train Loss: 17.8067   val Loss: 23.8219   time: 433.87s   best: 23.0780
2023-10-14 02:12:39,100:INFO:  Epoch 454/500:  train Loss: 17.9730   val Loss: 23.9828   time: 439.40s   best: 23.0780
2023-10-14 02:19:54,763:INFO:  Epoch 455/500:  train Loss: 17.7305   val Loss: 23.6651   time: 435.64s   best: 23.0780
2023-10-14 02:27:09,881:INFO:  Epoch 456/500:  train Loss: 17.8633   val Loss: 23.7087   time: 435.08s   best: 23.0780
2023-10-14 02:34:27,935:INFO:  Epoch 457/500:  train Loss: 17.7293   val Loss: 24.3558   time: 438.05s   best: 23.0780
2023-10-14 02:41:47,696:INFO:  Epoch 458/500:  train Loss: 17.8645   val Loss: 24.0086   time: 439.75s   best: 23.0780
2023-10-14 02:49:04,131:INFO:  Epoch 459/500:  train Loss: 17.7407   val Loss: 26.2765   time: 436.41s   best: 23.0780
2023-10-14 02:56:21,603:INFO:  Epoch 460/500:  train Loss: 17.9247   val Loss: 23.7945   time: 437.43s   best: 23.0780
2023-10-14 03:03:41,584:INFO:  Epoch 461/500:  train Loss: 17.8961   val Loss: 23.8323   time: 439.93s   best: 23.0780
2023-10-14 03:11:01,273:INFO:  Epoch 462/500:  train Loss: 17.6614   val Loss: 23.6375   time: 439.64s   best: 23.0780
2023-10-14 03:18:16,267:INFO:  Epoch 463/500:  train Loss: 17.8170   val Loss: 23.5054   time: 434.98s   best: 23.0780
2023-10-14 03:25:34,995:INFO:  Epoch 464/500:  train Loss: 17.6445   val Loss: 24.3023   time: 438.67s   best: 23.0780
2023-10-14 03:32:50,855:INFO:  Epoch 465/500:  train Loss: 17.6466   val Loss: 23.5971   time: 435.84s   best: 23.0780
2023-10-14 03:40:09,703:INFO:  Epoch 466/500:  train Loss: 17.7400   val Loss: 23.7559   time: 438.82s   best: 23.0780
2023-10-14 03:47:27,369:INFO:  Epoch 467/500:  train Loss: 17.8256   val Loss: 23.7819   time: 437.64s   best: 23.0780
2023-10-14 03:54:46,643:INFO:  Epoch 468/500:  train Loss: 17.7818   val Loss: 23.8297   time: 439.26s   best: 23.0780
2023-10-14 04:02:05,654:INFO:  Epoch 469/500:  train Loss: 18.1218   val Loss: 24.1513   time: 439.00s   best: 23.0780
2023-10-14 04:09:20,776:INFO:  Epoch 470/500:  train Loss: 17.7697   val Loss: 23.8323   time: 435.10s   best: 23.0780
2023-10-14 04:16:37,056:INFO:  Epoch 471/500:  train Loss: 17.7044   val Loss: 24.4864   time: 436.27s   best: 23.0780
2023-10-14 04:23:53,068:INFO:  Epoch 472/500:  train Loss: 17.8347   val Loss: 23.1793   time: 435.98s   best: 23.0780
2023-10-14 04:31:11,177:INFO:  Epoch 473/500:  train Loss: 17.6375   val Loss: 23.5797   time: 438.08s   best: 23.0780
2023-10-14 04:38:25,539:INFO:  Epoch 474/500:  train Loss: 17.6887   val Loss: 23.5195   time: 434.33s   best: 23.0780
2023-10-14 04:45:39,198:INFO:  Epoch 475/500:  train Loss: 17.6880   val Loss: 23.7138   time: 433.63s   best: 23.0780
2023-10-14 04:52:54,847:INFO:  Epoch 476/500:  train Loss: 17.7174   val Loss: 23.7664   time: 435.63s   best: 23.0780
2023-10-14 05:00:13,296:INFO:  Epoch 477/500:  train Loss: 18.1573   val Loss: 23.3556   time: 438.43s   best: 23.0780
2023-10-14 05:07:28,884:INFO:  Epoch 478/500:  train Loss: 17.7729   val Loss: 24.7392   time: 435.55s   best: 23.0780
2023-10-14 05:14:45,102:INFO:  Epoch 479/500:  train Loss: 17.6029   val Loss: 23.6733   time: 436.20s   best: 23.0780
2023-10-14 05:22:04,423:INFO:  Epoch 480/500:  train Loss: 17.7585   val Loss: 24.7974   time: 439.31s   best: 23.0780
2023-10-14 05:29:18,639:INFO:  Epoch 481/500:  train Loss: 17.9990   val Loss: 23.9394   time: 434.18s   best: 23.0780
2023-10-14 05:36:35,046:INFO:  Epoch 482/500:  train Loss: 17.7529   val Loss: 23.5934   time: 436.40s   best: 23.0780
2023-10-14 05:43:50,398:INFO:  Epoch 483/500:  train Loss: 17.6391   val Loss: 23.5936   time: 435.33s   best: 23.0780
2023-10-14 05:51:08,081:INFO:  Epoch 484/500:  train Loss: 17.5862   val Loss: 23.8980   time: 437.65s   best: 23.0780
2023-10-14 05:58:22,944:INFO:  Epoch 485/500:  train Loss: 17.5720   val Loss: 23.7554   time: 434.84s   best: 23.0780
2023-10-14 06:05:39,284:INFO:  Epoch 486/500:  train Loss: 17.6836   val Loss: 24.6733   time: 436.31s   best: 23.0780
2023-10-14 06:12:56,437:INFO:  Epoch 487/500:  train Loss: 17.6158   val Loss: 24.0159   time: 437.12s   best: 23.0780
2023-10-14 06:20:14,785:INFO:  Epoch 488/500:  train Loss: 17.7747   val Loss: 23.6533   time: 438.34s   best: 23.0780
2023-10-14 06:27:30,742:INFO:  Epoch 489/500:  train Loss: 17.8081   val Loss: 24.2149   time: 435.94s   best: 23.0780
2023-10-14 06:34:51,865:INFO:  Epoch 490/500:  train Loss: 17.6785   val Loss: 23.1868   time: 441.09s   best: 23.0780
2023-10-14 06:42:09,380:INFO:  Epoch 491/500:  train Loss: 17.7863   val Loss: 23.4314   time: 437.50s   best: 23.0780
2023-10-14 06:49:27,608:INFO:  Epoch 492/500:  train Loss: 17.7183   val Loss: 23.6230   time: 438.20s   best: 23.0780
2023-10-14 06:56:43,257:INFO:  Epoch 493/500:  train Loss: 17.7354   val Loss: 23.7761   time: 435.64s   best: 23.0780
2023-10-14 07:04:00,792:INFO:  Epoch 494/500:  train Loss: 17.5846   val Loss: 23.6559   time: 437.50s   best: 23.0780
2023-10-14 07:11:16,126:INFO:  Epoch 495/500:  train Loss: 17.5908   val Loss: 27.3556   time: 435.32s   best: 23.0780
2023-10-14 07:18:32,444:INFO:  Epoch 496/500:  train Loss: 17.6528   val Loss: 23.9680   time: 436.30s   best: 23.0780
2023-10-14 07:25:52,751:INFO:  Epoch 497/500:  train Loss: 17.6443   val Loss: 23.5383   time: 440.28s   best: 23.0780
2023-10-14 07:33:06,925:INFO:  Epoch 498/500:  train Loss: 17.5678   val Loss: 23.9552   time: 434.15s   best: 23.0780
2023-10-14 07:40:22,746:INFO:  Epoch 499/500:  train Loss: 17.6613   val Loss: 23.6850   time: 435.81s   best: 23.0780
2023-10-14 07:47:38,274:INFO:  Epoch 500/500:  train Loss: 17.6567   val Loss: 23.5859   time: 435.50s   best: 23.0780
2023-10-14 07:47:38,297:INFO:  -----> Training complete in 3656m 19s   best validation loss: 23.0780
 
2023-10-15 10:05:45,614:INFO:  Starting experiment lstm autoencoder Debug (2 layer + 0.2 dropout)
2023-10-15 10:05:45,623:INFO:  Defining the model
2023-10-15 10:05:45,667:INFO:  Reading the dataset
2023-10-15 10:05:52,039:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:05:52,060:INFO:  Epoch 1/500:  train Loss: 98.6569   val Loss: 96.4529   time: 1.46s   best: 96.4529
2023-10-15 10:05:52,317:INFO:  Epoch 2/500:  train Loss: 98.4505   val Loss: 99.6226   time: 0.25s   best: 96.4529
2023-10-15 10:05:52,621:INFO:  Epoch 3/500:  train Loss: 99.0749   val Loss: 100.0379   time: 0.30s   best: 96.4529
2023-10-15 10:05:52,862:INFO:  Epoch 4/500:  train Loss: 98.9858   val Loss: 99.6594   time: 0.24s   best: 96.4529
2023-10-15 10:05:53,151:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:05:53,171:INFO:  Epoch 5/500:  train Loss: 98.3538   val Loss: 96.3505   time: 0.26s   best: 96.3505
2023-10-15 10:05:53,415:INFO:  Epoch 6/500:  train Loss: 97.8690   val Loss: 96.4543   time: 0.24s   best: 96.3505
2023-10-15 10:05:53,679:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:05:53,803:INFO:  Epoch 7/500:  train Loss: 96.9572   val Loss: 93.7107   time: 0.26s   best: 93.7107
2023-10-15 10:05:54,047:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:05:54,067:INFO:  Epoch 8/500:  train Loss: 96.3264   val Loss: 93.0173   time: 0.24s   best: 93.0173
2023-10-15 10:05:54,325:INFO:  Epoch 9/500:  train Loss: 96.3486   val Loss: 96.6239   time: 0.25s   best: 93.0173
2023-10-15 10:05:54,566:INFO:  Epoch 10/500:  train Loss: 96.7995   val Loss: 96.4581   time: 0.24s   best: 93.0173
2023-10-15 10:05:54,872:INFO:  Epoch 11/500:  train Loss: 95.3713   val Loss: 95.0721   time: 0.30s   best: 93.0173
2023-10-15 10:05:55,115:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:05:55,243:INFO:  Epoch 12/500:  train Loss: 93.9705   val Loss: 91.8461   time: 0.24s   best: 91.8461
2023-10-15 10:05:55,487:INFO:  Epoch 13/500:  train Loss: 93.7293   val Loss: 92.1765   time: 0.24s   best: 91.8461
2023-10-15 10:05:55,748:INFO:  Epoch 14/500:  train Loss: 94.1294   val Loss: 94.0495   time: 0.26s   best: 91.8461
2023-10-15 10:05:55,991:INFO:  Epoch 15/500:  train Loss: 94.2770   val Loss: 94.3067   time: 0.24s   best: 91.8461
2023-10-15 10:05:56,234:INFO:  Epoch 16/500:  train Loss: 94.1041   val Loss: 94.4584   time: 0.24s   best: 91.8461
2023-10-15 10:05:56,498:INFO:  Epoch 17/500:  train Loss: 94.2866   val Loss: 94.3773   time: 0.26s   best: 91.8461
2023-10-15 10:05:56,802:INFO:  Epoch 18/500:  train Loss: 94.0962   val Loss: 94.2529   time: 0.30s   best: 91.8461
2023-10-15 10:05:57,043:INFO:  Epoch 19/500:  train Loss: 94.0202   val Loss: 94.2079   time: 0.24s   best: 91.8461
2023-10-15 10:05:57,285:INFO:  Epoch 20/500:  train Loss: 94.4402   val Loss: 94.2838   time: 0.24s   best: 91.8461
2023-10-15 10:05:57,549:INFO:  Epoch 21/500:  train Loss: 94.6129   val Loss: 94.3461   time: 0.26s   best: 91.8461
2023-10-15 10:05:57,791:INFO:  Epoch 22/500:  train Loss: 94.6025   val Loss: 94.2498   time: 0.24s   best: 91.8461
2023-10-15 10:05:58,052:INFO:  Epoch 23/500:  train Loss: 94.2512   val Loss: 94.1762   time: 0.26s   best: 91.8461
2023-10-15 10:05:58,294:INFO:  Epoch 24/500:  train Loss: 94.1747   val Loss: 94.1204   time: 0.24s   best: 91.8461
2023-10-15 10:05:58,558:INFO:  Epoch 25/500:  train Loss: 93.7219   val Loss: 94.0958   time: 0.26s   best: 91.8461
2023-10-15 10:05:58,844:INFO:  Epoch 26/500:  train Loss: 94.7522   val Loss: 94.1843   time: 0.28s   best: 91.8461
2023-10-15 10:05:59,105:INFO:  Epoch 27/500:  train Loss: 94.1833   val Loss: 94.1761   time: 0.26s   best: 91.8461
2023-10-15 10:05:59,347:INFO:  Epoch 28/500:  train Loss: 94.2267   val Loss: 94.1450   time: 0.24s   best: 91.8461
2023-10-15 10:05:59,609:INFO:  Epoch 29/500:  train Loss: 94.3190   val Loss: 94.1277   time: 0.26s   best: 91.8461
2023-10-15 10:05:59,851:INFO:  Epoch 30/500:  train Loss: 94.2445   val Loss: 94.0946   time: 0.24s   best: 91.8461
2023-10-15 10:06:00,113:INFO:  Epoch 31/500:  train Loss: 93.8899   val Loss: 94.0627   time: 0.26s   best: 91.8461
2023-10-15 10:06:00,355:INFO:  Epoch 32/500:  train Loss: 94.3340   val Loss: 94.0833   time: 0.24s   best: 91.8461
2023-10-15 10:06:00,621:INFO:  Epoch 33/500:  train Loss: 94.1754   val Loss: 93.9548   time: 0.26s   best: 91.8461
2023-10-15 10:06:00,903:INFO:  Epoch 34/500:  train Loss: 93.7638   val Loss: 93.8703   time: 0.28s   best: 91.8461
2023-10-15 10:06:01,164:INFO:  Epoch 35/500:  train Loss: 93.9673   val Loss: 93.8431   time: 0.26s   best: 91.8461
2023-10-15 10:06:01,407:INFO:  Epoch 36/500:  train Loss: 94.3209   val Loss: 93.7779   time: 0.24s   best: 91.8461
2023-10-15 10:06:01,669:INFO:  Epoch 37/500:  train Loss: 93.6932   val Loss: 93.6385   time: 0.26s   best: 91.8461
2023-10-15 10:06:01,911:INFO:  Epoch 38/500:  train Loss: 93.6506   val Loss: 93.3929   time: 0.24s   best: 91.8461
2023-10-15 10:06:02,172:INFO:  Epoch 39/500:  train Loss: 93.3812   val Loss: 92.9326   time: 0.26s   best: 91.8461
2023-10-15 10:06:02,415:INFO:  Epoch 40/500:  train Loss: 92.8337   val Loss: 92.6010   time: 0.24s   best: 91.8461
2023-10-15 10:06:02,687:INFO:  Epoch 41/500:  train Loss: 92.3421   val Loss: 92.0442   time: 0.27s   best: 91.8461
2023-10-15 10:06:02,971:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:03,061:INFO:  Epoch 42/500:  train Loss: 92.1156   val Loss: 91.5221   time: 0.28s   best: 91.5221
2023-10-15 10:06:03,322:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:03,384:INFO:  Epoch 43/500:  train Loss: 91.5470   val Loss: 91.2506   time: 0.26s   best: 91.2506
2023-10-15 10:06:03,628:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:03,718:INFO:  Epoch 44/500:  train Loss: 91.9250   val Loss: 90.8928   time: 0.24s   best: 90.8928
2023-10-15 10:06:03,962:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:03,982:INFO:  Epoch 45/500:  train Loss: 90.6868   val Loss: 90.0675   time: 0.24s   best: 90.0675
2023-10-15 10:06:04,241:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:04,341:INFO:  Epoch 46/500:  train Loss: 90.5581   val Loss: 89.7583   time: 0.25s   best: 89.7583
2023-10-15 10:06:04,585:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:04,690:INFO:  Epoch 47/500:  train Loss: 89.8635   val Loss: 89.3860   time: 0.24s   best: 89.3860
2023-10-15 10:06:04,934:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:04,988:INFO:  Epoch 48/500:  train Loss: 89.6998   val Loss: 88.7463   time: 0.24s   best: 88.7463
2023-10-15 10:06:05,264:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:05,392:INFO:  Epoch 49/500:  train Loss: 88.6649   val Loss: 87.9827   time: 0.27s   best: 87.9827
2023-10-15 10:06:05,636:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:05,656:INFO:  Epoch 50/500:  train Loss: 88.5816   val Loss: 87.9331   time: 0.24s   best: 87.9331
2023-10-15 10:06:05,914:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:06,068:INFO:  Epoch 51/500:  train Loss: 88.1033   val Loss: 87.1444   time: 0.25s   best: 87.1444
2023-10-15 10:06:06,327:INFO:  Epoch 52/500:  train Loss: 87.6323   val Loss: 88.9413   time: 0.26s   best: 87.1444
2023-10-15 10:06:06,569:INFO:  Epoch 53/500:  train Loss: 90.3932   val Loss: 90.6494   time: 0.24s   best: 87.1444
2023-10-15 10:06:06,913:INFO:  Epoch 54/500:  train Loss: 90.4585   val Loss: 89.9168   time: 0.34s   best: 87.1444
2023-10-15 10:06:07,194:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:07,266:INFO:  Epoch 55/500:  train Loss: 88.5975   val Loss: 87.1226   time: 0.28s   best: 87.1226
2023-10-15 10:06:07,509:INFO:  Epoch 56/500:  train Loss: 87.8080   val Loss: 87.5041   time: 0.24s   best: 87.1226
2023-10-15 10:06:07,751:INFO:  Epoch 57/500:  train Loss: 88.6610   val Loss: 88.5782   time: 0.24s   best: 87.1226
2023-10-15 10:06:08,011:INFO:  Epoch 58/500:  train Loss: 88.5983   val Loss: 88.3766   time: 0.26s   best: 87.1226
2023-10-15 10:06:08,254:INFO:  Epoch 59/500:  train Loss: 88.2956   val Loss: 87.3641   time: 0.24s   best: 87.1226
2023-10-15 10:06:08,516:INFO:  Epoch 60/500:  train Loss: 87.3388   val Loss: 88.1386   time: 0.26s   best: 87.1226
2023-10-15 10:06:08,768:INFO:  Epoch 61/500:  train Loss: 88.5221   val Loss: 88.3395   time: 0.25s   best: 87.1226
2023-10-15 10:06:09,030:INFO:  Epoch 62/500:  train Loss: 88.6841   val Loss: 88.0461   time: 0.26s   best: 87.1226
2023-10-15 10:06:09,313:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:09,360:INFO:  Epoch 63/500:  train Loss: 87.9063   val Loss: 87.0352   time: 0.28s   best: 87.0352
2023-10-15 10:06:09,603:INFO:  Epoch 64/500:  train Loss: 87.6349   val Loss: 87.2484   time: 0.24s   best: 87.0352
2023-10-15 10:06:09,844:INFO:  Epoch 65/500:  train Loss: 87.4893   val Loss: 87.2342   time: 0.24s   best: 87.0352
2023-10-15 10:06:10,105:INFO:  Epoch 66/500:  train Loss: 87.2862   val Loss: 87.4041   time: 0.26s   best: 87.0352
2023-10-15 10:06:10,347:INFO:  Epoch 67/500:  train Loss: 87.6788   val Loss: 87.4442   time: 0.24s   best: 87.0352
2023-10-15 10:06:10,610:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:10,787:INFO:  Epoch 68/500:  train Loss: 87.4165   val Loss: 86.8813   time: 0.26s   best: 86.8813
2023-10-15 10:06:11,045:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:11,075:INFO:  Epoch 69/500:  train Loss: 87.2864   val Loss: 86.6074   time: 0.25s   best: 86.6074
2023-10-15 10:06:11,358:INFO:  Epoch 70/500:  train Loss: 87.0331   val Loss: 86.7612   time: 0.28s   best: 86.6074
2023-10-15 10:06:11,619:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:11,732:INFO:  Epoch 71/500:  train Loss: 86.8849   val Loss: 86.2347   time: 0.26s   best: 86.2347
2023-10-15 10:06:11,992:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:12,056:INFO:  Epoch 72/500:  train Loss: 86.8687   val Loss: 85.9006   time: 0.26s   best: 85.9006
2023-10-15 10:06:12,301:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:12,349:INFO:  Epoch 73/500:  train Loss: 86.7847   val Loss: 85.6167   time: 0.24s   best: 85.6167
2023-10-15 10:06:12,607:INFO:  Epoch 74/500:  train Loss: 86.3074   val Loss: 85.6394   time: 0.25s   best: 85.6167
2023-10-15 10:06:12,857:INFO:  Epoch 75/500:  train Loss: 86.5390   val Loss: 85.6528   time: 0.25s   best: 85.6167
2023-10-15 10:06:13,122:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:13,225:INFO:  Epoch 76/500:  train Loss: 85.8181   val Loss: 85.3214   time: 0.26s   best: 85.3214
2023-10-15 10:06:13,508:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:13,577:INFO:  Epoch 77/500:  train Loss: 86.1599   val Loss: 85.2449   time: 0.28s   best: 85.2449
2023-10-15 10:06:13,820:INFO:  Epoch 78/500:  train Loss: 85.6009   val Loss: 85.3574   time: 0.24s   best: 85.2449
2023-10-15 10:06:14,080:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:14,205:INFO:  Epoch 79/500:  train Loss: 86.0110   val Loss: 84.8042   time: 0.26s   best: 84.8042
2023-10-15 10:06:14,450:INFO:  Epoch 80/500:  train Loss: 85.3449   val Loss: 85.2762   time: 0.24s   best: 84.8042
2023-10-15 10:06:14,712:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:15,076:INFO:  Epoch 81/500:  train Loss: 86.1187   val Loss: 84.5650   time: 0.26s   best: 84.5650
2023-10-15 10:06:15,320:INFO:  Epoch 82/500:  train Loss: 84.8747   val Loss: 84.8892   time: 0.24s   best: 84.5650
2023-10-15 10:06:15,617:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:15,658:INFO:  Epoch 83/500:  train Loss: 85.4964   val Loss: 84.3435   time: 0.29s   best: 84.3435
2023-10-15 10:06:15,903:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:16,000:INFO:  Epoch 84/500:  train Loss: 84.9063   val Loss: 84.2131   time: 0.24s   best: 84.2131
2023-10-15 10:06:16,257:INFO:  Epoch 85/500:  train Loss: 84.5642   val Loss: 84.8442   time: 0.25s   best: 84.2131
2023-10-15 10:06:16,503:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:16,570:INFO:  Epoch 86/500:  train Loss: 84.9507   val Loss: 83.9623   time: 0.24s   best: 83.9623
2023-10-15 10:06:16,838:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:16,908:INFO:  Epoch 87/500:  train Loss: 84.9905   val Loss: 83.8089   time: 0.26s   best: 83.8089
2023-10-15 10:06:17,171:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:17,208:INFO:  Epoch 88/500:  train Loss: 84.5769   val Loss: 83.3644   time: 0.26s   best: 83.3644
2023-10-15 10:06:17,480:INFO:  Epoch 89/500:  train Loss: 85.0282   val Loss: 83.7328   time: 0.27s   best: 83.3644
2023-10-15 10:06:17,785:INFO:  Epoch 90/500:  train Loss: 85.3521   val Loss: 85.0900   time: 0.30s   best: 83.3644
2023-10-15 10:06:18,032:INFO:  Epoch 91/500:  train Loss: 85.0474   val Loss: 84.0405   time: 0.24s   best: 83.3644
2023-10-15 10:06:18,301:INFO:  Epoch 92/500:  train Loss: 84.5836   val Loss: 84.4520   time: 0.27s   best: 83.3644
2023-10-15 10:06:18,554:INFO:  Epoch 93/500:  train Loss: 84.8037   val Loss: 84.2279   time: 0.25s   best: 83.3644
2023-10-15 10:06:18,821:INFO:  Epoch 94/500:  train Loss: 83.9483   val Loss: 83.7153   time: 0.26s   best: 83.3644
2023-10-15 10:06:19,068:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:19,133:INFO:  Epoch 95/500:  train Loss: 84.5015   val Loss: 83.3285   time: 0.24s   best: 83.3285
2023-10-15 10:06:19,396:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:19,477:INFO:  Epoch 96/500:  train Loss: 83.4120   val Loss: 83.1557   time: 0.26s   best: 83.1557
2023-10-15 10:06:19,775:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:19,859:INFO:  Epoch 97/500:  train Loss: 83.5254   val Loss: 82.7722   time: 0.29s   best: 82.7722
2023-10-15 10:06:20,103:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:20,238:INFO:  Epoch 98/500:  train Loss: 83.2804   val Loss: 82.5577   time: 0.24s   best: 82.5577
2023-10-15 10:06:20,493:INFO:  Epoch 99/500:  train Loss: 83.4782   val Loss: 83.4437   time: 0.25s   best: 82.5577
2023-10-15 10:06:20,830:INFO:  Epoch 100/500:  train Loss: 83.9690   val Loss: 82.7881   time: 0.31s   best: 82.5577
2023-10-15 10:06:21,083:INFO:  Epoch 101/500:  train Loss: 88.5159   val Loss: 83.1941   time: 0.24s   best: 82.5577
2023-10-15 10:06:21,360:INFO:  Epoch 102/500:  train Loss: 88.8218   val Loss: 93.2126   time: 0.27s   best: 82.5577
2023-10-15 10:06:21,605:INFO:  Epoch 103/500:  train Loss: 94.1453   val Loss: 93.9022   time: 0.24s   best: 82.5577
2023-10-15 10:06:21,923:INFO:  Epoch 104/500:  train Loss: 93.1341   val Loss: 91.8417   time: 0.31s   best: 82.5577
2023-10-15 10:06:22,173:INFO:  Epoch 105/500:  train Loss: 90.7971   val Loss: 89.6713   time: 0.24s   best: 82.5577
2023-10-15 10:06:22,446:INFO:  Epoch 106/500:  train Loss: 89.6358   val Loss: 89.6356   time: 0.26s   best: 82.5577
2023-10-15 10:06:22,701:INFO:  Epoch 107/500:  train Loss: 89.7397   val Loss: 89.7470   time: 0.24s   best: 82.5577
2023-10-15 10:06:22,971:INFO:  Epoch 108/500:  train Loss: 89.5208   val Loss: 88.9982   time: 0.26s   best: 82.5577
2023-10-15 10:06:23,221:INFO:  Epoch 109/500:  train Loss: 88.9105   val Loss: 88.2639   time: 0.24s   best: 82.5577
2023-10-15 10:06:23,492:INFO:  Epoch 110/500:  train Loss: 88.1871   val Loss: 87.7591   time: 0.26s   best: 82.5577
2023-10-15 10:06:23,742:INFO:  Epoch 111/500:  train Loss: 87.6356   val Loss: 87.0716   time: 0.24s   best: 82.5577
2023-10-15 10:06:24,047:INFO:  Epoch 112/500:  train Loss: 86.7531   val Loss: 86.1068   time: 0.29s   best: 82.5577
2023-10-15 10:06:24,296:INFO:  Epoch 113/500:  train Loss: 86.0921   val Loss: 85.1577   time: 0.24s   best: 82.5577
2023-10-15 10:06:24,567:INFO:  Epoch 114/500:  train Loss: 85.1237   val Loss: 84.3540   time: 0.26s   best: 82.5577
2023-10-15 10:06:24,823:INFO:  Epoch 115/500:  train Loss: 84.4524   val Loss: 83.3248   time: 0.25s   best: 82.5577
2023-10-15 10:06:25,091:INFO:  Epoch 116/500:  train Loss: 83.9132   val Loss: 83.5269   time: 0.26s   best: 82.5577
2023-10-15 10:06:25,339:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:25,367:INFO:  Epoch 117/500:  train Loss: 83.8632   val Loss: 82.3966   time: 0.24s   best: 82.3966
2023-10-15 10:06:25,631:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:25,789:INFO:  Epoch 118/500:  train Loss: 83.0364   val Loss: 82.1110   time: 0.26s   best: 82.1110
2023-10-15 10:06:26,083:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:26,111:INFO:  Epoch 119/500:  train Loss: 82.2947   val Loss: 81.7568   time: 0.29s   best: 81.7568
2023-10-15 10:06:26,355:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:26,549:INFO:  Epoch 120/500:  train Loss: 82.2884   val Loss: 81.3706   time: 0.24s   best: 81.3706
2023-10-15 10:06:26,800:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:27,105:INFO:  Epoch 121/500:  train Loss: 81.9533   val Loss: 81.1356   time: 0.25s   best: 81.1356
2023-10-15 10:06:27,358:INFO:  Epoch 122/500:  train Loss: 81.7275   val Loss: 81.3342   time: 0.24s   best: 81.1356
2023-10-15 10:06:27,620:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:27,652:INFO:  Epoch 123/500:  train Loss: 81.6759   val Loss: 80.8293   time: 0.26s   best: 80.8293
2023-10-15 10:06:27,898:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:28,006:INFO:  Epoch 124/500:  train Loss: 81.3260   val Loss: 80.7763   time: 0.24s   best: 80.7763
2023-10-15 10:06:28,274:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:28,303:INFO:  Epoch 125/500:  train Loss: 81.3250   val Loss: 80.3800   time: 0.26s   best: 80.3800
2023-10-15 10:06:28,555:INFO:  Epoch 126/500:  train Loss: 81.3178   val Loss: 81.3387   time: 0.24s   best: 80.3800
2023-10-15 10:06:28,830:INFO:  Epoch 127/500:  train Loss: 80.9189   val Loss: 81.1110   time: 0.26s   best: 80.3800
2023-10-15 10:06:29,081:INFO:  Epoch 128/500:  train Loss: 81.1582   val Loss: 80.5206   time: 0.24s   best: 80.3800
2023-10-15 10:06:29,353:INFO:  Epoch 129/500:  train Loss: 81.2938   val Loss: 81.8350   time: 0.26s   best: 80.3800
2023-10-15 10:06:29,606:INFO:  Epoch 130/500:  train Loss: 81.5208   val Loss: 80.9852   time: 0.24s   best: 80.3800
2023-10-15 10:06:29,874:INFO:  Epoch 131/500:  train Loss: 81.5091   val Loss: 80.5131   time: 0.26s   best: 80.3800
2023-10-15 10:06:30,162:INFO:  Epoch 132/500:  train Loss: 82.0178   val Loss: 81.0667   time: 0.28s   best: 80.3800
2023-10-15 10:06:30,425:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:30,501:INFO:  Epoch 133/500:  train Loss: 80.8368   val Loss: 80.2971   time: 0.26s   best: 80.2971
2023-10-15 10:06:30,768:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:30,851:INFO:  Epoch 134/500:  train Loss: 80.7861   val Loss: 79.9614   time: 0.26s   best: 79.9614
2023-10-15 10:06:31,097:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:31,127:INFO:  Epoch 135/500:  train Loss: 80.8754   val Loss: 79.6231   time: 0.24s   best: 79.6231
2023-10-15 10:06:31,390:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:31,546:INFO:  Epoch 136/500:  train Loss: 80.5379   val Loss: 79.6057   time: 0.26s   best: 79.6057
2023-10-15 10:06:31,805:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:31,833:INFO:  Epoch 137/500:  train Loss: 79.9356   val Loss: 79.3268   time: 0.25s   best: 79.3268
2023-10-15 10:06:32,107:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:32,209:INFO:  Epoch 138/500:  train Loss: 80.5635   val Loss: 79.2364   time: 0.26s   best: 79.2364
2023-10-15 10:06:32,474:INFO:  Epoch 139/500:  train Loss: 80.3447   val Loss: 79.6535   time: 0.25s   best: 79.2364
2023-10-15 10:06:32,729:INFO:  Epoch 140/500:  train Loss: 80.4283   val Loss: 79.5735   time: 0.24s   best: 79.2364
2023-10-15 10:06:32,999:INFO:  Epoch 141/500:  train Loss: 80.7494   val Loss: 79.3334   time: 0.26s   best: 79.2364
2023-10-15 10:06:33,246:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:33,319:INFO:  Epoch 142/500:  train Loss: 80.0632   val Loss: 79.1859   time: 0.24s   best: 79.1859
2023-10-15 10:06:33,573:INFO:  Epoch 143/500:  train Loss: 79.6970   val Loss: 79.6028   time: 0.24s   best: 79.1859
2023-10-15 10:06:33,840:INFO:  Epoch 144/500:  train Loss: 80.9353   val Loss: 81.4036   time: 0.26s   best: 79.1859
2023-10-15 10:06:34,090:INFO:  Epoch 145/500:  train Loss: 81.1951   val Loss: 80.5668   time: 0.24s   best: 79.1859
2023-10-15 10:06:34,388:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:34,439:INFO:  Epoch 146/500:  train Loss: 80.2708   val Loss: 78.7715   time: 0.29s   best: 78.7715
2023-10-15 10:06:34,693:INFO:  Epoch 147/500:  train Loss: 80.5015   val Loss: 79.2544   time: 0.24s   best: 78.7715
2023-10-15 10:06:34,962:INFO:  Epoch 148/500:  train Loss: 80.2692   val Loss: 79.2036   time: 0.26s   best: 78.7715
2023-10-15 10:06:35,212:INFO:  Epoch 149/500:  train Loss: 80.0567   val Loss: 79.6450   time: 0.24s   best: 78.7715
2023-10-15 10:06:35,478:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:35,628:INFO:  Epoch 150/500:  train Loss: 78.8528   val Loss: 78.7051   time: 0.26s   best: 78.7051
2023-10-15 10:06:35,873:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:35,933:INFO:  Epoch 151/500:  train Loss: 78.9491   val Loss: 78.1873   time: 0.24s   best: 78.1873
2023-10-15 10:06:36,184:INFO:  Epoch 152/500:  train Loss: 79.8104   val Loss: 78.6737   time: 0.24s   best: 78.1873
2023-10-15 10:06:36,488:INFO:  Epoch 153/500:  train Loss: 79.6099   val Loss: 78.8019   time: 0.29s   best: 78.1873
2023-10-15 10:06:36,746:INFO:  Epoch 154/500:  train Loss: 80.0247   val Loss: 78.8635   time: 0.25s   best: 78.1873
2023-10-15 10:06:37,021:INFO:  Epoch 155/500:  train Loss: 79.1311   val Loss: 78.6389   time: 0.26s   best: 78.1873
2023-10-15 10:06:37,271:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:37,489:INFO:  Epoch 156/500:  train Loss: 79.0444   val Loss: 78.0220   time: 0.25s   best: 78.0220
2023-10-15 10:06:37,743:INFO:  Epoch 157/500:  train Loss: 79.9298   val Loss: 82.5515   time: 0.24s   best: 78.0220
2023-10-15 10:06:38,014:INFO:  Epoch 158/500:  train Loss: 81.0310   val Loss: 81.7565   time: 0.24s   best: 78.0220
2023-10-15 10:06:38,266:INFO:  Epoch 159/500:  train Loss: 81.7729   val Loss: 79.7627   time: 0.24s   best: 78.0220
2023-10-15 10:06:38,571:INFO:  Epoch 160/500:  train Loss: 81.2883   val Loss: 82.0950   time: 0.29s   best: 78.0220
2023-10-15 10:06:38,825:INFO:  Epoch 161/500:  train Loss: 80.8780   val Loss: 78.8726   time: 0.24s   best: 78.0220
2023-10-15 10:06:39,097:INFO:  Epoch 162/500:  train Loss: 79.1241   val Loss: 78.8668   time: 0.26s   best: 78.0220
2023-10-15 10:06:39,341:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:39,427:INFO:  Epoch 163/500:  train Loss: 79.7266   val Loss: 77.9373   time: 0.24s   best: 77.9373
2023-10-15 10:06:39,690:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:39,829:INFO:  Epoch 164/500:  train Loss: 79.0550   val Loss: 77.7945   time: 0.26s   best: 77.7945
2023-10-15 10:06:40,098:INFO:  Epoch 165/500:  train Loss: 78.9819   val Loss: 78.0940   time: 0.24s   best: 77.7945
2023-10-15 10:06:40,342:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:40,425:INFO:  Epoch 166/500:  train Loss: 78.7933   val Loss: 77.7448   time: 0.24s   best: 77.7448
2023-10-15 10:06:40,708:INFO:  Epoch 167/500:  train Loss: 78.6680   val Loss: 77.7879   time: 0.27s   best: 77.7448
2023-10-15 10:06:40,961:INFO:  Epoch 168/500:  train Loss: 78.7305   val Loss: 77.8802   time: 0.24s   best: 77.7448
2023-10-15 10:06:41,224:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:41,304:INFO:  Epoch 169/500:  train Loss: 78.9368   val Loss: 77.4238   time: 0.26s   best: 77.4238
2023-10-15 10:06:41,557:INFO:  Epoch 170/500:  train Loss: 78.9746   val Loss: 78.7384   time: 0.24s   best: 77.4238
2023-10-15 10:06:41,823:INFO:  Epoch 171/500:  train Loss: 79.0969   val Loss: 79.2998   time: 0.26s   best: 77.4238
2023-10-15 10:06:42,076:INFO:  Epoch 172/500:  train Loss: 79.4095   val Loss: 78.0739   time: 0.24s   best: 77.4238
2023-10-15 10:06:42,345:INFO:  Epoch 173/500:  train Loss: 79.1498   val Loss: 78.5626   time: 0.26s   best: 77.4238
2023-10-15 10:06:42,628:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:42,951:INFO:  Epoch 174/500:  train Loss: 78.5627   val Loss: 76.7251   time: 0.28s   best: 76.7251
2023-10-15 10:06:43,202:INFO:  Epoch 175/500:  train Loss: 77.8363   val Loss: 77.1856   time: 0.24s   best: 76.7251
2023-10-15 10:06:43,466:INFO:  Epoch 176/500:  train Loss: 78.5141   val Loss: 78.1296   time: 0.25s   best: 76.7251
2023-10-15 10:06:43,718:INFO:  Epoch 177/500:  train Loss: 77.9608   val Loss: 77.4871   time: 0.24s   best: 76.7251
2023-10-15 10:06:43,988:INFO:  Epoch 178/500:  train Loss: 79.9779   val Loss: 77.3005   time: 0.26s   best: 76.7251
2023-10-15 10:06:44,239:INFO:  Epoch 179/500:  train Loss: 78.0393   val Loss: 77.5161   time: 0.24s   best: 76.7251
2023-10-15 10:06:44,509:INFO:  Epoch 180/500:  train Loss: 78.3029   val Loss: 77.9156   time: 0.26s   best: 76.7251
2023-10-15 10:06:44,818:INFO:  Epoch 181/500:  train Loss: 78.0487   val Loss: 77.0538   time: 0.30s   best: 76.7251
2023-10-15 10:06:45,069:INFO:  Epoch 182/500:  train Loss: 77.1955   val Loss: 77.6591   time: 0.24s   best: 76.7251
2023-10-15 10:06:45,339:INFO:  Epoch 183/500:  train Loss: 78.9069   val Loss: 77.1276   time: 0.26s   best: 76.7251
2023-10-15 10:06:45,594:INFO:  Epoch 184/500:  train Loss: 78.0915   val Loss: 77.1813   time: 0.24s   best: 76.7251
2023-10-15 10:06:45,855:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:45,925:INFO:  Epoch 185/500:  train Loss: 78.3066   val Loss: 76.6576   time: 0.26s   best: 76.6576
2023-10-15 10:06:46,169:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:46,220:INFO:  Epoch 186/500:  train Loss: 77.1797   val Loss: 76.5856   time: 0.24s   best: 76.5856
2023-10-15 10:06:46,485:INFO:  Epoch 187/500:  train Loss: 78.0740   val Loss: 76.5996   time: 0.25s   best: 76.5856
2023-10-15 10:06:46,773:INFO:  Epoch 188/500:  train Loss: 77.8003   val Loss: 79.1276   time: 0.28s   best: 76.5856
2023-10-15 10:06:47,043:INFO:  Epoch 189/500:  train Loss: 79.5013   val Loss: 78.5931   time: 0.26s   best: 76.5856
2023-10-15 10:06:47,293:INFO:  Epoch 190/500:  train Loss: 79.2593   val Loss: 78.1960   time: 0.24s   best: 76.5856
2023-10-15 10:06:47,564:INFO:  Epoch 191/500:  train Loss: 77.1480   val Loss: 76.7420   time: 0.26s   best: 76.5856
2023-10-15 10:06:47,808:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:47,996:INFO:  Epoch 192/500:  train Loss: 78.5668   val Loss: 76.2764   time: 0.24s   best: 76.2764
2023-10-15 10:06:48,247:INFO:  Epoch 193/500:  train Loss: 76.9956   val Loss: 77.0788   time: 0.24s   best: 76.2764
2023-10-15 10:06:48,514:INFO:  Epoch 194/500:  train Loss: 77.9399   val Loss: 78.1579   time: 0.26s   best: 76.2764
2023-10-15 10:06:48,821:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:48,869:INFO:  Epoch 195/500:  train Loss: 77.9501   val Loss: 75.7408   time: 0.30s   best: 75.7408
2023-10-15 10:06:49,136:INFO:  Epoch 196/500:  train Loss: 76.9605   val Loss: 77.5788   time: 0.26s   best: 75.7408
2023-10-15 10:06:49,387:INFO:  Epoch 197/500:  train Loss: 79.7819   val Loss: 77.1586   time: 0.24s   best: 75.7408
2023-10-15 10:06:49,658:INFO:  Epoch 198/500:  train Loss: 77.7475   val Loss: 78.8600   time: 0.26s   best: 75.7408
2023-10-15 10:06:49,909:INFO:  Epoch 199/500:  train Loss: 79.3211   val Loss: 78.4493   time: 0.24s   best: 75.7408
2023-10-15 10:06:50,242:INFO:  Epoch 200/500:  train Loss: 78.6370   val Loss: 77.3382   time: 0.31s   best: 75.7408
2023-10-15 10:06:50,495:INFO:  Epoch 201/500:  train Loss: 77.7254   val Loss: 75.9592   time: 0.24s   best: 75.7408
2023-10-15 10:06:50,758:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:50,888:INFO:  Epoch 202/500:  train Loss: 76.8717   val Loss: 75.7279   time: 0.26s   best: 75.7279
2023-10-15 10:06:51,170:INFO:  Epoch 203/500:  train Loss: 77.4343   val Loss: 76.7898   time: 0.27s   best: 75.7279
2023-10-15 10:06:51,421:INFO:  Epoch 204/500:  train Loss: 77.4938   val Loss: 77.5923   time: 0.24s   best: 75.7279
2023-10-15 10:06:51,690:INFO:  Epoch 205/500:  train Loss: 78.0069   val Loss: 77.1923   time: 0.26s   best: 75.7279
2023-10-15 10:06:51,941:INFO:  Epoch 206/500:  train Loss: 77.2756   val Loss: 77.6252   time: 0.24s   best: 75.7279
2023-10-15 10:06:52,219:INFO:  Epoch 207/500:  train Loss: 77.4282   val Loss: 76.1804   time: 0.27s   best: 75.7279
2023-10-15 10:06:52,479:INFO:  Epoch 208/500:  train Loss: 77.2991   val Loss: 76.4114   time: 0.25s   best: 75.7279
2023-10-15 10:06:52,743:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:52,842:INFO:  Epoch 209/500:  train Loss: 76.5324   val Loss: 75.7153   time: 0.26s   best: 75.7153
2023-10-15 10:06:53,130:INFO:  Epoch 210/500:  train Loss: 76.3045   val Loss: 75.9335   time: 0.28s   best: 75.7153
2023-10-15 10:06:53,397:INFO:  Epoch 211/500:  train Loss: 76.6823   val Loss: 77.6968   time: 0.26s   best: 75.7153
2023-10-15 10:06:53,649:INFO:  Epoch 212/500:  train Loss: 77.6065   val Loss: 77.2264   time: 0.24s   best: 75.7153
2023-10-15 10:06:53,918:INFO:  Epoch 213/500:  train Loss: 77.9785   val Loss: 76.0753   time: 0.26s   best: 75.7153
2023-10-15 10:06:54,168:INFO:  Epoch 214/500:  train Loss: 77.3361   val Loss: 76.3883   time: 0.24s   best: 75.7153
2023-10-15 10:06:54,436:INFO:  Epoch 215/500:  train Loss: 76.4523   val Loss: 76.8520   time: 0.26s   best: 75.7153
2023-10-15 10:06:54,688:INFO:  Epoch 216/500:  train Loss: 76.2242   val Loss: 76.3815   time: 0.24s   best: 75.7153
2023-10-15 10:06:54,962:INFO:  Epoch 217/500:  train Loss: 77.2151   val Loss: 75.7567   time: 0.26s   best: 75.7153
2023-10-15 10:06:55,267:INFO:  Epoch 218/500:  train Loss: 77.3215   val Loss: 76.8733   time: 0.28s   best: 75.7153
2023-10-15 10:06:55,518:INFO:  Epoch 219/500:  train Loss: 78.7321   val Loss: 78.9222   time: 0.24s   best: 75.7153
2023-10-15 10:06:55,771:INFO:  Epoch 220/500:  train Loss: 77.4214   val Loss: 79.6931   time: 0.24s   best: 75.7153
2023-10-15 10:06:56,042:INFO:  Epoch 221/500:  train Loss: 78.5282   val Loss: 77.9399   time: 0.26s   best: 75.7153
2023-10-15 10:06:56,292:INFO:  Epoch 222/500:  train Loss: 78.2495   val Loss: 76.4212   time: 0.24s   best: 75.7153
2023-10-15 10:06:56,557:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:06:56,615:INFO:  Epoch 223/500:  train Loss: 76.5282   val Loss: 75.3331   time: 0.26s   best: 75.3331
2023-10-15 10:06:56,886:INFO:  Epoch 224/500:  train Loss: 76.0822   val Loss: 75.3434   time: 0.26s   best: 75.3331
2023-10-15 10:06:57,163:INFO:  Epoch 225/500:  train Loss: 77.5095   val Loss: 76.2958   time: 0.27s   best: 75.3331
2023-10-15 10:06:57,442:INFO:  Epoch 226/500:  train Loss: 76.4002   val Loss: 75.9477   time: 0.27s   best: 75.3331
2023-10-15 10:06:57,693:INFO:  Epoch 227/500:  train Loss: 77.0788   val Loss: 82.2967   time: 0.24s   best: 75.3331
2023-10-15 10:06:57,964:INFO:  Epoch 228/500:  train Loss: 79.5024   val Loss: 82.4932   time: 0.26s   best: 75.3331
2023-10-15 10:06:58,214:INFO:  Epoch 229/500:  train Loss: 80.7754   val Loss: 82.8100   time: 0.24s   best: 75.3331
2023-10-15 10:06:58,483:INFO:  Epoch 230/500:  train Loss: 89.1657   val Loss: 79.4225   time: 0.26s   best: 75.3331
2023-10-15 10:06:58,736:INFO:  Epoch 231/500:  train Loss: 80.2860   val Loss: 80.6527   time: 0.24s   best: 75.3331
2023-10-15 10:06:59,007:INFO:  Epoch 232/500:  train Loss: 80.4083   val Loss: 79.5631   time: 0.26s   best: 75.3331
2023-10-15 10:06:59,293:INFO:  Epoch 233/500:  train Loss: 79.6368   val Loss: 78.1468   time: 0.28s   best: 75.3331
2023-10-15 10:06:59,564:INFO:  Epoch 234/500:  train Loss: 78.3227   val Loss: 77.3087   time: 0.26s   best: 75.3331
2023-10-15 10:06:59,814:INFO:  Epoch 235/500:  train Loss: 77.2390   val Loss: 77.3922   time: 0.24s   best: 75.3331
2023-10-15 10:07:00,085:INFO:  Epoch 236/500:  train Loss: 78.9834   val Loss: 78.7459   time: 0.26s   best: 75.3331
2023-10-15 10:07:00,335:INFO:  Epoch 237/500:  train Loss: 79.0944   val Loss: 77.6516   time: 0.24s   best: 75.3331
2023-10-15 10:07:00,605:INFO:  Epoch 238/500:  train Loss: 78.7110   val Loss: 78.6027   time: 0.26s   best: 75.3331
2023-10-15 10:07:00,860:INFO:  Epoch 239/500:  train Loss: 77.8670   val Loss: 78.2208   time: 0.24s   best: 75.3331
2023-10-15 10:07:01,130:INFO:  Epoch 240/500:  train Loss: 77.8366   val Loss: 75.9108   time: 0.26s   best: 75.3331
2023-10-15 10:07:01,412:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:01,440:INFO:  Epoch 241/500:  train Loss: 76.5101   val Loss: 75.2742   time: 0.28s   best: 75.2742
2023-10-15 10:07:01,708:INFO:  Epoch 242/500:  train Loss: 75.8253   val Loss: 76.3649   time: 0.26s   best: 75.2742
2023-10-15 10:07:01,960:INFO:  Epoch 243/500:  train Loss: 76.7822   val Loss: 77.3904   time: 0.24s   best: 75.2742
2023-10-15 10:07:02,228:INFO:  Epoch 244/500:  train Loss: 77.5656   val Loss: 75.8881   time: 0.26s   best: 75.2742
2023-10-15 10:07:02,480:INFO:  Epoch 245/500:  train Loss: 76.5560   val Loss: 75.7145   time: 0.24s   best: 75.2742
2023-10-15 10:07:02,751:INFO:  Epoch 246/500:  train Loss: 78.6700   val Loss: 75.4348   time: 0.26s   best: 75.2742
2023-10-15 10:07:03,004:INFO:  Epoch 247/500:  train Loss: 75.9371   val Loss: 78.2936   time: 0.24s   best: 75.2742
2023-10-15 10:07:03,273:INFO:  Epoch 248/500:  train Loss: 77.6400   val Loss: 77.3723   time: 0.26s   best: 75.2742
2023-10-15 10:07:03,562:INFO:  Epoch 249/500:  train Loss: 77.0467   val Loss: 75.7050   time: 0.28s   best: 75.2742
2023-10-15 10:07:03,824:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:03,989:INFO:  Epoch 250/500:  train Loss: 75.9048   val Loss: 74.6366   time: 0.26s   best: 74.6366
2023-10-15 10:07:04,253:INFO:  Epoch 251/500:  train Loss: 75.1341   val Loss: 74.7889   time: 0.25s   best: 74.6366
2023-10-15 10:07:04,506:INFO:  Epoch 252/500:  train Loss: 75.6959   val Loss: 75.5643   time: 0.24s   best: 74.6366
2023-10-15 10:07:04,779:INFO:  Epoch 253/500:  train Loss: 76.8756   val Loss: 76.3109   time: 0.26s   best: 74.6366
2023-10-15 10:07:05,032:INFO:  Epoch 254/500:  train Loss: 75.3374   val Loss: 74.9092   time: 0.24s   best: 74.6366
2023-10-15 10:07:05,302:INFO:  Epoch 255/500:  train Loss: 75.5390   val Loss: 76.1199   time: 0.26s   best: 74.6366
2023-10-15 10:07:05,592:INFO:  Epoch 256/500:  train Loss: 76.7700   val Loss: 78.3487   time: 0.28s   best: 74.6366
2023-10-15 10:07:05,862:INFO:  Epoch 257/500:  train Loss: 77.8558   val Loss: 77.4938   time: 0.26s   best: 74.6366
2023-10-15 10:07:06,113:INFO:  Epoch 258/500:  train Loss: 78.3881   val Loss: 76.1370   time: 0.24s   best: 74.6366
2023-10-15 10:07:06,383:INFO:  Epoch 259/500:  train Loss: 76.3999   val Loss: 74.8634   time: 0.26s   best: 74.6366
2023-10-15 10:07:06,630:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:06,659:INFO:  Epoch 260/500:  train Loss: 75.3586   val Loss: 73.5492   time: 0.24s   best: 73.5492
2023-10-15 10:07:06,931:INFO:  Epoch 261/500:  train Loss: 74.3375   val Loss: 73.8618   time: 0.26s   best: 73.5492
2023-10-15 10:07:07,176:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:07,282:INFO:  Epoch 262/500:  train Loss: 73.9198   val Loss: 72.7472   time: 0.24s   best: 72.7472
2023-10-15 10:07:07,595:INFO:  Epoch 263/500:  train Loss: 74.2244   val Loss: 72.9062   time: 0.31s   best: 72.7472
2023-10-15 10:07:07,848:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:07,929:INFO:  Epoch 264/500:  train Loss: 73.9890   val Loss: 72.0114   time: 0.25s   best: 72.0114
2023-10-15 10:07:08,181:INFO:  Epoch 265/500:  train Loss: 73.1851   val Loss: 72.7013   time: 0.24s   best: 72.0114
2023-10-15 10:07:08,441:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:08,481:INFO:  Epoch 266/500:  train Loss: 73.5052   val Loss: 71.6251   time: 0.26s   best: 71.6251
2023-10-15 10:07:08,730:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:08,917:INFO:  Epoch 267/500:  train Loss: 72.7667   val Loss: 71.4256   time: 0.24s   best: 71.4256
2023-10-15 10:07:09,169:INFO:  Epoch 268/500:  train Loss: 72.9178   val Loss: 73.0160   time: 0.24s   best: 71.4256
2023-10-15 10:07:09,438:INFO:  Epoch 269/500:  train Loss: 73.9480   val Loss: 73.1020   time: 0.26s   best: 71.4256
2023-10-15 10:07:09,725:INFO:  Epoch 270/500:  train Loss: 74.5861   val Loss: 72.2479   time: 0.28s   best: 71.4256
2023-10-15 10:07:09,991:INFO:  Epoch 271/500:  train Loss: 72.7838   val Loss: 74.3359   time: 0.25s   best: 71.4256
2023-10-15 10:07:10,241:INFO:  Epoch 272/500:  train Loss: 81.5812   val Loss: 81.5815   time: 0.24s   best: 71.4256
2023-10-15 10:07:10,512:INFO:  Epoch 273/500:  train Loss: 76.2936   val Loss: 78.8540   time: 0.26s   best: 71.4256
2023-10-15 10:07:10,764:INFO:  Epoch 274/500:  train Loss: 79.9634   val Loss: 78.7344   time: 0.24s   best: 71.4256
2023-10-15 10:07:11,036:INFO:  Epoch 275/500:  train Loss: 79.5468   val Loss: 79.7432   time: 0.26s   best: 71.4256
2023-10-15 10:07:11,286:INFO:  Epoch 276/500:  train Loss: 79.8410   val Loss: 78.3579   time: 0.24s   best: 71.4256
2023-10-15 10:07:11,559:INFO:  Epoch 277/500:  train Loss: 78.2733   val Loss: 91.0419   time: 0.26s   best: 71.4256
2023-10-15 10:07:11,846:INFO:  Epoch 278/500:  train Loss: 89.1341   val Loss: 82.3690   time: 0.28s   best: 71.4256
2023-10-15 10:07:12,115:INFO:  Epoch 279/500:  train Loss: 80.0715   val Loss: 79.0934   time: 0.26s   best: 71.4256
2023-10-15 10:07:12,364:INFO:  Epoch 280/500:  train Loss: 79.8863   val Loss: 77.8719   time: 0.24s   best: 71.4256
2023-10-15 10:07:12,637:INFO:  Epoch 281/500:  train Loss: 77.1588   val Loss: 75.9641   time: 0.26s   best: 71.4256
2023-10-15 10:07:12,893:INFO:  Epoch 282/500:  train Loss: 77.0796   val Loss: 75.5555   time: 0.24s   best: 71.4256
2023-10-15 10:07:13,195:INFO:  Epoch 283/500:  train Loss: 76.2216   val Loss: 75.7645   time: 0.29s   best: 71.4256
2023-10-15 10:07:13,445:INFO:  Epoch 284/500:  train Loss: 75.9404   val Loss: 74.8229   time: 0.24s   best: 71.4256
2023-10-15 10:07:13,715:INFO:  Epoch 285/500:  train Loss: 75.3976   val Loss: 74.2781   time: 0.26s   best: 71.4256
2023-10-15 10:07:14,003:INFO:  Epoch 286/500:  train Loss: 74.4830   val Loss: 74.3605   time: 0.28s   best: 71.4256
2023-10-15 10:07:14,271:INFO:  Epoch 287/500:  train Loss: 76.2017   val Loss: 75.2471   time: 0.26s   best: 71.4256
2023-10-15 10:07:14,522:INFO:  Epoch 288/500:  train Loss: 76.4706   val Loss: 75.8233   time: 0.24s   best: 71.4256
2023-10-15 10:07:14,795:INFO:  Epoch 289/500:  train Loss: 77.8027   val Loss: 78.3437   time: 0.26s   best: 71.4256
2023-10-15 10:07:15,047:INFO:  Epoch 290/500:  train Loss: 76.4910   val Loss: 76.0308   time: 0.24s   best: 71.4256
2023-10-15 10:07:15,315:INFO:  Epoch 291/500:  train Loss: 75.5316   val Loss: 74.1235   time: 0.26s   best: 71.4256
2023-10-15 10:07:15,568:INFO:  Epoch 292/500:  train Loss: 74.0119   val Loss: 73.3745   time: 0.24s   best: 71.4256
2023-10-15 10:07:15,838:INFO:  Epoch 293/500:  train Loss: 74.7886   val Loss: 73.4217   time: 0.26s   best: 71.4256
2023-10-15 10:07:16,130:INFO:  Epoch 294/500:  train Loss: 74.5566   val Loss: 72.6725   time: 0.28s   best: 71.4256
2023-10-15 10:07:16,398:INFO:  Epoch 295/500:  train Loss: 74.0340   val Loss: 74.2457   time: 0.26s   best: 71.4256
2023-10-15 10:07:16,650:INFO:  Epoch 296/500:  train Loss: 74.6495   val Loss: 73.7671   time: 0.24s   best: 71.4256
2023-10-15 10:07:16,924:INFO:  Epoch 297/500:  train Loss: 75.3560   val Loss: 75.4047   time: 0.26s   best: 71.4256
2023-10-15 10:07:17,175:INFO:  Epoch 298/500:  train Loss: 75.3425   val Loss: 79.5936   time: 0.24s   best: 71.4256
2023-10-15 10:07:17,445:INFO:  Epoch 299/500:  train Loss: 77.6082   val Loss: 76.7859   time: 0.26s   best: 71.4256
2023-10-15 10:07:17,761:INFO:  Epoch 300/500:  train Loss: 75.5672   val Loss: 73.4793   time: 0.29s   best: 71.4256
2023-10-15 10:07:18,067:INFO:  Epoch 301/500:  train Loss: 75.1390   val Loss: 73.5816   time: 0.29s   best: 71.4256
2023-10-15 10:07:18,310:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:18,356:INFO:  Epoch 302/500:  train Loss: 73.6378   val Loss: 71.3590   time: 0.24s   best: 71.3590
2023-10-15 10:07:18,602:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:18,730:INFO:  Epoch 303/500:  train Loss: 72.1366   val Loss: 71.3446   time: 0.24s   best: 71.3446
2023-10-15 10:07:18,990:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:19,093:INFO:  Epoch 304/500:  train Loss: 72.7523   val Loss: 70.2931   time: 0.25s   best: 70.2931
2023-10-15 10:07:19,343:INFO:  Epoch 305/500:  train Loss: 72.1545   val Loss: 71.8340   time: 0.24s   best: 70.2931
2023-10-15 10:07:19,611:INFO:  Epoch 306/500:  train Loss: 72.7413   val Loss: 71.7291   time: 0.26s   best: 70.2931
2023-10-15 10:07:19,862:INFO:  Epoch 307/500:  train Loss: 72.9323   val Loss: 70.3918   time: 0.24s   best: 70.2931
2023-10-15 10:07:20,201:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:20,297:INFO:  Epoch 308/500:  train Loss: 71.6056   val Loss: 70.0979   time: 0.33s   best: 70.0979
2023-10-15 10:07:20,560:INFO:  Epoch 309/500:  train Loss: 71.7378   val Loss: 71.8912   time: 0.25s   best: 70.0979
2023-10-15 10:07:20,812:INFO:  Epoch 310/500:  train Loss: 71.9168   val Loss: 73.3015   time: 0.24s   best: 70.0979
2023-10-15 10:07:21,083:INFO:  Epoch 311/500:  train Loss: 74.3701   val Loss: 74.5954   time: 0.26s   best: 70.0979
2023-10-15 10:07:21,328:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:21,499:INFO:  Epoch 312/500:  train Loss: 72.4636   val Loss: 70.0833   time: 0.24s   best: 70.0833
2023-10-15 10:07:21,752:INFO:  Epoch 313/500:  train Loss: 70.8078   val Loss: 71.6035   time: 0.24s   best: 70.0833
2023-10-15 10:07:22,092:INFO:  Epoch 314/500:  train Loss: 72.1096   val Loss: 71.0810   time: 0.33s   best: 70.0833
2023-10-15 10:07:22,373:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:22,411:INFO:  Epoch 315/500:  train Loss: 70.9892   val Loss: 69.6610   time: 0.28s   best: 69.6610
2023-10-15 10:07:22,673:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:22,773:INFO:  Epoch 316/500:  train Loss: 71.1678   val Loss: 68.8515   time: 0.26s   best: 68.8515
2023-10-15 10:07:23,018:INFO:  Epoch 317/500:  train Loss: 70.0912   val Loss: 69.0544   time: 0.24s   best: 68.8515
2023-10-15 10:07:23,299:INFO:  Epoch 318/500:  train Loss: 70.0319   val Loss: 74.3643   time: 0.27s   best: 68.8515
2023-10-15 10:07:23,552:INFO:  Epoch 319/500:  train Loss: 77.2573   val Loss: 78.1408   time: 0.24s   best: 68.8515
2023-10-15 10:07:23,820:INFO:  Epoch 320/500:  train Loss: 78.6377   val Loss: 81.0359   time: 0.26s   best: 68.8515
2023-10-15 10:07:24,071:INFO:  Epoch 321/500:  train Loss: 76.5472   val Loss: 75.3433   time: 0.24s   best: 68.8515
2023-10-15 10:07:24,377:INFO:  Epoch 322/500:  train Loss: 74.1891   val Loss: 73.0347   time: 0.29s   best: 68.8515
2023-10-15 10:07:24,629:INFO:  Epoch 323/500:  train Loss: 75.4326   val Loss: 74.9726   time: 0.24s   best: 68.8515
2023-10-15 10:07:24,900:INFO:  Epoch 324/500:  train Loss: 78.0405   val Loss: 77.6812   time: 0.26s   best: 68.8515
2023-10-15 10:07:25,151:INFO:  Epoch 325/500:  train Loss: 77.8790   val Loss: 78.2608   time: 0.24s   best: 68.8515
2023-10-15 10:07:25,420:INFO:  Epoch 326/500:  train Loss: 76.8758   val Loss: 76.4815   time: 0.26s   best: 68.8515
2023-10-15 10:07:25,673:INFO:  Epoch 327/500:  train Loss: 76.5906   val Loss: 74.3833   time: 0.24s   best: 68.8515
2023-10-15 10:07:25,941:INFO:  Epoch 328/500:  train Loss: 73.7068   val Loss: 71.7245   time: 0.26s   best: 68.8515
2023-10-15 10:07:26,194:INFO:  Epoch 329/500:  train Loss: 72.5650   val Loss: 71.3012   time: 0.24s   best: 68.8515
2023-10-15 10:07:26,500:INFO:  Epoch 330/500:  train Loss: 72.4841   val Loss: 71.9005   time: 0.29s   best: 68.8515
2023-10-15 10:07:26,753:INFO:  Epoch 331/500:  train Loss: 71.4709   val Loss: 69.9872   time: 0.24s   best: 68.8515
2023-10-15 10:07:27,023:INFO:  Epoch 332/500:  train Loss: 70.2299   val Loss: 69.1729   time: 0.26s   best: 68.8515
2023-10-15 10:07:27,273:INFO:  Epoch 333/500:  train Loss: 71.1253   val Loss: 70.7772   time: 0.24s   best: 68.8515
2023-10-15 10:07:27,543:INFO:  Epoch 334/500:  train Loss: 71.4284   val Loss: 80.1377   time: 0.26s   best: 68.8515
2023-10-15 10:07:27,794:INFO:  Epoch 335/500:  train Loss: 77.7124   val Loss: 73.5585   time: 0.24s   best: 68.8515
2023-10-15 10:07:28,062:INFO:  Epoch 336/500:  train Loss: 74.8956   val Loss: 75.5907   time: 0.26s   best: 68.8515
2023-10-15 10:07:28,313:INFO:  Epoch 337/500:  train Loss: 74.6502   val Loss: 73.2063   time: 0.24s   best: 68.8515
2023-10-15 10:07:28,622:INFO:  Epoch 338/500:  train Loss: 74.7610   val Loss: 74.1043   time: 0.30s   best: 68.8515
2023-10-15 10:07:28,894:INFO:  Epoch 339/500:  train Loss: 73.6884   val Loss: 74.0266   time: 0.24s   best: 68.8515
2023-10-15 10:07:29,145:INFO:  Epoch 340/500:  train Loss: 74.4850   val Loss: 76.0553   time: 0.24s   best: 68.8515
2023-10-15 10:07:29,394:INFO:  Epoch 341/500:  train Loss: 76.4639   val Loss: 75.3090   time: 0.24s   best: 68.8515
2023-10-15 10:07:29,666:INFO:  Epoch 342/500:  train Loss: 74.5841   val Loss: 72.3980   time: 0.26s   best: 68.8515
2023-10-15 10:07:29,916:INFO:  Epoch 343/500:  train Loss: 73.8273   val Loss: 74.4711   time: 0.24s   best: 68.8515
2023-10-15 10:07:30,186:INFO:  Epoch 344/500:  train Loss: 73.9199   val Loss: 75.6432   time: 0.26s   best: 68.8515
2023-10-15 10:07:30,435:INFO:  Epoch 345/500:  train Loss: 76.6310   val Loss: 78.0508   time: 0.24s   best: 68.8515
2023-10-15 10:07:30,744:INFO:  Epoch 346/500:  train Loss: 76.6208   val Loss: 76.6997   time: 0.30s   best: 68.8515
2023-10-15 10:07:30,998:INFO:  Epoch 347/500:  train Loss: 74.4573   val Loss: 74.9384   time: 0.24s   best: 68.8515
2023-10-15 10:07:31,267:INFO:  Epoch 348/500:  train Loss: 75.5400   val Loss: 74.3430   time: 0.26s   best: 68.8515
2023-10-15 10:07:31,517:INFO:  Epoch 349/500:  train Loss: 73.6132   val Loss: 72.5848   time: 0.24s   best: 68.8515
2023-10-15 10:07:31,787:INFO:  Epoch 350/500:  train Loss: 73.2107   val Loss: 71.7401   time: 0.26s   best: 68.8515
2023-10-15 10:07:32,038:INFO:  Epoch 351/500:  train Loss: 72.2317   val Loss: 70.6870   time: 0.24s   best: 68.8515
2023-10-15 10:07:32,308:INFO:  Epoch 352/500:  train Loss: 69.9029   val Loss: 69.3221   time: 0.26s   best: 68.8515
2023-10-15 10:07:32,559:INFO:  Epoch 353/500:  train Loss: 70.4195   val Loss: 68.9493   time: 0.24s   best: 68.8515
2023-10-15 10:07:32,867:INFO:  Epoch 354/500:  train Loss: 69.7567   val Loss: 69.0967   time: 0.30s   best: 68.8515
2023-10-15 10:07:33,119:INFO:  Epoch 355/500:  train Loss: 69.9164   val Loss: 69.8557   time: 0.24s   best: 68.8515
2023-10-15 10:07:33,382:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:33,410:INFO:  Epoch 356/500:  train Loss: 69.1574   val Loss: 67.8291   time: 0.26s   best: 67.8291
2023-10-15 10:07:33,664:INFO:  Epoch 357/500:  train Loss: 70.7791   val Loss: 74.2880   time: 0.24s   best: 67.8291
2023-10-15 10:07:33,933:INFO:  Epoch 358/500:  train Loss: 77.2101   val Loss: 75.6174   time: 0.26s   best: 67.8291
2023-10-15 10:07:34,184:INFO:  Epoch 359/500:  train Loss: 78.5064   val Loss: 79.3535   time: 0.24s   best: 67.8291
2023-10-15 10:07:34,454:INFO:  Epoch 360/500:  train Loss: 76.6901   val Loss: 76.9319   time: 0.26s   best: 67.8291
2023-10-15 10:07:34,762:INFO:  Epoch 361/500:  train Loss: 76.2008   val Loss: 74.8243   time: 0.28s   best: 67.8291
2023-10-15 10:07:35,015:INFO:  Epoch 362/500:  train Loss: 74.6556   val Loss: 72.8844   time: 0.24s   best: 67.8291
2023-10-15 10:07:35,286:INFO:  Epoch 363/500:  train Loss: 72.9937   val Loss: 71.8153   time: 0.24s   best: 67.8291
2023-10-15 10:07:35,536:INFO:  Epoch 364/500:  train Loss: 71.7471   val Loss: 71.2482   time: 0.24s   best: 67.8291
2023-10-15 10:07:35,788:INFO:  Epoch 365/500:  train Loss: 72.0630   val Loss: 72.8660   time: 0.24s   best: 67.8291
2023-10-15 10:07:36,059:INFO:  Epoch 366/500:  train Loss: 73.1943   val Loss: 69.7421   time: 0.26s   best: 67.8291
2023-10-15 10:07:36,312:INFO:  Epoch 367/500:  train Loss: 69.9201   val Loss: 68.8708   time: 0.24s   best: 67.8291
2023-10-15 10:07:36,581:INFO:  Epoch 368/500:  train Loss: 70.3150   val Loss: 69.6667   time: 0.26s   best: 67.8291
2023-10-15 10:07:36,889:INFO:  Epoch 369/500:  train Loss: 69.8524   val Loss: 68.7594   time: 0.30s   best: 67.8291
2023-10-15 10:07:37,141:INFO:  Epoch 370/500:  train Loss: 70.2359   val Loss: 71.0896   time: 0.24s   best: 67.8291
2023-10-15 10:07:37,405:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:37,767:INFO:  Epoch 371/500:  train Loss: 70.4227   val Loss: 67.7406   time: 0.26s   best: 67.7406
2023-10-15 10:07:38,023:INFO:  Epoch 372/500:  train Loss: 69.2479   val Loss: 67.9998   time: 0.24s   best: 67.7406
2023-10-15 10:07:38,281:INFO:  Epoch 373/500:  train Loss: 68.8585   val Loss: 68.6670   time: 0.25s   best: 67.7406
2023-10-15 10:07:38,560:INFO:  Epoch 374/500:  train Loss: 68.8766   val Loss: 68.6572   time: 0.27s   best: 67.7406
2023-10-15 10:07:38,812:INFO:  Epoch 375/500:  train Loss: 68.9185   val Loss: 67.7837   time: 0.24s   best: 67.7406
2023-10-15 10:07:39,119:INFO:  Epoch 376/500:  train Loss: 69.1342   val Loss: 68.3187   time: 0.30s   best: 67.7406
2023-10-15 10:07:39,370:INFO:  Epoch 377/500:  train Loss: 68.8589   val Loss: 69.9571   time: 0.24s   best: 67.7406
2023-10-15 10:07:39,642:INFO:  Epoch 378/500:  train Loss: 69.7374   val Loss: 69.3317   time: 0.26s   best: 67.7406
2023-10-15 10:07:39,892:INFO:  Epoch 379/500:  train Loss: 68.4559   val Loss: 70.0528   time: 0.24s   best: 67.7406
2023-10-15 10:07:40,161:INFO:  Epoch 380/500:  train Loss: 69.3837   val Loss: 68.3205   time: 0.26s   best: 67.7406
2023-10-15 10:07:40,405:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:40,673:INFO:  Epoch 381/500:  train Loss: 68.6077   val Loss: 67.4701   time: 0.24s   best: 67.4701
2023-10-15 10:07:40,956:INFO:  Epoch 382/500:  train Loss: 68.7010   val Loss: 68.5354   time: 0.27s   best: 67.4701
2023-10-15 10:07:41,222:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:41,377:INFO:  Epoch 383/500:  train Loss: 68.0195   val Loss: 66.8766   time: 0.26s   best: 66.8766
2023-10-15 10:07:41,635:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:41,735:INFO:  Epoch 384/500:  train Loss: 71.0662   val Loss: 66.7073   time: 0.25s   best: 66.7073
2023-10-15 10:07:41,979:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:42,015:INFO:  Epoch 385/500:  train Loss: 66.6302   val Loss: 66.0463   time: 0.24s   best: 66.0463
2023-10-15 10:07:42,280:INFO:  Epoch 386/500:  train Loss: 67.5934   val Loss: 66.1586   time: 0.25s   best: 66.0463
2023-10-15 10:07:42,531:INFO:  Epoch 387/500:  train Loss: 67.8212   val Loss: 66.3071   time: 0.24s   best: 66.0463
2023-10-15 10:07:42,804:INFO:  Epoch 388/500:  train Loss: 67.4075   val Loss: 67.2284   time: 0.26s   best: 66.0463
2023-10-15 10:07:43,086:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:43,197:INFO:  Epoch 389/500:  train Loss: 66.8503   val Loss: 65.7749   time: 0.28s   best: 65.7749
2023-10-15 10:07:43,449:INFO:  Epoch 390/500:  train Loss: 67.7691   val Loss: 65.9139   time: 0.24s   best: 65.7749
2023-10-15 10:07:43,717:INFO:  Epoch 391/500:  train Loss: 68.4118   val Loss: 68.2326   time: 0.26s   best: 65.7749
2023-10-15 10:07:43,968:INFO:  Epoch 392/500:  train Loss: 69.1093   val Loss: 70.0679   time: 0.24s   best: 65.7749
2023-10-15 10:07:44,239:INFO:  Epoch 393/500:  train Loss: 68.1636   val Loss: 67.4769   time: 0.26s   best: 65.7749
2023-10-15 10:07:44,484:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:44,512:INFO:  Epoch 394/500:  train Loss: 67.0254   val Loss: 65.7547   time: 0.24s   best: 65.7547
2023-10-15 10:07:44,784:INFO:  Epoch 395/500:  train Loss: 66.1964   val Loss: 66.2437   time: 0.26s   best: 65.7547
2023-10-15 10:07:45,036:INFO:  Epoch 396/500:  train Loss: 66.4172   val Loss: 66.5683   time: 0.24s   best: 65.7547
2023-10-15 10:07:45,342:INFO:  Epoch 397/500:  train Loss: 68.7670   val Loss: 69.5888   time: 0.30s   best: 65.7547
2023-10-15 10:07:45,594:INFO:  Epoch 398/500:  train Loss: 68.9791   val Loss: 68.2162   time: 0.24s   best: 65.7547
2023-10-15 10:07:45,865:INFO:  Epoch 399/500:  train Loss: 68.6589   val Loss: 68.0520   time: 0.26s   best: 65.7547
2023-10-15 10:07:46,178:INFO:  Epoch 400/500:  train Loss: 70.5281   val Loss: 66.8851   time: 0.29s   best: 65.7547
2023-10-15 10:07:46,439:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:46,469:INFO:  Epoch 401/500:  train Loss: 67.2677   val Loss: 65.5597   time: 0.26s   best: 65.5597
2023-10-15 10:07:46,715:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:46,864:INFO:  Epoch 402/500:  train Loss: 65.9894   val Loss: 65.3591   time: 0.24s   best: 65.3591
2023-10-15 10:07:47,117:INFO:  Epoch 403/500:  train Loss: 66.9865   val Loss: 65.6687   time: 0.24s   best: 65.3591
2023-10-15 10:07:47,418:INFO:  Epoch 404/500:  train Loss: 67.7365   val Loss: 67.5951   time: 0.29s   best: 65.3591
2023-10-15 10:07:47,669:INFO:  Epoch 405/500:  train Loss: 70.6183   val Loss: 70.9994   time: 0.24s   best: 65.3591
2023-10-15 10:07:47,938:INFO:  Epoch 406/500:  train Loss: 71.1909   val Loss: 70.5087   time: 0.26s   best: 65.3591
2023-10-15 10:07:48,189:INFO:  Epoch 407/500:  train Loss: 69.0591   val Loss: 70.0343   time: 0.24s   best: 65.3591
2023-10-15 10:07:48,459:INFO:  Epoch 408/500:  train Loss: 69.7180   val Loss: 67.9516   time: 0.26s   best: 65.3591
2023-10-15 10:07:48,711:INFO:  Epoch 409/500:  train Loss: 68.1240   val Loss: 66.0294   time: 0.24s   best: 65.3591
2023-10-15 10:07:48,977:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:49,005:INFO:  Epoch 410/500:  train Loss: 65.6957   val Loss: 64.8774   time: 0.26s   best: 64.8774
2023-10-15 10:07:49,292:INFO:  Epoch 411/500:  train Loss: 65.9247   val Loss: 65.2699   time: 0.28s   best: 64.8774
2023-10-15 10:07:49,555:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:49,812:INFO:  Epoch 412/500:  train Loss: 66.5026   val Loss: 64.3929   time: 0.26s   best: 64.3929
2023-10-15 10:07:50,072:INFO:  Epoch 413/500:  train Loss: 65.0499   val Loss: 64.5513   time: 0.25s   best: 64.3929
2023-10-15 10:07:50,323:INFO:  Epoch 414/500:  train Loss: 66.1945   val Loss: 64.4920   time: 0.24s   best: 64.3929
2023-10-15 10:07:50,587:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:50,631:INFO:  Epoch 415/500:  train Loss: 64.2256   val Loss: 63.6692   time: 0.26s   best: 63.6692
2023-10-15 10:07:50,884:INFO:  Epoch 416/500:  train Loss: 65.3328   val Loss: 66.5559   time: 0.24s   best: 63.6692
2023-10-15 10:07:51,154:INFO:  Epoch 417/500:  train Loss: 66.9223   val Loss: 66.7204   time: 0.26s   best: 63.6692
2023-10-15 10:07:51,496:INFO:  Epoch 418/500:  train Loss: 67.4483   val Loss: 67.1841   time: 0.33s   best: 63.6692
2023-10-15 10:07:51,750:INFO:  Epoch 419/500:  train Loss: 66.2329   val Loss: 69.4792   time: 0.24s   best: 63.6692
2023-10-15 10:07:52,020:INFO:  Epoch 420/500:  train Loss: 67.7003   val Loss: 66.2397   time: 0.24s   best: 63.6692
2023-10-15 10:07:52,270:INFO:  Epoch 421/500:  train Loss: 67.9595   val Loss: 64.5032   time: 0.24s   best: 63.6692
2023-10-15 10:07:52,540:INFO:  Epoch 422/500:  train Loss: 64.9371   val Loss: 63.6927   time: 0.24s   best: 63.6692
2023-10-15 10:07:52,792:INFO:  Epoch 423/500:  train Loss: 64.9036   val Loss: 64.5971   time: 0.24s   best: 63.6692
2023-10-15 10:07:53,065:INFO:  Epoch 424/500:  train Loss: 65.1479   val Loss: 64.3384   time: 0.24s   best: 63.6692
2023-10-15 10:07:53,315:INFO:  Epoch 425/500:  train Loss: 65.9040   val Loss: 65.0531   time: 0.24s   best: 63.6692
2023-10-15 10:07:53,629:INFO:  Epoch 426/500:  train Loss: 67.6472   val Loss: 65.0311   time: 0.31s   best: 63.6692
2023-10-15 10:07:53,889:INFO:  Epoch 427/500:  train Loss: 67.5529   val Loss: 66.2995   time: 0.25s   best: 63.6692
2023-10-15 10:07:54,158:INFO:  Epoch 428/500:  train Loss: 66.3928   val Loss: 64.5606   time: 0.26s   best: 63.6692
2023-10-15 10:07:54,409:INFO:  Epoch 429/500:  train Loss: 65.3675   val Loss: 63.9006   time: 0.24s   best: 63.6692
2023-10-15 10:07:54,671:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:54,725:INFO:  Epoch 430/500:  train Loss: 64.7878   val Loss: 63.0042   time: 0.26s   best: 63.0042
2023-10-15 10:07:54,980:INFO:  Epoch 431/500:  train Loss: 65.7233   val Loss: 64.3723   time: 0.24s   best: 63.0042
2023-10-15 10:07:55,247:INFO:  Epoch 432/500:  train Loss: 65.2313   val Loss: 64.8631   time: 0.26s   best: 63.0042
2023-10-15 10:07:55,497:INFO:  Epoch 433/500:  train Loss: 66.5366   val Loss: 66.5854   time: 0.24s   best: 63.0042
2023-10-15 10:07:55,806:INFO:  Epoch 434/500:  train Loss: 65.7907   val Loss: 66.4655   time: 0.30s   best: 63.0042
2023-10-15 10:07:56,056:INFO:  Epoch 435/500:  train Loss: 65.6199   val Loss: 64.3636   time: 0.24s   best: 63.0042
2023-10-15 10:07:56,320:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:56,466:INFO:  Epoch 436/500:  train Loss: 64.8869   val Loss: 62.9523   time: 0.26s   best: 62.9523
2023-10-15 10:07:56,727:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:07:56,758:INFO:  Epoch 437/500:  train Loss: 63.6984   val Loss: 62.2702   time: 0.24s   best: 62.2702
2023-10-15 10:07:57,012:INFO:  Epoch 438/500:  train Loss: 63.7414   val Loss: 63.9492   time: 0.24s   best: 62.2702
2023-10-15 10:07:57,279:INFO:  Epoch 439/500:  train Loss: 64.8124   val Loss: 63.2660   time: 0.26s   best: 62.2702
2023-10-15 10:07:57,530:INFO:  Epoch 440/500:  train Loss: 64.3767   val Loss: 63.3146   time: 0.24s   best: 62.2702
2023-10-15 10:07:57,836:INFO:  Epoch 441/500:  train Loss: 64.1052   val Loss: 64.0637   time: 0.29s   best: 62.2702
2023-10-15 10:07:58,086:INFO:  Epoch 442/500:  train Loss: 65.0032   val Loss: 62.5136   time: 0.24s   best: 62.2702
2023-10-15 10:07:58,355:INFO:  Epoch 443/500:  train Loss: 64.7410   val Loss: 62.8449   time: 0.26s   best: 62.2702
2023-10-15 10:07:58,606:INFO:  Epoch 444/500:  train Loss: 65.3771   val Loss: 65.6243   time: 0.24s   best: 62.2702
2023-10-15 10:07:58,879:INFO:  Epoch 445/500:  train Loss: 69.9330   val Loss: 74.5651   time: 0.26s   best: 62.2702
2023-10-15 10:07:59,132:INFO:  Epoch 446/500:  train Loss: 78.9040   val Loss: 80.7981   time: 0.24s   best: 62.2702
2023-10-15 10:07:59,402:INFO:  Epoch 447/500:  train Loss: 81.2460   val Loss: 79.2759   time: 0.26s   best: 62.2702
2023-10-15 10:07:59,653:INFO:  Epoch 448/500:  train Loss: 78.8637   val Loss: 76.9735   time: 0.24s   best: 62.2702
2023-10-15 10:07:59,957:INFO:  Epoch 449/500:  train Loss: 76.0800   val Loss: 72.6429   time: 0.29s   best: 62.2702
2023-10-15 10:08:00,207:INFO:  Epoch 450/500:  train Loss: 73.0719   val Loss: 71.1104   time: 0.24s   best: 62.2702
2023-10-15 10:08:00,476:INFO:  Epoch 451/500:  train Loss: 72.2652   val Loss: 70.1562   time: 0.26s   best: 62.2702
2023-10-15 10:08:00,727:INFO:  Epoch 452/500:  train Loss: 70.5973   val Loss: 69.3017   time: 0.24s   best: 62.2702
2023-10-15 10:08:01,000:INFO:  Epoch 453/500:  train Loss: 69.7561   val Loss: 69.5967   time: 0.26s   best: 62.2702
2023-10-15 10:08:01,251:INFO:  Epoch 454/500:  train Loss: 71.1465   val Loss: 70.9106   time: 0.24s   best: 62.2702
2023-10-15 10:08:01,520:INFO:  Epoch 455/500:  train Loss: 70.4892   val Loss: 68.9552   time: 0.26s   best: 62.2702
2023-10-15 10:08:01,772:INFO:  Epoch 456/500:  train Loss: 70.4485   val Loss: 74.9595   time: 0.24s   best: 62.2702
2023-10-15 10:08:02,076:INFO:  Epoch 457/500:  train Loss: 73.6113   val Loss: 73.8001   time: 0.29s   best: 62.2702
2023-10-15 10:08:02,327:INFO:  Epoch 458/500:  train Loss: 72.2341   val Loss: 69.7314   time: 0.24s   best: 62.2702
2023-10-15 10:08:02,595:INFO:  Epoch 459/500:  train Loss: 70.6018   val Loss: 68.9409   time: 0.26s   best: 62.2702
2023-10-15 10:08:02,848:INFO:  Epoch 460/500:  train Loss: 71.0384   val Loss: 68.6144   time: 0.24s   best: 62.2702
2023-10-15 10:08:03,119:INFO:  Epoch 461/500:  train Loss: 69.7707   val Loss: 67.9131   time: 0.26s   best: 62.2702
2023-10-15 10:08:03,370:INFO:  Epoch 462/500:  train Loss: 68.3335   val Loss: 66.7681   time: 0.24s   best: 62.2702
2023-10-15 10:08:03,640:INFO:  Epoch 463/500:  train Loss: 66.5803   val Loss: 64.8855   time: 0.26s   best: 62.2702
2023-10-15 10:08:03,905:INFO:  Epoch 464/500:  train Loss: 65.4099   val Loss: 65.4462   time: 0.24s   best: 62.2702
2023-10-15 10:08:04,196:INFO:  Epoch 465/500:  train Loss: 66.3445   val Loss: 64.7716   time: 0.28s   best: 62.2702
2023-10-15 10:08:04,448:INFO:  Epoch 466/500:  train Loss: 66.3563   val Loss: 67.2831   time: 0.24s   best: 62.2702
2023-10-15 10:08:04,719:INFO:  Epoch 467/500:  train Loss: 69.4309   val Loss: 69.8242   time: 0.26s   best: 62.2702
2023-10-15 10:08:04,973:INFO:  Epoch 468/500:  train Loss: 69.5707   val Loss: 66.4837   time: 0.24s   best: 62.2702
2023-10-15 10:08:05,243:INFO:  Epoch 469/500:  train Loss: 68.4218   val Loss: 66.8193   time: 0.26s   best: 62.2702
2023-10-15 10:08:05,493:INFO:  Epoch 470/500:  train Loss: 66.2564   val Loss: 64.7136   time: 0.24s   best: 62.2702
2023-10-15 10:08:05,764:INFO:  Epoch 471/500:  train Loss: 65.8791   val Loss: 65.1454   time: 0.26s   best: 62.2702
2023-10-15 10:08:06,053:INFO:  Epoch 472/500:  train Loss: 66.0098   val Loss: 65.0409   time: 0.28s   best: 62.2702
2023-10-15 10:08:06,322:INFO:  Epoch 473/500:  train Loss: 65.9880   val Loss: 64.0088   time: 0.26s   best: 62.2702
2023-10-15 10:08:06,572:INFO:  Epoch 474/500:  train Loss: 68.0064   val Loss: 67.3365   time: 0.24s   best: 62.2702
2023-10-15 10:08:06,848:INFO:  Epoch 475/500:  train Loss: 68.9779   val Loss: 70.8732   time: 0.27s   best: 62.2702
2023-10-15 10:08:07,100:INFO:  Epoch 476/500:  train Loss: 70.9701   val Loss: 70.8813   time: 0.24s   best: 62.2702
2023-10-15 10:08:07,371:INFO:  Epoch 477/500:  train Loss: 68.5251   val Loss: 68.5332   time: 0.26s   best: 62.2702
2023-10-15 10:08:07,622:INFO:  Epoch 478/500:  train Loss: 67.0916   val Loss: 67.9107   time: 0.24s   best: 62.2702
2023-10-15 10:08:07,893:INFO:  Epoch 479/500:  train Loss: 73.4257   val Loss: 63.8568   time: 0.26s   best: 62.2702
2023-10-15 10:08:08,180:INFO:  Epoch 480/500:  train Loss: 66.6989   val Loss: 65.5744   time: 0.28s   best: 62.2702
2023-10-15 10:08:08,449:INFO:  Epoch 481/500:  train Loss: 66.1322   val Loss: 66.2785   time: 0.26s   best: 62.2702
2023-10-15 10:08:08,701:INFO:  Epoch 482/500:  train Loss: 66.6938   val Loss: 66.1845   time: 0.24s   best: 62.2702
2023-10-15 10:08:08,983:INFO:  Epoch 483/500:  train Loss: 68.4399   val Loss: 64.4924   time: 0.28s   best: 62.2702
2023-10-15 10:08:09,242:INFO:  Epoch 484/500:  train Loss: 64.8620   val Loss: 63.6499   time: 0.25s   best: 62.2702
2023-10-15 10:08:09,510:INFO:  Epoch 485/500:  train Loss: 65.5765   val Loss: 63.2950   time: 0.26s   best: 62.2702
2023-10-15 10:08:09,761:INFO:  Epoch 486/500:  train Loss: 66.4108   val Loss: 67.4298   time: 0.24s   best: 62.2702
2023-10-15 10:08:10,031:INFO:  Epoch 487/500:  train Loss: 67.6069   val Loss: 65.3184   time: 0.26s   best: 62.2702
2023-10-15 10:08:10,319:INFO:  Epoch 488/500:  train Loss: 69.3511   val Loss: 65.4865   time: 0.28s   best: 62.2702
2023-10-15 10:08:10,586:INFO:  Epoch 489/500:  train Loss: 64.8102   val Loss: 63.0783   time: 0.26s   best: 62.2702
2023-10-15 10:08:10,840:INFO:  Epoch 490/500:  train Loss: 63.8479   val Loss: 63.1850   time: 0.24s   best: 62.2702
2023-10-15 10:08:11,104:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:08:11,176:INFO:  Epoch 491/500:  train Loss: 65.0500   val Loss: 61.9475   time: 0.26s   best: 61.9475
2023-10-15 10:08:11,421:INFO:  Epoch 492/500:  train Loss: 63.3379   val Loss: 62.4273   time: 0.24s   best: 61.9475
2023-10-15 10:08:11,696:INFO:  Epoch 493/500:  train Loss: 63.0724   val Loss: 62.2629   time: 0.26s   best: 61.9475
2023-10-15 10:08:11,948:INFO:  Epoch 494/500:  train Loss: 63.6171   val Loss: 62.6988   time: 0.24s   best: 61.9475
2023-10-15 10:08:12,210:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:08:12,256:INFO:  Epoch 495/500:  train Loss: 64.0201   val Loss: 61.5625   time: 0.26s   best: 61.5625
2023-10-15 10:08:12,546:INFO:  Epoch 496/500:  train Loss: 62.5539   val Loss: 62.0795   time: 0.28s   best: 61.5625
2023-10-15 10:08:12,792:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:08:12,843:INFO:  Epoch 497/500:  train Loss: 62.7215   val Loss: 61.3864   time: 0.24s   best: 61.3864
2023-10-15 10:08:13,113:INFO:  Epoch 498/500:  train Loss: 62.5362   val Loss: 61.4466   time: 0.26s   best: 61.3864
2023-10-15 10:08:13,364:INFO:  Epoch 499/500:  train Loss: 62.6688   val Loss: 61.4153   time: 0.24s   best: 61.3864
2023-10-15 10:08:13,678:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder Debug (2 layer + 0.2 dropout)_57a9.pt
2023-10-15 10:08:13,887:INFO:  Epoch 500/500:  train Loss: 61.9591   val Loss: 60.9009   time: 0.31s   best: 60.9009
2023-10-15 10:08:13,887:INFO:  -----> Training complete in 2m 23s   best validation loss: 60.9009
 
2023-10-15 18:40:31,313:INFO:  Starting experiment lstm autoencoder (2 layer + 0.2 dropout)
2023-10-15 18:40:31,330:INFO:  Defining the model
2023-10-15 18:40:31,387:INFO:  Reading the dataset
2023-10-15 19:09:47,038:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 19:09:47,073:INFO:  Epoch 1/500:  train Loss: 75.0752   val Loss: 71.0679   time: 426.32s   best: 71.0679
2023-10-15 19:16:55,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 19:16:55,116:INFO:  Epoch 2/500:  train Loss: 63.4727   val Loss: 62.9232   time: 428.01s   best: 62.9232
2023-10-15 19:24:04,461:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 19:24:04,494:INFO:  Epoch 3/500:  train Loss: 57.1153   val Loss: 54.3477   time: 429.32s   best: 54.3477
2023-10-15 19:31:11,591:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 19:31:11,623:INFO:  Epoch 4/500:  train Loss: 51.4480   val Loss: 49.3490   time: 427.09s   best: 49.3490
2023-10-15 19:38:19,153:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 19:38:19,176:INFO:  Epoch 5/500:  train Loss: 46.7728   val Loss: 47.4888   time: 427.52s   best: 47.4888
2023-10-15 19:45:25,201:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 19:45:25,230:INFO:  Epoch 6/500:  train Loss: 43.1290   val Loss: 42.8413   time: 426.01s   best: 42.8413
2023-10-15 19:52:31,245:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 19:52:31,284:INFO:  Epoch 7/500:  train Loss: 40.3811   val Loss: 40.8396   time: 426.01s   best: 40.8396
2023-10-15 19:59:41,877:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 19:59:42,012:INFO:  Epoch 8/500:  train Loss: 38.3724   val Loss: 38.4615   time: 430.58s   best: 38.4615
2023-10-15 20:06:49,849:INFO:  Epoch 9/500:  train Loss: 36.6894   val Loss: 39.7553   time: 427.81s   best: 38.4615
2023-10-15 20:13:54,874:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 20:13:55,062:INFO:  Epoch 10/500:  train Loss: 35.3709   val Loss: 37.5545   time: 425.00s   best: 37.5545
2023-10-15 20:21:01,789:INFO:  Epoch 11/500:  train Loss: 34.3374   val Loss: 37.7791   time: 426.72s   best: 37.5545
2023-10-15 20:28:11,582:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 20:28:11,609:INFO:  Epoch 12/500:  train Loss: 33.4857   val Loss: 36.5128   time: 429.78s   best: 36.5128
2023-10-15 20:35:17,545:INFO:  Epoch 13/500:  train Loss: 32.4839   val Loss: 36.8807   time: 425.93s   best: 36.5128
2023-10-15 20:42:22,748:INFO:  Epoch 14/500:  train Loss: 31.7814   val Loss: 38.8598   time: 425.19s   best: 36.5128
2023-10-15 20:49:31,984:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 20:49:32,021:INFO:  Epoch 15/500:  train Loss: 31.1892   val Loss: 31.8112   time: 429.20s   best: 31.8112
2023-10-15 20:56:41,528:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 20:56:41,556:INFO:  Epoch 16/500:  train Loss: 30.6018   val Loss: 31.8109   time: 429.50s   best: 31.8109
2023-10-15 21:03:49,788:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 21:03:49,837:INFO:  Epoch 17/500:  train Loss: 30.2228   val Loss: 30.4940   time: 428.23s   best: 30.4940
2023-10-15 21:10:58,862:INFO:  Epoch 18/500:  train Loss: 29.6948   val Loss: 31.3444   time: 429.01s   best: 30.4940
2023-10-15 21:18:04,018:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 21:18:04,049:INFO:  Epoch 19/500:  train Loss: 29.1693   val Loss: 29.7282   time: 425.14s   best: 29.7282
2023-10-15 21:25:13,655:INFO:  Epoch 20/500:  train Loss: 28.8931   val Loss: 30.1827   time: 429.60s   best: 29.7282
2023-10-15 21:32:20,436:INFO:  Epoch 21/500:  train Loss: 28.4347   val Loss: 29.9231   time: 426.78s   best: 29.7282
2023-10-15 21:39:26,102:INFO:  Epoch 22/500:  train Loss: 28.1142   val Loss: 31.5229   time: 425.64s   best: 29.7282
2023-10-15 21:46:31,709:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 21:46:31,755:INFO:  Epoch 23/500:  train Loss: 27.8591   val Loss: 28.9155   time: 425.59s   best: 28.9155
2023-10-15 21:53:36,799:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 21:53:36,826:INFO:  Epoch 24/500:  train Loss: 27.5672   val Loss: 28.7418   time: 425.03s   best: 28.7418
2023-10-15 22:00:42,912:INFO:  Epoch 25/500:  train Loss: 27.2561   val Loss: 28.9771   time: 426.09s   best: 28.7418
2023-10-15 22:07:51,328:INFO:  Epoch 26/500:  train Loss: 27.3020   val Loss: 28.8066   time: 428.39s   best: 28.7418
2023-10-15 22:14:57,722:INFO:  Epoch 27/500:  train Loss: 26.8717   val Loss: 29.8141   time: 426.36s   best: 28.7418
2023-10-15 22:22:06,740:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 22:22:06,765:INFO:  Epoch 28/500:  train Loss: 26.5132   val Loss: 28.5693   time: 429.01s   best: 28.5693
2023-10-15 22:29:16,506:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 22:29:16,543:INFO:  Epoch 29/500:  train Loss: 26.0666   val Loss: 27.8299   time: 429.74s   best: 27.8299
2023-10-15 22:36:25,668:INFO:  Epoch 30/500:  train Loss: 26.5184   val Loss: 28.4289   time: 429.11s   best: 27.8299
2023-10-15 22:43:35,983:INFO:  Epoch 31/500:  train Loss: 26.0325   val Loss: 37.8474   time: 430.29s   best: 27.8299
2023-10-15 22:50:45,847:INFO:  Epoch 32/500:  train Loss: 25.8676   val Loss: 28.0668   time: 429.84s   best: 27.8299
2023-10-15 22:57:55,578:INFO:  Epoch 33/500:  train Loss: 25.9636   val Loss: 28.5908   time: 429.71s   best: 27.8299
2023-10-15 23:05:00,500:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 23:05:00,601:INFO:  Epoch 34/500:  train Loss: 25.4470   val Loss: 27.4766   time: 424.88s   best: 27.4766
2023-10-15 23:12:11,034:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 23:12:11,062:INFO:  Epoch 35/500:  train Loss: 25.1148   val Loss: 27.1692   time: 430.41s   best: 27.1692
2023-10-15 23:19:20,787:INFO:  Epoch 36/500:  train Loss: 25.3432   val Loss: 27.2247   time: 429.71s   best: 27.1692
2023-10-15 23:26:30,966:INFO:  Epoch 37/500:  train Loss: 24.8526   val Loss: 27.3198   time: 430.14s   best: 27.1692
2023-10-15 23:33:36,637:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-15 23:33:36,665:INFO:  Epoch 38/500:  train Loss: 24.7396   val Loss: 26.7962   time: 425.65s   best: 26.7962
2023-10-15 23:40:42,124:INFO:  Epoch 39/500:  train Loss: 24.5768   val Loss: 28.4772   time: 425.46s   best: 26.7962
2023-10-15 23:47:49,817:INFO:  Epoch 40/500:  train Loss: 24.5826   val Loss: 27.8851   time: 427.68s   best: 26.7962
2023-10-15 23:54:59,925:INFO:  Epoch 41/500:  train Loss: 24.2439   val Loss: 26.9015   time: 430.10s   best: 26.7962
2023-10-16 00:02:10,186:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 00:02:10,249:INFO:  Epoch 42/500:  train Loss: 24.2820   val Loss: 26.6318   time: 430.23s   best: 26.6318
2023-10-16 00:09:20,533:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 00:09:20,580:INFO:  Epoch 43/500:  train Loss: 24.1039   val Loss: 26.1592   time: 430.26s   best: 26.1592
2023-10-16 00:16:30,655:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 00:16:30,691:INFO:  Epoch 44/500:  train Loss: 23.9524   val Loss: 26.1045   time: 430.07s   best: 26.1045
2023-10-16 00:23:40,461:INFO:  Epoch 45/500:  train Loss: 23.8775   val Loss: 27.1775   time: 429.77s   best: 26.1045
2023-10-16 00:30:49,510:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 00:30:49,548:INFO:  Epoch 46/500:  train Loss: 23.8680   val Loss: 25.9423   time: 429.02s   best: 25.9423
2023-10-16 00:37:55,590:INFO:  Epoch 47/500:  train Loss: 23.6896   val Loss: 26.6479   time: 426.04s   best: 25.9423
2023-10-16 00:45:05,285:INFO:  Epoch 48/500:  train Loss: 23.4895   val Loss: 26.1651   time: 429.68s   best: 25.9423
2023-10-16 00:52:10,416:INFO:  Epoch 49/500:  train Loss: 23.5223   val Loss: 25.9662   time: 425.11s   best: 25.9423
2023-10-16 00:59:18,071:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 00:59:18,103:INFO:  Epoch 50/500:  train Loss: 23.4333   val Loss: 25.7815   time: 427.63s   best: 25.7815
2023-10-16 01:06:25,273:INFO:  Epoch 51/500:  train Loss: 23.2444   val Loss: 26.0493   time: 427.17s   best: 25.7815
2023-10-16 01:13:32,849:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 01:13:32,899:INFO:  Epoch 52/500:  train Loss: 23.1425   val Loss: 25.6906   time: 427.56s   best: 25.6906
2023-10-16 01:20:40,270:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 01:20:40,296:INFO:  Epoch 53/500:  train Loss: 23.0016   val Loss: 25.3939   time: 427.35s   best: 25.3939
2023-10-16 01:27:50,014:INFO:  Epoch 54/500:  train Loss: 23.4991   val Loss: 26.1806   time: 429.71s   best: 25.3939
2023-10-16 01:34:56,694:INFO:  Epoch 55/500:  train Loss: 22.9807   val Loss: 25.5212   time: 426.65s   best: 25.3939
2023-10-16 01:42:02,551:INFO:  Epoch 56/500:  train Loss: 22.8104   val Loss: 25.7162   time: 425.83s   best: 25.3939
2023-10-16 01:49:13,243:INFO:  Epoch 57/500:  train Loss: 22.6694   val Loss: 25.7174   time: 430.67s   best: 25.3939
2023-10-16 01:56:20,406:INFO:  Epoch 58/500:  train Loss: 22.7328   val Loss: 25.6221   time: 427.13s   best: 25.3939
2023-10-16 02:03:30,473:INFO:  Epoch 59/500:  train Loss: 22.5667   val Loss: 25.4832   time: 430.04s   best: 25.3939
2023-10-16 02:10:38,946:INFO:  Epoch 60/500:  train Loss: 22.5149   val Loss: 25.5095   time: 428.45s   best: 25.3939
2023-10-16 02:17:50,149:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 02:17:50,197:INFO:  Epoch 61/500:  train Loss: 22.4102   val Loss: 24.5627   time: 431.17s   best: 24.5627
2023-10-16 02:24:57,732:INFO:  Epoch 62/500:  train Loss: 22.6146   val Loss: 25.5371   time: 427.51s   best: 24.5627
2023-10-16 02:32:06,933:INFO:  Epoch 63/500:  train Loss: 22.5817   val Loss: 26.7517   time: 429.17s   best: 24.5627
2023-10-16 02:39:13,151:INFO:  Epoch 64/500:  train Loss: 22.4959   val Loss: 24.9874   time: 426.20s   best: 24.5627
2023-10-16 02:46:23,216:INFO:  Epoch 65/500:  train Loss: 22.2064   val Loss: 26.7450   time: 430.06s   best: 24.5627
2023-10-16 02:53:34,253:INFO:  Epoch 66/500:  train Loss: 22.5369   val Loss: 25.4212   time: 431.01s   best: 24.5627
2023-10-16 03:00:43,477:INFO:  Epoch 67/500:  train Loss: 22.1158   val Loss: 25.6099   time: 429.22s   best: 24.5627
2023-10-16 03:07:52,996:INFO:  Epoch 68/500:  train Loss: 21.9607   val Loss: 24.9202   time: 429.51s   best: 24.5627
2023-10-16 03:14:58,705:INFO:  Epoch 69/500:  train Loss: 21.8684   val Loss: 25.1326   time: 425.68s   best: 24.5627
2023-10-16 03:22:06,260:INFO:  Epoch 70/500:  train Loss: 21.8066   val Loss: 25.8691   time: 427.54s   best: 24.5627
2023-10-16 03:29:12,306:INFO:  Epoch 71/500:  train Loss: 21.9952   val Loss: 24.6501   time: 426.04s   best: 24.5627
2023-10-16 03:36:19,362:INFO:  Epoch 72/500:  train Loss: 21.7288   val Loss: 25.1475   time: 427.03s   best: 24.5627
2023-10-16 03:43:25,499:INFO:  Epoch 73/500:  train Loss: 22.0803   val Loss: 25.4215   time: 426.10s   best: 24.5627
2023-10-16 03:50:36,633:INFO:  Epoch 74/500:  train Loss: 21.6508   val Loss: 26.0195   time: 431.11s   best: 24.5627
2023-10-16 03:57:46,610:INFO:  Epoch 75/500:  train Loss: 22.2294   val Loss: 24.6756   time: 429.96s   best: 24.5627
2023-10-16 04:04:52,343:INFO:  Epoch 76/500:  train Loss: 21.5662   val Loss: 24.9166   time: 425.71s   best: 24.5627
2023-10-16 04:11:59,515:INFO:  Epoch 77/500:  train Loss: 21.5480   val Loss: 24.9471   time: 427.15s   best: 24.5627
2023-10-16 04:19:05,848:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 04:19:05,901:INFO:  Epoch 78/500:  train Loss: 21.4792   val Loss: 24.4868   time: 426.28s   best: 24.4868
2023-10-16 04:26:13,147:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 04:26:13,179:INFO:  Epoch 79/500:  train Loss: 21.4702   val Loss: 24.4100   time: 427.22s   best: 24.4100
2023-10-16 04:33:20,621:INFO:  Epoch 80/500:  train Loss: 21.4188   val Loss: 24.7838   time: 427.43s   best: 24.4100
2023-10-16 04:40:30,539:INFO:  Epoch 81/500:  train Loss: 21.3857   val Loss: 24.9750   time: 429.90s   best: 24.4100
2023-10-16 04:47:40,503:INFO:  Epoch 82/500:  train Loss: 21.4110   val Loss: 25.1496   time: 429.93s   best: 24.4100
2023-10-16 04:54:47,824:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 04:54:47,859:INFO:  Epoch 83/500:  train Loss: 21.3424   val Loss: 24.2508   time: 427.30s   best: 24.2508
2023-10-16 05:01:55,109:INFO:  Epoch 84/500:  train Loss: 21.1359   val Loss: 25.7281   time: 427.25s   best: 24.2508
2023-10-16 05:09:05,659:INFO:  Epoch 85/500:  train Loss: 21.0190   val Loss: 24.8352   time: 430.51s   best: 24.2508
2023-10-16 05:16:11,094:INFO:  Epoch 86/500:  train Loss: 21.2015   val Loss: 24.6067   time: 425.39s   best: 24.2508
2023-10-16 05:23:20,939:INFO:  Epoch 87/500:  train Loss: 20.9549   val Loss: 24.6057   time: 429.81s   best: 24.2508
2023-10-16 05:30:26,209:INFO:  Epoch 88/500:  train Loss: 20.8934   val Loss: 24.9028   time: 425.24s   best: 24.2508
2023-10-16 05:37:36,239:INFO:  Epoch 89/500:  train Loss: 20.8635   val Loss: 24.5832   time: 430.01s   best: 24.2508
2023-10-16 05:44:46,166:INFO:  Epoch 90/500:  train Loss: 20.9043   val Loss: 24.7957   time: 429.91s   best: 24.2508
2023-10-16 05:51:56,401:INFO:  Epoch 91/500:  train Loss: 20.8133   val Loss: 26.1436   time: 430.20s   best: 24.2508
2023-10-16 05:59:05,885:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 05:59:05,941:INFO:  Epoch 92/500:  train Loss: 22.3725   val Loss: 24.2255   time: 429.45s   best: 24.2255
2023-10-16 06:06:13,231:INFO:  Epoch 93/500:  train Loss: 20.9098   val Loss: 24.5976   time: 427.27s   best: 24.2255
2023-10-16 06:13:23,628:INFO:  Epoch 94/500:  train Loss: 20.7122   val Loss: 24.4845   time: 430.38s   best: 24.2255
2023-10-16 06:20:32,757:INFO:  Epoch 95/500:  train Loss: 20.7416   val Loss: 24.2596   time: 429.11s   best: 24.2255
2023-10-16 06:27:41,018:INFO:  Epoch 96/500:  train Loss: 20.6583   val Loss: 24.3073   time: 428.22s   best: 24.2255
2023-10-16 06:34:47,744:INFO:  Epoch 97/500:  train Loss: 21.1420   val Loss: 24.8899   time: 426.69s   best: 24.2255
2023-10-16 06:41:53,325:INFO:  Epoch 98/500:  train Loss: 20.7036   val Loss: 24.6102   time: 425.56s   best: 24.2255
2023-10-16 06:48:59,165:INFO:  Epoch 99/500:  train Loss: 20.5741   val Loss: 24.4035   time: 425.81s   best: 24.2255
2023-10-16 06:56:05,231:INFO:  Epoch 100/500:  train Loss: 20.8554   val Loss: 25.8655   time: 426.04s   best: 24.2255
2023-10-16 07:03:14,613:INFO:  Epoch 101/500:  train Loss: 20.5180   val Loss: 25.0754   time: 429.37s   best: 24.2255
2023-10-16 07:10:20,828:INFO:  Epoch 102/500:  train Loss: 20.6493   val Loss: 24.6289   time: 426.19s   best: 24.2255
2023-10-16 07:17:28,375:INFO:  Epoch 103/500:  train Loss: 20.5847   val Loss: 24.6898   time: 427.50s   best: 24.2255
2023-10-16 07:24:36,820:INFO:  Epoch 104/500:  train Loss: 20.4333   val Loss: 24.3084   time: 428.43s   best: 24.2255
2023-10-16 07:31:45,426:INFO:  Epoch 105/500:  train Loss: 20.3213   val Loss: 24.3675   time: 428.59s   best: 24.2255
2023-10-16 07:38:52,200:INFO:  Epoch 106/500:  train Loss: 20.3139   val Loss: 25.8495   time: 426.75s   best: 24.2255
2023-10-16 07:46:01,618:INFO:  Epoch 107/500:  train Loss: 20.4542   val Loss: 26.2353   time: 429.39s   best: 24.2255
2023-10-16 07:53:11,558:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 07:53:11,600:INFO:  Epoch 108/500:  train Loss: 20.4280   val Loss: 24.0612   time: 429.90s   best: 24.0612
2023-10-16 08:00:18,713:INFO:  Epoch 109/500:  train Loss: 20.3905   val Loss: 24.1217   time: 427.10s   best: 24.0612
2023-10-16 08:07:27,548:INFO:  Epoch 110/500:  train Loss: 20.2481   val Loss: 24.2699   time: 428.82s   best: 24.0612
2023-10-16 08:14:35,767:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 08:14:35,792:INFO:  Epoch 111/500:  train Loss: 20.4719   val Loss: 23.8058   time: 428.21s   best: 23.8058
2023-10-16 08:21:44,257:INFO:  Epoch 112/500:  train Loss: 20.3262   val Loss: 24.1791   time: 428.46s   best: 23.8058
2023-10-16 08:28:51,181:INFO:  Epoch 113/500:  train Loss: 20.3714   val Loss: 24.2305   time: 426.91s   best: 23.8058
2023-10-16 08:36:00,560:INFO:  Epoch 114/500:  train Loss: 20.1733   val Loss: 24.1477   time: 429.35s   best: 23.8058
2023-10-16 08:43:11,676:INFO:  Epoch 115/500:  train Loss: 20.1157   val Loss: 24.3084   time: 431.08s   best: 23.8058
2023-10-16 08:50:19,236:INFO:  Epoch 116/500:  train Loss: 20.2441   val Loss: 24.1982   time: 427.55s   best: 23.8058
2023-10-16 08:57:29,286:INFO:  Epoch 117/500:  train Loss: 20.3693   val Loss: 24.0502   time: 430.04s   best: 23.8058
2023-10-16 09:04:34,465:INFO:  Epoch 118/500:  train Loss: 20.0182   val Loss: 24.6276   time: 425.15s   best: 23.8058
2023-10-16 09:11:40,902:INFO:  Epoch 119/500:  train Loss: 20.0044   val Loss: 25.5443   time: 426.40s   best: 23.8058
2023-10-16 09:18:51,247:INFO:  Epoch 120/500:  train Loss: 19.9773   val Loss: 24.3043   time: 430.32s   best: 23.8058
2023-10-16 09:26:01,381:INFO:  Epoch 121/500:  train Loss: 20.1428   val Loss: 24.0848   time: 430.12s   best: 23.8058
2023-10-16 09:33:10,903:INFO:  Epoch 122/500:  train Loss: 20.0221   val Loss: 24.0920   time: 429.51s   best: 23.8058
2023-10-16 09:40:20,483:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 09:40:20,525:INFO:  Epoch 123/500:  train Loss: 19.8882   val Loss: 23.7098   time: 429.53s   best: 23.7098
2023-10-16 09:47:27,747:INFO:  Epoch 124/500:  train Loss: 20.3107   val Loss: 24.0555   time: 427.20s   best: 23.7098
2023-10-16 09:54:36,809:INFO:  Epoch 125/500:  train Loss: 20.1759   val Loss: 24.5587   time: 429.05s   best: 23.7098
2023-10-16 10:01:47,870:INFO:  Epoch 126/500:  train Loss: 19.8409   val Loss: 23.9399   time: 431.02s   best: 23.7098
2023-10-16 10:08:53,538:INFO:  Epoch 127/500:  train Loss: 19.8761   val Loss: 24.0755   time: 425.64s   best: 23.7098
2023-10-16 10:16:00,059:INFO:  Epoch 128/500:  train Loss: 19.9232   val Loss: 26.8071   time: 426.50s   best: 23.7098
2023-10-16 10:23:05,298:INFO:  Epoch 129/500:  train Loss: 19.9394   val Loss: 23.7694   time: 425.22s   best: 23.7098
2023-10-16 10:30:13,032:INFO:  Epoch 130/500:  train Loss: 19.7771   val Loss: 23.9572   time: 427.72s   best: 23.7098
2023-10-16 10:37:23,288:INFO:  Epoch 131/500:  train Loss: 19.8759   val Loss: 35.0542   time: 430.23s   best: 23.7098
2023-10-16 10:44:28,885:INFO:  Epoch 132/500:  train Loss: 19.8606   val Loss: 24.0785   time: 425.57s   best: 23.7098
2023-10-16 10:51:34,536:INFO:  Epoch 133/500:  train Loss: 19.8480   val Loss: 23.9706   time: 425.64s   best: 23.7098
2023-10-16 10:58:42,908:INFO:  Epoch 134/500:  train Loss: 19.7267   val Loss: 24.2788   time: 428.35s   best: 23.7098
2023-10-16 11:05:51,045:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 11:05:51,095:INFO:  Epoch 135/500:  train Loss: 19.6858   val Loss: 23.6855   time: 428.10s   best: 23.6855
2023-10-16 11:12:58,218:INFO:  Epoch 136/500:  train Loss: 19.6192   val Loss: 23.9329   time: 427.11s   best: 23.6855
2023-10-16 11:20:08,602:INFO:  Epoch 137/500:  train Loss: 19.6317   val Loss: 23.9905   time: 430.36s   best: 23.6855
2023-10-16 11:27:18,863:INFO:  Epoch 138/500:  train Loss: 19.5747   val Loss: 24.0731   time: 430.23s   best: 23.6855
2023-10-16 11:34:24,274:INFO:  Epoch 139/500:  train Loss: 19.8972   val Loss: 24.4064   time: 425.38s   best: 23.6855
2023-10-16 11:41:33,907:INFO:  Epoch 140/500:  train Loss: 19.5756   val Loss: 24.1882   time: 429.60s   best: 23.6855
2023-10-16 11:48:43,030:INFO:  Epoch 141/500:  train Loss: 19.6034   val Loss: 23.7541   time: 429.11s   best: 23.6855
2023-10-16 11:55:52,908:INFO:  Epoch 142/500:  train Loss: 19.7323   val Loss: 24.7967   time: 429.83s   best: 23.6855
2023-10-16 12:03:01,940:INFO:  Epoch 143/500:  train Loss: 19.7502   val Loss: 23.8285   time: 429.01s   best: 23.6855
2023-10-16 12:10:07,033:INFO:  Epoch 144/500:  train Loss: 19.6534   val Loss: 24.0316   time: 425.06s   best: 23.6855
2023-10-16 12:17:12,408:INFO:  Epoch 145/500:  train Loss: 19.5451   val Loss: 24.4343   time: 425.34s   best: 23.6855
2023-10-16 12:24:15,938:INFO:  Epoch 146/500:  train Loss: 19.5395   val Loss: 24.3709   time: 423.51s   best: 23.6855
2023-10-16 12:31:20,535:INFO:  Epoch 147/500:  train Loss: 19.5352   val Loss: 23.8056   time: 424.55s   best: 23.6855
2023-10-16 12:38:26,442:INFO:  Epoch 148/500:  train Loss: 19.4727   val Loss: 24.2379   time: 425.90s   best: 23.6855
2023-10-16 12:48:34,492:INFO:  Epoch 149/500:  train Loss: 19.3910   val Loss: 24.8328   time: 608.02s   best: 23.6855
2023-10-16 13:07:39,849:INFO:  Epoch 150/500:  train Loss: 19.4903   val Loss: 24.0282   time: 1145.33s   best: 23.6855
2023-10-16 13:17:35,046:INFO:  Epoch 151/500:  train Loss: 19.6449   val Loss: 23.9486   time: 595.17s   best: 23.6855
2023-10-16 13:29:43,458:INFO:  Epoch 152/500:  train Loss: 19.6059   val Loss: 24.2648   time: 728.39s   best: 23.6855
2023-10-16 13:58:03,834:INFO:  Epoch 153/500:  train Loss: 19.3854   val Loss: 24.2601   time: 1700.36s   best: 23.6855
2023-10-16 14:18:55,845:INFO:  Epoch 154/500:  train Loss: 19.2633   val Loss: 24.4572   time: 1252.00s   best: 23.6855
2023-10-16 14:33:45,104:INFO:  Epoch 155/500:  train Loss: 19.3232   val Loss: 28.3051   time: 889.25s   best: 23.6855
2023-10-16 14:52:19,747:INFO:  Epoch 156/500:  train Loss: 19.5035   val Loss: 24.0324   time: 1114.62s   best: 23.6855
2023-10-16 15:08:21,844:INFO:  Epoch 157/500:  train Loss: 19.3828   val Loss: 24.2944   time: 962.06s   best: 23.6855
2023-10-16 15:22:59,744:INFO:  Epoch 158/500:  train Loss: 19.4115   val Loss: 24.3882   time: 877.88s   best: 23.6855
2023-10-16 15:37:50,259:INFO:  Epoch 159/500:  train Loss: 19.5370   val Loss: 24.1535   time: 890.50s   best: 23.6855
2023-10-16 15:53:10,076:INFO:  Epoch 160/500:  train Loss: 19.4937   val Loss: 24.4149   time: 919.80s   best: 23.6855
2023-10-16 16:08:30,393:INFO:  Epoch 161/500:  train Loss: 19.2660   val Loss: 23.7998   time: 920.30s   best: 23.6855
2023-10-16 16:23:49,215:INFO:  Epoch 162/500:  train Loss: 19.3164   val Loss: 24.2049   time: 918.81s   best: 23.6855
2023-10-16 16:38:39,325:INFO:  Epoch 163/500:  train Loss: 19.3233   val Loss: 24.1176   time: 890.09s   best: 23.6855
2023-10-16 16:53:06,934:INFO:  Epoch 164/500:  train Loss: 19.1677   val Loss: 24.2286   time: 867.59s   best: 23.6855
2023-10-16 17:07:59,254:INFO:  Epoch 165/500:  train Loss: 19.1306   val Loss: 24.4456   time: 892.29s   best: 23.6855
2023-10-16 17:22:19,342:INFO:  Epoch 166/500:  train Loss: 19.1917   val Loss: 24.4224   time: 860.07s   best: 23.6855
2023-10-16 17:36:58,707:INFO:  Epoch 167/500:  train Loss: 19.0920   val Loss: 24.1495   time: 879.35s   best: 23.6855
2023-10-16 17:51:25,656:INFO:  Epoch 168/500:  train Loss: 19.4846   val Loss: 24.0229   time: 866.91s   best: 23.6855
2023-10-16 18:05:59,136:INFO:  Epoch 169/500:  train Loss: 19.2567   val Loss: 23.8587   time: 873.39s   best: 23.6855
2023-10-16 18:20:04,753:INFO:  Epoch 170/500:  train Loss: 19.0645   val Loss: 24.4435   time: 845.60s   best: 23.6855
2023-10-16 18:34:12,311:INFO:  Epoch 171/500:  train Loss: 19.1753   val Loss: 24.2465   time: 847.52s   best: 23.6855
2023-10-16 18:48:11,570:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-16 18:48:11,640:INFO:  Epoch 172/500:  train Loss: 19.6038   val Loss: 23.4134   time: 839.21s   best: 23.4134
2023-10-16 19:01:41,025:INFO:  Epoch 173/500:  train Loss: 19.1389   val Loss: 24.2161   time: 809.34s   best: 23.4134
2023-10-16 19:14:58,502:INFO:  Epoch 174/500:  train Loss: 19.3695   val Loss: 24.4636   time: 797.47s   best: 23.4134
2023-10-16 19:28:02,852:INFO:  Epoch 175/500:  train Loss: 19.1323   val Loss: 24.0142   time: 784.34s   best: 23.4134
2023-10-16 19:41:09,085:INFO:  Epoch 176/500:  train Loss: 19.0396   val Loss: 23.8482   time: 786.23s   best: 23.4134
2023-10-16 19:54:14,134:INFO:  Epoch 177/500:  train Loss: 18.9256   val Loss: 23.9385   time: 785.03s   best: 23.4134
2023-10-16 20:07:29,442:INFO:  Epoch 178/500:  train Loss: 19.0765   val Loss: 24.2853   time: 795.29s   best: 23.4134
2023-10-16 20:20:17,512:INFO:  Epoch 179/500:  train Loss: 19.1405   val Loss: 24.4288   time: 768.05s   best: 23.4134
2023-10-16 20:33:16,352:INFO:  Epoch 180/500:  train Loss: 19.2202   val Loss: 24.0742   time: 778.82s   best: 23.4134
2023-10-16 20:46:11,625:INFO:  Epoch 181/500:  train Loss: 18.9929   val Loss: 24.2341   time: 775.26s   best: 23.4134
2023-10-16 20:59:00,249:INFO:  Epoch 182/500:  train Loss: 19.3641   val Loss: 24.9853   time: 768.61s   best: 23.4134
2023-10-16 21:11:51,817:INFO:  Epoch 183/500:  train Loss: 19.0437   val Loss: 24.1088   time: 771.56s   best: 23.4134
2023-10-16 21:24:37,936:INFO:  Epoch 184/500:  train Loss: 19.2596   val Loss: 24.7786   time: 766.11s   best: 23.4134
2023-10-16 21:37:21,509:INFO:  Epoch 185/500:  train Loss: 19.0757   val Loss: 24.3982   time: 763.55s   best: 23.4134
2023-10-16 21:50:38,657:INFO:  Epoch 186/500:  train Loss: 19.0569   val Loss: 24.2227   time: 797.13s   best: 23.4134
2023-10-16 22:03:53,457:INFO:  Epoch 187/500:  train Loss: 18.9746   val Loss: 24.1366   time: 794.78s   best: 23.4134
2023-10-16 22:17:24,786:INFO:  Epoch 188/500:  train Loss: 19.1352   val Loss: 24.6030   time: 811.31s   best: 23.4134
2023-10-16 22:30:39,571:INFO:  Epoch 189/500:  train Loss: 18.8791   val Loss: 24.7026   time: 794.76s   best: 23.4134
2023-10-16 22:44:03,105:INFO:  Epoch 190/500:  train Loss: 18.8158   val Loss: 24.5997   time: 803.53s   best: 23.4134
2023-10-16 22:57:21,641:INFO:  Epoch 191/500:  train Loss: 19.0126   val Loss: 24.4330   time: 798.52s   best: 23.4134
2023-10-16 23:10:48,895:INFO:  Epoch 192/500:  train Loss: 18.9721   val Loss: 23.8547   time: 807.24s   best: 23.4134
2023-10-16 23:24:01,591:INFO:  Epoch 193/500:  train Loss: 18.8454   val Loss: 24.7793   time: 792.67s   best: 23.4134
2023-10-16 23:37:10,669:INFO:  Epoch 194/500:  train Loss: 18.9991   val Loss: 24.6379   time: 789.06s   best: 23.4134
2023-10-16 23:50:13,269:INFO:  Epoch 195/500:  train Loss: 18.8727   val Loss: 23.8926   time: 782.59s   best: 23.4134
2023-10-17 00:03:28,555:INFO:  Epoch 196/500:  train Loss: 18.8173   val Loss: 24.1046   time: 795.27s   best: 23.4134
2023-10-17 00:17:00,842:INFO:  Epoch 197/500:  train Loss: 18.7487   val Loss: 24.1612   time: 812.28s   best: 23.4134
2023-10-17 00:30:28,053:INFO:  Epoch 198/500:  train Loss: 18.8181   val Loss: 24.3550   time: 807.19s   best: 23.4134
2023-10-17 00:43:59,108:INFO:  Epoch 199/500:  train Loss: 18.8987   val Loss: 24.2274   time: 811.03s   best: 23.4134
2023-10-17 00:57:29,285:INFO:  Epoch 200/500:  train Loss: 18.8460   val Loss: 26.2775   time: 810.15s   best: 23.4134
2023-10-17 01:11:11,608:INFO:  Epoch 201/500:  train Loss: 18.8180   val Loss: 24.0064   time: 822.31s   best: 23.4134
2023-10-17 01:24:53,370:INFO:  Epoch 202/500:  train Loss: 18.7613   val Loss: 24.4191   time: 821.75s   best: 23.4134
2023-10-17 01:38:35,636:INFO:  Epoch 203/500:  train Loss: 18.7978   val Loss: 24.7266   time: 822.25s   best: 23.4134
2023-10-17 01:52:26,271:INFO:  Epoch 204/500:  train Loss: 18.7314   val Loss: 24.1777   time: 830.61s   best: 23.4134
2023-10-17 02:06:15,393:INFO:  Epoch 205/500:  train Loss: 18.6583   val Loss: 24.4343   time: 829.02s   best: 23.4134
2023-10-17 02:19:49,418:INFO:  Epoch 206/500:  train Loss: 18.8212   val Loss: 23.5399   time: 814.00s   best: 23.4134
2023-10-17 02:32:59,918:INFO:  Epoch 207/500:  train Loss: 18.6838   val Loss: 24.5115   time: 790.48s   best: 23.4134
2023-10-17 02:46:17,757:INFO:  Epoch 208/500:  train Loss: 18.7380   val Loss: 26.4403   time: 797.81s   best: 23.4134
2023-10-17 02:59:36,721:INFO:  Epoch 209/500:  train Loss: 18.6395   val Loss: 24.6741   time: 798.95s   best: 23.4134
2023-10-17 03:12:41,699:INFO:  Epoch 210/500:  train Loss: 18.9598   val Loss: 24.6887   time: 784.96s   best: 23.4134
2023-10-17 03:25:38,987:INFO:  Epoch 211/500:  train Loss: 18.6239   val Loss: 24.4010   time: 777.28s   best: 23.4134
2023-10-17 03:38:54,650:INFO:  Epoch 212/500:  train Loss: 18.6186   val Loss: 24.2753   time: 795.66s   best: 23.4134
2023-10-17 03:52:29,071:INFO:  Epoch 213/500:  train Loss: 18.7491   val Loss: 24.0504   time: 814.41s   best: 23.4134
2023-10-17 04:06:14,137:INFO:  Epoch 214/500:  train Loss: 18.7400   val Loss: 24.1888   time: 825.05s   best: 23.4134
2023-10-17 04:19:51,733:INFO:  Epoch 215/500:  train Loss: 18.6045   val Loss: 24.2749   time: 817.54s   best: 23.4134
2023-10-17 04:33:34,740:INFO:  Epoch 216/500:  train Loss: 18.5488   val Loss: 24.4884   time: 822.99s   best: 23.4134
2023-10-17 04:47:20,132:INFO:  Epoch 217/500:  train Loss: 18.7725   val Loss: 24.2858   time: 825.38s   best: 23.4134
2023-10-17 05:00:45,569:INFO:  Epoch 218/500:  train Loss: 18.4709   val Loss: 24.3319   time: 805.40s   best: 23.4134
2023-10-17 05:14:29,649:INFO:  Epoch 219/500:  train Loss: 18.5830   val Loss: 24.0806   time: 824.07s   best: 23.4134
2023-10-17 05:28:03,977:INFO:  Epoch 220/500:  train Loss: 18.7287   val Loss: 24.0535   time: 814.31s   best: 23.4134
2023-10-17 05:41:48,049:INFO:  Epoch 221/500:  train Loss: 18.4832   val Loss: 24.0768   time: 824.06s   best: 23.4134
2023-10-17 05:55:27,451:INFO:  Epoch 222/500:  train Loss: 18.5882   val Loss: 23.6817   time: 819.39s   best: 23.4134
2023-10-17 06:08:59,418:INFO:  Epoch 223/500:  train Loss: 18.6023   val Loss: 23.6597   time: 811.94s   best: 23.4134
2023-10-17 06:21:57,551:INFO:  Epoch 224/500:  train Loss: 18.5418   val Loss: 24.3659   time: 778.12s   best: 23.4134
2023-10-17 06:35:01,486:INFO:  Epoch 225/500:  train Loss: 18.4981   val Loss: 23.8910   time: 783.91s   best: 23.4134
2023-10-17 06:48:01,959:INFO:  Epoch 226/500:  train Loss: 18.6945   val Loss: 24.1088   time: 780.46s   best: 23.4134
2023-10-17 07:00:59,730:INFO:  Epoch 227/500:  train Loss: 18.4323   val Loss: 24.2481   time: 777.76s   best: 23.4134
2023-10-17 07:13:58,353:INFO:  Epoch 228/500:  train Loss: 18.6350   val Loss: 26.2673   time: 778.62s   best: 23.4134
2023-10-17 07:27:03,746:INFO:  Epoch 229/500:  train Loss: 18.6146   val Loss: 25.7340   time: 785.38s   best: 23.4134
2023-10-17 07:40:03,909:INFO:  Epoch 230/500:  train Loss: 18.5945   val Loss: 23.7954   time: 780.15s   best: 23.4134
2023-10-17 07:53:02,605:INFO:  Epoch 231/500:  train Loss: 18.4428   val Loss: 24.4480   time: 778.67s   best: 23.4134
2023-10-17 08:06:03,390:INFO:  Epoch 232/500:  train Loss: 18.7013   val Loss: 24.8618   time: 780.77s   best: 23.4134
2023-10-17 08:18:54,297:INFO:  Epoch 233/500:  train Loss: 18.6393   val Loss: 23.9412   time: 770.90s   best: 23.4134
2023-10-17 08:30:38,853:INFO:  Epoch 234/500:  train Loss: 18.4965   val Loss: 23.9082   time: 704.54s   best: 23.4134
2023-10-17 08:38:30,945:INFO:  Epoch 235/500:  train Loss: 18.5885   val Loss: 23.6185   time: 470.58s   best: 23.4134
2023-10-17 08:49:55,230:INFO:  Epoch 236/500:  train Loss: 18.5521   val Loss: 24.2890   time: 684.22s   best: 23.4134
2023-10-17 09:01:34,729:INFO:  Epoch 237/500:  train Loss: 18.4420   val Loss: 24.0606   time: 699.43s   best: 23.4134
2023-10-17 09:13:30,336:INFO:  Epoch 238/500:  train Loss: 18.6061   val Loss: 24.0340   time: 715.59s   best: 23.4134
2023-10-17 09:25:11,728:INFO:  Epoch 239/500:  train Loss: 18.3905   val Loss: 24.1068   time: 701.38s   best: 23.4134
2023-10-17 09:36:46,684:INFO:  Epoch 240/500:  train Loss: 18.6626   val Loss: 24.8605   time: 694.94s   best: 23.4134
2023-10-17 09:40:59,831:INFO:  Starting experiment lstm autoencoder (2 layer + 0.1 dropout)
2023-10-17 09:40:59,847:INFO:  Defining the model
2023-10-17 09:40:59,901:INFO:  Reading the dataset
2023-10-17 09:48:34,505:INFO:  Epoch 241/500:  train Loss: 18.3851   val Loss: 23.6850   time: 707.55s   best: 23.4134
2023-10-17 10:01:11,352:INFO:  Epoch 242/500:  train Loss: 18.5765   val Loss: 23.7966   time: 756.83s   best: 23.4134
2023-10-17 10:13:40,426:INFO:  Epoch 243/500:  train Loss: 18.4985   val Loss: 23.9328   time: 749.06s   best: 23.4134
2023-10-17 10:16:06,070:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 10:16:06,105:INFO:  Epoch 1/500:  train Loss: 74.8038   val Loss: 66.5466   time: 430.57s   best: 66.5466
2023-10-17 10:23:17,388:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 10:23:17,463:INFO:  Epoch 2/500:  train Loss: 61.9163   val Loss: 57.6575   time: 431.27s   best: 57.6575
2023-10-17 10:25:12,273:INFO:  Epoch 244/500:  train Loss: 18.5375   val Loss: 24.2421   time: 691.84s   best: 23.4134
2023-10-17 10:30:28,852:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 10:30:28,884:INFO:  Epoch 3/500:  train Loss: 54.8011   val Loss: 52.3331   time: 431.36s   best: 52.3331
2023-10-17 10:35:57,957:INFO:  Epoch 245/500:  train Loss: 18.4777   val Loss: 24.3339   time: 645.68s   best: 23.4134
2023-10-17 10:37:40,236:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 10:37:40,269:INFO:  Epoch 4/500:  train Loss: 49.2709   val Loss: 46.8267   time: 431.34s   best: 46.8267
2023-10-17 10:44:54,800:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 10:44:54,821:INFO:  Epoch 5/500:  train Loss: 45.0200   val Loss: 44.5015   time: 434.52s   best: 44.5015
2023-10-17 10:45:46,086:INFO:  Epoch 246/500:  train Loss: 18.3492   val Loss: 24.1650   time: 588.12s   best: 23.4134
2023-10-17 10:52:10,154:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 10:52:10,181:INFO:  Epoch 6/500:  train Loss: 41.8870   val Loss: 41.2424   time: 435.33s   best: 41.2424
2023-10-17 10:55:50,030:INFO:  Epoch 247/500:  train Loss: 18.3111   val Loss: 23.8675   time: 603.94s   best: 23.4134
2023-10-17 10:59:24,635:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 10:59:24,682:INFO:  Epoch 7/500:  train Loss: 39.4406   val Loss: 38.6922   time: 434.44s   best: 38.6922
2023-10-17 11:06:03,763:INFO:  Epoch 248/500:  train Loss: 18.6175   val Loss: 24.0870   time: 613.72s   best: 23.4134
2023-10-17 11:06:39,778:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 11:06:39,808:INFO:  Epoch 8/500:  train Loss: 37.5559   val Loss: 37.1343   time: 435.08s   best: 37.1343
2023-10-17 11:13:53,442:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 11:13:53,468:INFO:  Epoch 9/500:  train Loss: 35.9784   val Loss: 35.5002   time: 433.62s   best: 35.5002
2023-10-17 11:16:48,777:INFO:  Epoch 249/500:  train Loss: 18.2134   val Loss: 25.2921   time: 645.00s   best: 23.4134
2023-10-17 11:21:02,667:INFO:  Epoch 10/500:  train Loss: 34.7407   val Loss: 44.0030   time: 429.19s   best: 35.5002
2023-10-17 11:28:15,071:INFO:  Epoch 250/500:  train Loss: 18.4817   val Loss: 24.1382   time: 686.27s   best: 23.4134
2023-10-17 11:28:15,190:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 11:28:15,217:INFO:  Epoch 11/500:  train Loss: 33.8167   val Loss: 33.3954   time: 432.51s   best: 33.3954
2023-10-17 11:35:25,493:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 11:35:25,520:INFO:  Epoch 12/500:  train Loss: 32.8573   val Loss: 32.9942   time: 430.26s   best: 32.9942
2023-10-17 11:40:42,679:INFO:  Epoch 251/500:  train Loss: 18.2445   val Loss: 23.8697   time: 747.58s   best: 23.4134
2023-10-17 11:42:36,899:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 11:42:36,934:INFO:  Epoch 13/500:  train Loss: 32.1224   val Loss: 31.9047   time: 431.36s   best: 31.9047
2023-10-17 11:49:48,102:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 11:49:48,129:INFO:  Epoch 14/500:  train Loss: 31.4484   val Loss: 31.2711   time: 431.14s   best: 31.2711
2023-10-17 11:53:16,121:INFO:  Epoch 252/500:  train Loss: 18.2439   val Loss: 24.2561   time: 753.41s   best: 23.4134
2023-10-17 11:57:02,096:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 11:57:02,147:INFO:  Epoch 15/500:  train Loss: 30.7174   val Loss: 30.5019   time: 433.95s   best: 30.5019
2023-10-17 12:04:12,637:INFO:  Epoch 16/500:  train Loss: 30.2088   val Loss: 31.0812   time: 430.47s   best: 30.5019
2023-10-17 12:04:53,831:INFO:  Epoch 253/500:  train Loss: 18.5089   val Loss: 24.1271   time: 697.68s   best: 23.4134
2023-10-17 12:11:23,479:INFO:  Epoch 17/500:  train Loss: 29.8214   val Loss: 33.9336   time: 430.82s   best: 30.5019
2023-10-17 12:17:23,716:INFO:  Epoch 254/500:  train Loss: 18.3027   val Loss: 23.5204   time: 749.87s   best: 23.4134
2023-10-17 12:18:34,135:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 12:18:34,161:INFO:  Epoch 18/500:  train Loss: 29.3991   val Loss: 30.3044   time: 430.61s   best: 30.3044
2023-10-17 12:25:50,018:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 12:25:50,137:INFO:  Epoch 19/500:  train Loss: 29.0789   val Loss: 29.6076   time: 435.84s   best: 29.6076
2023-10-17 12:29:49,473:INFO:  Epoch 255/500:  train Loss: 18.4332   val Loss: 23.9480   time: 745.73s   best: 23.4134
2023-10-17 12:33:04,912:INFO:  Epoch 20/500:  train Loss: 28.6250   val Loss: 29.8279   time: 434.76s   best: 29.6076
2023-10-17 12:40:17,120:INFO:  Epoch 21/500:  train Loss: 28.2405   val Loss: 29.6580   time: 432.13s   best: 29.6076
2023-10-17 12:42:25,554:INFO:  Epoch 256/500:  train Loss: 18.2281   val Loss: 23.9793   time: 756.06s   best: 23.4134
2023-10-17 12:47:31,080:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 12:47:31,109:INFO:  Epoch 22/500:  train Loss: 28.1201   val Loss: 28.8018   time: 433.95s   best: 28.8018
2023-10-17 12:54:42,231:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 12:54:42,252:INFO:  Epoch 23/500:  train Loss: 27.5415   val Loss: 28.7085   time: 431.12s   best: 28.7085
2023-10-17 12:55:07,727:INFO:  Epoch 257/500:  train Loss: 18.3618   val Loss: 23.8253   time: 762.16s   best: 23.4134
2023-10-17 13:01:55,688:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 13:01:55,720:INFO:  Epoch 24/500:  train Loss: 27.3246   val Loss: 28.0274   time: 433.41s   best: 28.0274
2023-10-17 13:07:03,824:INFO:  Epoch 258/500:  train Loss: 18.2804   val Loss: 24.1407   time: 716.08s   best: 23.4134
2023-10-17 13:09:11,584:INFO:  Epoch 25/500:  train Loss: 27.0608   val Loss: 28.0879   time: 435.86s   best: 28.0274
2023-10-17 13:16:27,970:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 13:16:27,998:INFO:  Epoch 26/500:  train Loss: 26.6689   val Loss: 27.8686   time: 436.36s   best: 27.8686
2023-10-17 13:19:38,682:INFO:  Epoch 259/500:  train Loss: 18.4604   val Loss: 27.3442   time: 754.85s   best: 23.4134
2023-10-17 13:23:39,997:INFO:  Epoch 27/500:  train Loss: 26.4696   val Loss: 27.9141   time: 431.98s   best: 27.8686
2023-10-17 13:30:53,285:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 13:30:53,317:INFO:  Epoch 28/500:  train Loss: 26.3276   val Loss: 27.8342   time: 433.27s   best: 27.8342
2023-10-17 13:32:02,872:INFO:  Epoch 260/500:  train Loss: 18.3107   val Loss: 24.9086   time: 744.18s   best: 23.4134
2023-10-17 13:38:08,032:INFO:  Epoch 29/500:  train Loss: 26.1129   val Loss: 27.8738   time: 434.71s   best: 27.8342
2023-10-17 13:44:40,072:INFO:  Epoch 261/500:  train Loss: 18.1437   val Loss: 23.5101   time: 757.19s   best: 23.4134
2023-10-17 13:45:23,668:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 13:45:23,695:INFO:  Epoch 30/500:  train Loss: 25.8765   val Loss: 27.2015   time: 435.59s   best: 27.2015
2023-10-17 13:52:35,387:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 13:52:35,413:INFO:  Epoch 31/500:  train Loss: 25.6035   val Loss: 26.9344   time: 431.67s   best: 26.9344
2023-10-17 13:57:16,713:INFO:  Epoch 262/500:  train Loss: 18.2892   val Loss: 23.7989   time: 756.62s   best: 23.4134
2023-10-17 13:59:50,308:INFO:  Epoch 32/500:  train Loss: 25.4207   val Loss: 27.2874   time: 434.88s   best: 26.9344
2023-10-17 14:07:04,243:INFO:  Epoch 33/500:  train Loss: 25.1825   val Loss: 27.8611   time: 433.92s   best: 26.9344
2023-10-17 14:10:18,692:INFO:  Epoch 263/500:  train Loss: 18.5441   val Loss: 24.6457   time: 781.96s   best: 23.4134
2023-10-17 14:14:20,020:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 14:14:20,059:INFO:  Epoch 34/500:  train Loss: 25.0948   val Loss: 26.5344   time: 435.75s   best: 26.5344
2023-10-17 14:21:32,647:INFO:  Epoch 35/500:  train Loss: 24.9161   val Loss: 26.5466   time: 432.59s   best: 26.5344
2023-10-17 14:22:59,359:INFO:  Epoch 264/500:  train Loss: 18.3600   val Loss: 24.0652   time: 760.66s   best: 23.4134
2023-10-17 14:28:47,699:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 14:28:47,740:INFO:  Epoch 36/500:  train Loss: 24.5669   val Loss: 26.5306   time: 435.02s   best: 26.5306
2023-10-17 14:35:12,412:INFO:  Epoch 265/500:  train Loss: 18.3212   val Loss: 24.3550   time: 733.03s   best: 23.4134
2023-10-17 14:35:58,778:INFO:  Epoch 37/500:  train Loss: 24.5850   val Loss: 29.5434   time: 431.03s   best: 26.5306
2023-10-17 14:43:12,236:INFO:  Epoch 38/500:  train Loss: 24.4189   val Loss: 26.8113   time: 433.42s   best: 26.5306
2023-10-17 14:47:01,688:INFO:  Epoch 266/500:  train Loss: 18.2456   val Loss: 24.4405   time: 709.25s   best: 23.4134
2023-10-17 14:50:23,867:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 14:50:23,893:INFO:  Epoch 39/500:  train Loss: 24.1351   val Loss: 26.0946   time: 431.62s   best: 26.0946
2023-10-17 14:57:35,130:INFO:  Epoch 40/500:  train Loss: 24.1941   val Loss: 33.0300   time: 431.24s   best: 26.0946
2023-10-17 14:58:23,647:INFO:  Epoch 267/500:  train Loss: 18.2964   val Loss: 24.2148   time: 681.93s   best: 23.4134
2023-10-17 15:04:48,436:INFO:  Epoch 41/500:  train Loss: 24.1111   val Loss: 30.3155   time: 433.08s   best: 26.0946
2023-10-17 15:09:41,947:INFO:  Epoch 268/500:  train Loss: 18.3964   val Loss: 24.2304   time: 678.28s   best: 23.4134
2023-10-17 15:12:04,664:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 15:12:04,689:INFO:  Epoch 42/500:  train Loss: 23.9648   val Loss: 25.7137   time: 436.20s   best: 25.7137
2023-10-17 15:19:20,112:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 15:19:20,140:INFO:  Epoch 43/500:  train Loss: 23.5461   val Loss: 25.7113   time: 435.42s   best: 25.7113
2023-10-17 15:20:49,913:INFO:  Epoch 269/500:  train Loss: 18.1873   val Loss: 24.0777   time: 667.95s   best: 23.4134
2023-10-17 15:26:37,258:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 15:26:37,291:INFO:  Epoch 44/500:  train Loss: 23.6448   val Loss: 25.6995   time: 437.10s   best: 25.6995
2023-10-17 15:31:56,438:INFO:  Epoch 270/500:  train Loss: 18.1642   val Loss: 23.5472   time: 666.51s   best: 23.4134
2023-10-17 15:33:53,054:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 15:33:53,084:INFO:  Epoch 45/500:  train Loss: 23.3746   val Loss: 25.2651   time: 435.76s   best: 25.2651
2023-10-17 15:41:06,200:INFO:  Epoch 46/500:  train Loss: 23.3111   val Loss: 26.4315   time: 433.10s   best: 25.2651
2023-10-17 15:43:06,245:INFO:  Epoch 271/500:  train Loss: 18.2472   val Loss: 23.8898   time: 669.80s   best: 23.4134
2023-10-17 15:48:19,062:INFO:  Epoch 47/500:  train Loss: 23.2377   val Loss: 25.3658   time: 432.85s   best: 25.2651
2023-10-17 15:54:05,475:INFO:  Epoch 272/500:  train Loss: 18.2529   val Loss: 23.8013   time: 659.22s   best: 23.4134
2023-10-17 15:55:34,164:INFO:  Epoch 48/500:  train Loss: 23.2864   val Loss: 25.7749   time: 435.07s   best: 25.2651
2023-10-17 16:02:46,219:INFO:  Epoch 49/500:  train Loss: 23.0083   val Loss: 26.0188   time: 432.05s   best: 25.2651
2023-10-17 16:05:27,710:INFO:  Epoch 273/500:  train Loss: 18.2698   val Loss: 26.6487   time: 682.22s   best: 23.4134
2023-10-17 16:09:57,413:INFO:  Epoch 50/500:  train Loss: 22.9645   val Loss: 27.6601   time: 431.18s   best: 25.2651
2023-10-17 16:16:52,125:INFO:  Epoch 274/500:  train Loss: 18.6211   val Loss: 26.6443   time: 684.40s   best: 23.4134
2023-10-17 16:17:10,704:INFO:  Epoch 51/500:  train Loss: 23.0031   val Loss: 25.6104   time: 433.27s   best: 25.2651
2023-10-17 16:24:24,064:INFO:  Epoch 52/500:  train Loss: 22.7806   val Loss: 25.3467   time: 433.34s   best: 25.2651
2023-10-17 16:28:03,352:INFO:  Epoch 275/500:  train Loss: 18.4301   val Loss: 23.8353   time: 671.21s   best: 23.4134
2023-10-17 16:31:38,289:INFO:  Epoch 53/500:  train Loss: 22.6737   val Loss: 25.7445   time: 434.19s   best: 25.2651
2023-10-17 16:38:53,475:INFO:  Epoch 54/500:  train Loss: 22.5170   val Loss: 25.5401   time: 435.16s   best: 25.2651
2023-10-17 16:39:11,505:INFO:  Epoch 276/500:  train Loss: 18.0627   val Loss: 23.9271   time: 668.13s   best: 23.4134
2023-10-17 16:46:09,496:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 16:46:09,525:INFO:  Epoch 55/500:  train Loss: 22.5223   val Loss: 25.1221   time: 435.98s   best: 25.1221
2023-10-17 16:50:35,534:INFO:  Epoch 277/500:  train Loss: 18.1517   val Loss: 24.3225   time: 684.01s   best: 23.4134
2023-10-17 16:53:24,718:INFO:  Epoch 56/500:  train Loss: 22.3975   val Loss: 25.2604   time: 435.18s   best: 25.1221
2023-10-17 17:00:40,769:INFO:  Epoch 57/500:  train Loss: 22.4430   val Loss: 25.1258   time: 436.03s   best: 25.1221
2023-10-17 17:01:55,773:INFO:  Epoch 278/500:  train Loss: 18.4076   val Loss: 24.0695   time: 680.23s   best: 23.4134
2023-10-17 17:07:56,504:INFO:  Epoch 58/500:  train Loss: 22.2749   val Loss: 25.3823   time: 435.71s   best: 25.1221
2023-10-17 17:13:07,204:INFO:  Epoch 279/500:  train Loss: 18.7000   val Loss: 23.6474   time: 671.40s   best: 23.4134
2023-10-17 17:15:11,776:INFO:  Epoch 59/500:  train Loss: 22.2628   val Loss: 26.4130   time: 435.25s   best: 25.1221
2023-10-17 17:22:25,461:INFO:  Epoch 60/500:  train Loss: 22.2460   val Loss: 25.5508   time: 433.66s   best: 25.1221
2023-10-17 17:24:23,956:INFO:  Epoch 280/500:  train Loss: 18.0724   val Loss: 24.7116   time: 676.73s   best: 23.4134
2023-10-17 17:29:37,114:INFO:  Epoch 61/500:  train Loss: 22.0771   val Loss: 26.0830   time: 431.63s   best: 25.1221
2023-10-17 17:35:46,013:INFO:  Epoch 281/500:  train Loss: 18.2116   val Loss: 23.8800   time: 682.04s   best: 23.4134
2023-10-17 17:36:51,671:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 17:36:51,693:INFO:  Epoch 62/500:  train Loss: 22.0242   val Loss: 24.9619   time: 433.95s   best: 24.9619
2023-10-17 17:44:02,964:INFO:  Epoch 63/500:  train Loss: 22.1668   val Loss: 24.9658   time: 431.27s   best: 24.9619
2023-10-17 17:46:57,209:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-17 17:46:57,300:INFO:  Epoch 282/500:  train Loss: 18.0800   val Loss: 23.4082   time: 671.18s   best: 23.4082
2023-10-17 17:51:16,285:INFO:  Epoch 64/500:  train Loss: 22.4373   val Loss: 25.3640   time: 433.29s   best: 24.9619
2023-10-17 17:58:14,166:INFO:  Epoch 283/500:  train Loss: 18.0657   val Loss: 24.2143   time: 676.86s   best: 23.4082
2023-10-17 17:58:27,416:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 17:58:27,440:INFO:  Epoch 65/500:  train Loss: 21.9660   val Loss: 24.5691   time: 431.09s   best: 24.5691
2023-10-17 18:05:38,680:INFO:  Epoch 66/500:  train Loss: 21.8682   val Loss: 24.8387   time: 431.23s   best: 24.5691
2023-10-17 18:09:26,446:INFO:  Epoch 284/500:  train Loss: 18.0000   val Loss: 23.4282   time: 672.24s   best: 23.4082
2023-10-17 18:12:53,736:INFO:  Epoch 67/500:  train Loss: 21.6755   val Loss: 24.5986   time: 435.02s   best: 24.5691
2023-10-17 18:20:04,881:INFO:  Epoch 68/500:  train Loss: 21.6979   val Loss: 25.3949   time: 431.12s   best: 24.5691
2023-10-17 18:20:42,523:INFO:  Epoch 285/500:  train Loss: 18.0246   val Loss: 24.3535   time: 676.06s   best: 23.4082
2023-10-17 18:27:17,180:INFO:  Epoch 69/500:  train Loss: 21.5362   val Loss: 24.8553   time: 432.28s   best: 24.5691
2023-10-17 18:31:36,400:INFO:  Epoch 286/500:  train Loss: 18.3159   val Loss: 23.7566   time: 653.87s   best: 23.4082
2023-10-17 18:34:29,290:INFO:  Epoch 70/500:  train Loss: 21.6337   val Loss: 24.9495   time: 432.08s   best: 24.5691
2023-10-17 18:41:43,669:INFO:  Epoch 71/500:  train Loss: 21.5811   val Loss: 24.8299   time: 434.37s   best: 24.5691
2023-10-17 18:43:01,877:INFO:  Epoch 287/500:  train Loss: 18.2112   val Loss: 23.7027   time: 685.45s   best: 23.4082
2023-10-17 18:48:55,308:INFO:  Epoch 72/500:  train Loss: 21.5231   val Loss: 25.1470   time: 431.60s   best: 24.5691
2023-10-17 18:54:17,671:INFO:  Epoch 288/500:  train Loss: 18.1234   val Loss: 23.9799   time: 675.76s   best: 23.4082
2023-10-17 18:56:06,461:INFO:  Epoch 73/500:  train Loss: 21.3257   val Loss: 25.0036   time: 431.12s   best: 24.5691
2023-10-17 19:03:21,851:INFO:  Epoch 74/500:  train Loss: 21.2478   val Loss: 31.1645   time: 435.35s   best: 24.5691
2023-10-17 19:05:30,822:INFO:  Epoch 289/500:  train Loss: 18.0191   val Loss: 23.5992   time: 673.14s   best: 23.4082
2023-10-17 19:10:33,663:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 19:10:33,693:INFO:  Epoch 75/500:  train Loss: 21.4498   val Loss: 24.4357   time: 431.79s   best: 24.4357
2023-10-17 19:16:31,535:INFO:  Epoch 290/500:  train Loss: 17.9823   val Loss: 23.6643   time: 660.70s   best: 23.4082
2023-10-17 19:17:44,952:INFO:  Epoch 76/500:  train Loss: 21.1813   val Loss: 24.8129   time: 431.24s   best: 24.4357
2023-10-17 19:24:58,714:INFO:  Epoch 77/500:  train Loss: 21.7600   val Loss: 24.5949   time: 433.74s   best: 24.4357
2023-10-17 19:27:52,210:INFO:  Epoch 291/500:  train Loss: 17.9815   val Loss: 23.9239   time: 680.64s   best: 23.4082
2023-10-17 19:32:11,265:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 19:32:11,296:INFO:  Epoch 78/500:  train Loss: 21.2550   val Loss: 24.1725   time: 432.53s   best: 24.1725
2023-10-17 19:39:07,500:INFO:  Epoch 292/500:  train Loss: 18.0847   val Loss: 23.5253   time: 675.27s   best: 23.4082
2023-10-17 19:39:26,269:INFO:  Epoch 79/500:  train Loss: 21.1494   val Loss: 26.7042   time: 434.95s   best: 24.1725
2023-10-17 19:46:41,057:INFO:  Epoch 80/500:  train Loss: 21.2394   val Loss: 25.5256   time: 434.75s   best: 24.1725
2023-10-17 19:50:24,564:INFO:  Epoch 293/500:  train Loss: 17.9450   val Loss: 23.6697   time: 677.05s   best: 23.4082
2023-10-17 19:53:56,705:INFO:  Epoch 81/500:  train Loss: 21.0532   val Loss: 24.4421   time: 435.64s   best: 24.1725
2023-10-17 20:01:09,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 20:01:10,031:INFO:  Epoch 82/500:  train Loss: 20.9972   val Loss: 24.1123   time: 433.26s   best: 24.1123
2023-10-17 20:01:48,271:INFO:  Epoch 294/500:  train Loss: 18.0946   val Loss: 23.9447   time: 683.69s   best: 23.4082
2023-10-17 20:08:22,025:INFO:  Epoch 83/500:  train Loss: 20.9367   val Loss: 25.1225   time: 431.99s   best: 24.1123
2023-10-17 20:13:02,765:INFO:  Epoch 295/500:  train Loss: 18.2084   val Loss: 23.9902   time: 674.48s   best: 23.4082
2023-10-17 20:15:34,357:INFO:  Epoch 84/500:  train Loss: 20.9225   val Loss: 24.1864   time: 432.31s   best: 24.1123
2023-10-17 20:22:45,753:INFO:  Epoch 85/500:  train Loss: 20.9151   val Loss: 25.5515   time: 431.36s   best: 24.1123
2023-10-17 20:24:18,468:INFO:  Epoch 296/500:  train Loss: 18.0519   val Loss: 24.1744   time: 675.69s   best: 23.4082
2023-10-17 20:29:56,964:INFO:  Epoch 86/500:  train Loss: 20.7832   val Loss: 25.1774   time: 431.17s   best: 24.1123
2023-10-17 20:35:27,257:INFO:  Epoch 297/500:  train Loss: 17.9500   val Loss: 23.4629   time: 668.77s   best: 23.4082
2023-10-17 20:37:08,181:INFO:  Epoch 87/500:  train Loss: 20.6865   val Loss: 24.2597   time: 431.20s   best: 24.1123
2023-10-17 20:44:25,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-17 20:44:25,121:INFO:  Epoch 88/500:  train Loss: 20.7598   val Loss: 23.7614   time: 436.87s   best: 23.7614
2023-10-17 20:46:31,392:INFO:  Epoch 298/500:  train Loss: 17.9568   val Loss: 23.7508   time: 664.12s   best: 23.4082
2023-10-17 20:51:38,033:INFO:  Epoch 89/500:  train Loss: 20.8006   val Loss: 24.4022   time: 432.90s   best: 23.7614
2023-10-17 20:57:29,107:INFO:  Epoch 299/500:  train Loss: 18.2262   val Loss: 24.6392   time: 657.70s   best: 23.4082
2023-10-17 20:58:51,159:INFO:  Epoch 90/500:  train Loss: 20.6610   val Loss: 24.0215   time: 433.10s   best: 23.7614
2023-10-17 21:06:02,487:INFO:  Epoch 91/500:  train Loss: 20.8507   val Loss: 24.0749   time: 431.31s   best: 23.7614
2023-10-17 21:08:31,759:INFO:  Epoch 300/500:  train Loss: 18.0766   val Loss: 25.1357   time: 662.63s   best: 23.4082
2023-10-17 21:13:15,563:INFO:  Epoch 92/500:  train Loss: 20.6962   val Loss: 28.3631   time: 433.06s   best: 23.7614
2023-10-17 21:19:33,616:INFO:  Epoch 301/500:  train Loss: 18.0593   val Loss: 23.8811   time: 661.85s   best: 23.4082
2023-10-17 21:20:29,682:INFO:  Epoch 93/500:  train Loss: 20.5357   val Loss: 24.0631   time: 434.10s   best: 23.7614
2023-10-17 21:27:40,985:INFO:  Epoch 94/500:  train Loss: 20.8207   val Loss: 24.5716   time: 431.26s   best: 23.7614
2023-10-17 21:30:32,520:INFO:  Epoch 302/500:  train Loss: 18.0369   val Loss: 24.0352   time: 658.89s   best: 23.4082
2023-10-17 21:34:55,264:INFO:  Epoch 95/500:  train Loss: 20.5171   val Loss: 24.4137   time: 434.26s   best: 23.7614
2023-10-17 21:41:24,243:INFO:  Epoch 303/500:  train Loss: 18.5785   val Loss: 23.5580   time: 651.71s   best: 23.4082
2023-10-17 21:42:06,350:INFO:  Epoch 96/500:  train Loss: 20.5039   val Loss: 25.3092   time: 431.05s   best: 23.7614
2023-10-17 21:49:16,475:INFO:  Epoch 97/500:  train Loss: 20.4275   val Loss: 23.8517   time: 430.12s   best: 23.7614
2023-10-17 21:52:17,305:INFO:  Epoch 304/500:  train Loss: 18.1424   val Loss: 23.9742   time: 653.04s   best: 23.4082
2023-10-17 21:56:31,041:INFO:  Epoch 98/500:  train Loss: 20.4283   val Loss: 25.6846   time: 434.54s   best: 23.7614
2023-10-17 22:03:14,811:INFO:  Epoch 305/500:  train Loss: 18.0973   val Loss: 25.8017   time: 657.49s   best: 23.4082
2023-10-17 22:03:41,980:INFO:  Epoch 99/500:  train Loss: 20.3995   val Loss: 23.9777   time: 430.90s   best: 23.7614
2023-10-17 22:10:53,243:INFO:  Epoch 100/500:  train Loss: 20.4817   val Loss: 24.4869   time: 431.24s   best: 23.7614
2023-10-17 22:14:22,548:INFO:  Epoch 306/500:  train Loss: 17.9460   val Loss: 23.8325   time: 667.73s   best: 23.4082
2023-10-17 22:18:05,078:INFO:  Epoch 101/500:  train Loss: 20.2949   val Loss: 25.6806   time: 431.81s   best: 23.7614
2023-10-17 22:25:20,224:INFO:  Epoch 102/500:  train Loss: 20.4928   val Loss: 29.2432   time: 435.11s   best: 23.7614
2023-10-17 22:25:28,676:INFO:  Epoch 307/500:  train Loss: 17.9333   val Loss: 23.7796   time: 666.12s   best: 23.4082
2023-10-17 22:32:31,255:INFO:  Epoch 103/500:  train Loss: 20.5838   val Loss: 24.2123   time: 431.01s   best: 23.7614
2023-10-17 22:36:34,189:INFO:  Epoch 308/500:  train Loss: 18.1489   val Loss: 23.4228   time: 665.49s   best: 23.4082
2023-10-17 22:39:47,580:INFO:  Epoch 104/500:  train Loss: 20.2025   val Loss: 24.5782   time: 436.32s   best: 23.7614
2023-10-17 22:47:01,457:INFO:  Epoch 105/500:  train Loss: 20.1800   val Loss: 26.0590   time: 433.83s   best: 23.7614
2023-10-17 22:47:43,407:INFO:  Epoch 309/500:  train Loss: 17.9116   val Loss: 24.0049   time: 669.19s   best: 23.4082
2023-10-17 22:54:15,201:INFO:  Epoch 106/500:  train Loss: 20.1205   val Loss: 25.3717   time: 433.73s   best: 23.7614
2023-10-17 22:58:40,571:INFO:  Epoch 310/500:  train Loss: 18.1117   val Loss: 23.8732   time: 657.14s   best: 23.4082
2023-10-17 23:01:27,463:INFO:  Epoch 107/500:  train Loss: 20.0576   val Loss: 24.3619   time: 432.24s   best: 23.7614
2023-10-17 23:08:38,685:INFO:  Epoch 108/500:  train Loss: 20.6120   val Loss: 24.0985   time: 431.21s   best: 23.7614
2023-10-17 23:09:47,535:INFO:  Epoch 311/500:  train Loss: 17.9054   val Loss: 23.9599   time: 666.95s   best: 23.4082
2023-10-17 23:15:53,230:INFO:  Epoch 109/500:  train Loss: 20.1393   val Loss: 24.3848   time: 434.52s   best: 23.7614
2023-10-17 23:20:57,996:INFO:  Epoch 312/500:  train Loss: 17.8998   val Loss: 24.0570   time: 670.43s   best: 23.4082
2023-10-17 23:23:07,224:INFO:  Epoch 110/500:  train Loss: 20.0296   val Loss: 24.1208   time: 433.98s   best: 23.7614
2023-10-17 23:30:21,252:INFO:  Epoch 111/500:  train Loss: 20.1313   val Loss: 24.0961   time: 434.00s   best: 23.7614
2023-10-17 23:31:55,163:INFO:  Epoch 313/500:  train Loss: 18.1047   val Loss: 24.0924   time: 657.16s   best: 23.4082
2023-10-17 23:37:35,524:INFO:  Epoch 112/500:  train Loss: 20.2139   val Loss: 24.8837   time: 434.25s   best: 23.7614
2023-10-17 23:42:53,234:INFO:  Epoch 314/500:  train Loss: 17.8553   val Loss: 23.9579   time: 658.05s   best: 23.4082
2023-10-17 23:44:50,322:INFO:  Epoch 113/500:  train Loss: 20.6142   val Loss: 24.4947   time: 434.76s   best: 23.7614
2023-10-17 23:52:01,274:INFO:  Epoch 114/500:  train Loss: 19.9422   val Loss: 24.1597   time: 430.89s   best: 23.7614
2023-10-17 23:53:44,059:INFO:  Epoch 315/500:  train Loss: 18.0625   val Loss: 24.0788   time: 650.81s   best: 23.4082
2023-10-17 23:59:11,968:INFO:  Epoch 115/500:  train Loss: 20.1451   val Loss: 24.0943   time: 430.67s   best: 23.7614
2023-10-18 00:04:35,213:INFO:  Epoch 316/500:  train Loss: 18.0812   val Loss: 23.9501   time: 651.13s   best: 23.4082
2023-10-18 00:06:23,309:INFO:  Epoch 116/500:  train Loss: 19.9873   val Loss: 23.8721   time: 431.31s   best: 23.7614
2023-10-18 00:13:35,417:INFO:  Epoch 117/500:  train Loss: 19.9210   val Loss: 26.4640   time: 432.08s   best: 23.7614
2023-10-18 00:15:37,277:INFO:  Epoch 317/500:  train Loss: 18.0812   val Loss: 23.6217   time: 662.05s   best: 23.4082
2023-10-18 00:20:45,980:INFO:  Epoch 118/500:  train Loss: 20.0559   val Loss: 24.0727   time: 430.54s   best: 23.7614
2023-10-18 00:26:46,448:INFO:  Epoch 318/500:  train Loss: 18.1142   val Loss: 23.8795   time: 669.15s   best: 23.4082
2023-10-18 00:27:57,062:INFO:  Epoch 119/500:  train Loss: 19.9451   val Loss: 23.8171   time: 431.06s   best: 23.7614
2023-10-18 00:35:08,389:INFO:  Epoch 120/500:  train Loss: 19.9531   val Loss: 24.2701   time: 431.29s   best: 23.7614
2023-10-18 00:38:00,188:INFO:  Epoch 319/500:  train Loss: 18.9485   val Loss: 24.3730   time: 673.73s   best: 23.4082
2023-10-18 00:42:18,139:INFO:  Epoch 121/500:  train Loss: 19.8432   val Loss: 25.4232   time: 429.71s   best: 23.7614
2023-10-18 00:49:17,555:INFO:  Epoch 320/500:  train Loss: 18.1139   val Loss: 25.6522   time: 677.35s   best: 23.4082
2023-10-18 00:49:34,454:INFO:  Epoch 122/500:  train Loss: 19.8617   val Loss: 23.9016   time: 436.29s   best: 23.7614
2023-10-18 00:56:48,831:INFO:  Epoch 123/500:  train Loss: 19.7288   val Loss: 36.7815   time: 434.37s   best: 23.7614
2023-10-18 01:00:59,448:INFO:  Epoch 321/500:  train Loss: 17.9940   val Loss: 24.5007   time: 701.88s   best: 23.4082
2023-10-18 01:04:00,775:INFO:  Epoch 124/500:  train Loss: 19.8560   val Loss: 23.9819   time: 431.94s   best: 23.7614
2023-10-18 01:11:12,963:INFO:  Epoch 125/500:  train Loss: 19.6545   val Loss: 24.1812   time: 432.16s   best: 23.7614
2023-10-18 01:12:28,802:INFO:  Epoch 322/500:  train Loss: 18.0336   val Loss: 24.7230   time: 689.33s   best: 23.4082
2023-10-18 01:18:23,889:INFO:  Epoch 126/500:  train Loss: 19.6694   val Loss: 23.9791   time: 430.91s   best: 23.7614
2023-10-18 01:23:55,863:INFO:  Epoch 323/500:  train Loss: 17.8367   val Loss: 23.9469   time: 687.02s   best: 23.4082
2023-10-18 01:25:37,519:INFO:  Epoch 127/500:  train Loss: 19.8676   val Loss: 26.2671   time: 433.60s   best: 23.7614
2023-10-18 01:32:51,131:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-18 01:32:51,156:INFO:  Epoch 128/500:  train Loss: 19.6275   val Loss: 23.6841   time: 433.58s   best: 23.6841
2023-10-18 01:35:24,866:INFO:  Epoch 324/500:  train Loss: 17.9565   val Loss: 23.8033   time: 688.99s   best: 23.4082
2023-10-18 01:40:01,973:INFO:  Epoch 129/500:  train Loss: 19.6771   val Loss: 24.2324   time: 430.81s   best: 23.6841
2023-10-18 01:46:54,833:INFO:  Epoch 325/500:  train Loss: 18.1815   val Loss: 24.9498   time: 689.95s   best: 23.4082
2023-10-18 01:47:18,061:INFO:  Epoch 130/500:  train Loss: 19.6863   val Loss: 26.4633   time: 436.08s   best: 23.6841
2023-10-18 01:54:34,128:INFO:  Epoch 131/500:  train Loss: 19.7053   val Loss: 24.0919   time: 436.01s   best: 23.6841
2023-10-18 01:58:13,331:INFO:  Epoch 326/500:  train Loss: 18.0400   val Loss: 24.0760   time: 678.48s   best: 23.4082
2023-10-18 02:01:47,456:INFO:  Epoch 132/500:  train Loss: 19.7462   val Loss: 23.7402   time: 433.30s   best: 23.6841
2023-10-18 02:09:04,763:INFO:  Epoch 133/500:  train Loss: 19.5614   val Loss: 23.9294   time: 437.28s   best: 23.6841
2023-10-18 02:09:41,121:INFO:  Epoch 327/500:  train Loss: 17.9646   val Loss: 24.5919   time: 687.77s   best: 23.4082
2023-10-18 02:16:21,641:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-18 02:16:21,669:INFO:  Epoch 134/500:  train Loss: 19.4902   val Loss: 23.5921   time: 436.84s   best: 23.5921
2023-10-18 02:20:52,948:INFO:  Epoch 328/500:  train Loss: 18.0769   val Loss: 23.8605   time: 671.82s   best: 23.4082
2023-10-18 02:23:33,939:INFO:  Epoch 135/500:  train Loss: 19.9716   val Loss: 25.8981   time: 432.27s   best: 23.5921
2023-10-18 02:30:49,410:INFO:  Epoch 136/500:  train Loss: 19.7821   val Loss: 25.2107   time: 435.45s   best: 23.5921
2023-10-18 02:32:08,779:INFO:  Epoch 329/500:  train Loss: 17.8565   val Loss: 23.7111   time: 675.82s   best: 23.4082
2023-10-18 02:38:02,624:INFO:  Epoch 137/500:  train Loss: 19.6667   val Loss: 24.3225   time: 433.19s   best: 23.5921
2023-10-18 02:43:15,757:INFO:  Epoch 330/500:  train Loss: 17.9631   val Loss: 34.0939   time: 666.96s   best: 23.4082
2023-10-18 02:45:18,134:INFO:  Epoch 138/500:  train Loss: 19.4055   val Loss: 24.0615   time: 435.48s   best: 23.5921
2023-10-18 02:52:33,562:INFO:  Epoch 139/500:  train Loss: 19.4056   val Loss: 23.8451   time: 435.41s   best: 23.5921
2023-10-18 02:54:16,390:INFO:  Epoch 331/500:  train Loss: 17.9766   val Loss: 23.8982   time: 660.60s   best: 23.4082
2023-10-18 02:59:49,139:INFO:  Epoch 140/500:  train Loss: 19.5873   val Loss: 24.5096   time: 435.55s   best: 23.5921
2023-10-18 03:05:16,087:INFO:  Epoch 332/500:  train Loss: 17.8826   val Loss: 24.0767   time: 659.68s   best: 23.4082
2023-10-18 03:07:01,347:INFO:  Epoch 141/500:  train Loss: 19.3650   val Loss: 24.3775   time: 432.18s   best: 23.5921
2023-10-18 03:14:13,729:INFO:  Epoch 142/500:  train Loss: 19.4701   val Loss: 23.6014   time: 432.38s   best: 23.5921
2023-10-18 03:16:17,849:INFO:  Epoch 333/500:  train Loss: 17.9946   val Loss: 23.6814   time: 661.75s   best: 23.4082
2023-10-18 03:21:25,496:INFO:  Epoch 143/500:  train Loss: 19.3745   val Loss: 24.3148   time: 431.73s   best: 23.5921
2023-10-18 03:27:15,144:INFO:  Epoch 334/500:  train Loss: 17.9263   val Loss: 23.8303   time: 657.28s   best: 23.4082
2023-10-18 03:28:37,068:INFO:  Epoch 144/500:  train Loss: 20.1451   val Loss: 24.9380   time: 431.55s   best: 23.5921
2023-10-18 03:35:49,144:INFO:  Epoch 145/500:  train Loss: 19.3079   val Loss: 23.8846   time: 432.05s   best: 23.5921
2023-10-18 03:38:19,712:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-18 03:38:19,802:INFO:  Epoch 335/500:  train Loss: 18.0454   val Loss: 23.2175   time: 664.53s   best: 23.2175
2023-10-18 03:43:03,994:INFO:  Epoch 146/500:  train Loss: 19.3514   val Loss: 24.3858   time: 434.84s   best: 23.5921
2023-10-18 03:48:55,109:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-18 03:48:55,166:INFO:  Epoch 336/500:  train Loss: 17.8586   val Loss: 23.0989   time: 635.30s   best: 23.0989
2023-10-18 03:50:18,643:INFO:  Epoch 147/500:  train Loss: 19.3051   val Loss: 24.2489   time: 434.62s   best: 23.5921
2023-10-18 03:57:29,454:INFO:  Epoch 148/500:  train Loss: 19.2499   val Loss: 24.2432   time: 430.80s   best: 23.5921
2023-10-18 03:59:30,653:INFO:  Epoch 337/500:  train Loss: 17.7977   val Loss: 24.0246   time: 635.47s   best: 23.0989
2023-10-18 04:04:40,918:INFO:  Epoch 149/500:  train Loss: 19.2840   val Loss: 26.8606   time: 431.44s   best: 23.5921
2023-10-18 04:10:14,153:INFO:  Epoch 338/500:  train Loss: 18.3424   val Loss: 23.7995   time: 643.49s   best: 23.0989
2023-10-18 04:11:51,599:INFO:  Epoch 150/500:  train Loss: 19.4029   val Loss: 24.9159   time: 430.65s   best: 23.5921
2023-10-18 04:19:06,373:INFO:  Epoch 151/500:  train Loss: 19.2500   val Loss: 24.7317   time: 434.75s   best: 23.5921
2023-10-18 04:21:02,240:INFO:  Epoch 339/500:  train Loss: 18.2016   val Loss: 24.0612   time: 648.06s   best: 23.0989
2023-10-18 04:26:17,298:INFO:  Epoch 152/500:  train Loss: 19.1615   val Loss: 23.7225   time: 430.90s   best: 23.5921
2023-10-18 04:32:00,640:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-18 04:32:00,731:INFO:  Epoch 340/500:  train Loss: 17.8309   val Loss: 22.9020   time: 658.38s   best: 22.9020
2023-10-18 04:33:29,324:INFO:  Epoch 153/500:  train Loss: 19.6756   val Loss: 23.9772   time: 432.00s   best: 23.5921
2023-10-18 04:40:42,473:INFO:  Epoch 154/500:  train Loss: 19.3124   val Loss: 24.1933   time: 433.12s   best: 23.5921
2023-10-18 04:42:43,283:INFO:  Epoch 341/500:  train Loss: 18.0752   val Loss: 24.1159   time: 642.55s   best: 22.9020
2023-10-18 04:47:54,825:INFO:  Epoch 155/500:  train Loss: 19.4761   val Loss: 24.3499   time: 432.33s   best: 23.5921
2023-10-18 04:53:38,832:INFO:  Epoch 342/500:  train Loss: 17.7686   val Loss: 23.8211   time: 655.53s   best: 22.9020
2023-10-18 04:55:05,819:INFO:  Epoch 156/500:  train Loss: 19.3194   val Loss: 23.7752   time: 430.97s   best: 23.5921
2023-10-18 05:02:17,052:INFO:  Epoch 157/500:  train Loss: 19.2065   val Loss: 25.9954   time: 431.21s   best: 23.5921
2023-10-18 05:04:38,827:INFO:  Epoch 343/500:  train Loss: 17.7510   val Loss: 23.1311   time: 659.99s   best: 22.9020
2023-10-18 05:09:28,421:INFO:  Epoch 158/500:  train Loss: 19.1822   val Loss: 24.6754   time: 431.36s   best: 23.5921
2023-10-18 05:15:28,117:INFO:  Epoch 344/500:  train Loss: 17.7761   val Loss: 24.2916   time: 649.27s   best: 22.9020
2023-10-18 05:16:39,654:INFO:  Epoch 159/500:  train Loss: 19.1490   val Loss: 24.0818   time: 431.20s   best: 23.5921
2023-10-18 05:23:54,796:INFO:  Epoch 160/500:  train Loss: 19.5092   val Loss: 23.8648   time: 435.12s   best: 23.5921
2023-10-18 05:26:26,594:INFO:  Epoch 345/500:  train Loss: 17.8698   val Loss: 23.9286   time: 658.45s   best: 22.9020
2023-10-18 05:31:06,028:INFO:  Epoch 161/500:  train Loss: 19.3549   val Loss: 24.0781   time: 431.22s   best: 23.5921
2023-10-18 05:33:38,655:INFO:  Epoch 346/500:  train Loss: 17.7968   val Loss: 23.4314   time: 432.05s   best: 22.9020
2023-10-18 05:38:17,163:INFO:  Epoch 162/500:  train Loss: 19.1668   val Loss: 23.6963   time: 431.12s   best: 23.5921
2023-10-18 05:40:48,859:INFO:  Epoch 347/500:  train Loss: 17.7232   val Loss: 23.9568   time: 430.19s   best: 22.9020
2023-10-18 05:45:28,419:INFO:  Epoch 163/500:  train Loss: 19.1391   val Loss: 24.5032   time: 431.24s   best: 23.5921
2023-10-18 05:47:53,583:INFO:  Epoch 348/500:  train Loss: 17.8046   val Loss: 23.8780   time: 424.70s   best: 22.9020
2023-10-18 05:52:39,271:INFO:  Epoch 164/500:  train Loss: 19.1593   val Loss: 24.0189   time: 430.81s   best: 23.5921
2023-10-18 05:55:12,021:INFO:  Epoch 349/500:  train Loss: 18.0129   val Loss: 23.2146   time: 438.44s   best: 22.9020
2023-10-18 05:59:49,815:INFO:  Epoch 165/500:  train Loss: 19.0431   val Loss: 24.5433   time: 430.52s   best: 23.5921
2023-10-18 06:05:47,514:INFO:  Epoch 350/500:  train Loss: 17.8896   val Loss: 24.5211   time: 635.48s   best: 22.9020
2023-10-18 06:07:00,988:INFO:  Epoch 166/500:  train Loss: 19.1674   val Loss: 26.7927   time: 431.16s   best: 23.5921
2023-10-18 06:14:16,145:INFO:  Epoch 167/500:  train Loss: 19.0857   val Loss: 24.2581   time: 435.16s   best: 23.5921
2023-10-18 06:16:23,160:INFO:  Epoch 351/500:  train Loss: 17.7362   val Loss: 23.7605   time: 635.63s   best: 22.9020
2023-10-18 06:21:31,745:INFO:  Epoch 168/500:  train Loss: 18.9668   val Loss: 24.6058   time: 435.58s   best: 23.5921
2023-10-18 06:27:07,352:INFO:  Epoch 352/500:  train Loss: 17.7509   val Loss: 23.5100   time: 644.18s   best: 22.9020
2023-10-18 06:28:45,501:INFO:  Epoch 169/500:  train Loss: 19.1937   val Loss: 23.8990   time: 433.74s   best: 23.5921
2023-10-18 06:35:57,925:INFO:  Epoch 170/500:  train Loss: 18.9831   val Loss: 24.0061   time: 432.40s   best: 23.5921
2023-10-18 06:37:55,415:INFO:  Epoch 353/500:  train Loss: 17.9022   val Loss: 23.4173   time: 648.05s   best: 22.9020
2023-10-18 06:43:09,915:INFO:  Epoch 171/500:  train Loss: 19.1722   val Loss: 24.0311   time: 431.96s   best: 23.5921
2023-10-18 06:48:43,125:INFO:  Epoch 354/500:  train Loss: 18.0870   val Loss: 23.2755   time: 647.68s   best: 22.9020
2023-10-18 06:50:26,053:INFO:  Epoch 172/500:  train Loss: 19.0865   val Loss: 25.5448   time: 436.12s   best: 23.5921
2023-10-18 06:57:37,505:INFO:  Epoch 173/500:  train Loss: 19.1137   val Loss: 25.3413   time: 431.43s   best: 23.5921
2023-10-18 06:59:31,562:INFO:  Epoch 355/500:  train Loss: 17.8508   val Loss: 23.5509   time: 648.41s   best: 22.9020
2023-10-18 07:04:50,231:INFO:  Epoch 174/500:  train Loss: 19.3232   val Loss: 33.8565   time: 432.71s   best: 23.5921
2023-10-18 07:10:22,195:INFO:  Epoch 356/500:  train Loss: 17.7860   val Loss: 23.1827   time: 650.62s   best: 22.9020
2023-10-18 07:12:06,683:INFO:  Epoch 175/500:  train Loss: 19.0335   val Loss: 23.8654   time: 436.44s   best: 23.5921
2023-10-18 07:19:20,959:INFO:  Epoch 176/500:  train Loss: 19.0688   val Loss: 24.5201   time: 434.26s   best: 23.5921
2023-10-18 07:21:17,781:INFO:  Epoch 357/500:  train Loss: 17.7691   val Loss: 24.5049   time: 655.56s   best: 22.9020
2023-10-18 07:26:35,129:INFO:  Epoch 177/500:  train Loss: 19.0556   val Loss: 23.8813   time: 434.14s   best: 23.5921
2023-10-18 07:32:07,949:INFO:  Epoch 358/500:  train Loss: 18.1031   val Loss: 23.4542   time: 650.15s   best: 22.9020
2023-10-18 07:33:51,249:INFO:  Epoch 178/500:  train Loss: 18.9396   val Loss: 23.9079   time: 436.11s   best: 23.5921
2023-10-18 07:40:25,887:INFO:  Epoch 359/500:  train Loss: 17.8412   val Loss: 23.2387   time: 497.92s   best: 22.9020
2023-10-18 07:41:07,689:INFO:  Epoch 179/500:  train Loss: 18.9391   val Loss: 24.0902   time: 436.42s   best: 23.5921
2023-10-18 07:48:21,637:INFO:  Epoch 180/500:  train Loss: 18.7815   val Loss: 23.7742   time: 433.94s   best: 23.5921
2023-10-18 07:51:08,629:INFO:  Epoch 360/500:  train Loss: 17.7828   val Loss: 22.9140   time: 642.73s   best: 22.9020
2023-10-18 07:55:37,683:INFO:  Epoch 181/500:  train Loss: 19.1282   val Loss: 24.5227   time: 436.02s   best: 23.5921
2023-10-18 08:01:48,506:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-18 08:01:48,561:INFO:  Epoch 361/500:  train Loss: 17.7858   val Loss: 22.7924   time: 639.83s   best: 22.7924
2023-10-18 08:02:49,181:INFO:  Epoch 182/500:  train Loss: 18.8058   val Loss: 24.0866   time: 431.46s   best: 23.5921
2023-10-18 08:10:02,764:INFO:  Epoch 183/500:  train Loss: 19.2066   val Loss: 23.9300   time: 433.57s   best: 23.5921
2023-10-18 08:12:35,957:INFO:  Epoch 362/500:  train Loss: 17.6502   val Loss: 23.8946   time: 647.37s   best: 22.7924
2023-10-18 08:17:15,602:INFO:  Epoch 184/500:  train Loss: 18.9808   val Loss: 23.7290   time: 432.81s   best: 23.5921
2023-10-18 08:23:16,816:INFO:  Epoch 363/500:  train Loss: 17.7342   val Loss: 23.8163   time: 640.84s   best: 22.7924
2023-10-18 08:24:30,572:INFO:  Epoch 185/500:  train Loss: 18.8456   val Loss: 23.7371   time: 434.94s   best: 23.5921
2023-10-18 08:31:42,344:INFO:  Epoch 186/500:  train Loss: 18.9090   val Loss: 23.7544   time: 431.74s   best: 23.5921
2023-10-18 08:33:55,438:INFO:  Epoch 364/500:  train Loss: 17.6704   val Loss: 25.9428   time: 638.60s   best: 22.7924
2023-10-18 08:38:56,606:INFO:  Epoch 187/500:  train Loss: 18.8723   val Loss: 23.6737   time: 434.24s   best: 23.5921
2023-10-18 08:44:36,816:INFO:  Epoch 365/500:  train Loss: 17.9522   val Loss: 24.6710   time: 641.36s   best: 22.7924
2023-10-18 08:46:09,218:INFO:  Epoch 188/500:  train Loss: 18.9950   val Loss: 26.6067   time: 432.58s   best: 23.5921
2023-10-18 08:53:20,178:INFO:  Epoch 189/500:  train Loss: 18.8710   val Loss: 25.1147   time: 430.94s   best: 23.5921
2023-10-18 08:55:10,395:INFO:  Epoch 366/500:  train Loss: 17.6552   val Loss: 23.4056   time: 633.57s   best: 22.7924
2023-10-18 09:00:34,207:INFO:  Epoch 190/500:  train Loss: 18.9011   val Loss: 24.0847   time: 434.00s   best: 23.5921
2023-10-18 09:05:39,889:INFO:  Epoch 367/500:  train Loss: 17.6849   val Loss: 24.0554   time: 629.48s   best: 22.7924
2023-10-18 09:07:50,483:INFO:  Epoch 191/500:  train Loss: 18.9441   val Loss: 24.3785   time: 436.26s   best: 23.5921
2023-10-18 09:15:06,847:INFO:  Epoch 192/500:  train Loss: 19.6641   val Loss: 24.6963   time: 436.34s   best: 23.5921
2023-10-18 09:16:17,349:INFO:  Epoch 368/500:  train Loss: 17.7574   val Loss: 23.5387   time: 637.45s   best: 22.7924
2023-10-18 09:22:19,199:INFO:  Epoch 193/500:  train Loss: 18.8427   val Loss: 24.1862   time: 432.33s   best: 23.5921
2023-10-18 09:26:29,923:INFO:  Epoch 369/500:  train Loss: 17.8965   val Loss: 24.2984   time: 612.55s   best: 22.7924
2023-10-18 09:29:35,329:INFO:  Epoch 194/500:  train Loss: 19.7542   val Loss: 24.4380   time: 436.11s   best: 23.5921
2023-10-18 09:36:48,610:INFO:  Epoch 195/500:  train Loss: 18.7006   val Loss: 26.4513   time: 433.25s   best: 23.5921
2023-10-18 09:37:09,912:INFO:  Epoch 370/500:  train Loss: 17.7537   val Loss: 23.4701   time: 639.97s   best: 22.7924
2023-10-18 09:44:04,524:INFO:  Epoch 196/500:  train Loss: 18.7835   val Loss: 24.5816   time: 435.88s   best: 23.5921
2023-10-18 09:47:46,748:INFO:  Epoch 371/500:  train Loss: 17.6900   val Loss: 23.7655   time: 636.82s   best: 22.7924
2023-10-18 09:51:21,352:INFO:  Epoch 197/500:  train Loss: 18.6738   val Loss: 24.1489   time: 436.79s   best: 23.5921
2023-10-18 09:58:21,897:INFO:  Epoch 372/500:  train Loss: 17.6029   val Loss: 23.4315   time: 635.12s   best: 22.7924
2023-10-18 09:58:36,869:INFO:  Epoch 198/500:  train Loss: 18.8394   val Loss: 24.0420   time: 435.50s   best: 23.5921
2023-10-18 10:05:53,121:INFO:  Epoch 199/500:  train Loss: 19.0425   val Loss: 24.1379   time: 436.24s   best: 23.5921
2023-10-18 10:09:06,531:INFO:  Epoch 373/500:  train Loss: 17.9330   val Loss: 23.2172   time: 644.62s   best: 22.7924
2023-10-18 10:13:09,152:INFO:  Epoch 200/500:  train Loss: 18.5895   val Loss: 24.2285   time: 436.02s   best: 23.5921
2023-10-18 10:19:42,928:INFO:  Epoch 374/500:  train Loss: 17.9387   val Loss: 23.6558   time: 636.37s   best: 22.7924
2023-10-18 10:20:22,623:INFO:  Epoch 201/500:  train Loss: 18.7531   val Loss: 25.2342   time: 433.45s   best: 23.5921
2023-10-18 10:27:37,463:INFO:  Epoch 202/500:  train Loss: 18.6126   val Loss: 24.0483   time: 434.83s   best: 23.5921
2023-10-18 10:30:25,953:INFO:  Epoch 375/500:  train Loss: 17.7615   val Loss: 23.1224   time: 643.01s   best: 22.7924
2023-10-18 10:34:50,811:INFO:  Epoch 203/500:  train Loss: 18.6815   val Loss: 24.1012   time: 433.33s   best: 23.5921
2023-10-18 10:41:14,883:INFO:  Epoch 376/500:  train Loss: 17.8027   val Loss: 23.7423   time: 648.92s   best: 22.7924
2023-10-18 10:42:02,933:INFO:  Epoch 204/500:  train Loss: 18.7794   val Loss: 23.6348   time: 432.09s   best: 23.5921
2023-10-18 10:49:14,527:INFO:  Epoch 205/500:  train Loss: 18.7867   val Loss: 24.7343   time: 431.55s   best: 23.5921
2023-10-18 10:51:57,547:INFO:  Epoch 377/500:  train Loss: 17.6333   val Loss: 24.2546   time: 642.65s   best: 22.7924
2023-10-18 10:56:27,032:INFO:  Epoch 206/500:  train Loss: 18.7667   val Loss: 23.8599   time: 432.49s   best: 23.5921
2023-10-18 11:02:37,198:INFO:  Epoch 378/500:  train Loss: 17.7210   val Loss: 22.8598   time: 639.63s   best: 22.7924
2023-10-18 11:03:39,303:INFO:  Epoch 207/500:  train Loss: 18.8966   val Loss: 24.7765   time: 432.23s   best: 23.5921
2023-10-18 11:10:55,599:INFO:  Epoch 208/500:  train Loss: 18.9521   val Loss: 24.9169   time: 436.27s   best: 23.5921
2023-10-18 11:13:07,165:INFO:  Epoch 379/500:  train Loss: 17.6142   val Loss: 23.6100   time: 629.94s   best: 22.7924
2023-10-18 11:18:11,482:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-18 11:18:11,512:INFO:  Epoch 209/500:  train Loss: 18.6285   val Loss: 23.4408   time: 435.86s   best: 23.4408
2023-10-18 11:23:46,513:INFO:  Epoch 380/500:  train Loss: 17.7800   val Loss: 24.2919   time: 639.32s   best: 22.7924
2023-10-18 11:25:27,077:INFO:  Epoch 210/500:  train Loss: 18.5574   val Loss: 24.2600   time: 435.54s   best: 23.4408
2023-10-18 11:32:42,664:INFO:  Epoch 211/500:  train Loss: 18.6551   val Loss: 23.6541   time: 435.56s   best: 23.4408
2023-10-18 11:34:39,669:INFO:  Epoch 381/500:  train Loss: 17.6897   val Loss: 24.2071   time: 653.13s   best: 22.7924
2023-10-18 11:39:57,814:INFO:  Epoch 212/500:  train Loss: 18.9154   val Loss: 23.8179   time: 435.12s   best: 23.4408
2023-10-18 11:45:17,974:INFO:  Epoch 382/500:  train Loss: 18.1009   val Loss: 24.3293   time: 638.30s   best: 22.7924
2023-10-18 11:47:13,482:INFO:  Epoch 213/500:  train Loss: 18.6002   val Loss: 23.7687   time: 435.66s   best: 23.4408
2023-10-18 11:54:24,676:INFO:  Epoch 214/500:  train Loss: 18.7096   val Loss: 24.6374   time: 431.18s   best: 23.4408
2023-10-18 11:56:06,652:INFO:  Epoch 383/500:  train Loss: 17.6050   val Loss: 24.0516   time: 648.66s   best: 22.7924
2023-10-18 12:01:35,535:INFO:  Epoch 215/500:  train Loss: 18.6376   val Loss: 23.9456   time: 430.85s   best: 23.4408
2023-10-18 12:07:13,289:INFO:  Epoch 384/500:  train Loss: 17.6859   val Loss: 23.1309   time: 666.62s   best: 22.7924
2023-10-18 12:08:50,912:INFO:  Epoch 216/500:  train Loss: 18.6005   val Loss: 23.6577   time: 435.34s   best: 23.4408
2023-10-18 12:16:01,757:INFO:  Epoch 217/500:  train Loss: 18.5388   val Loss: 23.5768   time: 430.81s   best: 23.4408
2023-10-18 12:18:05,752:INFO:  Epoch 385/500:  train Loss: 17.6483   val Loss: 24.2348   time: 652.43s   best: 22.7924
2023-10-18 12:23:13,459:INFO:  Epoch 218/500:  train Loss: 18.4757   val Loss: 24.9152   time: 431.68s   best: 23.4408
2023-10-18 12:29:03,885:INFO:  Epoch 386/500:  train Loss: 17.8438   val Loss: 24.1106   time: 658.10s   best: 22.7924
2023-10-18 12:30:28,288:INFO:  Epoch 219/500:  train Loss: 18.5666   val Loss: 23.8631   time: 434.80s   best: 23.4408
2023-10-18 12:37:42,446:INFO:  Epoch 220/500:  train Loss: 18.7128   val Loss: 23.8923   time: 434.13s   best: 23.4408
2023-10-18 12:39:53,763:INFO:  Epoch 387/500:  train Loss: 17.5767   val Loss: 24.9046   time: 649.86s   best: 22.7924
2023-10-18 12:44:57,811:INFO:  Epoch 221/500:  train Loss: 18.6326   val Loss: 24.1065   time: 435.34s   best: 23.4408
2023-10-18 12:50:45,813:INFO:  Epoch 388/500:  train Loss: 17.7264   val Loss: 22.9992   time: 652.03s   best: 22.7924
2023-10-18 12:52:10,671:INFO:  Epoch 222/500:  train Loss: 18.5826   val Loss: 23.8736   time: 432.84s   best: 23.4408
2023-10-18 12:59:23,601:INFO:  Epoch 223/500:  train Loss: 18.4048   val Loss: 23.6719   time: 432.92s   best: 23.4408
2023-10-18 13:01:45,602:INFO:  Epoch 389/500:  train Loss: 17.4991   val Loss: 23.9222   time: 659.77s   best: 22.7924
2023-10-18 13:06:40,151:INFO:  Epoch 224/500:  train Loss: 18.3636   val Loss: 23.8390   time: 436.52s   best: 23.4408
2023-10-18 13:12:33,197:INFO:  Epoch 390/500:  train Loss: 17.5074   val Loss: 23.9399   time: 647.57s   best: 22.7924
2023-10-18 13:13:55,010:INFO:  Epoch 225/500:  train Loss: 18.4314   val Loss: 23.6314   time: 434.83s   best: 23.4408
2023-10-18 13:21:06,678:INFO:  Epoch 226/500:  train Loss: 19.1069   val Loss: 23.9839   time: 431.66s   best: 23.4408
2023-10-18 13:23:31,496:INFO:  Epoch 391/500:  train Loss: 17.4574   val Loss: 23.2515   time: 658.29s   best: 22.7924
2023-10-18 13:28:19,766:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-18 13:28:19,806:INFO:  Epoch 227/500:  train Loss: 18.5313   val Loss: 23.3861   time: 433.05s   best: 23.3861
2023-10-18 13:34:33,407:INFO:  Epoch 392/500:  train Loss: 17.4732   val Loss: 23.1067   time: 661.90s   best: 22.7924
2023-10-18 13:35:35,089:INFO:  Epoch 228/500:  train Loss: 18.4632   val Loss: 24.4314   time: 435.27s   best: 23.3861
2023-10-18 13:42:51,167:INFO:  Epoch 229/500:  train Loss: 18.3544   val Loss: 25.2432   time: 436.06s   best: 23.3861
2023-10-18 13:45:37,053:INFO:  Epoch 393/500:  train Loss: 17.4722   val Loss: 23.5052   time: 663.62s   best: 22.7924
2023-10-18 13:50:02,777:INFO:  Epoch 230/500:  train Loss: 18.5481   val Loss: 23.6816   time: 431.59s   best: 23.3861
2023-10-18 13:56:40,220:INFO:  Epoch 394/500:  train Loss: 17.7894   val Loss: 23.7047   time: 663.15s   best: 22.7924
2023-10-18 13:57:15,746:INFO:  Epoch 231/500:  train Loss: 18.3812   val Loss: 23.4428   time: 432.96s   best: 23.3861
2023-10-18 14:04:27,406:INFO:  Epoch 232/500:  train Loss: 18.4391   val Loss: 23.6958   time: 431.61s   best: 23.3861
2023-10-18 14:07:52,085:INFO:  Epoch 395/500:  train Loss: 17.8360   val Loss: 23.9161   time: 671.84s   best: 22.7924
2023-10-18 14:11:41,761:INFO:  Epoch 233/500:  train Loss: 18.3934   val Loss: 24.2315   time: 434.34s   best: 23.3861
2023-10-18 14:18:52,987:INFO:  Epoch 396/500:  train Loss: 17.6626   val Loss: 23.2348   time: 660.87s   best: 22.7924
2023-10-18 14:18:55,371:INFO:  Epoch 234/500:  train Loss: 18.4633   val Loss: 25.2074   time: 433.60s   best: 23.3861
2023-10-18 14:26:06,817:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-18 14:26:06,858:INFO:  Epoch 235/500:  train Loss: 18.3693   val Loss: 23.2369   time: 431.42s   best: 23.2369
2023-10-18 14:29:56,142:INFO:  Epoch 397/500:  train Loss: 17.4755   val Loss: 24.1881   time: 663.14s   best: 22.7924
2023-10-18 14:33:21,853:INFO:  Epoch 236/500:  train Loss: 18.3006   val Loss: 26.6428   time: 434.99s   best: 23.2369
2023-10-18 14:40:38,479:INFO:  Epoch 237/500:  train Loss: 18.4285   val Loss: 23.3626   time: 436.59s   best: 23.2369
2023-10-18 14:40:58,773:INFO:  Epoch 398/500:  train Loss: 17.4848   val Loss: 23.7033   time: 662.62s   best: 22.7924
2023-10-18 14:47:54,345:INFO:  Epoch 238/500:  train Loss: 18.2478   val Loss: 23.8753   time: 435.84s   best: 23.2369
2023-10-18 14:52:01,646:INFO:  Epoch 399/500:  train Loss: 17.6417   val Loss: 23.9356   time: 662.86s   best: 22.7924
2023-10-18 14:55:06,809:INFO:  Epoch 239/500:  train Loss: 18.2203   val Loss: 31.1673   time: 432.44s   best: 23.2369
2023-10-18 15:02:18,892:INFO:  Epoch 240/500:  train Loss: 18.2740   val Loss: 23.7345   time: 432.07s   best: 23.2369
2023-10-18 15:02:53,227:INFO:  Epoch 400/500:  train Loss: 17.6854   val Loss: 23.9396   time: 651.57s   best: 22.7924
2023-10-18 15:09:31,109:INFO:  Epoch 241/500:  train Loss: 18.8356   val Loss: 23.5375   time: 432.19s   best: 23.2369
2023-10-18 15:13:37,366:INFO:  Epoch 401/500:  train Loss: 17.4529   val Loss: 23.7599   time: 644.11s   best: 22.7924
2023-10-18 15:16:45,289:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-18 15:16:45,334:INFO:  Epoch 242/500:  train Loss: 18.4710   val Loss: 23.0844   time: 434.16s   best: 23.0844
2023-10-18 15:22:15,917:INFO:  Epoch 402/500:  train Loss: 17.4288   val Loss: 24.1310   time: 518.53s   best: 22.7924
2023-10-18 15:24:01,534:INFO:  Epoch 243/500:  train Loss: 18.2545   val Loss: 23.3554   time: 436.19s   best: 23.0844
2023-10-18 15:29:22,403:INFO:  Epoch 403/500:  train Loss: 17.7148   val Loss: 23.5424   time: 426.47s   best: 22.7924
2023-10-18 15:31:14,667:INFO:  Epoch 244/500:  train Loss: 18.5522   val Loss: 23.6626   time: 433.11s   best: 23.0844
2023-10-18 15:36:34,579:INFO:  Epoch 404/500:  train Loss: 17.5281   val Loss: 23.7866   time: 432.15s   best: 22.7924
2023-10-18 15:38:31,737:INFO:  Epoch 245/500:  train Loss: 18.4601   val Loss: 23.6351   time: 437.05s   best: 23.0844
2023-10-18 15:43:45,682:INFO:  Epoch 405/500:  train Loss: 17.6454   val Loss: 23.8035   time: 431.09s   best: 22.7924
2023-10-18 15:45:47,746:INFO:  Epoch 246/500:  train Loss: 18.1982   val Loss: 23.5718   time: 436.00s   best: 23.0844
2023-10-18 15:50:51,757:INFO:  Epoch 406/500:  train Loss: 17.7680   val Loss: 23.9914   time: 426.06s   best: 22.7924
2023-10-18 15:53:03,539:INFO:  Epoch 247/500:  train Loss: 18.1658   val Loss: 23.1813   time: 435.77s   best: 23.0844
2023-10-18 15:58:01,250:INFO:  Epoch 407/500:  train Loss: 17.6899   val Loss: 23.5491   time: 429.49s   best: 22.7924
2023-10-18 16:00:19,140:INFO:  Epoch 248/500:  train Loss: 18.1321   val Loss: 24.1174   time: 435.56s   best: 23.0844
2023-10-18 16:05:10,987:INFO:  Epoch 408/500:  train Loss: 17.8317   val Loss: 25.1168   time: 429.69s   best: 22.7924
2023-10-18 16:07:35,048:INFO:  Epoch 249/500:  train Loss: 18.1667   val Loss: 23.2019   time: 435.87s   best: 23.0844
2023-10-18 16:12:20,693:INFO:  Epoch 409/500:  train Loss: 17.5260   val Loss: 24.1697   time: 429.68s   best: 22.7924
2023-10-18 16:14:48,579:INFO:  Epoch 250/500:  train Loss: 18.2651   val Loss: 23.8096   time: 433.50s   best: 23.0844
2023-10-18 16:19:26,368:INFO:  Epoch 410/500:  train Loss: 17.6081   val Loss: 23.7364   time: 425.65s   best: 22.7924
2023-10-18 16:21:59,652:INFO:  Epoch 251/500:  train Loss: 18.2276   val Loss: 23.6690   time: 431.05s   best: 23.0844
2023-10-18 16:26:35,664:INFO:  Epoch 411/500:  train Loss: 17.4377   val Loss: 24.0949   time: 429.28s   best: 22.7924
2023-10-18 16:29:15,708:INFO:  Epoch 252/500:  train Loss: 18.3657   val Loss: 25.8010   time: 436.05s   best: 23.0844
2023-10-18 16:33:45,977:INFO:  Epoch 412/500:  train Loss: 18.0811   val Loss: 23.5939   time: 430.28s   best: 22.7924
2023-10-18 16:36:30,216:INFO:  Epoch 253/500:  train Loss: 18.2957   val Loss: 23.8127   time: 434.49s   best: 23.0844
2023-10-18 16:40:52,438:INFO:  Epoch 413/500:  train Loss: 17.4555   val Loss: 23.7225   time: 426.45s   best: 22.7924
2023-10-18 16:43:44,672:INFO:  Epoch 254/500:  train Loss: 18.1963   val Loss: 24.0824   time: 434.44s   best: 23.0844
2023-10-18 16:48:02,151:INFO:  Epoch 414/500:  train Loss: 17.4615   val Loss: 23.7567   time: 429.68s   best: 22.7924
2023-10-18 16:51:00,496:INFO:  Epoch 255/500:  train Loss: 18.6015   val Loss: 23.9010   time: 435.79s   best: 23.0844
2023-10-18 16:55:09,112:INFO:  Epoch 415/500:  train Loss: 17.5192   val Loss: 23.9754   time: 426.94s   best: 22.7924
2023-10-18 16:58:11,918:INFO:  Epoch 256/500:  train Loss: 18.1758   val Loss: 23.9248   time: 431.16s   best: 23.0844
2023-10-18 17:02:15,078:INFO:  Epoch 416/500:  train Loss: 17.6163   val Loss: 23.5578   time: 425.94s   best: 22.7924
2023-10-18 17:05:23,507:INFO:  Epoch 257/500:  train Loss: 18.1354   val Loss: 23.7274   time: 431.58s   best: 23.0844
2023-10-18 17:09:25,341:INFO:  Epoch 417/500:  train Loss: 17.6321   val Loss: 23.5738   time: 430.23s   best: 22.7924
2023-10-18 17:12:34,819:INFO:  Epoch 258/500:  train Loss: 18.1780   val Loss: 24.2367   time: 431.28s   best: 23.0844
2023-10-18 17:16:32,697:INFO:  Epoch 418/500:  train Loss: 17.6037   val Loss: 23.4472   time: 427.33s   best: 22.7924
2023-10-18 17:19:45,729:INFO:  Epoch 259/500:  train Loss: 18.2890   val Loss: 23.9398   time: 430.88s   best: 23.0844
2023-10-18 17:23:39,106:INFO:  Epoch 419/500:  train Loss: 17.4810   val Loss: 23.7269   time: 426.38s   best: 22.7924
2023-10-18 17:27:02,098:INFO:  Epoch 260/500:  train Loss: 18.2269   val Loss: 24.0351   time: 436.33s   best: 23.0844
2023-10-18 17:30:44,729:INFO:  Epoch 420/500:  train Loss: 17.3921   val Loss: 24.0392   time: 425.59s   best: 22.7924
2023-10-18 17:34:17,051:INFO:  Epoch 261/500:  train Loss: 18.7227   val Loss: 24.6594   time: 434.93s   best: 23.0844
2023-10-18 17:37:54,573:INFO:  Epoch 421/500:  train Loss: 17.5961   val Loss: 23.7109   time: 429.82s   best: 22.7924
2023-10-18 17:41:32,756:INFO:  Epoch 262/500:  train Loss: 18.1052   val Loss: 23.7618   time: 435.70s   best: 23.0844
2023-10-18 17:45:01,372:INFO:  Epoch 422/500:  train Loss: 17.4816   val Loss: 23.6598   time: 426.77s   best: 22.7924
2023-10-18 17:48:46,192:INFO:  Epoch 263/500:  train Loss: 18.4348   val Loss: 24.6090   time: 433.41s   best: 23.0844
2023-10-18 17:52:07,772:INFO:  Epoch 423/500:  train Loss: 17.5027   val Loss: 23.6241   time: 426.37s   best: 22.7924
2023-10-18 17:55:59,521:INFO:  Epoch 264/500:  train Loss: 18.2404   val Loss: 24.0928   time: 433.31s   best: 23.0844
2023-10-18 17:59:14,120:INFO:  Epoch 424/500:  train Loss: 17.5342   val Loss: 23.4597   time: 426.32s   best: 22.7924
2023-10-18 18:03:10,520:INFO:  Epoch 265/500:  train Loss: 18.2499   val Loss: 23.9083   time: 430.99s   best: 23.0844
2023-10-18 18:06:20,853:INFO:  Epoch 425/500:  train Loss: 17.8350   val Loss: 23.1796   time: 426.72s   best: 22.7924
2023-10-18 18:10:22,502:INFO:  Epoch 266/500:  train Loss: 18.0542   val Loss: 24.1732   time: 431.95s   best: 23.0844
2023-10-18 18:13:29,231:INFO:  Epoch 426/500:  train Loss: 17.4899   val Loss: 23.5494   time: 428.35s   best: 22.7924
2023-10-18 18:17:34,958:INFO:  Epoch 267/500:  train Loss: 18.1669   val Loss: 24.3354   time: 432.42s   best: 23.0844
2023-10-18 18:20:38,682:INFO:  Epoch 427/500:  train Loss: 17.4762   val Loss: 24.0234   time: 429.41s   best: 22.7924
2023-10-18 18:24:47,593:INFO:  Epoch 268/500:  train Loss: 18.1691   val Loss: 23.7693   time: 432.62s   best: 23.0844
2023-10-18 18:27:47,581:INFO:  Epoch 428/500:  train Loss: 17.5123   val Loss: 23.5072   time: 428.87s   best: 22.7924
2023-10-18 18:32:00,745:INFO:  Epoch 269/500:  train Loss: 18.0684   val Loss: 27.1959   time: 433.12s   best: 23.0844
2023-10-18 18:34:54,398:INFO:  Epoch 429/500:  train Loss: 17.4672   val Loss: 23.6821   time: 426.79s   best: 22.7924
2023-10-18 18:39:11,857:INFO:  Epoch 270/500:  train Loss: 18.6762   val Loss: 25.3656   time: 431.10s   best: 23.0844
2023-10-18 18:42:04,258:INFO:  Epoch 430/500:  train Loss: 17.5194   val Loss: 23.6743   time: 429.81s   best: 22.7924
2023-10-18 18:46:27,332:INFO:  Epoch 271/500:  train Loss: 18.2506   val Loss: 26.7386   time: 435.44s   best: 23.0844
2023-10-18 18:49:15,339:INFO:  Epoch 431/500:  train Loss: 17.6094   val Loss: 23.7242   time: 431.06s   best: 22.7924
2023-10-18 18:53:43,140:INFO:  Epoch 272/500:  train Loss: 18.0882   val Loss: 24.8151   time: 435.79s   best: 23.0844
2023-10-18 18:56:21,276:INFO:  Epoch 432/500:  train Loss: 17.3369   val Loss: 23.8879   time: 425.90s   best: 22.7924
2023-10-18 19:00:55,853:INFO:  Epoch 273/500:  train Loss: 18.0203   val Loss: 24.6640   time: 432.70s   best: 23.0844
2023-10-18 19:03:31,671:INFO:  Epoch 433/500:  train Loss: 17.7193   val Loss: 27.1501   time: 430.35s   best: 22.7924
2023-10-18 19:08:07,674:INFO:  Epoch 274/500:  train Loss: 18.2744   val Loss: 23.2974   time: 431.79s   best: 23.0844
2023-10-18 19:10:37,861:INFO:  Epoch 434/500:  train Loss: 17.5332   val Loss: 23.5353   time: 426.17s   best: 22.7924
2023-10-18 19:15:24,493:INFO:  Epoch 275/500:  train Loss: 18.0606   val Loss: 23.9094   time: 436.79s   best: 23.0844
2023-10-18 19:17:43,956:INFO:  Epoch 435/500:  train Loss: 17.4877   val Loss: 24.0928   time: 426.07s   best: 22.7924
2023-10-18 19:22:39,058:INFO:  Epoch 276/500:  train Loss: 18.1323   val Loss: 23.7907   time: 434.56s   best: 23.0844
2023-10-18 19:24:54,397:INFO:  Epoch 436/500:  train Loss: 17.7054   val Loss: 23.6000   time: 430.41s   best: 22.7924
2023-10-18 19:29:54,384:INFO:  Epoch 277/500:  train Loss: 18.0277   val Loss: 23.5244   time: 435.29s   best: 23.0844
2023-10-18 19:32:05,190:INFO:  Epoch 437/500:  train Loss: 17.5917   val Loss: 24.6781   time: 430.78s   best: 22.7924
2023-10-18 19:37:06,667:INFO:  Epoch 278/500:  train Loss: 18.2087   val Loss: 23.8116   time: 432.28s   best: 23.0844
2023-10-18 19:39:13,969:INFO:  Epoch 438/500:  train Loss: 17.3833   val Loss: 24.0251   time: 428.75s   best: 22.7924
2023-10-18 19:44:18,722:INFO:  Epoch 279/500:  train Loss: 18.2303   val Loss: 23.4379   time: 432.03s   best: 23.0844
2023-10-18 19:46:24,552:INFO:  Epoch 439/500:  train Loss: 17.6940   val Loss: 23.4262   time: 430.55s   best: 22.7924
2023-10-18 19:51:35,345:INFO:  Epoch 280/500:  train Loss: 17.9793   val Loss: 23.9836   time: 436.62s   best: 23.0844
2023-10-18 19:53:31,027:INFO:  Epoch 440/500:  train Loss: 17.5320   val Loss: 24.1630   time: 426.44s   best: 22.7924
2023-10-18 19:58:51,897:INFO:  Epoch 281/500:  train Loss: 17.9749   val Loss: 24.8622   time: 436.53s   best: 23.0844
2023-10-18 20:00:41,566:INFO:  Epoch 441/500:  train Loss: 17.4395   val Loss: 24.0221   time: 430.51s   best: 22.7924
2023-10-18 20:06:05,018:INFO:  Epoch 282/500:  train Loss: 17.9829   val Loss: 23.5767   time: 433.09s   best: 23.0844
2023-10-18 20:07:50,614:INFO:  Epoch 442/500:  train Loss: 17.6540   val Loss: 23.8464   time: 429.04s   best: 22.7924
2023-10-18 20:13:21,049:INFO:  Epoch 283/500:  train Loss: 17.9974   val Loss: 23.6213   time: 436.00s   best: 23.0844
2023-10-18 20:14:56,828:INFO:  Epoch 443/500:  train Loss: 17.5249   val Loss: 24.0518   time: 426.08s   best: 22.7924
2023-10-18 20:20:34,363:INFO:  Epoch 284/500:  train Loss: 18.1066   val Loss: 25.1900   time: 433.28s   best: 23.0844
2023-10-18 20:22:04,190:INFO:  Epoch 444/500:  train Loss: 17.3982   val Loss: 24.0737   time: 427.36s   best: 22.7924
2023-10-18 20:27:50,576:INFO:  Epoch 285/500:  train Loss: 18.0639   val Loss: 23.7848   time: 436.18s   best: 23.0844
2023-10-18 20:29:10,471:INFO:  Epoch 445/500:  train Loss: 17.4464   val Loss: 24.3828   time: 426.28s   best: 22.7924
2023-10-18 20:35:06,540:INFO:  Epoch 286/500:  train Loss: 18.0779   val Loss: 23.2088   time: 435.94s   best: 23.0844
2023-10-18 20:36:20,227:INFO:  Epoch 446/500:  train Loss: 17.5265   val Loss: 23.8397   time: 429.75s   best: 22.7924
2023-10-18 20:42:18,442:INFO:  Epoch 287/500:  train Loss: 18.2290   val Loss: 23.3115   time: 431.89s   best: 23.0844
2023-10-18 20:43:29,193:INFO:  Epoch 447/500:  train Loss: 17.3349   val Loss: 23.3062   time: 428.94s   best: 22.7924
2023-10-18 20:49:35,593:INFO:  Epoch 288/500:  train Loss: 18.0463   val Loss: 23.4388   time: 437.14s   best: 23.0844
2023-10-18 20:50:35,054:INFO:  Epoch 448/500:  train Loss: 17.2518   val Loss: 23.1840   time: 425.84s   best: 22.7924
2023-10-18 20:56:50,856:INFO:  Epoch 289/500:  train Loss: 17.9406   val Loss: 25.6881   time: 435.23s   best: 23.0844
2023-10-18 20:57:44,294:INFO:  Epoch 449/500:  train Loss: 17.3059   val Loss: 24.3858   time: 429.22s   best: 22.7924
2023-10-18 21:04:05,515:INFO:  Epoch 290/500:  train Loss: 18.1781   val Loss: 23.6742   time: 434.63s   best: 23.0844
2023-10-18 21:04:51,428:INFO:  Epoch 450/500:  train Loss: 17.4975   val Loss: 23.4881   time: 427.11s   best: 22.7924
2023-10-18 21:11:21,453:INFO:  Epoch 291/500:  train Loss: 17.9929   val Loss: 24.0713   time: 435.90s   best: 23.0844
2023-10-18 21:12:01,390:INFO:  Epoch 451/500:  train Loss: 17.4386   val Loss: 24.3626   time: 429.93s   best: 22.7924
2023-10-18 21:18:36,964:INFO:  Epoch 292/500:  train Loss: 18.0417   val Loss: 23.2841   time: 435.50s   best: 23.0844
2023-10-18 21:19:07,877:INFO:  Epoch 452/500:  train Loss: 17.4734   val Loss: 23.9174   time: 426.47s   best: 22.7924
2023-10-18 21:25:52,941:INFO:  Epoch 293/500:  train Loss: 17.9198   val Loss: 23.5493   time: 435.95s   best: 23.0844
2023-10-18 21:26:17,616:INFO:  Epoch 453/500:  train Loss: 17.4714   val Loss: 24.0426   time: 429.73s   best: 22.7924
2023-10-18 21:33:05,243:INFO:  Epoch 294/500:  train Loss: 17.9561   val Loss: 24.6518   time: 432.27s   best: 23.0844
2023-10-18 21:33:23,251:INFO:  Epoch 454/500:  train Loss: 17.3858   val Loss: 23.3930   time: 425.59s   best: 22.7924
2023-10-18 21:40:20,923:INFO:  Epoch 295/500:  train Loss: 17.9475   val Loss: 23.3730   time: 435.65s   best: 23.0844
2023-10-18 21:40:29,908:INFO:  Epoch 455/500:  train Loss: 17.6286   val Loss: 24.0138   time: 426.66s   best: 22.7924
2023-10-18 21:47:33,599:INFO:  Epoch 296/500:  train Loss: 18.1270   val Loss: 23.6152   time: 432.64s   best: 23.0844
2023-10-18 21:47:35,289:INFO:  Epoch 456/500:  train Loss: 17.5058   val Loss: 23.7123   time: 425.36s   best: 22.7924
2023-10-18 21:54:41,254:INFO:  Epoch 457/500:  train Loss: 17.2894   val Loss: 23.7996   time: 425.92s   best: 22.7924
2023-10-18 21:54:44,547:INFO:  Epoch 297/500:  train Loss: 17.9048   val Loss: 23.9549   time: 430.93s   best: 23.0844
2023-10-18 22:01:51,496:INFO:  Epoch 458/500:  train Loss: 17.3536   val Loss: 23.7103   time: 430.22s   best: 22.7924
2023-10-18 22:01:57,337:INFO:  Epoch 298/500:  train Loss: 18.0151   val Loss: 23.4450   time: 432.74s   best: 23.0844
2023-10-18 22:08:57,353:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.2 dropout)_7268.pt
2023-10-18 22:08:57,381:INFO:  Epoch 459/500:  train Loss: 17.7542   val Loss: 22.5325   time: 425.83s   best: 22.5325
2023-10-18 22:09:10,402:INFO:  Epoch 299/500:  train Loss: 17.9446   val Loss: 23.7138   time: 433.04s   best: 23.0844
2023-10-18 22:16:03,455:INFO:  Epoch 460/500:  train Loss: 17.3134   val Loss: 24.4201   time: 426.07s   best: 22.5325
2023-10-18 22:16:25,604:INFO:  Epoch 300/500:  train Loss: 17.8935   val Loss: 23.8734   time: 435.19s   best: 23.0844
2023-10-18 22:23:09,476:INFO:  Epoch 461/500:  train Loss: 17.4865   val Loss: 23.8106   time: 426.00s   best: 22.5325
2023-10-18 22:23:37,333:INFO:  Epoch 301/500:  train Loss: 17.7843   val Loss: 23.7074   time: 431.69s   best: 23.0844
2023-10-18 22:30:16,222:INFO:  Epoch 462/500:  train Loss: 17.2048   val Loss: 24.1443   time: 426.72s   best: 22.5325
2023-10-18 22:30:48,925:INFO:  Epoch 302/500:  train Loss: 17.8711   val Loss: 29.4444   time: 431.57s   best: 23.0844
2023-10-18 22:37:26,219:INFO:  Epoch 463/500:  train Loss: 17.3939   val Loss: 23.4921   time: 429.98s   best: 22.5325
2023-10-18 22:38:02,505:INFO:  Epoch 303/500:  train Loss: 17.9640   val Loss: 23.9222   time: 433.56s   best: 23.0844
2023-10-18 22:44:34,682:INFO:  Epoch 464/500:  train Loss: 17.2419   val Loss: 23.6850   time: 428.43s   best: 22.5325
2023-10-18 22:45:19,024:INFO:  Epoch 304/500:  train Loss: 17.9637   val Loss: 24.3314   time: 436.50s   best: 23.0844
2023-10-18 22:51:42,066:INFO:  Epoch 465/500:  train Loss: 17.2020   val Loss: 25.2196   time: 427.36s   best: 22.5325
2023-10-18 22:52:31,149:INFO:  Epoch 305/500:  train Loss: 18.2918   val Loss: 24.5536   time: 432.11s   best: 23.0844
2023-10-18 22:58:48,264:INFO:  Epoch 466/500:  train Loss: 17.2817   val Loss: 23.3604   time: 426.15s   best: 22.5325
2023-10-18 22:59:46,023:INFO:  Epoch 306/500:  train Loss: 17.8553   val Loss: 23.4602   time: 434.87s   best: 23.0844
2023-10-18 23:05:54,867:INFO:  Epoch 467/500:  train Loss: 17.2989   val Loss: 23.9562   time: 426.56s   best: 22.5325
2023-10-18 23:06:58,462:INFO:  Epoch 307/500:  train Loss: 18.5198   val Loss: 25.0936   time: 432.41s   best: 23.0844
2023-10-18 23:13:00,113:INFO:  Epoch 468/500:  train Loss: 17.2544   val Loss: 24.3317   time: 425.22s   best: 22.5325
2023-10-18 23:14:12,551:INFO:  Epoch 308/500:  train Loss: 18.0397   val Loss: 23.3860   time: 434.05s   best: 23.0844
2023-10-18 23:20:06,694:INFO:  Epoch 469/500:  train Loss: 17.4863   val Loss: 23.0969   time: 426.57s   best: 22.5325
2023-10-18 23:21:25,234:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-18 23:21:25,271:INFO:  Epoch 309/500:  train Loss: 17.8519   val Loss: 22.9557   time: 432.67s   best: 22.9557
2023-10-18 23:27:12,923:INFO:  Epoch 470/500:  train Loss: 17.3105   val Loss: 23.1099   time: 426.21s   best: 22.5325
2023-10-18 23:28:40,783:INFO:  Epoch 310/500:  train Loss: 17.9761   val Loss: 23.2625   time: 435.51s   best: 22.9557
2023-10-18 23:34:19,905:INFO:  Epoch 471/500:  train Loss: 17.1968   val Loss: 23.7021   time: 426.96s   best: 22.5325
2023-10-18 23:35:53,758:INFO:  Epoch 311/500:  train Loss: 17.9894   val Loss: 23.4827   time: 432.95s   best: 22.9557
2023-10-18 23:41:26,769:INFO:  Epoch 472/500:  train Loss: 17.2441   val Loss: 23.8464   time: 426.83s   best: 22.5325
2023-10-18 23:43:06,481:INFO:  Epoch 312/500:  train Loss: 17.8920   val Loss: 23.0316   time: 432.69s   best: 22.9557
2023-10-18 23:48:33,549:INFO:  Epoch 473/500:  train Loss: 17.4541   val Loss: 23.8447   time: 426.76s   best: 22.5325
2023-10-18 23:50:22,208:INFO:  Epoch 313/500:  train Loss: 17.9524   val Loss: 24.6042   time: 435.59s   best: 22.9557
2023-10-18 23:55:41,938:INFO:  Epoch 474/500:  train Loss: 17.2631   val Loss: 23.4119   time: 428.36s   best: 22.5325
2023-10-18 23:57:37,967:INFO:  Epoch 314/500:  train Loss: 17.7882   val Loss: 23.9092   time: 435.73s   best: 22.9557
2023-10-19 00:02:49,309:INFO:  Epoch 475/500:  train Loss: 17.2285   val Loss: 24.4089   time: 427.36s   best: 22.5325
2023-10-19 00:04:51,028:INFO:  Epoch 315/500:  train Loss: 17.7917   val Loss: 23.8868   time: 433.03s   best: 22.9557
2023-10-19 00:09:58,896:INFO:  Epoch 476/500:  train Loss: 17.4596   val Loss: 23.6426   time: 429.57s   best: 22.5325
2023-10-19 00:12:06,898:INFO:  Epoch 316/500:  train Loss: 17.9188   val Loss: 23.9777   time: 435.84s   best: 22.9557
2023-10-19 00:17:05,096:INFO:  Epoch 477/500:  train Loss: 17.4398   val Loss: 23.9587   time: 426.17s   best: 22.5325
2023-10-19 00:19:19,140:INFO:  Epoch 317/500:  train Loss: 18.0431   val Loss: 23.3288   time: 432.23s   best: 22.9557
2023-10-19 00:24:15,012:INFO:  Epoch 478/500:  train Loss: 17.2226   val Loss: 22.5550   time: 429.90s   best: 22.5325
2023-10-19 00:26:35,574:INFO:  Epoch 318/500:  train Loss: 17.9042   val Loss: 23.7694   time: 436.42s   best: 22.9557
2023-10-19 00:31:25,844:INFO:  Epoch 479/500:  train Loss: 17.2982   val Loss: 23.4611   time: 430.80s   best: 22.5325
2023-10-19 00:33:52,374:INFO:  Epoch 319/500:  train Loss: 17.9288   val Loss: 23.8043   time: 436.76s   best: 22.9557
2023-10-19 00:38:34,311:INFO:  Epoch 480/500:  train Loss: 17.5489   val Loss: 23.6530   time: 428.44s   best: 22.5325
2023-10-19 00:41:07,406:INFO:  Epoch 320/500:  train Loss: 18.1164   val Loss: 23.2318   time: 435.00s   best: 22.9557
2023-10-19 00:45:45,387:INFO:  Epoch 481/500:  train Loss: 17.2910   val Loss: 23.1080   time: 431.05s   best: 22.5325
2023-10-19 00:48:22,244:INFO:  Epoch 321/500:  train Loss: 17.9199   val Loss: 23.6475   time: 434.81s   best: 22.9557
2023-10-19 00:52:52,310:INFO:  Epoch 482/500:  train Loss: 17.2317   val Loss: 24.3315   time: 426.88s   best: 22.5325
2023-10-19 00:55:34,364:INFO:  Epoch 322/500:  train Loss: 17.8969   val Loss: 23.5082   time: 432.10s   best: 22.9557
2023-10-19 00:59:58,872:INFO:  Epoch 483/500:  train Loss: 17.2250   val Loss: 23.3501   time: 426.54s   best: 22.5325
2023-10-19 01:02:46,683:INFO:  Epoch 323/500:  train Loss: 17.9960   val Loss: 23.1345   time: 432.30s   best: 22.9557
2023-10-19 01:07:10,455:INFO:  Epoch 484/500:  train Loss: 17.3484   val Loss: 23.3771   time: 431.55s   best: 22.5325
2023-10-19 01:09:59,899:INFO:  Epoch 324/500:  train Loss: 17.8985   val Loss: 23.8476   time: 433.18s   best: 22.9557
2023-10-19 01:14:16,684:INFO:  Epoch 485/500:  train Loss: 17.1543   val Loss: 23.0834   time: 426.21s   best: 22.5325
2023-10-19 01:17:17,532:INFO:  Epoch 325/500:  train Loss: 18.1026   val Loss: 23.5290   time: 437.59s   best: 22.9557
2023-10-19 01:21:25,972:INFO:  Epoch 486/500:  train Loss: 17.3628   val Loss: 23.5380   time: 429.27s   best: 22.5325
2023-10-19 01:24:34,141:INFO:  Epoch 326/500:  train Loss: 18.2008   val Loss: 24.7503   time: 436.58s   best: 22.9557
2023-10-19 01:28:33,196:INFO:  Epoch 487/500:  train Loss: 17.3072   val Loss: 23.1871   time: 427.21s   best: 22.5325
2023-10-19 01:31:50,015:INFO:  Epoch 327/500:  train Loss: 17.8993   val Loss: 23.2186   time: 435.84s   best: 22.9557
2023-10-19 01:35:39,446:INFO:  Epoch 488/500:  train Loss: 17.2085   val Loss: 23.6036   time: 426.23s   best: 22.5325
2023-10-19 01:39:02,191:INFO:  Epoch 328/500:  train Loss: 18.3570   val Loss: 23.8826   time: 432.15s   best: 22.9557
2023-10-19 01:42:50,561:INFO:  Epoch 489/500:  train Loss: 17.3516   val Loss: 25.0149   time: 431.11s   best: 22.5325
2023-10-19 01:46:14,673:INFO:  Epoch 329/500:  train Loss: 17.9544   val Loss: 23.1472   time: 432.45s   best: 22.9557
2023-10-19 01:49:56,353:INFO:  Epoch 490/500:  train Loss: 17.2058   val Loss: 23.5519   time: 425.78s   best: 22.5325
2023-10-19 01:53:26,951:INFO:  Epoch 330/500:  train Loss: 17.8906   val Loss: 23.8850   time: 432.26s   best: 22.9557
2023-10-19 01:57:06,786:INFO:  Epoch 491/500:  train Loss: 17.4437   val Loss: 23.8228   time: 430.41s   best: 22.5325
2023-10-19 02:00:39,636:INFO:  Epoch 331/500:  train Loss: 18.0326   val Loss: 23.0661   time: 432.65s   best: 22.9557
2023-10-19 02:04:15,010:INFO:  Epoch 492/500:  train Loss: 17.2289   val Loss: 24.0266   time: 428.22s   best: 22.5325
2023-10-19 02:07:51,653:INFO:  Epoch 332/500:  train Loss: 17.7710   val Loss: 23.5000   time: 432.00s   best: 22.9557
2023-10-19 02:11:26,153:INFO:  Epoch 493/500:  train Loss: 17.5020   val Loss: 23.5546   time: 431.13s   best: 22.5325
2023-10-19 02:15:07,359:INFO:  Epoch 333/500:  train Loss: 17.9254   val Loss: 24.7825   time: 435.68s   best: 22.9557
2023-10-19 02:18:34,931:INFO:  Epoch 494/500:  train Loss: 17.3176   val Loss: 24.2241   time: 428.75s   best: 22.5325
2023-10-19 02:22:21,239:INFO:  Epoch 334/500:  train Loss: 17.8727   val Loss: 23.5261   time: 433.85s   best: 22.9557
2023-10-19 02:25:43,142:INFO:  Epoch 495/500:  train Loss: 17.4142   val Loss: 23.7747   time: 428.18s   best: 22.5325
2023-10-19 02:29:38,196:INFO:  Epoch 335/500:  train Loss: 17.8433   val Loss: 23.7539   time: 436.93s   best: 22.9557
2023-10-19 02:32:50,564:INFO:  Epoch 496/500:  train Loss: 17.2059   val Loss: 23.2187   time: 427.41s   best: 22.5325
2023-10-19 02:36:52,418:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-19 02:36:52,453:INFO:  Epoch 336/500:  train Loss: 17.7970   val Loss: 22.9319   time: 434.21s   best: 22.9319
2023-10-19 02:40:00,540:INFO:  Epoch 497/500:  train Loss: 17.3275   val Loss: 23.8016   time: 429.95s   best: 22.5325
2023-10-19 02:44:08,346:INFO:  Epoch 337/500:  train Loss: 18.1007   val Loss: 23.4142   time: 435.89s   best: 22.9319
2023-10-19 02:47:10,704:INFO:  Epoch 498/500:  train Loss: 17.2512   val Loss: 23.9349   time: 430.14s   best: 22.5325
2023-10-19 02:51:24,579:INFO:  Epoch 338/500:  train Loss: 17.8180   val Loss: 23.6191   time: 436.21s   best: 22.9319
2023-10-19 02:54:16,595:INFO:  Epoch 499/500:  train Loss: 17.4435   val Loss: 24.7104   time: 425.87s   best: 22.5325
2023-10-19 02:58:41,009:INFO:  Epoch 339/500:  train Loss: 17.6702   val Loss: 23.7171   time: 436.39s   best: 22.9319
2023-10-19 03:01:24,280:INFO:  Epoch 500/500:  train Loss: 17.1942   val Loss: 23.4509   time: 427.66s   best: 22.5325
2023-10-19 03:01:24,304:INFO:  -----> Training complete in 4798m 44s   best validation loss: 22.5325
 
2023-10-19 03:05:58,568:INFO:  Epoch 340/500:  train Loss: 17.6861   val Loss: 24.0666   time: 437.53s   best: 22.9319
2023-10-19 03:13:14,715:INFO:  Epoch 341/500:  train Loss: 17.8115   val Loss: 23.3640   time: 436.11s   best: 22.9319
2023-10-19 03:20:28,628:INFO:  Epoch 342/500:  train Loss: 17.7419   val Loss: 24.0873   time: 433.88s   best: 22.9319
2023-10-19 03:27:43,749:INFO:  Epoch 343/500:  train Loss: 17.6027   val Loss: 23.8561   time: 435.10s   best: 22.9319
2023-10-19 03:34:58,113:INFO:  Epoch 344/500:  train Loss: 17.7432   val Loss: 23.6518   time: 434.33s   best: 22.9319
2023-10-19 03:42:14,331:INFO:  Epoch 345/500:  train Loss: 17.6013   val Loss: 23.3929   time: 436.19s   best: 22.9319
2023-10-19 03:49:31,522:INFO:  Epoch 346/500:  train Loss: 17.5534   val Loss: 23.4567   time: 437.16s   best: 22.9319
2023-10-19 03:56:46,993:INFO:  Epoch 347/500:  train Loss: 17.8641   val Loss: 23.1383   time: 435.45s   best: 22.9319
2023-10-19 04:04:03,149:INFO:  Epoch 348/500:  train Loss: 17.8222   val Loss: 25.1428   time: 436.15s   best: 22.9319
2023-10-19 04:11:14,984:INFO:  Epoch 349/500:  train Loss: 17.6398   val Loss: 24.0394   time: 431.82s   best: 22.9319
2023-10-19 04:18:27,839:INFO:  Epoch 350/500:  train Loss: 17.8907   val Loss: 23.4615   time: 432.83s   best: 22.9319
2023-10-19 04:25:44,193:INFO:  Epoch 351/500:  train Loss: 17.8183   val Loss: 24.2445   time: 436.31s   best: 22.9319
2023-10-19 04:32:59,739:INFO:  Epoch 352/500:  train Loss: 17.7934   val Loss: 23.3842   time: 435.52s   best: 22.9319
2023-10-19 04:40:15,925:INFO:  Epoch 353/500:  train Loss: 18.3003   val Loss: 24.0060   time: 436.16s   best: 22.9319
2023-10-19 04:47:31,541:INFO:  Epoch 354/500:  train Loss: 18.0699   val Loss: 23.5256   time: 435.58s   best: 22.9319
2023-10-19 04:54:45,940:INFO:  Epoch 355/500:  train Loss: 17.7455   val Loss: 23.5647   time: 434.37s   best: 22.9319
2023-10-19 05:02:00,648:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-19 05:02:00,680:INFO:  Epoch 356/500:  train Loss: 17.7879   val Loss: 22.6742   time: 434.69s   best: 22.6742
2023-10-19 05:09:14,575:INFO:  Epoch 357/500:  train Loss: 17.6105   val Loss: 23.1809   time: 433.88s   best: 22.6742
2023-10-19 05:16:28,312:INFO:  Epoch 358/500:  train Loss: 17.7896   val Loss: 23.8278   time: 433.71s   best: 22.6742
2023-10-19 05:23:40,327:INFO:  Epoch 359/500:  train Loss: 17.7331   val Loss: 23.6566   time: 431.99s   best: 22.6742
2023-10-19 05:30:55,675:INFO:  Epoch 360/500:  train Loss: 17.6967   val Loss: 23.2916   time: 435.34s   best: 22.6742
2023-10-19 05:38:09,884:INFO:  Epoch 361/500:  train Loss: 17.7576   val Loss: 23.4216   time: 434.19s   best: 22.6742
2023-10-19 05:45:25,439:INFO:  Epoch 362/500:  train Loss: 17.5998   val Loss: 23.7065   time: 435.52s   best: 22.6742
2023-10-19 05:52:40,391:INFO:  Epoch 363/500:  train Loss: 17.5852   val Loss: 23.2235   time: 434.94s   best: 22.6742
2023-10-19 05:59:56,381:INFO:  Epoch 364/500:  train Loss: 17.7803   val Loss: 24.3222   time: 435.96s   best: 22.6742
2023-10-19 06:07:12,984:INFO:  Epoch 365/500:  train Loss: 17.6764   val Loss: 26.6713   time: 436.58s   best: 22.6742
2023-10-19 06:14:27,841:INFO:  Epoch 366/500:  train Loss: 17.7779   val Loss: 25.0483   time: 434.85s   best: 22.6742
2023-10-19 06:21:43,802:INFO:  Epoch 367/500:  train Loss: 17.7897   val Loss: 24.3123   time: 435.95s   best: 22.6742
2023-10-19 06:28:55,616:INFO:  Epoch 368/500:  train Loss: 17.7038   val Loss: 23.7572   time: 431.79s   best: 22.6742
2023-10-19 06:36:09,609:INFO:  Epoch 369/500:  train Loss: 17.6738   val Loss: 23.4485   time: 433.97s   best: 22.6742
2023-10-19 06:43:24,896:INFO:  Epoch 370/500:  train Loss: 17.5310   val Loss: 23.1810   time: 435.27s   best: 22.6742
2023-10-19 06:50:40,260:INFO:  Epoch 371/500:  train Loss: 17.7864   val Loss: 23.2145   time: 435.34s   best: 22.6742
2023-10-19 06:57:54,490:INFO:  Epoch 372/500:  train Loss: 17.7110   val Loss: 23.2153   time: 434.21s   best: 22.6742
2023-10-19 07:05:09,611:INFO:  Epoch 373/500:  train Loss: 17.4839   val Loss: 23.3089   time: 435.09s   best: 22.6742
2023-10-19 07:12:26,608:INFO:  Epoch 374/500:  train Loss: 17.7001   val Loss: 23.6007   time: 436.96s   best: 22.6742
2023-10-19 07:19:42,077:INFO:  Epoch 375/500:  train Loss: 17.5731   val Loss: 23.8681   time: 435.46s   best: 22.6742
2023-10-19 07:26:53,490:INFO:  Epoch 376/500:  train Loss: 17.4928   val Loss: 24.6623   time: 431.39s   best: 22.6742
2023-10-19 07:34:05,628:INFO:  Epoch 377/500:  train Loss: 17.8581   val Loss: 23.2572   time: 432.12s   best: 22.6742
2023-10-19 07:41:19,690:INFO:  Epoch 378/500:  train Loss: 17.6246   val Loss: 23.5676   time: 434.04s   best: 22.6742
2023-10-19 07:48:32,118:INFO:  Epoch 379/500:  train Loss: 17.7069   val Loss: 23.8023   time: 432.42s   best: 22.6742
2023-10-19 07:55:45,648:INFO:  Epoch 380/500:  train Loss: 17.5909   val Loss: 23.1953   time: 433.51s   best: 22.6742
2023-10-19 08:03:00,855:INFO:  Epoch 381/500:  train Loss: 17.5535   val Loss: 23.7843   time: 435.18s   best: 22.6742
2023-10-19 08:10:16,639:INFO:  Epoch 382/500:  train Loss: 17.6667   val Loss: 23.4175   time: 435.78s   best: 22.6742
2023-10-19 08:17:28,949:INFO:  Epoch 383/500:  train Loss: 17.8047   val Loss: 23.6089   time: 432.28s   best: 22.6742
2023-10-19 08:24:44,096:INFO:  Epoch 384/500:  train Loss: 17.5088   val Loss: 23.2260   time: 435.13s   best: 22.6742
2023-10-19 08:31:59,045:INFO:  Epoch 385/500:  train Loss: 17.5943   val Loss: 23.8911   time: 434.90s   best: 22.6742
2023-10-19 08:39:15,355:INFO:  Epoch 386/500:  train Loss: 17.7420   val Loss: 23.8486   time: 436.29s   best: 22.6742
2023-10-19 08:46:26,912:INFO:  Epoch 387/500:  train Loss: 17.5302   val Loss: 23.7651   time: 431.53s   best: 22.6742
2023-10-19 08:53:41,819:INFO:  Epoch 388/500:  train Loss: 17.4808   val Loss: 23.2245   time: 434.87s   best: 22.6742
2023-10-19 09:00:52,599:INFO:  Epoch 389/500:  train Loss: 17.7180   val Loss: 27.5791   time: 430.76s   best: 22.6742
2023-10-19 09:08:09,394:INFO:  Epoch 390/500:  train Loss: 17.5583   val Loss: 25.1864   time: 436.79s   best: 22.6742
2023-10-19 09:15:23,703:INFO:  Epoch 391/500:  train Loss: 17.4904   val Loss: 23.8297   time: 434.28s   best: 22.6742
2023-10-19 09:22:37,721:INFO:  Epoch 392/500:  train Loss: 17.6074   val Loss: 23.7618   time: 433.99s   best: 22.6742
2023-10-19 09:29:49,818:INFO:  Epoch 393/500:  train Loss: 17.5342   val Loss: 23.6345   time: 432.07s   best: 22.6742
2023-10-19 09:37:01,421:INFO:  Epoch 394/500:  train Loss: 17.5203   val Loss: 23.8056   time: 431.59s   best: 22.6742
2023-10-19 09:44:14,468:INFO:  Epoch 395/500:  train Loss: 17.6884   val Loss: 23.4092   time: 433.04s   best: 22.6742
2023-10-19 09:51:26,614:INFO:  Epoch 396/500:  train Loss: 17.5417   val Loss: 22.9379   time: 432.12s   best: 22.6742
2023-10-19 09:58:40,064:INFO:  Epoch 397/500:  train Loss: 17.8140   val Loss: 22.8735   time: 433.43s   best: 22.6742
2023-10-19 10:05:54,152:INFO:  Epoch 398/500:  train Loss: 17.6759   val Loss: 23.3827   time: 434.07s   best: 22.6742
2023-10-19 10:13:06,764:INFO:  Epoch 399/500:  train Loss: 17.5238   val Loss: 23.4985   time: 432.59s   best: 22.6742
2023-10-19 10:20:19,212:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + 0.1 dropout)_a855.pt
2023-10-19 10:20:19,242:INFO:  Epoch 400/500:  train Loss: 17.5234   val Loss: 22.4419   time: 432.40s   best: 22.4419
2023-10-19 10:27:33,830:INFO:  Epoch 401/500:  train Loss: 17.6526   val Loss: 22.9543   time: 434.59s   best: 22.4419
2023-10-19 10:34:50,109:INFO:  Epoch 402/500:  train Loss: 17.4319   val Loss: 23.5549   time: 436.25s   best: 22.4419
2023-10-19 10:42:06,523:INFO:  Epoch 403/500:  train Loss: 17.6074   val Loss: 23.2722   time: 436.41s   best: 22.4419
2023-10-19 10:49:19,478:INFO:  Epoch 404/500:  train Loss: 17.4801   val Loss: 22.6163   time: 432.93s   best: 22.4419
2023-10-19 10:56:34,097:INFO:  Epoch 405/500:  train Loss: 17.9693   val Loss: 23.9527   time: 434.61s   best: 22.4419
2023-10-19 11:03:50,209:INFO:  Epoch 406/500:  train Loss: 17.6535   val Loss: 22.9581   time: 436.09s   best: 22.4419
2023-10-19 11:11:02,391:INFO:  Epoch 407/500:  train Loss: 17.7808   val Loss: 24.5760   time: 432.15s   best: 22.4419
2023-10-19 11:18:15,961:INFO:  Epoch 408/500:  train Loss: 17.5211   val Loss: 23.6206   time: 433.53s   best: 22.4419
2023-10-19 11:25:28,379:INFO:  Epoch 409/500:  train Loss: 17.5373   val Loss: 24.7640   time: 432.41s   best: 22.4419
2023-10-19 11:32:41,798:INFO:  Epoch 410/500:  train Loss: 17.4262   val Loss: 23.2727   time: 433.39s   best: 22.4419
2023-10-19 11:39:56,753:INFO:  Epoch 411/500:  train Loss: 17.7908   val Loss: 23.3532   time: 434.93s   best: 22.4419
2023-10-19 11:47:08,716:INFO:  Epoch 412/500:  train Loss: 17.4523   val Loss: 23.4707   time: 431.94s   best: 22.4419
2023-10-19 11:54:22,985:INFO:  Epoch 413/500:  train Loss: 17.4665   val Loss: 23.2791   time: 434.24s   best: 22.4419
2023-10-19 12:01:38,341:INFO:  Epoch 414/500:  train Loss: 17.3614   val Loss: 23.3150   time: 435.32s   best: 22.4419
2023-10-19 12:08:49,826:INFO:  Epoch 415/500:  train Loss: 17.4853   val Loss: 23.0441   time: 431.46s   best: 22.4419
2023-10-19 12:16:04,902:INFO:  Epoch 416/500:  train Loss: 17.6021   val Loss: 23.3906   time: 435.07s   best: 22.4419
2023-10-19 12:23:17,508:INFO:  Epoch 417/500:  train Loss: 17.4164   val Loss: 23.4850   time: 432.60s   best: 22.4419
2023-10-19 12:30:29,655:INFO:  Epoch 418/500:  train Loss: 17.4348   val Loss: 27.2325   time: 432.12s   best: 22.4419
2023-10-19 12:37:41,870:INFO:  Epoch 419/500:  train Loss: 17.4820   val Loss: 22.9627   time: 432.19s   best: 22.4419
2023-10-19 12:44:58,683:INFO:  Epoch 420/500:  train Loss: 17.3755   val Loss: 23.1237   time: 436.79s   best: 22.4419
2023-10-19 12:52:11,558:INFO:  Epoch 421/500:  train Loss: 17.4309   val Loss: 23.5190   time: 432.86s   best: 22.4419
2023-10-19 12:59:29,624:INFO:  Epoch 422/500:  train Loss: 17.4623   val Loss: 23.6931   time: 438.04s   best: 22.4419
2023-10-19 13:06:46,665:INFO:  Epoch 423/500:  train Loss: 17.6200   val Loss: 22.8823   time: 437.02s   best: 22.4419
2023-10-19 13:13:59,370:INFO:  Epoch 424/500:  train Loss: 17.5254   val Loss: 24.0889   time: 432.67s   best: 22.4419
2023-10-19 13:21:14,413:INFO:  Epoch 425/500:  train Loss: 17.5473   val Loss: 22.8873   time: 435.02s   best: 22.4419
2023-10-19 13:28:29,856:INFO:  Epoch 426/500:  train Loss: 17.3848   val Loss: 23.6471   time: 435.43s   best: 22.4419
2023-10-19 13:35:44,818:INFO:  Epoch 427/500:  train Loss: 17.3987   val Loss: 23.6475   time: 434.94s   best: 22.4419
2023-10-19 13:42:58,653:INFO:  Epoch 428/500:  train Loss: 17.8464   val Loss: 23.4565   time: 433.81s   best: 22.4419
2023-10-19 13:50:11,392:INFO:  Epoch 429/500:  train Loss: 17.5970   val Loss: 23.6521   time: 432.71s   best: 22.4419
2023-10-19 13:57:24,116:INFO:  Epoch 430/500:  train Loss: 17.5887   val Loss: 23.5147   time: 432.71s   best: 22.4419
2023-10-19 14:04:39,617:INFO:  Epoch 431/500:  train Loss: 17.3903   val Loss: 23.0252   time: 435.49s   best: 22.4419
2023-10-19 14:11:51,158:INFO:  Epoch 432/500:  train Loss: 17.4295   val Loss: 23.6913   time: 431.50s   best: 22.4419
2023-10-19 14:19:03,871:INFO:  Epoch 433/500:  train Loss: 17.2717   val Loss: 23.0552   time: 432.69s   best: 22.4419
2023-10-19 14:26:17,271:INFO:  Epoch 434/500:  train Loss: 17.3341   val Loss: 23.9545   time: 433.38s   best: 22.4419
2023-10-19 14:33:28,492:INFO:  Epoch 435/500:  train Loss: 17.3071   val Loss: 23.4307   time: 431.20s   best: 22.4419
2023-10-19 14:40:39,718:INFO:  Epoch 436/500:  train Loss: 17.4867   val Loss: 23.3178   time: 431.22s   best: 22.4419
2023-10-19 14:47:52,865:INFO:  Epoch 437/500:  train Loss: 17.3584   val Loss: 22.8033   time: 433.11s   best: 22.4419
2023-10-19 14:55:05,742:INFO:  Epoch 438/500:  train Loss: 17.5195   val Loss: 23.8999   time: 432.85s   best: 22.4419
2023-10-19 15:02:21,403:INFO:  Epoch 439/500:  train Loss: 17.3554   val Loss: 23.3040   time: 435.64s   best: 22.4419
2023-10-19 15:09:37,124:INFO:  Epoch 440/500:  train Loss: 17.9348   val Loss: 24.6813   time: 435.70s   best: 22.4419
2023-10-19 15:16:49,228:INFO:  Epoch 441/500:  train Loss: 17.8239   val Loss: 23.2942   time: 432.07s   best: 22.4419
2023-10-19 15:24:01,700:INFO:  Epoch 442/500:  train Loss: 17.5398   val Loss: 23.5189   time: 432.45s   best: 22.4419
2023-10-19 15:31:14,928:INFO:  Epoch 443/500:  train Loss: 17.3230   val Loss: 22.9837   time: 433.22s   best: 22.4419
2023-10-19 15:38:29,975:INFO:  Epoch 444/500:  train Loss: 17.3101   val Loss: 23.0223   time: 435.02s   best: 22.4419
2023-10-19 15:45:46,484:INFO:  Epoch 445/500:  train Loss: 17.2552   val Loss: 23.7188   time: 436.50s   best: 22.4419
2023-10-19 15:52:58,618:INFO:  Epoch 446/500:  train Loss: 17.3778   val Loss: 23.5767   time: 432.12s   best: 22.4419
2023-10-19 16:00:14,046:INFO:  Epoch 447/500:  train Loss: 17.5323   val Loss: 23.7670   time: 435.40s   best: 22.4419
2023-10-19 16:07:27,475:INFO:  Epoch 448/500:  train Loss: 17.4604   val Loss: 22.5434   time: 433.41s   best: 22.4419
2023-10-19 16:14:39,330:INFO:  Epoch 449/500:  train Loss: 17.3133   val Loss: 23.4697   time: 431.83s   best: 22.4419
2023-10-19 16:21:51,629:INFO:  Epoch 450/500:  train Loss: 17.7368   val Loss: 22.8162   time: 432.28s   best: 22.4419
2023-10-19 16:29:06,874:INFO:  Epoch 451/500:  train Loss: 17.5093   val Loss: 23.0159   time: 435.23s   best: 22.4419
2023-10-19 16:36:19,390:INFO:  Epoch 452/500:  train Loss: 17.2850   val Loss: 24.0566   time: 432.49s   best: 22.4419
2023-10-19 16:43:33,117:INFO:  Epoch 453/500:  train Loss: 17.3615   val Loss: 23.1297   time: 433.71s   best: 22.4419
2023-10-19 16:50:48,831:INFO:  Epoch 454/500:  train Loss: 17.4667   val Loss: 23.7026   time: 435.69s   best: 22.4419
2023-10-19 16:58:04,782:INFO:  Epoch 455/500:  train Loss: 17.4950   val Loss: 23.5239   time: 435.93s   best: 22.4419
2023-10-19 17:05:21,679:INFO:  Epoch 456/500:  train Loss: 17.5033   val Loss: 22.5320   time: 436.87s   best: 22.4419
2023-10-19 17:12:36,447:INFO:  Epoch 457/500:  train Loss: 17.3297   val Loss: 23.5045   time: 434.74s   best: 22.4419
2023-10-19 17:19:49,602:INFO:  Epoch 458/500:  train Loss: 17.7989   val Loss: 23.2969   time: 433.14s   best: 22.4419
2023-10-19 17:27:02,085:INFO:  Epoch 459/500:  train Loss: 17.3118   val Loss: 26.1080   time: 432.46s   best: 22.4419
2023-10-19 17:34:13,824:INFO:  Epoch 460/500:  train Loss: 17.3677   val Loss: 24.9243   time: 431.71s   best: 22.4419
2023-10-19 17:41:26,623:INFO:  Epoch 461/500:  train Loss: 17.3660   val Loss: 22.8513   time: 432.76s   best: 22.4419
2023-10-19 17:48:41,664:INFO:  Epoch 462/500:  train Loss: 17.2091   val Loss: 22.7258   time: 435.01s   best: 22.4419
2023-10-19 17:55:58,552:INFO:  Epoch 463/500:  train Loss: 17.4944   val Loss: 23.7116   time: 436.86s   best: 22.4419
2023-10-19 18:03:10,626:INFO:  Epoch 464/500:  train Loss: 17.3627   val Loss: 23.0802   time: 432.04s   best: 22.4419
2023-10-19 18:10:23,233:INFO:  Epoch 465/500:  train Loss: 17.6169   val Loss: 23.3786   time: 432.59s   best: 22.4419
2023-10-19 18:17:38,755:INFO:  Epoch 466/500:  train Loss: 17.3744   val Loss: 23.2232   time: 435.51s   best: 22.4419
2023-10-19 18:24:50,440:INFO:  Epoch 467/500:  train Loss: 17.3159   val Loss: 23.4225   time: 431.66s   best: 22.4419
2023-10-19 18:32:02,985:INFO:  Epoch 468/500:  train Loss: 17.3779   val Loss: 23.8071   time: 432.52s   best: 22.4419
2023-10-19 18:39:14,634:INFO:  Epoch 469/500:  train Loss: 17.3313   val Loss: 24.3346   time: 431.62s   best: 22.4419
2023-10-19 18:46:28,956:INFO:  Epoch 470/500:  train Loss: 17.5746   val Loss: 24.3620   time: 434.28s   best: 22.4419
2023-10-19 18:53:41,014:INFO:  Epoch 471/500:  train Loss: 17.7511   val Loss: 22.9984   time: 432.02s   best: 22.4419
2023-10-19 19:00:53,396:INFO:  Epoch 472/500:  train Loss: 17.3356   val Loss: 23.4580   time: 432.35s   best: 22.4419
2023-10-19 19:08:05,968:INFO:  Epoch 473/500:  train Loss: 17.3050   val Loss: 23.9405   time: 432.55s   best: 22.4419
2023-10-19 19:15:20,319:INFO:  Epoch 474/500:  train Loss: 17.1090   val Loss: 23.3863   time: 434.32s   best: 22.4419
2023-10-19 19:22:32,576:INFO:  Epoch 475/500:  train Loss: 17.1846   val Loss: 23.4949   time: 432.23s   best: 22.4419
2023-10-19 19:29:46,261:INFO:  Epoch 476/500:  train Loss: 17.2036   val Loss: 25.2669   time: 433.66s   best: 22.4419
2023-10-19 19:37:03,040:INFO:  Epoch 477/500:  train Loss: 17.2958   val Loss: 23.1355   time: 436.76s   best: 22.4419
2023-10-19 19:44:15,176:INFO:  Epoch 478/500:  train Loss: 17.3801   val Loss: 22.9137   time: 432.12s   best: 22.4419
2023-10-19 19:51:28,605:INFO:  Epoch 479/500:  train Loss: 17.1980   val Loss: 23.3552   time: 433.40s   best: 22.4419
2023-10-19 19:58:42,393:INFO:  Epoch 480/500:  train Loss: 17.2952   val Loss: 24.7654   time: 433.76s   best: 22.4419
2023-10-19 20:05:58,757:INFO:  Epoch 481/500:  train Loss: 17.2122   val Loss: 23.1451   time: 436.34s   best: 22.4419
2023-10-19 20:13:13,999:INFO:  Epoch 482/500:  train Loss: 17.2136   val Loss: 23.0014   time: 435.22s   best: 22.4419
2023-10-19 20:20:28,614:INFO:  Epoch 483/500:  train Loss: 17.3808   val Loss: 23.4268   time: 434.61s   best: 22.4419
2023-10-19 20:27:44,873:INFO:  Epoch 484/500:  train Loss: 17.1392   val Loss: 23.2738   time: 436.23s   best: 22.4419
2023-10-19 20:35:00,048:INFO:  Epoch 485/500:  train Loss: 17.1498   val Loss: 23.3786   time: 435.15s   best: 22.4419
2023-10-19 20:42:15,836:INFO:  Epoch 486/500:  train Loss: 17.2045   val Loss: 22.4674   time: 435.76s   best: 22.4419
2023-10-19 20:49:28,441:INFO:  Epoch 487/500:  train Loss: 17.1094   val Loss: 23.3863   time: 432.59s   best: 22.4419
2023-10-19 20:56:44,975:INFO:  Epoch 488/500:  train Loss: 17.1919   val Loss: 23.1747   time: 436.50s   best: 22.4419
2023-10-19 21:03:58,535:INFO:  Epoch 489/500:  train Loss: 17.8669   val Loss: 23.5564   time: 433.54s   best: 22.4419
2023-10-19 21:11:14,677:INFO:  Epoch 490/500:  train Loss: 17.1919   val Loss: 23.1112   time: 436.12s   best: 22.4419
2023-10-19 21:18:26,447:INFO:  Epoch 491/500:  train Loss: 17.5569   val Loss: 23.5910   time: 431.74s   best: 22.4419
2023-10-19 21:25:39,744:INFO:  Epoch 492/500:  train Loss: 17.1715   val Loss: 22.7576   time: 433.27s   best: 22.4419
2023-10-19 21:32:56,589:INFO:  Epoch 493/500:  train Loss: 17.1539   val Loss: 23.2344   time: 436.82s   best: 22.4419
2023-10-19 21:40:12,599:INFO:  Epoch 494/500:  train Loss: 17.3215   val Loss: 23.2373   time: 436.00s   best: 22.4419
2023-10-19 21:47:26,978:INFO:  Epoch 495/500:  train Loss: 17.6156   val Loss: 23.1403   time: 434.37s   best: 22.4419
2023-10-19 21:54:41,128:INFO:  Epoch 496/500:  train Loss: 17.2788   val Loss: 23.7790   time: 434.13s   best: 22.4419
2023-10-19 22:01:57,642:INFO:  Epoch 497/500:  train Loss: 17.0399   val Loss: 23.3270   time: 436.49s   best: 22.4419
2023-10-19 22:09:10,832:INFO:  Epoch 498/500:  train Loss: 17.2340   val Loss: 23.3858   time: 433.16s   best: 22.4419
2023-10-19 22:16:25,672:INFO:  Epoch 499/500:  train Loss: 17.2286   val Loss: 24.4025   time: 434.82s   best: 22.4419
2023-10-19 22:23:37,707:INFO:  Epoch 500/500:  train Loss: 17.1913   val Loss: 23.2813   time: 432.03s   best: 22.4419
2023-10-19 22:23:37,723:INFO:  -----> Training complete in 3614m 42s   best validation loss: 22.4419
 
2023-10-27 09:37:44,151:INFO:  Starting experiment lstm autoencoder debug (2 layer + no dropout)
2023-10-27 09:37:44,194:INFO:  Defining the model
2023-10-27 09:37:44,238:INFO:  Reading the dataset
2023-10-27 09:37:50,883:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:37:50,950:INFO:  Epoch 1/500:  train Loss: 108.0818   val Loss: 99.9419   time: 1.65s   best: 99.9419
2023-10-27 09:37:51,224:INFO:  Epoch 2/500:  train Loss: 99.6187   val Loss: 100.1003   time: 0.27s   best: 99.9419
2023-10-27 09:37:51,483:INFO:  Epoch 3/500:  train Loss: 99.9469   val Loss: 100.2377   time: 0.25s   best: 99.9419
2023-10-27 09:37:51,721:INFO:  Epoch 4/500:  train Loss: 100.2174   val Loss: 100.3101   time: 0.23s   best: 99.9419
2023-10-27 09:37:51,980:INFO:  Epoch 5/500:  train Loss: 100.3218   val Loss: 100.3475   time: 0.26s   best: 99.9419
2023-10-27 09:37:52,216:INFO:  Epoch 6/500:  train Loss: 100.3113   val Loss: 100.3618   time: 0.23s   best: 99.9419
2023-10-27 09:37:52,473:INFO:  Epoch 7/500:  train Loss: 100.3470   val Loss: 100.3618   time: 0.25s   best: 99.9419
2023-10-27 09:37:52,709:INFO:  Epoch 8/500:  train Loss: 100.3299   val Loss: 100.3462   time: 0.23s   best: 99.9419
2023-10-27 09:37:52,970:INFO:  Epoch 9/500:  train Loss: 100.3425   val Loss: 100.3260   time: 0.26s   best: 99.9419
2023-10-27 09:37:53,228:INFO:  Epoch 10/500:  train Loss: 100.3005   val Loss: 100.3039   time: 0.26s   best: 99.9419
2023-10-27 09:37:53,502:INFO:  Epoch 11/500:  train Loss: 100.2624   val Loss: 100.2726   time: 0.27s   best: 99.9419
2023-10-27 09:37:53,739:INFO:  Epoch 12/500:  train Loss: 100.1801   val Loss: 100.2366   time: 0.23s   best: 99.9419
2023-10-27 09:37:53,998:INFO:  Epoch 13/500:  train Loss: 100.0858   val Loss: 100.2002   time: 0.26s   best: 99.9419
2023-10-27 09:37:54,235:INFO:  Epoch 14/500:  train Loss: 100.1155   val Loss: 100.1711   time: 0.23s   best: 99.9419
2023-10-27 09:37:54,491:INFO:  Epoch 15/500:  train Loss: 99.6843   val Loss: 99.9569   time: 0.25s   best: 99.9419
2023-10-27 09:37:54,730:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:37:54,846:INFO:  Epoch 16/500:  train Loss: 99.1421   val Loss: 98.8016   time: 0.23s   best: 98.8016
2023-10-27 09:37:55,102:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:37:55,178:INFO:  Epoch 17/500:  train Loss: 97.3237   val Loss: 96.5174   time: 0.25s   best: 96.5174
2023-10-27 09:37:55,474:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:37:55,615:INFO:  Epoch 18/500:  train Loss: 96.0769   val Loss: 95.9856   time: 0.29s   best: 95.9856
2023-10-27 09:37:55,855:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:37:55,990:INFO:  Epoch 19/500:  train Loss: 96.0438   val Loss: 95.4365   time: 0.24s   best: 95.4365
2023-10-27 09:37:56,229:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:37:56,311:INFO:  Epoch 20/500:  train Loss: 94.9750   val Loss: 95.0371   time: 0.24s   best: 95.0371
2023-10-27 09:37:56,564:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:37:56,723:INFO:  Epoch 21/500:  train Loss: 94.5418   val Loss: 94.5678   time: 0.25s   best: 94.5678
2023-10-27 09:37:56,964:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:37:57,042:INFO:  Epoch 22/500:  train Loss: 93.5932   val Loss: 93.7389   time: 0.23s   best: 93.7389
2023-10-27 09:37:57,288:INFO:  Epoch 23/500:  train Loss: 94.0610   val Loss: 94.1704   time: 0.24s   best: 93.7389
2023-10-27 09:37:57,577:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:37:57,717:INFO:  Epoch 24/500:  train Loss: 93.3531   val Loss: 90.6494   time: 0.28s   best: 90.6494
2023-10-27 09:37:57,955:INFO:  Epoch 25/500:  train Loss: 94.1563   val Loss: 92.5922   time: 0.24s   best: 90.6494
2023-10-27 09:37:58,207:INFO:  Epoch 26/500:  train Loss: 94.0638   val Loss: 94.4395   time: 0.25s   best: 90.6494
2023-10-27 09:37:58,445:INFO:  Epoch 27/500:  train Loss: 94.7887   val Loss: 94.7135   time: 0.23s   best: 90.6494
2023-10-27 09:37:58,702:INFO:  Epoch 28/500:  train Loss: 94.9278   val Loss: 94.6480   time: 0.25s   best: 90.6494
2023-10-27 09:37:58,938:INFO:  Epoch 29/500:  train Loss: 94.0163   val Loss: 94.3806   time: 0.23s   best: 90.6494
2023-10-27 09:37:59,196:INFO:  Epoch 30/500:  train Loss: 94.1276   val Loss: 94.2458   time: 0.26s   best: 90.6494
2023-10-27 09:37:59,442:INFO:  Epoch 31/500:  train Loss: 93.6598   val Loss: 94.0545   time: 0.24s   best: 90.6494
2023-10-27 09:37:59,734:INFO:  Epoch 32/500:  train Loss: 94.0176   val Loss: 93.9633   time: 0.29s   best: 90.6494
2023-10-27 09:37:59,972:INFO:  Epoch 33/500:  train Loss: 93.9611   val Loss: 94.3126   time: 0.24s   best: 90.6494
2023-10-27 09:38:00,228:INFO:  Epoch 34/500:  train Loss: 94.2202   val Loss: 94.4253   time: 0.25s   best: 90.6494
2023-10-27 09:38:00,465:INFO:  Epoch 35/500:  train Loss: 94.2815   val Loss: 94.4184   time: 0.23s   best: 90.6494
2023-10-27 09:38:00,722:INFO:  Epoch 36/500:  train Loss: 93.5442   val Loss: 94.1754   time: 0.25s   best: 90.6494
2023-10-27 09:38:00,959:INFO:  Epoch 37/500:  train Loss: 93.5912   val Loss: 93.8917   time: 0.23s   best: 90.6494
2023-10-27 09:38:01,217:INFO:  Epoch 38/500:  train Loss: 93.6868   val Loss: 93.7184   time: 0.26s   best: 90.6494
2023-10-27 09:38:01,464:INFO:  Epoch 39/500:  train Loss: 93.6571   val Loss: 94.0188   time: 0.24s   best: 90.6494
2023-10-27 09:38:01,753:INFO:  Epoch 40/500:  train Loss: 93.9008   val Loss: 94.1030   time: 0.29s   best: 90.6494
2023-10-27 09:38:01,992:INFO:  Epoch 41/500:  train Loss: 93.8918   val Loss: 94.1181   time: 0.24s   best: 90.6494
2023-10-27 09:38:02,228:INFO:  Epoch 42/500:  train Loss: 93.2924   val Loss: 93.7257   time: 0.23s   best: 90.6494
2023-10-27 09:38:02,486:INFO:  Epoch 43/500:  train Loss: 93.1055   val Loss: 92.9805   time: 0.26s   best: 90.6494
2023-10-27 09:38:02,723:INFO:  Epoch 44/500:  train Loss: 92.5290   val Loss: 92.3647   time: 0.23s   best: 90.6494
2023-10-27 09:38:02,982:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:03,069:INFO:  Epoch 45/500:  train Loss: 92.3191   val Loss: 90.5613   time: 0.25s   best: 90.5613
2023-10-27 09:38:03,330:INFO:  Epoch 46/500:  train Loss: 92.8154   val Loss: 92.9079   time: 0.26s   best: 90.5613
2023-10-27 09:38:03,569:INFO:  Epoch 47/500:  train Loss: 92.6507   val Loss: 93.2442   time: 0.24s   best: 90.5613
2023-10-27 09:38:03,858:INFO:  Epoch 48/500:  train Loss: 93.1650   val Loss: 93.0535   time: 0.29s   best: 90.5613
2023-10-27 09:38:04,097:INFO:  Epoch 49/500:  train Loss: 92.8168   val Loss: 92.7079   time: 0.24s   best: 90.5613
2023-10-27 09:38:04,334:INFO:  Epoch 50/500:  train Loss: 92.5695   val Loss: 92.1649   time: 0.24s   best: 90.5613
2023-10-27 09:38:04,604:INFO:  Epoch 51/500:  train Loss: 91.4060   val Loss: 91.0575   time: 0.27s   best: 90.5613
2023-10-27 09:38:04,842:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:04,983:INFO:  Epoch 52/500:  train Loss: 91.1192   val Loss: 90.3744   time: 0.23s   best: 90.3744
2023-10-27 09:38:05,222:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:05,301:INFO:  Epoch 53/500:  train Loss: 90.4924   val Loss: 90.0701   time: 0.23s   best: 90.0701
2023-10-27 09:38:05,554:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:05,580:INFO:  Epoch 54/500:  train Loss: 89.2387   val Loss: 89.4118   time: 0.25s   best: 89.4118
2023-10-27 09:38:05,943:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:06,056:INFO:  Epoch 55/500:  train Loss: 89.2511   val Loss: 88.3744   time: 0.27s   best: 88.3744
2023-10-27 09:38:06,296:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:06,430:INFO:  Epoch 56/500:  train Loss: 88.8458   val Loss: 87.9712   time: 0.23s   best: 87.9712
2023-10-27 09:38:06,678:INFO:  Epoch 57/500:  train Loss: 88.1254   val Loss: 88.2170   time: 0.24s   best: 87.9712
2023-10-27 09:38:06,915:INFO:  Epoch 58/500:  train Loss: 88.8538   val Loss: 88.4379   time: 0.24s   best: 87.9712
2023-10-27 09:38:07,175:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:07,448:INFO:  Epoch 59/500:  train Loss: 88.7379   val Loss: 87.8292   time: 0.26s   best: 87.8292
2023-10-27 09:38:07,700:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:07,809:INFO:  Epoch 60/500:  train Loss: 88.3407   val Loss: 87.8061   time: 0.25s   best: 87.8061
2023-10-27 09:38:08,066:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:08,094:INFO:  Epoch 61/500:  train Loss: 87.9179   val Loss: 87.5152   time: 0.25s   best: 87.5152
2023-10-27 09:38:08,334:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:08,386:INFO:  Epoch 62/500:  train Loss: 87.4208   val Loss: 86.9712   time: 0.24s   best: 86.9712
2023-10-27 09:38:08,640:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:08,707:INFO:  Epoch 63/500:  train Loss: 87.6820   val Loss: 86.8187   time: 0.25s   best: 86.8187
2023-10-27 09:38:08,945:INFO:  Epoch 64/500:  train Loss: 88.0390   val Loss: 87.5389   time: 0.24s   best: 86.8187
2023-10-27 09:38:09,202:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:09,295:INFO:  Epoch 65/500:  train Loss: 87.2377   val Loss: 86.3668   time: 0.25s   best: 86.3668
2023-10-27 09:38:09,533:INFO:  Epoch 66/500:  train Loss: 87.3940   val Loss: 86.5946   time: 0.23s   best: 86.3668
2023-10-27 09:38:09,808:INFO:  Epoch 67/500:  train Loss: 88.7010   val Loss: 89.1426   time: 0.27s   best: 86.3668
2023-10-27 09:38:10,064:INFO:  Epoch 68/500:  train Loss: 88.8900   val Loss: 87.9887   time: 0.25s   best: 86.3668
2023-10-27 09:38:10,320:INFO:  Epoch 69/500:  train Loss: 87.4111   val Loss: 87.0714   time: 0.25s   best: 86.3668
2023-10-27 09:38:10,558:INFO:  Epoch 70/500:  train Loss: 87.3159   val Loss: 86.5584   time: 0.24s   best: 86.3668
2023-10-27 09:38:10,815:INFO:  Epoch 71/500:  train Loss: 88.5687   val Loss: 88.9576   time: 0.25s   best: 86.3668
2023-10-27 09:38:11,053:INFO:  Epoch 72/500:  train Loss: 88.0366   val Loss: 86.4871   time: 0.24s   best: 86.3668
2023-10-27 09:38:11,320:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:11,436:INFO:  Epoch 73/500:  train Loss: 87.1475   val Loss: 86.2377   time: 0.26s   best: 86.2377
2023-10-27 09:38:11,690:INFO:  Epoch 74/500:  train Loss: 88.1262   val Loss: 88.3004   time: 0.25s   best: 86.2377
2023-10-27 09:38:11,964:INFO:  Epoch 75/500:  train Loss: 88.6345   val Loss: 87.8963   time: 0.27s   best: 86.2377
2023-10-27 09:38:12,220:INFO:  Epoch 76/500:  train Loss: 87.4819   val Loss: 86.5701   time: 0.25s   best: 86.2377
2023-10-27 09:38:12,459:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:12,504:INFO:  Epoch 77/500:  train Loss: 86.6089   val Loss: 86.2103   time: 0.23s   best: 86.2103
2023-10-27 09:38:12,759:INFO:  Epoch 78/500:  train Loss: 87.3564   val Loss: 87.6049   time: 0.25s   best: 86.2103
2023-10-27 09:38:12,998:INFO:  Epoch 79/500:  train Loss: 88.4421   val Loss: 88.0269   time: 0.24s   best: 86.2103
2023-10-27 09:38:13,258:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:13,376:INFO:  Epoch 80/500:  train Loss: 87.1849   val Loss: 85.9930   time: 0.26s   best: 85.9930
2023-10-27 09:38:13,616:INFO:  Epoch 81/500:  train Loss: 86.0601   val Loss: 86.1889   time: 0.24s   best: 85.9930
2023-10-27 09:38:13,866:INFO:  Epoch 82/500:  train Loss: 87.1264   val Loss: 86.2665   time: 0.25s   best: 85.9930
2023-10-27 09:38:14,139:INFO:  Epoch 83/500:  train Loss: 87.0519   val Loss: 86.4746   time: 0.27s   best: 85.9930
2023-10-27 09:38:14,397:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:14,418:INFO:  Epoch 84/500:  train Loss: 86.0231   val Loss: 84.7699   time: 0.25s   best: 84.7699
2023-10-27 09:38:14,657:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:14,729:INFO:  Epoch 85/500:  train Loss: 85.6364   val Loss: 84.7151   time: 0.23s   best: 84.7151
2023-10-27 09:38:14,986:INFO:  Epoch 86/500:  train Loss: 85.8814   val Loss: 85.5779   time: 0.25s   best: 84.7151
2023-10-27 09:38:15,224:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:15,272:INFO:  Epoch 87/500:  train Loss: 85.5442   val Loss: 84.5776   time: 0.23s   best: 84.5776
2023-10-27 09:38:15,533:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:15,631:INFO:  Epoch 88/500:  train Loss: 84.9124   val Loss: 84.1838   time: 0.26s   best: 84.1838
2023-10-27 09:38:15,885:INFO:  Epoch 89/500:  train Loss: 85.5280   val Loss: 84.9088   time: 0.25s   best: 84.1838
2023-10-27 09:38:16,194:INFO:  Epoch 90/500:  train Loss: 85.6222   val Loss: 84.5698   time: 0.30s   best: 84.1838
2023-10-27 09:38:16,449:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:16,561:INFO:  Epoch 91/500:  train Loss: 84.6951   val Loss: 83.9171   time: 0.25s   best: 83.9171
2023-10-27 09:38:16,809:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:16,928:INFO:  Epoch 92/500:  train Loss: 84.1634   val Loss: 83.5303   time: 0.24s   best: 83.5303
2023-10-27 09:38:17,169:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:17,488:INFO:  Epoch 93/500:  train Loss: 84.2226   val Loss: 83.1822   time: 0.24s   best: 83.1822
2023-10-27 09:38:17,728:INFO:  Epoch 94/500:  train Loss: 83.3150   val Loss: 84.7482   time: 0.24s   best: 83.1822
2023-10-27 09:38:17,982:INFO:  Epoch 95/500:  train Loss: 85.6385   val Loss: 87.1201   time: 0.25s   best: 83.1822
2023-10-27 09:38:18,255:INFO:  Epoch 96/500:  train Loss: 87.5500   val Loss: 86.3193   time: 0.27s   best: 83.1822
2023-10-27 09:38:18,511:INFO:  Epoch 97/500:  train Loss: 85.3031   val Loss: 84.3383   time: 0.25s   best: 83.1822
2023-10-27 09:38:18,748:INFO:  Epoch 98/500:  train Loss: 84.6658   val Loss: 84.6695   time: 0.23s   best: 83.1822
2023-10-27 09:38:19,007:INFO:  Epoch 99/500:  train Loss: 85.1221   val Loss: 83.7838   time: 0.26s   best: 83.1822
2023-10-27 09:38:19,313:INFO:  Epoch 100/500:  train Loss: 84.5870   val Loss: 84.0285   time: 0.28s   best: 83.1822
2023-10-27 09:38:19,573:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:19,622:INFO:  Epoch 101/500:  train Loss: 83.8673   val Loss: 82.6636   time: 0.26s   best: 82.6636
2023-10-27 09:38:19,869:INFO:  Epoch 102/500:  train Loss: 83.4272   val Loss: 82.7981   time: 0.24s   best: 82.6636
2023-10-27 09:38:20,138:INFO:  Epoch 103/500:  train Loss: 84.1797   val Loss: 82.9815   time: 0.26s   best: 82.6636
2023-10-27 09:38:20,428:INFO:  Epoch 104/500:  train Loss: 83.4842   val Loss: 82.8723   time: 0.28s   best: 82.6636
2023-10-27 09:38:20,684:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:20,858:INFO:  Epoch 105/500:  train Loss: 83.2308   val Loss: 82.5137   time: 0.25s   best: 82.5137
2023-10-27 09:38:21,112:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:21,140:INFO:  Epoch 106/500:  train Loss: 83.3098   val Loss: 82.4636   time: 0.25s   best: 82.4636
2023-10-27 09:38:21,395:INFO:  Epoch 107/500:  train Loss: 82.8240   val Loss: 82.6547   time: 0.24s   best: 82.4636
2023-10-27 09:38:21,650:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:21,827:INFO:  Epoch 108/500:  train Loss: 82.5159   val Loss: 81.9671   time: 0.25s   best: 81.9671
2023-10-27 09:38:22,070:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:22,110:INFO:  Epoch 109/500:  train Loss: 82.5340   val Loss: 81.4477   time: 0.24s   best: 81.4477
2023-10-27 09:38:22,392:INFO:  Epoch 110/500:  train Loss: 82.9774   val Loss: 82.1200   time: 0.27s   best: 81.4477
2023-10-27 09:38:22,654:INFO:  Epoch 111/500:  train Loss: 82.9355   val Loss: 81.5618   time: 0.25s   best: 81.4477
2023-10-27 09:38:22,901:INFO:  Epoch 112/500:  train Loss: 82.5004   val Loss: 82.5886   time: 0.24s   best: 81.4477
2023-10-27 09:38:23,167:INFO:  Epoch 113/500:  train Loss: 82.2365   val Loss: 81.7358   time: 0.26s   best: 81.4477
2023-10-27 09:38:23,414:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:23,582:INFO:  Epoch 114/500:  train Loss: 81.3436   val Loss: 81.2113   time: 0.24s   best: 81.2113
2023-10-27 09:38:23,839:INFO:  Epoch 115/500:  train Loss: 83.0390   val Loss: 82.8199   time: 0.25s   best: 81.2113
2023-10-27 09:38:24,081:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:24,137:INFO:  Epoch 116/500:  train Loss: 81.8354   val Loss: 80.8799   time: 0.24s   best: 80.8799
2023-10-27 09:38:24,432:INFO:  Epoch 117/500:  train Loss: 81.6477   val Loss: 81.2362   time: 0.28s   best: 80.8799
2023-10-27 09:38:24,679:INFO:  Epoch 118/500:  train Loss: 81.2939   val Loss: 80.9560   time: 0.24s   best: 80.8799
2023-10-27 09:38:24,938:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:25,097:INFO:  Epoch 119/500:  train Loss: 81.6128   val Loss: 80.2301   time: 0.25s   best: 80.2301
2023-10-27 09:38:25,360:INFO:  Epoch 120/500:  train Loss: 81.7665   val Loss: 80.7387   time: 0.25s   best: 80.2301
2023-10-27 09:38:25,605:INFO:  Epoch 121/500:  train Loss: 81.9601   val Loss: 80.9867   time: 0.23s   best: 80.2301
2023-10-27 09:38:25,870:INFO:  Epoch 122/500:  train Loss: 81.4214   val Loss: 80.9465   time: 0.25s   best: 80.2301
2023-10-27 09:38:26,113:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:26,280:INFO:  Epoch 123/500:  train Loss: 82.0434   val Loss: 79.6436   time: 0.24s   best: 79.6436
2023-10-27 09:38:26,569:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:26,717:INFO:  Epoch 124/500:  train Loss: 80.3892   val Loss: 78.9734   time: 0.28s   best: 78.9734
2023-10-27 09:38:26,968:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:27,301:INFO:  Epoch 125/500:  train Loss: 79.9605   val Loss: 78.6966   time: 0.25s   best: 78.6966
2023-10-27 09:38:27,556:INFO:  Epoch 126/500:  train Loss: 79.3385   val Loss: 79.0834   time: 0.24s   best: 78.6966
2023-10-27 09:38:27,801:INFO:  Epoch 127/500:  train Loss: 81.0182   val Loss: 80.7755   time: 0.23s   best: 78.6966
2023-10-27 09:38:28,070:INFO:  Epoch 128/500:  train Loss: 81.0657   val Loss: 79.9934   time: 0.26s   best: 78.6966
2023-10-27 09:38:28,316:INFO:  Epoch 129/500:  train Loss: 84.8197   val Loss: 79.2081   time: 0.24s   best: 78.6966
2023-10-27 09:38:28,615:INFO:  Epoch 130/500:  train Loss: 81.5940   val Loss: 82.1068   time: 0.29s   best: 78.6966
2023-10-27 09:38:28,861:INFO:  Epoch 131/500:  train Loss: 81.0824   val Loss: 80.3161   time: 0.23s   best: 78.6966
2023-10-27 09:38:29,126:INFO:  Epoch 132/500:  train Loss: 82.5585   val Loss: 79.8715   time: 0.25s   best: 78.6966
2023-10-27 09:38:29,379:INFO:  Epoch 133/500:  train Loss: 80.2675   val Loss: 79.1049   time: 0.24s   best: 78.6966
2023-10-27 09:38:29,636:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:29,666:INFO:  Epoch 134/500:  train Loss: 79.6564   val Loss: 78.3577   time: 0.25s   best: 78.3577
2023-10-27 09:38:29,906:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:30,115:INFO:  Epoch 135/500:  train Loss: 79.8391   val Loss: 77.9309   time: 0.24s   best: 77.9309
2023-10-27 09:38:30,362:INFO:  Epoch 136/500:  train Loss: 79.3865   val Loss: 78.4085   time: 0.24s   best: 77.9309
2023-10-27 09:38:30,654:INFO:  Epoch 137/500:  train Loss: 79.0917   val Loss: 78.7189   time: 0.28s   best: 77.9309
2023-10-27 09:38:30,900:INFO:  Epoch 138/500:  train Loss: 79.2626   val Loss: 78.6160   time: 0.24s   best: 77.9309
2023-10-27 09:38:31,159:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:31,187:INFO:  Epoch 139/500:  train Loss: 79.5633   val Loss: 76.9275   time: 0.25s   best: 76.9275
2023-10-27 09:38:31,433:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:31,659:INFO:  Epoch 140/500:  train Loss: 77.8523   val Loss: 76.2099   time: 0.24s   best: 76.2099
2023-10-27 09:38:31,904:INFO:  Epoch 141/500:  train Loss: 78.5182   val Loss: 78.1768   time: 0.23s   best: 76.2099
2023-10-27 09:38:32,164:INFO:  Epoch 142/500:  train Loss: 79.1792   val Loss: 78.5372   time: 0.25s   best: 76.2099
2023-10-27 09:38:32,409:INFO:  Epoch 143/500:  train Loss: 79.1976   val Loss: 77.8720   time: 0.23s   best: 76.2099
2023-10-27 09:38:32,687:INFO:  Epoch 144/500:  train Loss: 78.5950   val Loss: 76.8195   time: 0.25s   best: 76.2099
2023-10-27 09:38:32,946:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:33,001:INFO:  Epoch 145/500:  train Loss: 77.1968   val Loss: 75.4799   time: 0.25s   best: 75.4799
2023-10-27 09:38:33,263:INFO:  Epoch 146/500:  train Loss: 77.6612   val Loss: 75.7372   time: 0.25s   best: 75.4799
2023-10-27 09:38:33,515:INFO:  Epoch 147/500:  train Loss: 76.6979   val Loss: 76.4819   time: 0.24s   best: 75.4799
2023-10-27 09:38:33,779:INFO:  Epoch 148/500:  train Loss: 78.1635   val Loss: 78.5152   time: 0.25s   best: 75.4799
2023-10-27 09:38:34,024:INFO:  Epoch 149/500:  train Loss: 78.6630   val Loss: 77.8945   time: 0.23s   best: 75.4799
2023-10-27 09:38:34,292:INFO:  Epoch 150/500:  train Loss: 78.8313   val Loss: 76.7169   time: 0.26s   best: 75.4799
2023-10-27 09:38:34,538:INFO:  Epoch 151/500:  train Loss: 77.5457   val Loss: 75.5867   time: 0.24s   best: 75.4799
2023-10-27 09:38:34,830:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:34,936:INFO:  Epoch 152/500:  train Loss: 77.4923   val Loss: 75.0032   time: 0.29s   best: 75.0032
2023-10-27 09:38:35,197:INFO:  Epoch 153/500:  train Loss: 76.4174   val Loss: 77.3219   time: 0.25s   best: 75.0032
2023-10-27 09:38:35,453:INFO:  Epoch 154/500:  train Loss: 78.7129   val Loss: 76.1452   time: 0.25s   best: 75.0032
2023-10-27 09:38:35,726:INFO:  Epoch 155/500:  train Loss: 76.7170   val Loss: 76.1037   time: 0.26s   best: 75.0032
2023-10-27 09:38:35,972:INFO:  Epoch 156/500:  train Loss: 77.3715   val Loss: 78.1125   time: 0.24s   best: 75.0032
2023-10-27 09:38:36,237:INFO:  Epoch 157/500:  train Loss: 79.0487   val Loss: 77.3226   time: 0.25s   best: 75.0032
2023-10-27 09:38:36,486:INFO:  Epoch 158/500:  train Loss: 79.7955   val Loss: 78.0411   time: 0.24s   best: 75.0032
2023-10-27 09:38:36,752:INFO:  Epoch 159/500:  train Loss: 79.5385   val Loss: 79.0653   time: 0.24s   best: 75.0032
2023-10-27 09:38:37,033:INFO:  Epoch 160/500:  train Loss: 79.1623   val Loss: 78.9864   time: 0.27s   best: 75.0032
2023-10-27 09:38:37,296:INFO:  Epoch 161/500:  train Loss: 80.2592   val Loss: 77.5776   time: 0.25s   best: 75.0032
2023-10-27 09:38:37,548:INFO:  Epoch 162/500:  train Loss: 77.6496   val Loss: 75.8131   time: 0.24s   best: 75.0032
2023-10-27 09:38:37,810:INFO:  Epoch 163/500:  train Loss: 77.0130   val Loss: 75.0725   time: 0.25s   best: 75.0032
2023-10-27 09:38:38,054:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:38,197:INFO:  Epoch 164/500:  train Loss: 75.7276   val Loss: 74.8488   time: 0.24s   best: 74.8488
2023-10-27 09:38:38,456:INFO:  Epoch 165/500:  train Loss: 75.6855   val Loss: 75.6018   time: 0.25s   best: 74.8488
2023-10-27 09:38:38,702:INFO:  Epoch 166/500:  train Loss: 77.8254   val Loss: 78.1044   time: 0.24s   best: 74.8488
2023-10-27 09:38:39,000:INFO:  Epoch 167/500:  train Loss: 77.5828   val Loss: 75.8111   time: 0.29s   best: 74.8488
2023-10-27 09:38:39,240:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:39,311:INFO:  Epoch 168/500:  train Loss: 77.3683   val Loss: 74.6175   time: 0.24s   best: 74.6175
2023-10-27 09:38:39,614:INFO:  Epoch 169/500:  train Loss: 76.2372   val Loss: 75.9501   time: 0.29s   best: 74.6175
2023-10-27 09:38:39,860:INFO:  Epoch 170/500:  train Loss: 75.8772   val Loss: 75.0107   time: 0.23s   best: 74.6175
2023-10-27 09:38:40,125:INFO:  Epoch 171/500:  train Loss: 76.5517   val Loss: 76.3944   time: 0.25s   best: 74.6175
2023-10-27 09:38:40,372:INFO:  Epoch 172/500:  train Loss: 76.6150   val Loss: 75.9432   time: 0.24s   best: 74.6175
2023-10-27 09:38:40,631:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:40,778:INFO:  Epoch 173/500:  train Loss: 76.4110   val Loss: 74.4402   time: 0.25s   best: 74.4402
2023-10-27 09:38:41,065:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:41,104:INFO:  Epoch 174/500:  train Loss: 75.5055   val Loss: 73.8577   time: 0.28s   best: 73.8577
2023-10-27 09:38:41,353:INFO:  Epoch 175/500:  train Loss: 75.2310   val Loss: 75.1794   time: 0.24s   best: 73.8577
2023-10-27 09:38:41,620:INFO:  Epoch 176/500:  train Loss: 75.7574   val Loss: 75.6732   time: 0.25s   best: 73.8577
2023-10-27 09:38:41,860:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:41,949:INFO:  Epoch 177/500:  train Loss: 76.6422   val Loss: 73.8159   time: 0.24s   best: 73.8159
2023-10-27 09:38:42,207:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:42,274:INFO:  Epoch 178/500:  train Loss: 75.6842   val Loss: 73.6882   time: 0.25s   best: 73.6882
2023-10-27 09:38:42,536:INFO:  Epoch 179/500:  train Loss: 75.3597   val Loss: 74.2300   time: 0.25s   best: 73.6882
2023-10-27 09:38:42,781:INFO:  Epoch 180/500:  train Loss: 76.0204   val Loss: 75.6922   time: 0.23s   best: 73.6882
2023-10-27 09:38:43,047:INFO:  Epoch 181/500:  train Loss: 76.2166   val Loss: 75.2343   time: 0.25s   best: 73.6882
2023-10-27 09:38:43,323:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:43,439:INFO:  Epoch 182/500:  train Loss: 75.2795   val Loss: 73.6690   time: 0.27s   best: 73.6690
2023-10-27 09:38:43,691:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:43,867:INFO:  Epoch 183/500:  train Loss: 74.1230   val Loss: 72.4707   time: 0.25s   best: 72.4707
2023-10-27 09:38:44,122:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:44,202:INFO:  Epoch 184/500:  train Loss: 73.7031   val Loss: 72.4294   time: 0.25s   best: 72.4294
2023-10-27 09:38:44,448:INFO:  Epoch 185/500:  train Loss: 73.7354   val Loss: 72.4325   time: 0.24s   best: 72.4294
2023-10-27 09:38:44,709:INFO:  Epoch 186/500:  train Loss: 73.8594   val Loss: 72.7835   time: 0.25s   best: 72.4294
2023-10-27 09:38:44,954:INFO:  Epoch 187/500:  train Loss: 74.3883   val Loss: 73.3653   time: 0.23s   best: 72.4294
2023-10-27 09:38:45,252:INFO:  Epoch 188/500:  train Loss: 75.1162   val Loss: 73.9872   time: 0.29s   best: 72.4294
2023-10-27 09:38:45,502:INFO:  Epoch 189/500:  train Loss: 75.3855   val Loss: 73.4740   time: 0.24s   best: 72.4294
2023-10-27 09:38:45,766:INFO:  Epoch 190/500:  train Loss: 74.4520   val Loss: 72.9406   time: 0.25s   best: 72.4294
2023-10-27 09:38:46,011:INFO:  Epoch 191/500:  train Loss: 73.5494   val Loss: 72.4434   time: 0.24s   best: 72.4294
2023-10-27 09:38:46,271:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:46,414:INFO:  Epoch 192/500:  train Loss: 73.1250   val Loss: 72.1132   time: 0.26s   best: 72.1132
2023-10-27 09:38:46,669:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:46,726:INFO:  Epoch 193/500:  train Loss: 73.2062   val Loss: 71.9621   time: 0.25s   best: 71.9621
2023-10-27 09:38:46,972:INFO:  Epoch 194/500:  train Loss: 73.1986   val Loss: 72.1807   time: 0.23s   best: 71.9621
2023-10-27 09:38:47,289:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:47,388:INFO:  Epoch 195/500:  train Loss: 72.6193   val Loss: 71.0711   time: 0.30s   best: 71.0711
2023-10-27 09:38:47,628:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:47,767:INFO:  Epoch 196/500:  train Loss: 74.9212   val Loss: 70.5247   time: 0.24s   best: 70.5247
2023-10-27 09:38:48,013:INFO:  Epoch 197/500:  train Loss: 73.7823   val Loss: 73.7734   time: 0.23s   best: 70.5247
2023-10-27 09:38:48,275:INFO:  Epoch 198/500:  train Loss: 73.8878   val Loss: 72.0082   time: 0.25s   best: 70.5247
2023-10-27 09:38:48,521:INFO:  Epoch 199/500:  train Loss: 73.3068   val Loss: 73.6369   time: 0.24s   best: 70.5247
2023-10-27 09:38:48,840:INFO:  Epoch 200/500:  train Loss: 73.8575   val Loss: 71.9556   time: 0.30s   best: 70.5247
2023-10-27 09:38:49,087:INFO:  Epoch 201/500:  train Loss: 72.9969   val Loss: 71.8455   time: 0.24s   best: 70.5247
2023-10-27 09:38:49,359:INFO:  Epoch 202/500:  train Loss: 72.5285   val Loss: 71.6603   time: 0.27s   best: 70.5247
2023-10-27 09:38:49,632:INFO:  Epoch 203/500:  train Loss: 73.9986   val Loss: 71.7765   time: 0.26s   best: 70.5247
2023-10-27 09:38:49,896:INFO:  Epoch 204/500:  train Loss: 72.8509   val Loss: 72.6772   time: 0.25s   best: 70.5247
2023-10-27 09:38:50,137:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:50,205:INFO:  Epoch 205/500:  train Loss: 71.8464   val Loss: 70.1785   time: 0.24s   best: 70.1785
2023-10-27 09:38:50,461:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:50,620:INFO:  Epoch 206/500:  train Loss: 74.9111   val Loss: 69.8967   time: 0.25s   best: 69.8967
2023-10-27 09:38:50,873:INFO:  Epoch 207/500:  train Loss: 75.4284   val Loss: 75.0499   time: 0.25s   best: 69.8967
2023-10-27 09:38:51,135:INFO:  Epoch 208/500:  train Loss: 75.4507   val Loss: 75.2291   time: 0.25s   best: 69.8967
2023-10-27 09:38:51,402:INFO:  Epoch 209/500:  train Loss: 74.9652   val Loss: 71.7857   time: 0.26s   best: 69.8967
2023-10-27 09:38:51,682:INFO:  Epoch 210/500:  train Loss: 72.7822   val Loss: 71.6545   time: 0.27s   best: 69.8967
2023-10-27 09:38:51,945:INFO:  Epoch 211/500:  train Loss: 73.2946   val Loss: 71.6547   time: 0.25s   best: 69.8967
2023-10-27 09:38:52,191:INFO:  Epoch 212/500:  train Loss: 72.4632   val Loss: 71.3169   time: 0.24s   best: 69.8967
2023-10-27 09:38:52,456:INFO:  Epoch 213/500:  train Loss: 72.2072   val Loss: 71.9289   time: 0.25s   best: 69.8967
2023-10-27 09:38:52,701:INFO:  Epoch 214/500:  train Loss: 83.5026   val Loss: 88.7150   time: 0.23s   best: 69.8967
2023-10-27 09:38:52,966:INFO:  Epoch 215/500:  train Loss: 87.0600   val Loss: 81.1893   time: 0.25s   best: 69.8967
2023-10-27 09:38:53,211:INFO:  Epoch 216/500:  train Loss: 80.4306   val Loss: 82.9300   time: 0.23s   best: 69.8967
2023-10-27 09:38:53,478:INFO:  Epoch 217/500:  train Loss: 83.1929   val Loss: 79.6438   time: 0.26s   best: 69.8967
2023-10-27 09:38:53,758:INFO:  Epoch 218/500:  train Loss: 79.9379   val Loss: 77.1766   time: 0.27s   best: 69.8967
2023-10-27 09:38:54,024:INFO:  Epoch 219/500:  train Loss: 77.2487   val Loss: 74.7915   time: 0.25s   best: 69.8967
2023-10-27 09:38:54,270:INFO:  Epoch 220/500:  train Loss: 75.4376   val Loss: 73.5962   time: 0.24s   best: 69.8967
2023-10-27 09:38:54,533:INFO:  Epoch 221/500:  train Loss: 73.7864   val Loss: 71.5148   time: 0.25s   best: 69.8967
2023-10-27 09:38:54,777:INFO:  Epoch 222/500:  train Loss: 72.6430   val Loss: 70.3621   time: 0.23s   best: 69.8967
2023-10-27 09:38:55,042:INFO:  Epoch 223/500:  train Loss: 73.0066   val Loss: 72.3809   time: 0.25s   best: 69.8967
2023-10-27 09:38:55,288:INFO:  Epoch 224/500:  train Loss: 73.1775   val Loss: 71.8833   time: 0.24s   best: 69.8967
2023-10-27 09:38:55,556:INFO:  Epoch 225/500:  train Loss: 72.3320   val Loss: 70.8468   time: 0.24s   best: 69.8967
2023-10-27 09:38:55,835:INFO:  Epoch 226/500:  train Loss: 72.7605   val Loss: 70.4561   time: 0.27s   best: 69.8967
2023-10-27 09:38:56,098:INFO:  Epoch 227/500:  train Loss: 71.7878   val Loss: 70.7516   time: 0.25s   best: 69.8967
2023-10-27 09:38:56,343:INFO:  Epoch 228/500:  train Loss: 71.5869   val Loss: 70.6326   time: 0.23s   best: 69.8967
2023-10-27 09:38:56,609:INFO:  Epoch 229/500:  train Loss: 71.4110   val Loss: 70.3887   time: 0.23s   best: 69.8967
2023-10-27 09:38:56,852:INFO:  Epoch 230/500:  train Loss: 71.6592   val Loss: 70.2748   time: 0.23s   best: 69.8967
2023-10-27 09:38:57,098:INFO:  Epoch 231/500:  train Loss: 71.1032   val Loss: 70.1661   time: 0.24s   best: 69.8967
2023-10-27 09:38:57,364:INFO:  Epoch 232/500:  train Loss: 71.0547   val Loss: 70.0705   time: 0.25s   best: 69.8967
2023-10-27 09:38:57,605:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:57,676:INFO:  Epoch 233/500:  train Loss: 70.6175   val Loss: 69.8191   time: 0.24s   best: 69.8191
2023-10-27 09:38:57,945:INFO:  Epoch 234/500:  train Loss: 72.3718   val Loss: 70.2755   time: 0.26s   best: 69.8191
2023-10-27 09:38:58,211:INFO:  Epoch 235/500:  train Loss: 70.9737   val Loss: 69.8399   time: 0.25s   best: 69.8191
2023-10-27 09:38:58,450:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:58,632:INFO:  Epoch 236/500:  train Loss: 70.5946   val Loss: 69.2595   time: 0.23s   best: 69.2595
2023-10-27 09:38:58,883:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:58,959:INFO:  Epoch 237/500:  train Loss: 70.6620   val Loss: 69.2208   time: 0.25s   best: 69.2208
2023-10-27 09:38:59,218:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:59,373:INFO:  Epoch 238/500:  train Loss: 70.5066   val Loss: 68.9465   time: 0.24s   best: 68.9465
2023-10-27 09:38:59,620:INFO:  Epoch 239/500:  train Loss: 70.1125   val Loss: 69.0548   time: 0.24s   best: 68.9465
2023-10-27 09:38:59,904:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:38:59,932:INFO:  Epoch 240/500:  train Loss: 69.8735   val Loss: 68.6966   time: 0.28s   best: 68.6966
2023-10-27 09:39:00,179:INFO:  Epoch 241/500:  train Loss: 69.3573   val Loss: 68.7304   time: 0.24s   best: 68.6966
2023-10-27 09:39:00,441:INFO:  Epoch 242/500:  train Loss: 69.3973   val Loss: 68.6970   time: 0.25s   best: 68.6966
2023-10-27 09:39:00,681:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:00,816:INFO:  Epoch 243/500:  train Loss: 69.9587   val Loss: 68.3595   time: 0.23s   best: 68.3595
2023-10-27 09:39:01,062:INFO:  Epoch 244/500:  train Loss: 69.8692   val Loss: 68.7256   time: 0.24s   best: 68.3595
2023-10-27 09:39:01,325:INFO:  Epoch 245/500:  train Loss: 69.6092   val Loss: 68.9555   time: 0.25s   best: 68.3595
2023-10-27 09:39:01,573:INFO:  Epoch 246/500:  train Loss: 69.4800   val Loss: 68.3650   time: 0.24s   best: 68.3595
2023-10-27 09:39:01,854:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:02,005:INFO:  Epoch 247/500:  train Loss: 70.0879   val Loss: 68.3350   time: 0.25s   best: 68.3350
2023-10-27 09:39:02,252:INFO:  Epoch 248/500:  train Loss: 70.8215   val Loss: 70.0759   time: 0.24s   best: 68.3350
2023-10-27 09:39:02,509:INFO:  Epoch 249/500:  train Loss: 71.1728   val Loss: 70.7643   time: 0.25s   best: 68.3350
2023-10-27 09:39:02,754:INFO:  Epoch 250/500:  train Loss: 70.6029   val Loss: 69.6313   time: 0.23s   best: 68.3350
2023-10-27 09:39:03,019:INFO:  Epoch 251/500:  train Loss: 70.9468   val Loss: 68.5392   time: 0.25s   best: 68.3350
2023-10-27 09:39:03,266:INFO:  Epoch 252/500:  train Loss: 71.1204   val Loss: 70.2540   time: 0.24s   best: 68.3350
2023-10-27 09:39:03,534:INFO:  Epoch 253/500:  train Loss: 70.1040   val Loss: 68.4173   time: 0.26s   best: 68.3350
2023-10-27 09:39:03,779:INFO:  Epoch 254/500:  train Loss: 70.2972   val Loss: 68.9843   time: 0.23s   best: 68.3350
2023-10-27 09:39:04,080:INFO:  Epoch 255/500:  train Loss: 69.6183   val Loss: 69.6131   time: 0.29s   best: 68.3350
2023-10-27 09:39:04,327:INFO:  Epoch 256/500:  train Loss: 69.5310   val Loss: 69.4557   time: 0.24s   best: 68.3350
2023-10-27 09:39:04,590:INFO:  Epoch 257/500:  train Loss: 69.5995   val Loss: 68.4431   time: 0.25s   best: 68.3350
2023-10-27 09:39:04,830:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:04,969:INFO:  Epoch 258/500:  train Loss: 69.6218   val Loss: 67.8589   time: 0.23s   best: 67.8589
2023-10-27 09:39:05,212:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:05,300:INFO:  Epoch 259/500:  train Loss: 68.5070   val Loss: 67.5423   time: 0.24s   best: 67.5423
2023-10-27 09:39:05,565:INFO:  Epoch 260/500:  train Loss: 68.4982   val Loss: 68.2747   time: 0.25s   best: 67.5423
2023-10-27 09:39:05,810:INFO:  Epoch 261/500:  train Loss: 69.8673   val Loss: 67.8834   time: 0.23s   best: 67.5423
2023-10-27 09:39:06,118:INFO:  Epoch 262/500:  train Loss: 69.0369   val Loss: 68.1059   time: 0.30s   best: 67.5423
2023-10-27 09:39:06,371:INFO:  Epoch 263/500:  train Loss: 70.4915   val Loss: 68.8614   time: 0.24s   best: 67.5423
2023-10-27 09:39:06,636:INFO:  Epoch 264/500:  train Loss: 71.0388   val Loss: 68.1054   time: 0.25s   best: 67.5423
2023-10-27 09:39:06,881:INFO:  Epoch 265/500:  train Loss: 69.9423   val Loss: 67.6098   time: 0.23s   best: 67.5423
2023-10-27 09:39:07,148:INFO:  Epoch 266/500:  train Loss: 70.4278   val Loss: 72.3126   time: 0.26s   best: 67.5423
2023-10-27 09:39:07,396:INFO:  Epoch 267/500:  train Loss: 72.3130   val Loss: 72.4275   time: 0.24s   best: 67.5423
2023-10-27 09:39:07,663:INFO:  Epoch 268/500:  train Loss: 72.6479   val Loss: 69.6242   time: 0.26s   best: 67.5423
2023-10-27 09:39:07,908:INFO:  Epoch 269/500:  train Loss: 70.2601   val Loss: 69.2987   time: 0.23s   best: 67.5423
2023-10-27 09:39:08,208:INFO:  Epoch 270/500:  train Loss: 71.5863   val Loss: 70.1366   time: 0.29s   best: 67.5423
2023-10-27 09:39:08,454:INFO:  Epoch 271/500:  train Loss: 71.1886   val Loss: 70.2550   time: 0.23s   best: 67.5423
2023-10-27 09:39:08,719:INFO:  Epoch 272/500:  train Loss: 71.2434   val Loss: 70.0504   time: 0.26s   best: 67.5423
2023-10-27 09:39:08,965:INFO:  Epoch 273/500:  train Loss: 70.6666   val Loss: 70.2876   time: 0.23s   best: 67.5423
2023-10-27 09:39:09,232:INFO:  Epoch 274/500:  train Loss: 70.9980   val Loss: 69.1002   time: 0.26s   best: 67.5423
2023-10-27 09:39:09,481:INFO:  Epoch 275/500:  train Loss: 70.6851   val Loss: 68.6703   time: 0.24s   best: 67.5423
2023-10-27 09:39:09,745:INFO:  Epoch 276/500:  train Loss: 70.4857   val Loss: 68.7561   time: 0.25s   best: 67.5423
2023-10-27 09:39:09,990:INFO:  Epoch 277/500:  train Loss: 69.2089   val Loss: 68.8681   time: 0.23s   best: 67.5423
2023-10-27 09:39:10,289:INFO:  Epoch 278/500:  train Loss: 70.6734   val Loss: 69.9686   time: 0.29s   best: 67.5423
2023-10-27 09:39:10,535:INFO:  Epoch 279/500:  train Loss: 70.0622   val Loss: 68.1610   time: 0.24s   best: 67.5423
2023-10-27 09:39:10,798:INFO:  Epoch 280/500:  train Loss: 69.3216   val Loss: 67.6205   time: 0.25s   best: 67.5423
2023-10-27 09:39:11,047:INFO:  Epoch 281/500:  train Loss: 69.6589   val Loss: 68.6512   time: 0.24s   best: 67.5423
2023-10-27 09:39:11,294:INFO:  Epoch 282/500:  train Loss: 69.2706   val Loss: 68.2904   time: 0.24s   best: 67.5423
2023-10-27 09:39:11,562:INFO:  Epoch 283/500:  train Loss: 68.5399   val Loss: 68.1291   time: 0.26s   best: 67.5423
2023-10-27 09:39:11,807:INFO:  Epoch 284/500:  train Loss: 69.1171   val Loss: 68.1555   time: 0.23s   best: 67.5423
2023-10-27 09:39:12,066:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:12,098:INFO:  Epoch 285/500:  train Loss: 69.2007   val Loss: 66.7569   time: 0.25s   best: 66.7569
2023-10-27 09:39:12,390:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:12,446:INFO:  Epoch 286/500:  train Loss: 68.8844   val Loss: 66.7427   time: 0.29s   best: 66.7427
2023-10-27 09:39:12,692:INFO:  Epoch 287/500:  train Loss: 68.4405   val Loss: 68.6971   time: 0.24s   best: 66.7427
2023-10-27 09:39:12,954:INFO:  Epoch 288/500:  train Loss: 69.8194   val Loss: 69.0888   time: 0.25s   best: 66.7427
2023-10-27 09:39:13,200:INFO:  Epoch 289/500:  train Loss: 69.9601   val Loss: 67.5805   time: 0.24s   best: 66.7427
2023-10-27 09:39:13,468:INFO:  Epoch 290/500:  train Loss: 70.4497   val Loss: 70.9861   time: 0.26s   best: 66.7427
2023-10-27 09:39:13,714:INFO:  Epoch 291/500:  train Loss: 70.3539   val Loss: 69.4326   time: 0.24s   best: 66.7427
2023-10-27 09:39:13,978:INFO:  Epoch 292/500:  train Loss: 68.9992   val Loss: 71.0294   time: 0.25s   best: 66.7427
2023-10-27 09:39:14,228:INFO:  Epoch 293/500:  train Loss: 69.6809   val Loss: 67.3107   time: 0.24s   best: 66.7427
2023-10-27 09:39:14,524:INFO:  Epoch 294/500:  train Loss: 68.2992   val Loss: 67.3083   time: 0.28s   best: 66.7427
2023-10-27 09:39:14,769:INFO:  Epoch 295/500:  train Loss: 67.7217   val Loss: 67.0037   time: 0.23s   best: 66.7427
2023-10-27 09:39:15,035:INFO:  Epoch 296/500:  train Loss: 68.6229   val Loss: 68.9117   time: 0.25s   best: 66.7427
2023-10-27 09:39:15,282:INFO:  Epoch 297/500:  train Loss: 68.7621   val Loss: 68.1215   time: 0.24s   best: 66.7427
2023-10-27 09:39:15,548:INFO:  Epoch 298/500:  train Loss: 68.6227   val Loss: 67.3795   time: 0.26s   best: 66.7427
2023-10-27 09:39:15,793:INFO:  Epoch 299/500:  train Loss: 68.8788   val Loss: 66.7607   time: 0.23s   best: 66.7427
2023-10-27 09:39:16,113:INFO:  Epoch 300/500:  train Loss: 69.1355   val Loss: 68.3012   time: 0.30s   best: 66.7427
2023-10-27 09:39:16,361:INFO:  Epoch 301/500:  train Loss: 69.2297   val Loss: 67.1971   time: 0.24s   best: 66.7427
2023-10-27 09:39:16,650:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:16,678:INFO:  Epoch 302/500:  train Loss: 68.5356   val Loss: 66.6484   time: 0.28s   best: 66.6484
2023-10-27 09:39:16,924:INFO:  Epoch 303/500:  train Loss: 69.6118   val Loss: 68.7885   time: 0.23s   best: 66.6484
2023-10-27 09:39:17,188:INFO:  Epoch 304/500:  train Loss: 69.1407   val Loss: 67.3508   time: 0.25s   best: 66.6484
2023-10-27 09:39:17,437:INFO:  Epoch 305/500:  train Loss: 71.5739   val Loss: 71.3428   time: 0.24s   best: 66.6484
2023-10-27 09:39:17,700:INFO:  Epoch 306/500:  train Loss: 71.5370   val Loss: 73.1973   time: 0.25s   best: 66.6484
2023-10-27 09:39:17,945:INFO:  Epoch 307/500:  train Loss: 72.6334   val Loss: 70.6049   time: 0.23s   best: 66.6484
2023-10-27 09:39:18,213:INFO:  Epoch 308/500:  train Loss: 71.5011   val Loss: 68.5909   time: 0.26s   best: 66.6484
2023-10-27 09:39:18,458:INFO:  Epoch 309/500:  train Loss: 69.1598   val Loss: 68.1054   time: 0.23s   best: 66.6484
2023-10-27 09:39:18,790:INFO:  Epoch 310/500:  train Loss: 69.2809   val Loss: 67.5429   time: 0.32s   best: 66.6484
2023-10-27 09:39:19,035:INFO:  Epoch 311/500:  train Loss: 68.2884   val Loss: 67.1276   time: 0.23s   best: 66.6484
2023-10-27 09:39:19,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:19,483:INFO:  Epoch 312/500:  train Loss: 68.3093   val Loss: 66.3175   time: 0.26s   best: 66.3175
2023-10-27 09:39:19,808:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:19,882:INFO:  Epoch 313/500:  train Loss: 67.5819   val Loss: 65.7163   time: 0.25s   best: 65.7163
2023-10-27 09:39:20,129:INFO:  Epoch 314/500:  train Loss: 66.9102   val Loss: 66.0549   time: 0.24s   best: 65.7163
2023-10-27 09:39:20,387:INFO:  Epoch 315/500:  train Loss: 67.4450   val Loss: 67.7067   time: 0.25s   best: 65.7163
2023-10-27 09:39:20,632:INFO:  Epoch 316/500:  train Loss: 70.3648   val Loss: 68.7529   time: 0.23s   best: 65.7163
2023-10-27 09:39:20,930:INFO:  Epoch 317/500:  train Loss: 71.9336   val Loss: 68.2090   time: 0.29s   best: 65.7163
2023-10-27 09:39:21,177:INFO:  Epoch 318/500:  train Loss: 68.5206   val Loss: 66.5617   time: 0.24s   best: 65.7163
2023-10-27 09:39:21,453:INFO:  Epoch 319/500:  train Loss: 68.4787   val Loss: 67.4668   time: 0.27s   best: 65.7163
2023-10-27 09:39:21,707:INFO:  Epoch 320/500:  train Loss: 70.2265   val Loss: 66.8156   time: 0.24s   best: 65.7163
2023-10-27 09:39:21,971:INFO:  Epoch 321/500:  train Loss: 67.8915   val Loss: 66.2713   time: 0.25s   best: 65.7163
2023-10-27 09:39:22,218:INFO:  Epoch 322/500:  train Loss: 68.8291   val Loss: 67.4795   time: 0.24s   best: 65.7163
2023-10-27 09:39:22,483:INFO:  Epoch 323/500:  train Loss: 67.9887   val Loss: 67.2208   time: 0.25s   best: 65.7163
2023-10-27 09:39:22,729:INFO:  Epoch 324/500:  train Loss: 68.1401   val Loss: 65.7216   time: 0.23s   best: 65.7163
2023-10-27 09:39:23,025:INFO:  Epoch 325/500:  train Loss: 68.1417   val Loss: 66.4092   time: 0.29s   best: 65.7163
2023-10-27 09:39:23,273:INFO:  Epoch 326/500:  train Loss: 67.1468   val Loss: 66.0948   time: 0.24s   best: 65.7163
2023-10-27 09:39:23,541:INFO:  Epoch 327/500:  train Loss: 67.6590   val Loss: 67.3618   time: 0.26s   best: 65.7163
2023-10-27 09:39:23,787:INFO:  Epoch 328/500:  train Loss: 67.9831   val Loss: 66.2723   time: 0.24s   best: 65.7163
2023-10-27 09:39:24,045:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:24,155:INFO:  Epoch 329/500:  train Loss: 66.5067   val Loss: 64.5717   time: 0.25s   best: 64.5717
2023-10-27 09:39:24,395:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:24,546:INFO:  Epoch 330/500:  train Loss: 65.8499   val Loss: 64.4956   time: 0.23s   best: 64.4956
2023-10-27 09:39:24,792:INFO:  Epoch 331/500:  train Loss: 66.9426   val Loss: 67.3579   time: 0.24s   best: 64.4956
2023-10-27 09:39:25,083:INFO:  Epoch 332/500:  train Loss: 68.4944   val Loss: 67.0835   time: 0.28s   best: 64.4956
2023-10-27 09:39:25,329:INFO:  Epoch 333/500:  train Loss: 69.4938   val Loss: 67.9635   time: 0.23s   best: 64.4956
2023-10-27 09:39:25,598:INFO:  Epoch 334/500:  train Loss: 67.2479   val Loss: 65.4362   time: 0.26s   best: 64.4956
2023-10-27 09:39:25,843:INFO:  Epoch 335/500:  train Loss: 67.3986   val Loss: 64.5139   time: 0.23s   best: 64.4956
2023-10-27 09:39:26,108:INFO:  Epoch 336/500:  train Loss: 66.2454   val Loss: 64.5715   time: 0.25s   best: 64.4956
2023-10-27 09:39:26,349:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:26,448:INFO:  Epoch 337/500:  train Loss: 64.7321   val Loss: 64.2127   time: 0.24s   best: 64.2127
2023-10-27 09:39:26,704:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:26,775:INFO:  Epoch 338/500:  train Loss: 65.2497   val Loss: 64.0246   time: 0.25s   best: 64.0246
2023-10-27 09:39:27,064:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:27,116:INFO:  Epoch 339/500:  train Loss: 65.1199   val Loss: 63.3418   time: 0.28s   best: 63.3418
2023-10-27 09:39:27,363:INFO:  Epoch 340/500:  train Loss: 65.1824   val Loss: 63.7045   time: 0.24s   best: 63.3418
2023-10-27 09:39:27,626:INFO:  Epoch 341/500:  train Loss: 66.8383   val Loss: 65.6506   time: 0.25s   best: 63.3418
2023-10-27 09:39:27,871:INFO:  Epoch 342/500:  train Loss: 65.5908   val Loss: 66.3009   time: 0.23s   best: 63.3418
2023-10-27 09:39:28,136:INFO:  Epoch 343/500:  train Loss: 67.9918   val Loss: 69.7662   time: 0.25s   best: 63.3418
2023-10-27 09:39:28,381:INFO:  Epoch 344/500:  train Loss: 70.5941   val Loss: 68.8191   time: 0.23s   best: 63.3418
2023-10-27 09:39:28,646:INFO:  Epoch 345/500:  train Loss: 69.9499   val Loss: 67.8923   time: 0.25s   best: 63.3418
2023-10-27 09:39:28,892:INFO:  Epoch 346/500:  train Loss: 68.7191   val Loss: 66.8061   time: 0.23s   best: 63.3418
2023-10-27 09:39:29,189:INFO:  Epoch 347/500:  train Loss: 67.7096   val Loss: 65.3383   time: 0.29s   best: 63.3418
2023-10-27 09:39:29,436:INFO:  Epoch 348/500:  train Loss: 65.9394   val Loss: 64.4292   time: 0.24s   best: 63.3418
2023-10-27 09:39:29,704:INFO:  Epoch 349/500:  train Loss: 66.3985   val Loss: 65.5869   time: 0.26s   best: 63.3418
2023-10-27 09:39:29,949:INFO:  Epoch 350/500:  train Loss: 66.8949   val Loss: 64.1429   time: 0.23s   best: 63.3418
2023-10-27 09:39:30,215:INFO:  Epoch 351/500:  train Loss: 66.4605   val Loss: 65.9241   time: 0.26s   best: 63.3418
2023-10-27 09:39:30,460:INFO:  Epoch 352/500:  train Loss: 66.4967   val Loss: 65.5899   time: 0.23s   best: 63.3418
2023-10-27 09:39:30,725:INFO:  Epoch 353/500:  train Loss: 66.2611   val Loss: 68.0178   time: 0.25s   best: 63.3418
2023-10-27 09:39:30,970:INFO:  Epoch 354/500:  train Loss: 70.9000   val Loss: 70.1262   time: 0.23s   best: 63.3418
2023-10-27 09:39:31,268:INFO:  Epoch 355/500:  train Loss: 69.9251   val Loss: 70.2335   time: 0.29s   best: 63.3418
2023-10-27 09:39:31,516:INFO:  Epoch 356/500:  train Loss: 71.7650   val Loss: 71.4140   time: 0.24s   best: 63.3418
2023-10-27 09:39:31,783:INFO:  Epoch 357/500:  train Loss: 69.7955   val Loss: 68.3682   time: 0.26s   best: 63.3418
2023-10-27 09:39:32,028:INFO:  Epoch 358/500:  train Loss: 70.0260   val Loss: 68.1560   time: 0.23s   best: 63.3418
2023-10-27 09:39:32,295:INFO:  Epoch 359/500:  train Loss: 67.5134   val Loss: 67.9397   time: 0.24s   best: 63.3418
2023-10-27 09:39:32,539:INFO:  Epoch 360/500:  train Loss: 68.7668   val Loss: 67.4417   time: 0.23s   best: 63.3418
2023-10-27 09:39:32,784:INFO:  Epoch 361/500:  train Loss: 67.2703   val Loss: 66.9193   time: 0.23s   best: 63.3418
2023-10-27 09:39:33,049:INFO:  Epoch 362/500:  train Loss: 73.1956   val Loss: 74.1893   time: 0.25s   best: 63.3418
2023-10-27 09:39:33,330:INFO:  Epoch 363/500:  train Loss: 74.0550   val Loss: 77.7517   time: 0.27s   best: 63.3418
2023-10-27 09:39:33,596:INFO:  Epoch 364/500:  train Loss: 73.2547   val Loss: 71.1330   time: 0.26s   best: 63.3418
2023-10-27 09:39:33,842:INFO:  Epoch 365/500:  train Loss: 72.6003   val Loss: 67.4914   time: 0.24s   best: 63.3418
2023-10-27 09:39:34,107:INFO:  Epoch 366/500:  train Loss: 72.3488   val Loss: 72.6750   time: 0.25s   best: 63.3418
2023-10-27 09:39:34,354:INFO:  Epoch 367/500:  train Loss: 74.3125   val Loss: 74.9587   time: 0.24s   best: 63.3418
2023-10-27 09:39:34,622:INFO:  Epoch 368/500:  train Loss: 74.3715   val Loss: 73.0948   time: 0.25s   best: 63.3418
2023-10-27 09:39:34,867:INFO:  Epoch 369/500:  train Loss: 73.6374   val Loss: 73.6090   time: 0.23s   best: 63.3418
2023-10-27 09:39:35,132:INFO:  Epoch 370/500:  train Loss: 72.8951   val Loss: 72.8564   time: 0.25s   best: 63.3418
2023-10-27 09:39:35,413:INFO:  Epoch 371/500:  train Loss: 72.2556   val Loss: 71.7281   time: 0.27s   best: 63.3418
2023-10-27 09:39:35,680:INFO:  Epoch 372/500:  train Loss: 72.8203   val Loss: 72.9856   time: 0.25s   best: 63.3418
2023-10-27 09:39:35,925:INFO:  Epoch 373/500:  train Loss: 71.1774   val Loss: 71.8556   time: 0.23s   best: 63.3418
2023-10-27 09:39:36,191:INFO:  Epoch 374/500:  train Loss: 72.0387   val Loss: 71.2999   time: 0.26s   best: 63.3418
2023-10-27 09:39:36,436:INFO:  Epoch 375/500:  train Loss: 70.7008   val Loss: 74.2413   time: 0.23s   best: 63.3418
2023-10-27 09:39:36,707:INFO:  Epoch 376/500:  train Loss: 74.2232   val Loss: 72.1292   time: 0.27s   best: 63.3418
2023-10-27 09:39:36,946:INFO:  Epoch 377/500:  train Loss: 73.0052   val Loss: 67.8303   time: 0.23s   best: 63.3418
2023-10-27 09:39:37,225:INFO:  Epoch 378/500:  train Loss: 70.3455   val Loss: 69.5712   time: 0.27s   best: 63.3418
2023-10-27 09:39:37,508:INFO:  Epoch 379/500:  train Loss: 72.3065   val Loss: 72.5196   time: 0.27s   best: 63.3418
2023-10-27 09:39:37,771:INFO:  Epoch 380/500:  train Loss: 75.2427   val Loss: 72.9165   time: 0.25s   best: 63.3418
2023-10-27 09:39:38,016:INFO:  Epoch 381/500:  train Loss: 72.8629   val Loss: 70.0463   time: 0.23s   best: 63.3418
2023-10-27 09:39:38,282:INFO:  Epoch 382/500:  train Loss: 70.7300   val Loss: 69.0453   time: 0.25s   best: 63.3418
2023-10-27 09:39:38,527:INFO:  Epoch 383/500:  train Loss: 72.0867   val Loss: 70.9493   time: 0.23s   best: 63.3418
2023-10-27 09:39:38,791:INFO:  Epoch 384/500:  train Loss: 69.4238   val Loss: 68.0621   time: 0.25s   best: 63.3418
2023-10-27 09:39:39,037:INFO:  Epoch 385/500:  train Loss: 69.1989   val Loss: 67.8154   time: 0.24s   best: 63.3418
2023-10-27 09:39:39,305:INFO:  Epoch 386/500:  train Loss: 68.6600   val Loss: 67.5286   time: 0.26s   best: 63.3418
2023-10-27 09:39:39,587:INFO:  Epoch 387/500:  train Loss: 69.0210   val Loss: 66.6807   time: 0.27s   best: 63.3418
2023-10-27 09:39:39,850:INFO:  Epoch 388/500:  train Loss: 67.5067   val Loss: 66.2982   time: 0.25s   best: 63.3418
2023-10-27 09:39:40,095:INFO:  Epoch 389/500:  train Loss: 68.7309   val Loss: 67.2962   time: 0.23s   best: 63.3418
2023-10-27 09:39:40,362:INFO:  Epoch 390/500:  train Loss: 67.7920   val Loss: 67.5349   time: 0.26s   best: 63.3418
2023-10-27 09:39:40,607:INFO:  Epoch 391/500:  train Loss: 68.5785   val Loss: 66.6625   time: 0.23s   best: 63.3418
2023-10-27 09:39:40,871:INFO:  Epoch 392/500:  train Loss: 67.2344   val Loss: 67.9687   time: 0.25s   best: 63.3418
2023-10-27 09:39:41,116:INFO:  Epoch 393/500:  train Loss: 69.5994   val Loss: 67.1755   time: 0.23s   best: 63.3418
2023-10-27 09:39:41,396:INFO:  Epoch 394/500:  train Loss: 69.3452   val Loss: 69.7433   time: 0.25s   best: 63.3418
2023-10-27 09:39:41,665:INFO:  Epoch 395/500:  train Loss: 70.4857   val Loss: 70.0573   time: 0.26s   best: 63.3418
2023-10-27 09:39:41,927:INFO:  Epoch 396/500:  train Loss: 72.2562   val Loss: 69.3985   time: 0.25s   best: 63.3418
2023-10-27 09:39:42,173:INFO:  Epoch 397/500:  train Loss: 69.2045   val Loss: 67.3358   time: 0.24s   best: 63.3418
2023-10-27 09:39:42,438:INFO:  Epoch 398/500:  train Loss: 69.7458   val Loss: 68.4719   time: 0.25s   best: 63.3418
2023-10-27 09:39:42,683:INFO:  Epoch 399/500:  train Loss: 70.2545   val Loss: 67.1512   time: 0.23s   best: 63.3418
2023-10-27 09:39:43,003:INFO:  Epoch 400/500:  train Loss: 67.9837   val Loss: 67.6364   time: 0.30s   best: 63.3418
2023-10-27 09:39:43,249:INFO:  Epoch 401/500:  train Loss: 67.5224   val Loss: 65.6924   time: 0.24s   best: 63.3418
2023-10-27 09:39:43,549:INFO:  Epoch 402/500:  train Loss: 67.4726   val Loss: 66.8527   time: 0.29s   best: 63.3418
2023-10-27 09:39:43,794:INFO:  Epoch 403/500:  train Loss: 68.7597   val Loss: 64.3848   time: 0.23s   best: 63.3418
2023-10-27 09:39:44,058:INFO:  Epoch 404/500:  train Loss: 67.2972   val Loss: 69.0852   time: 0.25s   best: 63.3418
2023-10-27 09:39:44,305:INFO:  Epoch 405/500:  train Loss: 69.8777   val Loss: 68.1830   time: 0.24s   best: 63.3418
2023-10-27 09:39:44,568:INFO:  Epoch 406/500:  train Loss: 68.1322   val Loss: 65.1718   time: 0.25s   best: 63.3418
2023-10-27 09:39:44,815:INFO:  Epoch 407/500:  train Loss: 66.5008   val Loss: 64.8415   time: 0.23s   best: 63.3418
2023-10-27 09:39:45,079:INFO:  Epoch 408/500:  train Loss: 67.1839   val Loss: 65.6421   time: 0.25s   best: 63.3418
2023-10-27 09:39:45,326:INFO:  Epoch 409/500:  train Loss: 66.5253   val Loss: 64.1279   time: 0.24s   best: 63.3418
2023-10-27 09:39:45,627:INFO:  Epoch 410/500:  train Loss: 65.7316   val Loss: 63.6463   time: 0.29s   best: 63.3418
2023-10-27 09:39:45,872:INFO:  Epoch 411/500:  train Loss: 66.1610   val Loss: 64.8840   time: 0.23s   best: 63.3418
2023-10-27 09:39:46,136:INFO:  Epoch 412/500:  train Loss: 67.0931   val Loss: 65.7000   time: 0.25s   best: 63.3418
2023-10-27 09:39:46,383:INFO:  Epoch 413/500:  train Loss: 66.3348   val Loss: 64.4740   time: 0.24s   best: 63.3418
2023-10-27 09:39:46,646:INFO:  Epoch 414/500:  train Loss: 64.8448   val Loss: 65.6432   time: 0.25s   best: 63.3418
2023-10-27 09:39:46,891:INFO:  Epoch 415/500:  train Loss: 66.7745   val Loss: 64.4896   time: 0.23s   best: 63.3418
2023-10-27 09:39:47,150:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:47,205:INFO:  Epoch 416/500:  train Loss: 70.5396   val Loss: 62.8524   time: 0.25s   best: 62.8524
2023-10-27 09:39:47,452:INFO:  Epoch 417/500:  train Loss: 67.8927   val Loss: 69.5954   time: 0.24s   best: 62.8524
2023-10-27 09:39:47,750:INFO:  Epoch 418/500:  train Loss: 71.3422   val Loss: 70.8124   time: 0.29s   best: 62.8524
2023-10-27 09:39:47,995:INFO:  Epoch 419/500:  train Loss: 69.4957   val Loss: 71.1547   time: 0.23s   best: 62.8524
2023-10-27 09:39:48,261:INFO:  Epoch 420/500:  train Loss: 71.2406   val Loss: 68.9166   time: 0.25s   best: 62.8524
2023-10-27 09:39:48,505:INFO:  Epoch 421/500:  train Loss: 67.1835   val Loss: 66.2459   time: 0.23s   best: 62.8524
2023-10-27 09:39:48,768:INFO:  Epoch 422/500:  train Loss: 66.8395   val Loss: 65.5613   time: 0.25s   best: 62.8524
2023-10-27 09:39:49,013:INFO:  Epoch 423/500:  train Loss: 66.7098   val Loss: 63.9808   time: 0.23s   best: 62.8524
2023-10-27 09:39:49,278:INFO:  Epoch 424/500:  train Loss: 66.8941   val Loss: 64.2025   time: 0.25s   best: 62.8524
2023-10-27 09:39:49,526:INFO:  Epoch 425/500:  train Loss: 65.4564   val Loss: 64.1494   time: 0.24s   best: 62.8524
2023-10-27 09:39:49,858:INFO:  Epoch 426/500:  train Loss: 64.4757   val Loss: 62.9695   time: 0.32s   best: 62.8524
2023-10-27 09:39:50,097:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:50,290:INFO:  Epoch 427/500:  train Loss: 63.2995   val Loss: 61.8614   time: 0.23s   best: 61.8614
2023-10-27 09:39:50,536:INFO:  Epoch 428/500:  train Loss: 62.7497   val Loss: 62.0693   time: 0.23s   best: 61.8614
2023-10-27 09:39:50,796:INFO:  Epoch 429/500:  train Loss: 63.8519   val Loss: 63.0079   time: 0.25s   best: 61.8614
2023-10-27 09:39:51,035:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + no dropout)_cff8.pt
2023-10-27 09:39:51,063:INFO:  Epoch 430/500:  train Loss: 64.6203   val Loss: 60.8143   time: 0.23s   best: 60.8143
2023-10-27 09:39:51,328:INFO:  Epoch 431/500:  train Loss: 62.9748   val Loss: 61.7634   time: 0.25s   best: 60.8143
2023-10-27 09:39:51,578:INFO:  Epoch 432/500:  train Loss: 64.3578   val Loss: 61.8440   time: 0.24s   best: 60.8143
2023-10-27 09:39:51,839:INFO:  Epoch 433/500:  train Loss: 68.4615   val Loss: 71.1880   time: 0.25s   best: 60.8143
2023-10-27 09:39:52,126:INFO:  Epoch 434/500:  train Loss: 74.4592   val Loss: 72.7852   time: 0.28s   best: 60.8143
2023-10-27 09:39:52,385:INFO:  Epoch 435/500:  train Loss: 72.6550   val Loss: 71.6856   time: 0.25s   best: 60.8143
2023-10-27 09:39:52,644:INFO:  Epoch 436/500:  train Loss: 73.3669   val Loss: 71.5472   time: 0.25s   best: 60.8143
2023-10-27 09:39:52,910:INFO:  Epoch 437/500:  train Loss: 71.2761   val Loss: 69.5761   time: 0.26s   best: 60.8143
2023-10-27 09:39:53,155:INFO:  Epoch 438/500:  train Loss: 70.4614   val Loss: 68.4858   time: 0.23s   best: 60.8143
2023-10-27 09:39:53,421:INFO:  Epoch 439/500:  train Loss: 69.4367   val Loss: 68.9767   time: 0.25s   best: 60.8143
2023-10-27 09:39:53,670:INFO:  Epoch 440/500:  train Loss: 68.6608   val Loss: 66.5729   time: 0.24s   best: 60.8143
2023-10-27 09:39:53,947:INFO:  Epoch 441/500:  train Loss: 69.2877   val Loss: 65.2441   time: 0.25s   best: 60.8143
2023-10-27 09:39:54,214:INFO:  Epoch 442/500:  train Loss: 67.2638   val Loss: 67.2775   time: 0.26s   best: 60.8143
2023-10-27 09:39:54,477:INFO:  Epoch 443/500:  train Loss: 69.0301   val Loss: 67.2932   time: 0.25s   best: 60.8143
2023-10-27 09:39:54,723:INFO:  Epoch 444/500:  train Loss: 67.0803   val Loss: 66.3390   time: 0.23s   best: 60.8143
2023-10-27 09:39:54,988:INFO:  Epoch 445/500:  train Loss: 65.8497   val Loss: 64.1497   time: 0.25s   best: 60.8143
2023-10-27 09:39:55,236:INFO:  Epoch 446/500:  train Loss: 65.7532   val Loss: 62.2105   time: 0.24s   best: 60.8143
2023-10-27 09:39:55,499:INFO:  Epoch 447/500:  train Loss: 64.4261   val Loss: 62.0519   time: 0.25s   best: 60.8143
2023-10-27 09:39:55,748:INFO:  Epoch 448/500:  train Loss: 62.7328   val Loss: 61.7879   time: 0.24s   best: 60.8143
2023-10-27 09:39:56,034:INFO:  Epoch 449/500:  train Loss: 63.9870   val Loss: 62.0994   time: 0.23s   best: 60.8143
2023-10-27 09:39:56,294:INFO:  Epoch 450/500:  train Loss: 62.8803   val Loss: 61.5392   time: 0.25s   best: 60.8143
2023-10-27 09:39:56,540:INFO:  Epoch 451/500:  train Loss: 63.4360   val Loss: 60.8456   time: 0.23s   best: 60.8143
2023-10-27 09:39:56,803:INFO:  Epoch 452/500:  train Loss: 63.2978   val Loss: 62.5435   time: 0.25s   best: 60.8143
2023-10-27 09:39:57,047:INFO:  Epoch 453/500:  train Loss: 65.1998   val Loss: 64.3436   time: 0.23s   best: 60.8143
2023-10-27 09:39:57,314:INFO:  Epoch 454/500:  train Loss: 64.6235   val Loss: 62.6823   time: 0.26s   best: 60.8143
2023-10-27 09:39:57,563:INFO:  Epoch 455/500:  train Loss: 63.2067   val Loss: 61.7750   time: 0.24s   best: 60.8143
2023-10-27 09:39:57,827:INFO:  Epoch 456/500:  train Loss: 62.8982   val Loss: 61.5736   time: 0.25s   best: 60.8143
2023-10-27 09:39:58,071:INFO:  Epoch 457/500:  train Loss: 63.3013   val Loss: 61.6692   time: 0.23s   best: 60.8143
2023-10-27 09:39:58,371:INFO:  Epoch 458/500:  train Loss: 64.1399   val Loss: 66.7006   time: 0.29s   best: 60.8143
2023-10-27 09:39:58,616:INFO:  Epoch 459/500:  train Loss: 82.7520   val Loss: 88.8721   time: 0.23s   best: 60.8143
2023-10-27 09:39:58,881:INFO:  Epoch 460/500:  train Loss: 90.2628   val Loss: 90.5717   time: 0.25s   best: 60.8143
2023-10-27 09:39:59,125:INFO:  Epoch 461/500:  train Loss: 91.3715   val Loss: 90.8905   time: 0.23s   best: 60.8143
2023-10-27 09:39:59,393:INFO:  Epoch 462/500:  train Loss: 91.4465   val Loss: 91.0222   time: 0.26s   best: 60.8143
2023-10-27 09:39:59,642:INFO:  Epoch 463/500:  train Loss: 90.3819   val Loss: 89.3447   time: 0.24s   best: 60.8143
2023-10-27 09:39:59,907:INFO:  Epoch 464/500:  train Loss: 88.6103   val Loss: 87.6889   time: 0.25s   best: 60.8143
2023-10-27 09:40:00,166:INFO:  Epoch 465/500:  train Loss: 87.3045   val Loss: 86.1449   time: 0.23s   best: 60.8143
2023-10-27 09:40:00,455:INFO:  Epoch 466/500:  train Loss: 85.6836   val Loss: 84.0860   time: 0.28s   best: 60.8143
2023-10-27 09:40:00,700:INFO:  Epoch 467/500:  train Loss: 83.6431   val Loss: 81.8522   time: 0.23s   best: 60.8143
2023-10-27 09:40:00,964:INFO:  Epoch 468/500:  train Loss: 81.3206   val Loss: 79.7358   time: 0.25s   best: 60.8143
2023-10-27 09:40:01,208:INFO:  Epoch 469/500:  train Loss: 79.4893   val Loss: 77.6572   time: 0.23s   best: 60.8143
2023-10-27 09:40:01,476:INFO:  Epoch 470/500:  train Loss: 77.2079   val Loss: 74.8493   time: 0.26s   best: 60.8143
2023-10-27 09:40:01,723:INFO:  Epoch 471/500:  train Loss: 73.1573   val Loss: 71.7787   time: 0.24s   best: 60.8143
2023-10-27 09:40:01,987:INFO:  Epoch 472/500:  train Loss: 71.2554   val Loss: 68.9482   time: 0.25s   best: 60.8143
2023-10-27 09:40:02,248:INFO:  Epoch 473/500:  train Loss: 69.5952   val Loss: 68.4134   time: 0.24s   best: 60.8143
2023-10-27 09:40:02,531:INFO:  Epoch 474/500:  train Loss: 68.9786   val Loss: 66.1444   time: 0.27s   best: 60.8143
2023-10-27 09:40:02,777:INFO:  Epoch 475/500:  train Loss: 67.6541   val Loss: 68.1837   time: 0.23s   best: 60.8143
2023-10-27 09:40:03,042:INFO:  Epoch 476/500:  train Loss: 68.5907   val Loss: 66.7808   time: 0.25s   best: 60.8143
2023-10-27 09:40:03,289:INFO:  Epoch 477/500:  train Loss: 67.5020   val Loss: 67.8622   time: 0.24s   best: 60.8143
2023-10-27 09:40:03,555:INFO:  Epoch 478/500:  train Loss: 69.1664   val Loss: 69.7228   time: 0.26s   best: 60.8143
2023-10-27 09:40:03,800:INFO:  Epoch 479/500:  train Loss: 70.9650   val Loss: 68.8640   time: 0.23s   best: 60.8143
2023-10-27 09:40:04,063:INFO:  Epoch 480/500:  train Loss: 69.4603   val Loss: 66.4783   time: 0.25s   best: 60.8143
2023-10-27 09:40:04,325:INFO:  Epoch 481/500:  train Loss: 67.4781   val Loss: 64.9587   time: 0.24s   best: 60.8143
2023-10-27 09:40:04,608:INFO:  Epoch 482/500:  train Loss: 66.7180   val Loss: 64.8259   time: 0.27s   best: 60.8143
2023-10-27 09:40:04,854:INFO:  Epoch 483/500:  train Loss: 67.5303   val Loss: 64.6086   time: 0.23s   best: 60.8143
2023-10-27 09:40:05,120:INFO:  Epoch 484/500:  train Loss: 66.7311   val Loss: 64.5147   time: 0.25s   best: 60.8143
2023-10-27 09:40:05,367:INFO:  Epoch 485/500:  train Loss: 75.7971   val Loss: 67.2268   time: 0.24s   best: 60.8143
2023-10-27 09:40:05,633:INFO:  Epoch 486/500:  train Loss: 74.5590   val Loss: 77.1321   time: 0.26s   best: 60.8143
2023-10-27 09:40:05,879:INFO:  Epoch 487/500:  train Loss: 77.0825   val Loss: 75.3818   time: 0.23s   best: 60.8143
2023-10-27 09:40:06,145:INFO:  Epoch 488/500:  train Loss: 77.6476   val Loss: 77.3574   time: 0.26s   best: 60.8143
2023-10-27 09:40:06,407:INFO:  Epoch 489/500:  train Loss: 77.2047   val Loss: 75.0058   time: 0.24s   best: 60.8143
2023-10-27 09:40:06,690:INFO:  Epoch 490/500:  train Loss: 75.9907   val Loss: 73.8120   time: 0.27s   best: 60.8143
2023-10-27 09:40:06,935:INFO:  Epoch 491/500:  train Loss: 74.4849   val Loss: 73.1658   time: 0.23s   best: 60.8143
2023-10-27 09:40:07,199:INFO:  Epoch 492/500:  train Loss: 73.3934   val Loss: 71.3613   time: 0.25s   best: 60.8143
2023-10-27 09:40:07,454:INFO:  Epoch 493/500:  train Loss: 72.1215   val Loss: 69.9891   time: 0.25s   best: 60.8143
2023-10-27 09:40:07,714:INFO:  Epoch 494/500:  train Loss: 70.2471   val Loss: 69.0630   time: 0.25s   best: 60.8143
2023-10-27 09:40:07,974:INFO:  Epoch 495/500:  train Loss: 70.2044   val Loss: 70.7318   time: 0.25s   best: 60.8143
2023-10-27 09:40:08,239:INFO:  Epoch 496/500:  train Loss: 73.7059   val Loss: 70.1361   time: 0.25s   best: 60.8143
2023-10-27 09:40:08,517:INFO:  Epoch 497/500:  train Loss: 72.7829   val Loss: 72.0214   time: 0.27s   best: 60.8143
2023-10-27 09:40:08,781:INFO:  Epoch 498/500:  train Loss: 72.8537   val Loss: 70.6532   time: 0.25s   best: 60.8143
2023-10-27 09:40:09,026:INFO:  Epoch 499/500:  train Loss: 70.8959   val Loss: 68.6590   time: 0.23s   best: 60.8143
2023-10-27 09:40:09,349:INFO:  Epoch 500/500:  train Loss: 69.2126   val Loss: 67.9412   time: 0.30s   best: 60.8143
2023-10-27 09:40:09,349:INFO:  -----> Training complete in 2m 20s   best validation loss: 60.8143
 
2023-10-27 16:53:57,065:INFO:  Starting experiment lstm autoencoder (2 layer + no dropout)
2023-10-27 16:53:57,075:INFO:  Defining the model
2023-10-27 16:53:57,127:INFO:  Reading the dataset
2023-10-27 17:21:21,701:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 17:21:21,732:INFO:  Epoch 1/500:  train Loss: 75.4043   val Loss: 68.6115   time: 436.37s   best: 68.6115
2023-10-27 17:28:37,387:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 17:28:37,412:INFO:  Epoch 2/500:  train Loss: 63.0609   val Loss: 60.3283   time: 435.65s   best: 60.3283
2023-10-27 17:35:52,904:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 17:35:52,934:INFO:  Epoch 3/500:  train Loss: 56.6453   val Loss: 54.5084   time: 435.48s   best: 54.5084
2023-10-27 17:43:04,463:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 17:43:04,499:INFO:  Epoch 4/500:  train Loss: 50.9611   val Loss: 49.9015   time: 431.52s   best: 49.9015
2023-10-27 17:50:20,764:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 17:50:20,788:INFO:  Epoch 5/500:  train Loss: 46.7009   val Loss: 45.8230   time: 436.26s   best: 45.8230
2023-10-27 17:57:32,464:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 17:57:32,494:INFO:  Epoch 6/500:  train Loss: 43.3669   val Loss: 42.5826   time: 431.67s   best: 42.5826
2023-10-27 18:04:44,953:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 18:04:44,984:INFO:  Epoch 7/500:  train Loss: 40.8595   val Loss: 42.1687   time: 432.44s   best: 42.1687
2023-10-27 18:11:58,619:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 18:11:58,648:INFO:  Epoch 8/500:  train Loss: 38.6247   val Loss: 39.0714   time: 433.63s   best: 39.0714
2023-10-27 18:19:15,578:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 18:19:15,605:INFO:  Epoch 9/500:  train Loss: 36.8751   val Loss: 38.4576   time: 436.91s   best: 38.4576
2023-10-27 18:26:29,528:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 18:26:29,552:INFO:  Epoch 10/500:  train Loss: 35.5812   val Loss: 35.3696   time: 433.92s   best: 35.3696
2023-10-27 18:33:45,896:INFO:  Epoch 11/500:  train Loss: 34.2844   val Loss: 36.2813   time: 436.34s   best: 35.3696
2023-10-27 18:40:59,825:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 18:40:59,864:INFO:  Epoch 12/500:  train Loss: 33.3087   val Loss: 33.5349   time: 433.90s   best: 33.5349
2023-10-27 18:48:17,384:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 18:48:17,428:INFO:  Epoch 13/500:  train Loss: 32.7416   val Loss: 32.8238   time: 437.51s   best: 32.8238
2023-10-27 18:55:33,462:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 18:55:33,493:INFO:  Epoch 14/500:  train Loss: 31.8815   val Loss: 32.3114   time: 436.01s   best: 32.3114
2023-10-27 19:02:49,634:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 19:02:49,665:INFO:  Epoch 15/500:  train Loss: 31.2961   val Loss: 31.4186   time: 436.12s   best: 31.4186
2023-10-27 19:10:06,053:INFO:  Epoch 16/500:  train Loss: 30.5477   val Loss: 33.1234   time: 436.39s   best: 31.4186
2023-10-27 19:17:23,156:INFO:  Epoch 17/500:  train Loss: 29.9227   val Loss: 32.3560   time: 437.07s   best: 31.4186
2023-10-27 19:24:35,982:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 19:24:36,149:INFO:  Epoch 18/500:  train Loss: 29.4554   val Loss: 29.8950   time: 432.80s   best: 29.8950
2023-10-27 19:31:53,209:INFO:  Epoch 19/500:  train Loss: 29.0197   val Loss: 32.2308   time: 437.05s   best: 29.8950
2023-10-27 19:39:10,836:INFO:  Epoch 20/500:  train Loss: 28.9084   val Loss: 32.0623   time: 437.60s   best: 29.8950
2023-10-27 19:46:24,790:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 19:46:24,823:INFO:  Epoch 21/500:  train Loss: 28.2926   val Loss: 28.7980   time: 433.93s   best: 28.7980
2023-10-27 19:53:41,839:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 19:53:41,871:INFO:  Epoch 22/500:  train Loss: 28.0096   val Loss: 28.7563   time: 436.99s   best: 28.7563
2023-10-27 20:00:55,328:INFO:  Epoch 23/500:  train Loss: 27.5251   val Loss: 29.3123   time: 433.43s   best: 28.7563
2023-10-27 20:08:11,740:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 20:08:11,775:INFO:  Epoch 24/500:  train Loss: 27.1857   val Loss: 28.7528   time: 436.38s   best: 28.7528
2023-10-27 20:15:27,755:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 20:15:27,797:INFO:  Epoch 25/500:  train Loss: 27.0700   val Loss: 28.3181   time: 435.98s   best: 28.3181
2023-10-27 20:22:43,596:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 20:22:43,628:INFO:  Epoch 26/500:  train Loss: 27.0135   val Loss: 27.8097   time: 435.78s   best: 27.8097
2023-10-27 20:29:58,315:INFO:  Epoch 27/500:  train Loss: 26.4808   val Loss: 28.4624   time: 434.69s   best: 27.8097
2023-10-27 20:37:12,014:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 20:37:12,041:INFO:  Epoch 28/500:  train Loss: 26.1930   val Loss: 27.5219   time: 433.67s   best: 27.5219
2023-10-27 20:44:25,955:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 20:44:25,979:INFO:  Epoch 29/500:  train Loss: 26.0889   val Loss: 27.4448   time: 433.90s   best: 27.4448
2023-10-27 20:51:37,845:INFO:  Epoch 30/500:  train Loss: 25.8748   val Loss: 29.2174   time: 431.84s   best: 27.4448
2023-10-27 20:58:54,271:INFO:  Epoch 31/500:  train Loss: 25.6630   val Loss: 29.3056   time: 436.42s   best: 27.4448
2023-10-27 21:06:11,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 21:06:11,331:INFO:  Epoch 32/500:  train Loss: 25.5670   val Loss: 27.2839   time: 437.00s   best: 27.2839
2023-10-27 21:13:28,987:INFO:  Epoch 33/500:  train Loss: 25.2859   val Loss: 27.4472   time: 437.66s   best: 27.2839
2023-10-27 21:20:44,894:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 21:20:44,942:INFO:  Epoch 34/500:  train Loss: 25.1165   val Loss: 26.9036   time: 435.89s   best: 26.9036
2023-10-27 21:28:01,658:INFO:  Epoch 35/500:  train Loss: 24.9202   val Loss: 27.3013   time: 436.72s   best: 26.9036
2023-10-27 21:35:17,427:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 21:35:17,453:INFO:  Epoch 36/500:  train Loss: 24.8971   val Loss: 26.8919   time: 435.76s   best: 26.8919
2023-10-27 21:42:34,482:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 21:42:34,509:INFO:  Epoch 37/500:  train Loss: 24.6809   val Loss: 26.7996   time: 437.01s   best: 26.7996
2023-10-27 21:49:49,035:INFO:  Epoch 38/500:  train Loss: 24.4977   val Loss: 28.5441   time: 434.51s   best: 26.7996
2023-10-27 21:57:01,479:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 21:57:01,521:INFO:  Epoch 39/500:  train Loss: 24.3951   val Loss: 26.4578   time: 432.42s   best: 26.4578
2023-10-27 22:04:15,863:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 22:04:15,903:INFO:  Epoch 40/500:  train Loss: 24.3219   val Loss: 26.4233   time: 434.33s   best: 26.4233
2023-10-27 22:11:31,722:INFO:  Epoch 41/500:  train Loss: 24.1343   val Loss: 26.6117   time: 435.82s   best: 26.4233
2023-10-27 22:18:49,698:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 22:18:49,763:INFO:  Epoch 42/500:  train Loss: 24.0444   val Loss: 26.1417   time: 437.93s   best: 26.1417
2023-10-27 22:26:05,849:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 22:26:05,878:INFO:  Epoch 43/500:  train Loss: 24.0754   val Loss: 25.8094   time: 436.07s   best: 25.8094
2023-10-27 22:33:21,799:INFO:  Epoch 44/500:  train Loss: 23.7748   val Loss: 27.3504   time: 435.91s   best: 25.8094
2023-10-27 22:40:34,491:INFO:  Epoch 45/500:  train Loss: 23.7112   val Loss: 26.1520   time: 432.67s   best: 25.8094
2023-10-27 22:47:50,209:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 22:47:50,258:INFO:  Epoch 46/500:  train Loss: 23.5737   val Loss: 25.7985   time: 435.71s   best: 25.7985
2023-10-27 22:55:07,527:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 22:55:07,569:INFO:  Epoch 47/500:  train Loss: 23.4356   val Loss: 25.7716   time: 437.27s   best: 25.7716
2023-10-27 23:02:25,061:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 23:02:25,091:INFO:  Epoch 48/500:  train Loss: 23.3139   val Loss: 25.4741   time: 437.49s   best: 25.4741
2023-10-27 23:09:37,527:INFO:  Epoch 49/500:  train Loss: 23.2761   val Loss: 25.7286   time: 432.44s   best: 25.4741
2023-10-27 23:16:53,904:INFO:  Epoch 50/500:  train Loss: 23.1819   val Loss: 27.0031   time: 436.35s   best: 25.4741
2023-10-27 23:24:10,093:INFO:  Epoch 51/500:  train Loss: 23.4313   val Loss: 26.4181   time: 436.16s   best: 25.4741
2023-10-27 23:31:26,395:INFO:  Epoch 52/500:  train Loss: 23.1152   val Loss: 27.5747   time: 436.29s   best: 25.4741
2023-10-27 23:38:43,286:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 23:38:43,312:INFO:  Epoch 53/500:  train Loss: 22.9465   val Loss: 25.1592   time: 436.88s   best: 25.1592
2023-10-27 23:46:00,616:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 23:46:00,643:INFO:  Epoch 54/500:  train Loss: 23.0271   val Loss: 25.1340   time: 437.30s   best: 25.1340
2023-10-27 23:53:14,010:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-27 23:53:14,048:INFO:  Epoch 55/500:  train Loss: 22.7341   val Loss: 25.0524   time: 433.36s   best: 25.0524
2023-10-28 00:00:28,747:INFO:  Epoch 56/500:  train Loss: 22.6591   val Loss: 25.1419   time: 434.69s   best: 25.0524
2023-10-28 00:07:41,502:INFO:  Epoch 57/500:  train Loss: 22.6232   val Loss: 26.4767   time: 432.72s   best: 25.0524
2023-10-28 00:14:58,679:INFO:  Epoch 58/500:  train Loss: 22.4736   val Loss: 25.6407   time: 437.15s   best: 25.0524
2023-10-28 00:22:14,645:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 00:22:14,673:INFO:  Epoch 59/500:  train Loss: 22.5486   val Loss: 24.7301   time: 435.94s   best: 24.7301
2023-10-28 00:29:30,335:INFO:  Epoch 60/500:  train Loss: 22.3814   val Loss: 25.0740   time: 435.65s   best: 24.7301
2023-10-28 00:36:46,027:INFO:  Epoch 61/500:  train Loss: 22.4735   val Loss: 25.2884   time: 435.67s   best: 24.7301
2023-10-28 00:44:00,892:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 00:44:00,923:INFO:  Epoch 62/500:  train Loss: 22.2737   val Loss: 24.6354   time: 434.85s   best: 24.6354
2023-10-28 00:51:16,621:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 00:51:16,646:INFO:  Epoch 63/500:  train Loss: 22.2796   val Loss: 24.6029   time: 435.68s   best: 24.6029
2023-10-28 00:58:30,915:INFO:  Epoch 64/500:  train Loss: 22.1196   val Loss: 24.7739   time: 434.25s   best: 24.6029
2023-10-28 01:05:48,669:INFO:  Epoch 65/500:  train Loss: 22.0810   val Loss: 25.0856   time: 437.73s   best: 24.6029
2023-10-28 01:13:04,538:INFO:  Epoch 66/500:  train Loss: 22.0899   val Loss: 24.8775   time: 435.86s   best: 24.6029
2023-10-28 01:20:18,768:INFO:  Epoch 67/500:  train Loss: 21.9070   val Loss: 25.2083   time: 434.20s   best: 24.6029
2023-10-28 01:27:33,930:INFO:  Epoch 68/500:  train Loss: 22.1282   val Loss: 24.9404   time: 435.15s   best: 24.6029
2023-10-28 01:34:50,995:INFO:  Epoch 69/500:  train Loss: 21.9845   val Loss: 25.4613   time: 437.04s   best: 24.6029
2023-10-28 01:42:04,148:INFO:  Epoch 70/500:  train Loss: 21.8343   val Loss: 24.7349   time: 433.13s   best: 24.6029
2023-10-28 01:49:16,429:INFO:  Epoch 71/500:  train Loss: 21.7766   val Loss: 24.6709   time: 432.25s   best: 24.6029
2023-10-28 01:56:30,846:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 01:56:30,875:INFO:  Epoch 72/500:  train Loss: 21.7790   val Loss: 24.3903   time: 434.39s   best: 24.3903
2023-10-28 02:03:43,605:INFO:  Epoch 73/500:  train Loss: 21.6565   val Loss: 25.0568   time: 432.73s   best: 24.3903
2023-10-28 02:11:00,530:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 02:11:00,561:INFO:  Epoch 74/500:  train Loss: 21.5300   val Loss: 24.1975   time: 436.90s   best: 24.1975
2023-10-28 02:18:15,051:INFO:  Epoch 75/500:  train Loss: 21.6191   val Loss: 24.5880   time: 434.49s   best: 24.1975
2023-10-28 02:25:28,444:INFO:  Epoch 76/500:  train Loss: 21.6854   val Loss: 24.6283   time: 433.37s   best: 24.1975
2023-10-28 02:32:45,244:INFO:  Epoch 77/500:  train Loss: 21.4655   val Loss: 26.5120   time: 436.75s   best: 24.1975
2023-10-28 02:40:01,603:INFO:  Epoch 78/500:  train Loss: 21.5933   val Loss: 24.6526   time: 436.34s   best: 24.1975
2023-10-28 02:47:16,891:INFO:  Epoch 79/500:  train Loss: 21.3612   val Loss: 24.6134   time: 435.26s   best: 24.1975
2023-10-28 02:54:34,087:INFO:  Epoch 80/500:  train Loss: 21.2744   val Loss: 24.9948   time: 437.17s   best: 24.1975
2023-10-28 03:01:50,288:INFO:  Epoch 81/500:  train Loss: 21.3695   val Loss: 24.6831   time: 436.16s   best: 24.1975
2023-10-28 03:09:03,168:INFO:  Epoch 82/500:  train Loss: 21.1651   val Loss: 25.9194   time: 432.87s   best: 24.1975
2023-10-28 03:16:20,043:INFO:  Epoch 83/500:  train Loss: 21.2586   val Loss: 24.8249   time: 436.84s   best: 24.1975
2023-10-28 03:23:38,419:INFO:  Epoch 84/500:  train Loss: 21.3407   val Loss: 24.5820   time: 438.35s   best: 24.1975
2023-10-28 03:30:54,834:INFO:  Epoch 85/500:  train Loss: 21.0104   val Loss: 24.6379   time: 436.39s   best: 24.1975
2023-10-28 03:38:11,685:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 03:38:11,723:INFO:  Epoch 86/500:  train Loss: 21.0113   val Loss: 23.9083   time: 436.83s   best: 23.9083
2023-10-28 03:45:25,101:INFO:  Epoch 87/500:  train Loss: 21.1048   val Loss: 24.2580   time: 433.38s   best: 23.9083
2023-10-28 03:52:38,208:INFO:  Epoch 88/500:  train Loss: 20.9456   val Loss: 24.4361   time: 433.10s   best: 23.9083
2023-10-28 03:59:52,435:INFO:  Epoch 89/500:  train Loss: 21.6399   val Loss: 24.0925   time: 434.20s   best: 23.9083
2023-10-28 04:07:08,037:INFO:  Epoch 90/500:  train Loss: 20.8913   val Loss: 27.5737   time: 435.59s   best: 23.9083
2023-10-28 04:14:23,127:INFO:  Epoch 91/500:  train Loss: 21.0529   val Loss: 24.1770   time: 435.08s   best: 23.9083
2023-10-28 04:21:37,407:INFO:  Epoch 92/500:  train Loss: 20.8296   val Loss: 24.5429   time: 434.24s   best: 23.9083
2023-10-28 04:28:53,786:INFO:  Epoch 93/500:  train Loss: 20.9693   val Loss: 25.6606   time: 436.37s   best: 23.9083
2023-10-28 04:36:10,988:INFO:  Epoch 94/500:  train Loss: 20.7682   val Loss: 24.0788   time: 437.18s   best: 23.9083
2023-10-28 04:43:23,475:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 04:43:23,502:INFO:  Epoch 95/500:  train Loss: 20.7874   val Loss: 23.7880   time: 432.45s   best: 23.7880
2023-10-28 04:50:37,132:INFO:  Epoch 96/500:  train Loss: 20.9554   val Loss: 25.9666   time: 433.63s   best: 23.7880
2023-10-28 04:57:50,191:INFO:  Epoch 97/500:  train Loss: 20.7107   val Loss: 24.0999   time: 433.05s   best: 23.7880
2023-10-28 05:05:05,454:INFO:  Epoch 98/500:  train Loss: 20.6919   val Loss: 24.1029   time: 435.26s   best: 23.7880
2023-10-28 05:12:17,709:INFO:  Epoch 99/500:  train Loss: 20.6527   val Loss: 24.6028   time: 432.21s   best: 23.7880
2023-10-28 05:19:32,860:INFO:  Epoch 100/500:  train Loss: 20.7053   val Loss: 24.7723   time: 435.12s   best: 23.7880
2023-10-28 05:26:49,703:INFO:  Epoch 101/500:  train Loss: 20.5502   val Loss: 25.4197   time: 436.83s   best: 23.7880
2023-10-28 05:34:03,503:INFO:  Epoch 102/500:  train Loss: 20.5244   val Loss: 24.2840   time: 433.77s   best: 23.7880
2023-10-28 05:41:21,033:INFO:  Epoch 103/500:  train Loss: 20.4974   val Loss: 24.7548   time: 437.51s   best: 23.7880
2023-10-28 05:48:38,350:INFO:  Epoch 104/500:  train Loss: 20.5859   val Loss: 24.4361   time: 437.29s   best: 23.7880
2023-10-28 05:55:51,324:INFO:  Epoch 105/500:  train Loss: 20.6364   val Loss: 25.4495   time: 432.94s   best: 23.7880
2023-10-28 06:03:04,459:INFO:  Epoch 106/500:  train Loss: 20.3441   val Loss: 24.0764   time: 433.10s   best: 23.7880
2023-10-28 06:10:17,967:INFO:  Epoch 107/500:  train Loss: 20.3781   val Loss: 23.7947   time: 433.48s   best: 23.7880
2023-10-28 06:17:32,329:INFO:  Epoch 108/500:  train Loss: 20.4673   val Loss: 24.1854   time: 434.35s   best: 23.7880
2023-10-28 06:24:47,357:INFO:  Epoch 109/500:  train Loss: 20.3448   val Loss: 24.1079   time: 435.00s   best: 23.7880
2023-10-28 06:32:03,335:INFO:  Epoch 110/500:  train Loss: 20.2513   val Loss: 24.0231   time: 435.95s   best: 23.7880
2023-10-28 06:39:18,849:INFO:  Epoch 111/500:  train Loss: 20.2485   val Loss: 25.3150   time: 435.49s   best: 23.7880
2023-10-28 06:46:35,623:INFO:  Epoch 112/500:  train Loss: 20.4376   val Loss: 25.1237   time: 436.68s   best: 23.7880
2023-10-28 06:53:50,057:INFO:  Epoch 113/500:  train Loss: 20.2641   val Loss: 24.1204   time: 434.40s   best: 23.7880
2023-10-28 07:01:05,952:INFO:  Epoch 114/500:  train Loss: 20.0883   val Loss: 24.6101   time: 435.88s   best: 23.7880
2023-10-28 07:08:18,005:INFO:  Epoch 115/500:  train Loss: 20.1775   val Loss: 24.0805   time: 432.04s   best: 23.7880
2023-10-28 07:15:30,568:INFO:  Epoch 116/500:  train Loss: 20.4042   val Loss: 24.7824   time: 432.53s   best: 23.7880
2023-10-28 07:22:46,173:INFO:  Epoch 117/500:  train Loss: 20.0280   val Loss: 24.0904   time: 435.58s   best: 23.7880
2023-10-28 07:30:02,349:INFO:  Epoch 118/500:  train Loss: 20.1432   val Loss: 24.6051   time: 436.17s   best: 23.7880
2023-10-28 07:37:16,117:INFO:  Epoch 119/500:  train Loss: 20.0503   val Loss: 26.6934   time: 433.74s   best: 23.7880
2023-10-28 07:44:30,244:INFO:  Epoch 120/500:  train Loss: 20.1432   val Loss: 24.2499   time: 434.10s   best: 23.7880
2023-10-28 07:51:44,637:INFO:  Epoch 121/500:  train Loss: 19.9732   val Loss: 25.1967   time: 434.35s   best: 23.7880
2023-10-28 07:58:57,048:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 07:58:57,073:INFO:  Epoch 122/500:  train Loss: 20.1193   val Loss: 23.7536   time: 432.38s   best: 23.7536
2023-10-28 08:06:10,074:INFO:  Epoch 123/500:  train Loss: 20.1606   val Loss: 23.9775   time: 432.99s   best: 23.7536
2023-10-28 08:13:21,778:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 08:13:21,818:INFO:  Epoch 124/500:  train Loss: 20.0802   val Loss: 23.4697   time: 431.67s   best: 23.4697
2023-10-28 08:20:34,952:INFO:  Epoch 125/500:  train Loss: 19.9002   val Loss: 23.9097   time: 433.12s   best: 23.4697
2023-10-28 08:27:50,338:INFO:  Epoch 126/500:  train Loss: 19.9205   val Loss: 24.1712   time: 435.38s   best: 23.4697
2023-10-28 08:35:02,483:INFO:  Epoch 127/500:  train Loss: 20.1009   val Loss: 24.7618   time: 432.12s   best: 23.4697
2023-10-28 08:42:20,148:INFO:  Epoch 128/500:  train Loss: 20.0281   val Loss: 24.5455   time: 437.64s   best: 23.4697
2023-10-28 08:49:33,861:INFO:  Epoch 129/500:  train Loss: 19.9517   val Loss: 24.0255   time: 433.70s   best: 23.4697
2023-10-28 08:56:50,162:INFO:  Epoch 130/500:  train Loss: 19.7575   val Loss: 24.1195   time: 436.28s   best: 23.4697
2023-10-28 09:04:06,624:INFO:  Epoch 131/500:  train Loss: 19.8590   val Loss: 24.1700   time: 436.45s   best: 23.4697
2023-10-28 09:11:22,328:INFO:  Epoch 132/500:  train Loss: 19.9294   val Loss: 24.1439   time: 435.68s   best: 23.4697
2023-10-28 09:18:39,129:INFO:  Epoch 133/500:  train Loss: 19.6704   val Loss: 24.0715   time: 436.77s   best: 23.4697
2023-10-28 09:25:53,033:INFO:  Epoch 134/500:  train Loss: 19.7183   val Loss: 24.1070   time: 433.89s   best: 23.4697
2023-10-28 09:33:06,314:INFO:  Epoch 135/500:  train Loss: 19.9618   val Loss: 24.8862   time: 433.27s   best: 23.4697
2023-10-28 09:40:22,418:INFO:  Epoch 136/500:  train Loss: 19.9116   val Loss: 24.0747   time: 436.09s   best: 23.4697
2023-10-28 09:47:36,574:INFO:  Epoch 137/500:  train Loss: 19.7179   val Loss: 26.1328   time: 434.14s   best: 23.4697
2023-10-28 09:54:49,381:INFO:  Epoch 138/500:  train Loss: 19.6763   val Loss: 23.5652   time: 432.79s   best: 23.4697
2023-10-28 10:02:04,312:INFO:  Epoch 139/500:  train Loss: 19.5904   val Loss: 23.8257   time: 434.92s   best: 23.4697
2023-10-28 10:09:21,473:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 10:09:21,514:INFO:  Epoch 140/500:  train Loss: 19.7292   val Loss: 23.3786   time: 437.13s   best: 23.3786
2023-10-28 10:16:34,919:INFO:  Epoch 141/500:  train Loss: 19.5851   val Loss: 23.5400   time: 433.39s   best: 23.3786
2023-10-28 10:23:50,198:INFO:  Epoch 142/500:  train Loss: 19.6002   val Loss: 23.8892   time: 435.25s   best: 23.3786
2023-10-28 10:31:08,438:INFO:  Epoch 143/500:  train Loss: 19.5717   val Loss: 23.5087   time: 438.23s   best: 23.3786
2023-10-28 10:38:27,104:INFO:  Epoch 144/500:  train Loss: 19.5775   val Loss: 24.4812   time: 438.64s   best: 23.3786
2023-10-28 10:45:44,433:INFO:  Epoch 145/500:  train Loss: 19.4907   val Loss: 23.8522   time: 437.31s   best: 23.3786
2023-10-28 10:53:02,053:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 10:53:02,080:INFO:  Epoch 146/500:  train Loss: 19.4087   val Loss: 23.2231   time: 437.59s   best: 23.2231
2023-10-28 11:00:19,174:INFO:  Epoch 147/500:  train Loss: 19.4160   val Loss: 23.8407   time: 437.08s   best: 23.2231
2023-10-28 11:07:33,851:INFO:  Epoch 148/500:  train Loss: 19.4490   val Loss: 25.3940   time: 434.67s   best: 23.2231
2023-10-28 11:14:51,692:INFO:  Epoch 149/500:  train Loss: 19.8915   val Loss: 23.5763   time: 437.82s   best: 23.2231
2023-10-28 11:22:07,634:INFO:  Epoch 150/500:  train Loss: 19.4267   val Loss: 23.6300   time: 435.92s   best: 23.2231
2023-10-28 11:29:19,948:INFO:  Epoch 151/500:  train Loss: 19.5590   val Loss: 23.4392   time: 432.28s   best: 23.2231
2023-10-28 11:36:34,595:INFO:  Epoch 152/500:  train Loss: 19.3588   val Loss: 23.5071   time: 434.64s   best: 23.2231
2023-10-28 11:43:51,179:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 11:43:51,205:INFO:  Epoch 153/500:  train Loss: 19.2426   val Loss: 23.1546   time: 436.54s   best: 23.1546
2023-10-28 11:51:04,023:INFO:  Epoch 154/500:  train Loss: 19.2839   val Loss: 23.7721   time: 432.82s   best: 23.1546
2023-10-28 11:58:16,901:INFO:  Epoch 155/500:  train Loss: 19.3492   val Loss: 24.0178   time: 432.85s   best: 23.1546
2023-10-28 12:05:32,257:INFO:  Epoch 156/500:  train Loss: 19.3147   val Loss: 23.5144   time: 435.35s   best: 23.1546
2023-10-28 12:12:45,765:INFO:  Epoch 157/500:  train Loss: 19.2455   val Loss: 25.8402   time: 433.48s   best: 23.1546
2023-10-28 12:20:00,771:INFO:  Epoch 158/500:  train Loss: 19.4317   val Loss: 24.2246   time: 434.99s   best: 23.1546
2023-10-28 12:27:16,279:INFO:  Epoch 159/500:  train Loss: 19.2230   val Loss: 23.5802   time: 435.49s   best: 23.1546
2023-10-28 12:34:31,532:INFO:  Epoch 160/500:  train Loss: 19.2649   val Loss: 23.6496   time: 435.22s   best: 23.1546
2023-10-28 12:41:45,148:INFO:  Epoch 161/500:  train Loss: 19.4090   val Loss: 25.0921   time: 433.60s   best: 23.1546
2023-10-28 12:49:02,465:INFO:  Epoch 162/500:  train Loss: 19.4633   val Loss: 23.2398   time: 437.29s   best: 23.1546
2023-10-28 12:56:16,538:INFO:  Epoch 163/500:  train Loss: 19.4023   val Loss: 23.8211   time: 434.05s   best: 23.1546
2023-10-28 13:03:33,441:INFO:  Epoch 164/500:  train Loss: 19.2560   val Loss: 23.8185   time: 436.87s   best: 23.1546
2023-10-28 13:10:50,951:INFO:  Epoch 165/500:  train Loss: 19.3482   val Loss: 24.0292   time: 437.48s   best: 23.1546
2023-10-28 13:18:09,552:INFO:  Epoch 166/500:  train Loss: 19.5285   val Loss: 23.4800   time: 438.58s   best: 23.1546
2023-10-28 13:25:24,617:INFO:  Epoch 167/500:  train Loss: 19.1200   val Loss: 23.7669   time: 435.05s   best: 23.1546
2023-10-28 13:32:41,758:INFO:  Epoch 168/500:  train Loss: 19.1258   val Loss: 23.5635   time: 437.12s   best: 23.1546
2023-10-28 13:39:56,186:INFO:  Epoch 169/500:  train Loss: 19.1309   val Loss: 24.3608   time: 434.39s   best: 23.1546
2023-10-28 13:47:11,974:INFO:  Epoch 170/500:  train Loss: 19.2584   val Loss: 23.6047   time: 435.77s   best: 23.1546
2023-10-28 13:54:29,145:INFO:  Epoch 171/500:  train Loss: 19.3019   val Loss: 23.6148   time: 437.15s   best: 23.1546
2023-10-28 14:01:41,373:INFO:  Epoch 172/500:  train Loss: 19.3366   val Loss: 23.6247   time: 432.20s   best: 23.1546
2023-10-28 14:08:57,939:INFO:  Epoch 173/500:  train Loss: 19.2445   val Loss: 24.2201   time: 436.55s   best: 23.1546
2023-10-28 14:16:10,532:INFO:  Epoch 174/500:  train Loss: 19.3196   val Loss: 23.1921   time: 432.58s   best: 23.1546
2023-10-28 14:23:28,307:INFO:  Epoch 175/500:  train Loss: 19.1288   val Loss: 24.4301   time: 437.75s   best: 23.1546
2023-10-28 14:30:40,897:INFO:  Epoch 176/500:  train Loss: 19.1772   val Loss: 23.4335   time: 432.57s   best: 23.1546
2023-10-28 14:37:53,857:INFO:  Epoch 177/500:  train Loss: 19.1029   val Loss: 23.4265   time: 432.94s   best: 23.1546
2023-10-28 14:45:07,335:INFO:  Epoch 178/500:  train Loss: 19.2421   val Loss: 24.1374   time: 433.45s   best: 23.1546
2023-10-28 14:52:20,924:INFO:  Epoch 179/500:  train Loss: 19.1024   val Loss: 23.3965   time: 433.58s   best: 23.1546
2023-10-28 14:59:38,406:INFO:  Epoch 180/500:  train Loss: 19.4646   val Loss: 23.2401   time: 437.45s   best: 23.1546
2023-10-28 15:06:51,209:INFO:  Epoch 181/500:  train Loss: 19.0894   val Loss: 24.0951   time: 432.78s   best: 23.1546
2023-10-28 15:14:09,268:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 15:14:09,294:INFO:  Epoch 182/500:  train Loss: 19.1802   val Loss: 22.9396   time: 438.04s   best: 22.9396
2023-10-28 15:21:27,463:INFO:  Epoch 183/500:  train Loss: 19.0168   val Loss: 23.3733   time: 438.17s   best: 22.9396
2023-10-28 15:28:46,576:INFO:  Epoch 184/500:  train Loss: 18.9340   val Loss: 23.5854   time: 439.09s   best: 22.9396
2023-10-28 15:35:59,599:INFO:  Epoch 185/500:  train Loss: 19.0030   val Loss: 24.0217   time: 433.01s   best: 22.9396
2023-10-28 15:43:16,301:INFO:  Epoch 186/500:  train Loss: 18.9663   val Loss: 23.4065   time: 436.68s   best: 22.9396
2023-10-28 15:50:34,218:INFO:  Epoch 187/500:  train Loss: 18.9136   val Loss: 23.3435   time: 437.89s   best: 22.9396
2023-10-28 15:57:52,696:INFO:  Epoch 188/500:  train Loss: 18.9270   val Loss: 23.3075   time: 438.45s   best: 22.9396
2023-10-28 16:05:05,891:INFO:  Epoch 189/500:  train Loss: 19.0166   val Loss: 23.3897   time: 433.16s   best: 22.9396
2023-10-28 16:12:18,880:INFO:  Epoch 190/500:  train Loss: 19.0812   val Loss: 24.0499   time: 432.96s   best: 22.9396
2023-10-28 16:19:33,586:INFO:  Epoch 191/500:  train Loss: 18.9194   val Loss: 25.8411   time: 434.68s   best: 22.9396
2023-10-28 16:26:46,070:INFO:  Epoch 192/500:  train Loss: 18.8249   val Loss: 23.5600   time: 432.46s   best: 22.9396
2023-10-28 16:34:02,060:INFO:  Epoch 193/500:  train Loss: 18.8475   val Loss: 23.3301   time: 435.97s   best: 22.9396
2023-10-28 16:41:20,455:INFO:  Epoch 194/500:  train Loss: 18.7738   val Loss: 23.5631   time: 438.37s   best: 22.9396
2023-10-28 16:48:37,984:INFO:  Epoch 195/500:  train Loss: 18.8322   val Loss: 24.2738   time: 437.50s   best: 22.9396
2023-10-28 16:55:55,340:INFO:  Epoch 196/500:  train Loss: 18.8301   val Loss: 23.5844   time: 437.32s   best: 22.9396
2023-10-28 17:03:11,836:INFO:  Epoch 197/500:  train Loss: 18.9388   val Loss: 24.2980   time: 436.47s   best: 22.9396
2023-10-28 17:10:24,535:INFO:  Epoch 198/500:  train Loss: 19.0054   val Loss: 24.0246   time: 432.65s   best: 22.9396
2023-10-28 17:17:37,421:INFO:  Epoch 199/500:  train Loss: 19.0602   val Loss: 23.6405   time: 432.84s   best: 22.9396
2023-10-28 17:24:51,632:INFO:  Epoch 200/500:  train Loss: 18.8878   val Loss: 23.9230   time: 434.18s   best: 22.9396
2023-10-28 17:32:07,684:INFO:  Epoch 201/500:  train Loss: 18.7987   val Loss: 24.5438   time: 436.03s   best: 22.9396
2023-10-28 17:39:23,205:INFO:  Epoch 202/500:  train Loss: 18.9069   val Loss: 24.0939   time: 435.51s   best: 22.9396
2023-10-28 17:46:40,758:INFO:  Epoch 203/500:  train Loss: 19.2007   val Loss: 25.0664   time: 437.53s   best: 22.9396
2023-10-28 17:53:58,862:INFO:  Epoch 204/500:  train Loss: 18.9339   val Loss: 22.9768   time: 438.08s   best: 22.9396
2023-10-28 18:01:12,443:INFO:  Epoch 205/500:  train Loss: 18.9812   val Loss: 23.9921   time: 433.56s   best: 22.9396
2023-10-28 18:08:27,672:INFO:  Epoch 206/500:  train Loss: 18.6699   val Loss: 23.9147   time: 435.19s   best: 22.9396
2023-10-28 18:15:40,628:INFO:  Epoch 207/500:  train Loss: 18.7909   val Loss: 23.6810   time: 432.93s   best: 22.9396
2023-10-28 18:22:58,400:INFO:  Epoch 208/500:  train Loss: 18.7274   val Loss: 23.6902   time: 437.74s   best: 22.9396
2023-10-28 18:30:11,709:INFO:  Epoch 209/500:  train Loss: 18.8055   val Loss: 24.2650   time: 433.30s   best: 22.9396
2023-10-28 18:37:25,146:INFO:  Epoch 210/500:  train Loss: 18.7909   val Loss: 23.5258   time: 433.42s   best: 22.9396
2023-10-28 18:44:40,928:INFO:  Epoch 211/500:  train Loss: 18.6855   val Loss: 23.6188   time: 435.77s   best: 22.9396
2023-10-28 18:51:57,481:INFO:  Epoch 212/500:  train Loss: 18.6494   val Loss: 24.9705   time: 436.54s   best: 22.9396
2023-10-28 18:59:14,283:INFO:  Epoch 213/500:  train Loss: 18.8879   val Loss: 23.4195   time: 436.77s   best: 22.9396
2023-10-28 19:06:31,652:INFO:  Epoch 214/500:  train Loss: 18.6203   val Loss: 24.3567   time: 437.34s   best: 22.9396
2023-10-28 19:13:44,711:INFO:  Epoch 215/500:  train Loss: 18.6883   val Loss: 23.2130   time: 433.03s   best: 22.9396
2023-10-28 19:21:01,797:INFO:  Epoch 216/500:  train Loss: 18.6577   val Loss: 24.2098   time: 437.06s   best: 22.9396
2023-10-28 19:28:16,332:INFO:  Epoch 217/500:  train Loss: 18.9294   val Loss: 23.2845   time: 434.52s   best: 22.9396
2023-10-28 19:35:32,348:INFO:  Epoch 218/500:  train Loss: 18.5958   val Loss: 23.7851   time: 436.00s   best: 22.9396
2023-10-28 19:42:48,202:INFO:  Epoch 219/500:  train Loss: 18.6290   val Loss: 23.9339   time: 435.83s   best: 22.9396
2023-10-28 19:50:02,892:INFO:  Epoch 220/500:  train Loss: 18.5939   val Loss: 24.2371   time: 434.67s   best: 22.9396
2023-10-28 19:57:20,340:INFO:  Epoch 221/500:  train Loss: 18.6253   val Loss: 26.2286   time: 437.42s   best: 22.9396
2023-10-28 20:04:35,664:INFO:  Epoch 222/500:  train Loss: 18.7489   val Loss: 23.6112   time: 435.31s   best: 22.9396
2023-10-28 20:11:48,584:INFO:  Epoch 223/500:  train Loss: 18.5252   val Loss: 23.3833   time: 432.89s   best: 22.9396
2023-10-28 20:19:01,360:INFO:  Epoch 224/500:  train Loss: 18.7233   val Loss: 29.5680   time: 432.76s   best: 22.9396
2023-10-28 20:26:17,178:INFO:  Epoch 225/500:  train Loss: 18.5607   val Loss: 24.4902   time: 435.78s   best: 22.9396
2023-10-28 20:33:33,865:INFO:  Epoch 226/500:  train Loss: 18.4382   val Loss: 23.7144   time: 436.66s   best: 22.9396
2023-10-28 20:40:48,823:INFO:  Epoch 227/500:  train Loss: 18.9573   val Loss: 23.7478   time: 434.95s   best: 22.9396
2023-10-28 20:48:06,941:INFO:  Epoch 228/500:  train Loss: 18.6437   val Loss: 23.5617   time: 438.11s   best: 22.9396
2023-10-28 20:55:23,419:INFO:  Epoch 229/500:  train Loss: 18.5799   val Loss: 23.7389   time: 436.46s   best: 22.9396
2023-10-28 21:02:40,464:INFO:  Epoch 230/500:  train Loss: 18.6789   val Loss: 23.9784   time: 437.02s   best: 22.9396
2023-10-28 21:09:55,963:INFO:  Epoch 231/500:  train Loss: 19.0571   val Loss: 23.7074   time: 435.48s   best: 22.9396
2023-10-28 21:17:09,331:INFO:  Epoch 232/500:  train Loss: 18.5031   val Loss: 23.2765   time: 433.34s   best: 22.9396
2023-10-28 21:24:22,630:INFO:  Epoch 233/500:  train Loss: 18.4708   val Loss: 23.3715   time: 433.27s   best: 22.9396
2023-10-28 21:31:37,975:INFO:  Epoch 234/500:  train Loss: 18.6606   val Loss: 24.8603   time: 435.32s   best: 22.9396
2023-10-28 21:38:51,502:INFO:  Epoch 235/500:  train Loss: 18.5229   val Loss: 23.9926   time: 433.50s   best: 22.9396
2023-10-28 21:46:06,987:INFO:  Epoch 236/500:  train Loss: 18.5947   val Loss: 23.1747   time: 435.46s   best: 22.9396
2023-10-28 21:53:23,636:INFO:  Epoch 237/500:  train Loss: 18.4053   val Loss: 23.3659   time: 436.62s   best: 22.9396
2023-10-28 22:00:36,777:INFO:  Epoch 238/500:  train Loss: 18.3914   val Loss: 23.1947   time: 433.11s   best: 22.9396
2023-10-28 22:07:51,594:INFO:  Epoch 239/500:  train Loss: 18.5028   val Loss: 23.4329   time: 434.79s   best: 22.9396
2023-10-28 22:15:07,034:INFO:  Epoch 240/500:  train Loss: 18.4246   val Loss: 23.3560   time: 435.41s   best: 22.9396
2023-10-28 22:22:19,446:INFO:  Epoch 241/500:  train Loss: 18.3900   val Loss: 23.9064   time: 432.40s   best: 22.9396
2023-10-28 22:29:35,166:INFO:  Epoch 242/500:  train Loss: 18.3492   val Loss: 23.1748   time: 435.71s   best: 22.9396
2023-10-28 22:36:52,793:INFO:  Epoch 243/500:  train Loss: 18.9903   val Loss: 24.7957   time: 437.59s   best: 22.9396
2023-10-28 22:44:05,450:INFO:  Epoch 244/500:  train Loss: 18.7049   val Loss: 23.6272   time: 432.63s   best: 22.9396
2023-10-28 22:51:22,136:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-28 22:51:22,165:INFO:  Epoch 245/500:  train Loss: 18.3303   val Loss: 22.9324   time: 436.64s   best: 22.9324
2023-10-28 22:58:36,242:INFO:  Epoch 246/500:  train Loss: 18.3013   val Loss: 23.2036   time: 434.08s   best: 22.9324
2023-10-28 23:05:49,077:INFO:  Epoch 247/500:  train Loss: 18.4446   val Loss: 23.7802   time: 432.81s   best: 22.9324
2023-10-28 23:13:01,779:INFO:  Epoch 248/500:  train Loss: 18.5126   val Loss: 23.5617   time: 432.67s   best: 22.9324
2023-10-28 23:20:18,952:INFO:  Epoch 249/500:  train Loss: 18.2862   val Loss: 23.3230   time: 437.16s   best: 22.9324
2023-10-28 23:27:34,134:INFO:  Epoch 250/500:  train Loss: 18.3713   val Loss: 23.5325   time: 435.15s   best: 22.9324
2023-10-28 23:34:51,408:INFO:  Epoch 251/500:  train Loss: 18.4096   val Loss: 23.5646   time: 437.26s   best: 22.9324
2023-10-28 23:42:06,715:INFO:  Epoch 252/500:  train Loss: 18.3112   val Loss: 22.9478   time: 435.28s   best: 22.9324
2023-10-28 23:49:23,959:INFO:  Epoch 253/500:  train Loss: 18.3611   val Loss: 24.3475   time: 437.22s   best: 22.9324
2023-10-28 23:56:35,999:INFO:  Epoch 254/500:  train Loss: 18.3743   val Loss: 23.3100   time: 432.00s   best: 22.9324
2023-10-29 00:03:51,767:INFO:  Epoch 255/500:  train Loss: 18.4803   val Loss: 22.9879   time: 435.74s   best: 22.9324
2023-10-29 00:11:09,473:INFO:  Epoch 256/500:  train Loss: 18.3693   val Loss: 23.0437   time: 437.68s   best: 22.9324
2023-10-29 00:18:27,295:INFO:  Epoch 257/500:  train Loss: 18.2983   val Loss: 23.1822   time: 437.81s   best: 22.9324
2023-10-29 00:25:42,422:INFO:  Epoch 258/500:  train Loss: 18.2629   val Loss: 23.4766   time: 435.10s   best: 22.9324
2023-10-29 00:32:55,453:INFO:  Epoch 259/500:  train Loss: 18.1672   val Loss: 23.1085   time: 432.99s   best: 22.9324
2023-10-29 00:40:12,245:INFO:  Epoch 260/500:  train Loss: 18.4360   val Loss: 23.4018   time: 436.76s   best: 22.9324
2023-10-29 00:47:25,004:INFO:  Epoch 261/500:  train Loss: 18.2695   val Loss: 23.2725   time: 432.73s   best: 22.9324
2023-10-29 00:54:42,519:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-29 00:54:42,557:INFO:  Epoch 262/500:  train Loss: 18.1818   val Loss: 22.7510   time: 437.49s   best: 22.7510
2023-10-29 01:01:59,348:INFO:  Epoch 263/500:  train Loss: 18.2030   val Loss: 23.3322   time: 436.78s   best: 22.7510
2023-10-29 01:09:13,512:INFO:  Epoch 264/500:  train Loss: 18.1395   val Loss: 23.7621   time: 434.14s   best: 22.7510
2023-10-29 01:16:31,359:INFO:  Epoch 265/500:  train Loss: 18.0394   val Loss: 23.5136   time: 437.83s   best: 22.7510
2023-10-29 01:23:44,163:INFO:  Epoch 266/500:  train Loss: 18.1569   val Loss: 23.6813   time: 432.78s   best: 22.7510
2023-10-29 01:30:58,579:INFO:  Epoch 267/500:  train Loss: 18.2200   val Loss: 23.6880   time: 434.38s   best: 22.7510
2023-10-29 01:38:13,339:INFO:  Epoch 268/500:  train Loss: 18.1266   val Loss: 23.0809   time: 434.75s   best: 22.7510
2023-10-29 01:45:29,956:INFO:  Epoch 269/500:  train Loss: 18.1237   val Loss: 23.3701   time: 436.60s   best: 22.7510
2023-10-29 01:52:44,563:INFO:  Epoch 270/500:  train Loss: 18.3483   val Loss: 23.5442   time: 434.59s   best: 22.7510
2023-10-29 02:00:02,317:INFO:  Epoch 271/500:  train Loss: 18.1219   val Loss: 23.3615   time: 437.73s   best: 22.7510
2023-10-29 02:07:16,094:INFO:  Epoch 272/500:  train Loss: 18.0125   val Loss: 23.7742   time: 433.75s   best: 22.7510
2023-10-29 02:14:34,101:INFO:  Epoch 273/500:  train Loss: 18.1672   val Loss: 23.4956   time: 437.98s   best: 22.7510
2023-10-29 02:21:52,033:INFO:  Epoch 274/500:  train Loss: 18.0904   val Loss: 24.1416   time: 437.92s   best: 22.7510
2023-10-29 02:29:09,672:INFO:  Epoch 275/500:  train Loss: 18.1498   val Loss: 23.7389   time: 437.62s   best: 22.7510
2023-10-29 02:36:25,396:INFO:  Epoch 276/500:  train Loss: 18.0181   val Loss: 22.9229   time: 435.69s   best: 22.7510
2023-10-29 02:43:37,626:INFO:  Epoch 277/500:  train Loss: 18.2159   val Loss: 23.6347   time: 432.21s   best: 22.7510
2023-10-29 02:50:54,512:INFO:  Epoch 278/500:  train Loss: 18.1496   val Loss: 23.0172   time: 436.86s   best: 22.7510
2023-10-29 02:58:07,554:INFO:  Epoch 279/500:  train Loss: 18.0543   val Loss: 23.8226   time: 433.03s   best: 22.7510
2023-10-29 03:05:22,202:INFO:  Epoch 280/500:  train Loss: 18.0272   val Loss: 26.0472   time: 434.63s   best: 22.7510
2023-10-29 03:12:38,963:INFO:  Epoch 281/500:  train Loss: 18.1199   val Loss: 23.2017   time: 436.75s   best: 22.7510
2023-10-29 03:19:52,349:INFO:  Epoch 282/500:  train Loss: 17.9347   val Loss: 22.9606   time: 433.37s   best: 22.7510
2023-10-29 03:27:08,282:INFO:  Epoch 283/500:  train Loss: 17.9942   val Loss: 24.6291   time: 435.91s   best: 22.7510
2023-10-29 03:34:22,195:INFO:  Epoch 284/500:  train Loss: 18.1220   val Loss: 23.4773   time: 433.89s   best: 22.7510
2023-10-29 03:41:37,239:INFO:  Epoch 285/500:  train Loss: 18.1133   val Loss: 23.7446   time: 435.01s   best: 22.7510
2023-10-29 03:48:55,085:INFO:  Epoch 286/500:  train Loss: 17.9576   val Loss: 23.9980   time: 437.82s   best: 22.7510
2023-10-29 03:56:12,636:INFO:  Epoch 287/500:  train Loss: 18.2689   val Loss: 23.1920   time: 437.52s   best: 22.7510
2023-10-29 04:03:30,936:INFO:  Epoch 288/500:  train Loss: 17.9086   val Loss: 23.4329   time: 438.27s   best: 22.7510
2023-10-29 04:10:48,619:INFO:  Epoch 289/500:  train Loss: 18.0482   val Loss: 23.6537   time: 437.65s   best: 22.7510
2023-10-29 04:18:05,733:INFO:  Epoch 290/500:  train Loss: 17.9970   val Loss: 23.1903   time: 437.10s   best: 22.7510
2023-10-29 04:25:20,748:INFO:  Epoch 291/500:  train Loss: 18.1013   val Loss: 23.7283   time: 435.00s   best: 22.7510
2023-10-29 04:32:38,713:INFO:  Epoch 292/500:  train Loss: 18.3323   val Loss: 23.4022   time: 437.96s   best: 22.7510
2023-10-29 04:39:55,355:INFO:  Epoch 293/500:  train Loss: 17.9382   val Loss: 23.3211   time: 436.62s   best: 22.7510
2023-10-29 04:47:12,298:INFO:  Epoch 294/500:  train Loss: 17.9153   val Loss: 24.7753   time: 436.92s   best: 22.7510
2023-10-29 04:54:25,164:INFO:  Epoch 295/500:  train Loss: 18.2197   val Loss: 23.2928   time: 432.84s   best: 22.7510
2023-10-29 05:01:38,024:INFO:  Epoch 296/500:  train Loss: 17.9894   val Loss: 23.6776   time: 432.83s   best: 22.7510
2023-10-29 05:08:50,753:INFO:  Epoch 297/500:  train Loss: 18.0661   val Loss: 23.3501   time: 432.72s   best: 22.7510
2023-10-29 05:16:04,207:INFO:  Epoch 298/500:  train Loss: 18.0589   val Loss: 23.8522   time: 433.43s   best: 22.7510
2023-10-29 05:23:20,763:INFO:  Epoch 299/500:  train Loss: 17.9859   val Loss: 23.7753   time: 436.53s   best: 22.7510
2023-10-29 05:30:37,238:INFO:  Epoch 300/500:  train Loss: 17.9098   val Loss: 23.1955   time: 436.46s   best: 22.7510
2023-10-29 05:37:53,661:INFO:  Epoch 301/500:  train Loss: 17.9267   val Loss: 23.1128   time: 436.41s   best: 22.7510
2023-10-29 05:45:12,071:INFO:  Epoch 302/500:  train Loss: 17.9881   val Loss: 23.8247   time: 438.38s   best: 22.7510
2023-10-29 05:52:26,589:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + no dropout)_95a9.pt
2023-10-29 05:52:26,612:INFO:  Epoch 303/500:  train Loss: 17.8834   val Loss: 22.3835   time: 434.48s   best: 22.3835
2023-10-29 05:59:41,230:INFO:  Epoch 304/500:  train Loss: 17.8380   val Loss: 23.3449   time: 434.62s   best: 22.3835
2023-10-29 06:06:57,492:INFO:  Epoch 305/500:  train Loss: 17.8018   val Loss: 23.9275   time: 436.25s   best: 22.3835
2023-10-29 06:14:14,918:INFO:  Epoch 306/500:  train Loss: 17.9230   val Loss: 23.6033   time: 437.40s   best: 22.3835
2023-10-29 06:21:28,815:INFO:  Epoch 307/500:  train Loss: 17.7350   val Loss: 23.1800   time: 433.87s   best: 22.3835
2023-10-29 06:28:41,698:INFO:  Epoch 308/500:  train Loss: 17.7913   val Loss: 23.2781   time: 432.85s   best: 22.3835
2023-10-29 06:35:58,672:INFO:  Epoch 309/500:  train Loss: 17.7639   val Loss: 23.4340   time: 436.96s   best: 22.3835
2023-10-29 06:43:11,946:INFO:  Epoch 310/500:  train Loss: 17.7581   val Loss: 23.7599   time: 433.25s   best: 22.3835
2023-10-29 06:50:26,676:INFO:  Epoch 311/500:  train Loss: 17.8468   val Loss: 23.0376   time: 434.71s   best: 22.3835
2023-10-29 06:57:44,396:INFO:  Epoch 312/500:  train Loss: 17.7496   val Loss: 23.6222   time: 437.70s   best: 22.3835
2023-10-29 07:05:01,558:INFO:  Epoch 313/500:  train Loss: 17.8333   val Loss: 23.0154   time: 437.15s   best: 22.3835
2023-10-29 07:12:18,940:INFO:  Epoch 314/500:  train Loss: 17.8808   val Loss: 23.7396   time: 437.35s   best: 22.3835
2023-10-29 07:19:36,200:INFO:  Epoch 315/500:  train Loss: 17.8581   val Loss: 24.1070   time: 437.22s   best: 22.3835
2023-10-29 07:26:53,556:INFO:  Epoch 316/500:  train Loss: 17.6681   val Loss: 23.7213   time: 437.33s   best: 22.3835
2023-10-29 07:34:10,610:INFO:  Epoch 317/500:  train Loss: 17.7890   val Loss: 23.1681   time: 437.03s   best: 22.3835
2023-10-29 07:41:23,282:INFO:  Epoch 318/500:  train Loss: 17.8611   val Loss: 23.4875   time: 432.65s   best: 22.3835
2023-10-29 07:48:37,268:INFO:  Epoch 319/500:  train Loss: 17.7520   val Loss: 23.1832   time: 433.96s   best: 22.3835
2023-10-29 07:55:53,334:INFO:  Epoch 320/500:  train Loss: 17.8402   val Loss: 23.5646   time: 436.04s   best: 22.3835
2023-10-29 08:03:07,176:INFO:  Epoch 321/500:  train Loss: 17.7457   val Loss: 23.2142   time: 433.83s   best: 22.3835
2023-10-29 08:10:24,148:INFO:  Epoch 322/500:  train Loss: 17.7709   val Loss: 23.6881   time: 436.94s   best: 22.3835
2023-10-29 08:17:39,981:INFO:  Epoch 323/500:  train Loss: 17.7317   val Loss: 22.8846   time: 435.81s   best: 22.3835
2023-10-29 08:24:53,497:INFO:  Epoch 324/500:  train Loss: 17.7246   val Loss: 23.5028   time: 433.49s   best: 22.3835
2023-10-29 08:32:11,142:INFO:  Epoch 325/500:  train Loss: 17.8490   val Loss: 23.6374   time: 437.62s   best: 22.3835
2023-10-29 08:39:28,790:INFO:  Epoch 326/500:  train Loss: 17.7334   val Loss: 23.2875   time: 437.62s   best: 22.3835
2023-10-29 08:46:45,787:INFO:  Epoch 327/500:  train Loss: 17.8425   val Loss: 24.1791   time: 436.99s   best: 22.3835
2023-10-29 08:54:03,983:INFO:  Epoch 328/500:  train Loss: 17.7681   val Loss: 22.4999   time: 438.17s   best: 22.3835
2023-10-29 09:01:16,961:INFO:  Epoch 329/500:  train Loss: 17.6830   val Loss: 24.1381   time: 432.94s   best: 22.3835
2023-10-29 09:08:34,865:INFO:  Epoch 330/500:  train Loss: 17.8263   val Loss: 26.1000   time: 437.89s   best: 22.3835
2023-10-29 09:15:48,913:INFO:  Epoch 331/500:  train Loss: 17.8648   val Loss: 23.1814   time: 434.02s   best: 22.3835
2023-10-29 09:23:05,129:INFO:  Epoch 332/500:  train Loss: 17.7855   val Loss: 23.3668   time: 436.19s   best: 22.3835
2023-10-29 09:30:22,908:INFO:  Epoch 333/500:  train Loss: 17.7350   val Loss: 23.3566   time: 437.75s   best: 22.3835
2023-10-29 09:37:35,589:INFO:  Epoch 334/500:  train Loss: 17.9612   val Loss: 23.1444   time: 432.64s   best: 22.3835
2023-10-29 09:44:47,874:INFO:  Epoch 335/500:  train Loss: 17.6430   val Loss: 23.5413   time: 432.25s   best: 22.3835
2023-10-29 09:52:02,419:INFO:  Epoch 336/500:  train Loss: 17.6341   val Loss: 23.6660   time: 434.51s   best: 22.3835
2023-10-29 09:59:14,047:INFO:  Epoch 337/500:  train Loss: 17.8029   val Loss: 23.2073   time: 431.62s   best: 22.3835
2023-10-29 10:06:30,964:INFO:  Epoch 338/500:  train Loss: 17.6738   val Loss: 23.9937   time: 436.89s   best: 22.3835
2023-10-29 10:13:47,667:INFO:  Epoch 339/500:  train Loss: 17.7538   val Loss: 23.8114   time: 436.69s   best: 22.3835
2023-10-29 10:20:59,763:INFO:  Epoch 340/500:  train Loss: 17.6821   val Loss: 23.7659   time: 432.07s   best: 22.3835
2023-10-29 10:28:12,430:INFO:  Epoch 341/500:  train Loss: 17.7760   val Loss: 23.8333   time: 432.62s   best: 22.3835
2023-10-29 10:35:29,429:INFO:  Epoch 342/500:  train Loss: 17.5677   val Loss: 23.7413   time: 436.97s   best: 22.3835
2023-10-29 10:42:46,388:INFO:  Epoch 343/500:  train Loss: 17.7617   val Loss: 23.2845   time: 436.93s   best: 22.3835
2023-10-29 10:50:00,635:INFO:  Epoch 344/500:  train Loss: 17.7067   val Loss: 22.8733   time: 434.21s   best: 22.3835
2023-10-29 10:57:13,384:INFO:  Epoch 345/500:  train Loss: 17.7513   val Loss: 23.7979   time: 432.72s   best: 22.3835
2023-10-29 11:04:25,491:INFO:  Epoch 346/500:  train Loss: 17.5811   val Loss: 23.8822   time: 432.08s   best: 22.3835
2023-10-29 11:11:41,012:INFO:  Epoch 347/500:  train Loss: 17.7240   val Loss: 23.1670   time: 435.51s   best: 22.3835
2023-10-29 11:18:57,419:INFO:  Epoch 348/500:  train Loss: 17.6856   val Loss: 23.4698   time: 436.37s   best: 22.3835
2023-10-29 11:26:13,878:INFO:  Epoch 349/500:  train Loss: 18.3915   val Loss: 23.4932   time: 436.44s   best: 22.3835
2023-10-29 11:33:29,876:INFO:  Epoch 350/500:  train Loss: 17.6626   val Loss: 22.9670   time: 435.96s   best: 22.3835
2023-10-29 11:40:43,915:INFO:  Epoch 351/500:  train Loss: 17.6017   val Loss: 23.2608   time: 434.01s   best: 22.3835
2023-10-29 11:43:28,779:INFO:  Starting experiment lstm autoencoder debug (2 layer + hidden)
2023-10-29 11:43:28,800:INFO:  Defining the model
2023-10-29 11:43:28,843:INFO:  Reading the dataset
2023-10-29 11:44:19,128:INFO:  Starting experiment lstm autoencoder debug (2 layer + hidden)
2023-10-29 11:44:19,129:INFO:  Defining the model
2023-10-29 11:44:19,173:INFO:  Reading the dataset
2023-10-29 11:44:25,237:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:25,258:INFO:  Epoch 1/500:  train Loss: 114.9115   val Loss: 106.9390   time: 1.52s   best: 106.9390
2023-10-29 11:44:25,499:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:25,520:INFO:  Epoch 2/500:  train Loss: 100.4393   val Loss: 95.1917   time: 0.24s   best: 95.1917
2023-10-29 11:44:25,782:INFO:  Epoch 3/500:  train Loss: 99.4510   val Loss: 100.1674   time: 0.26s   best: 95.1917
2023-10-29 11:44:26,055:INFO:  Epoch 4/500:  train Loss: 100.1097   val Loss: 100.2869   time: 0.27s   best: 95.1917
2023-10-29 11:44:26,311:INFO:  Epoch 5/500:  train Loss: 100.2889   val Loss: 100.3540   time: 0.25s   best: 95.1917
2023-10-29 11:44:26,549:INFO:  Epoch 6/500:  train Loss: 100.3809   val Loss: 100.3859   time: 0.24s   best: 95.1917
2023-10-29 11:44:26,808:INFO:  Epoch 7/500:  train Loss: 100.3868   val Loss: 100.3941   time: 0.26s   best: 95.1917
2023-10-29 11:44:27,045:INFO:  Epoch 8/500:  train Loss: 100.3673   val Loss: 100.3906   time: 0.23s   best: 95.1917
2023-10-29 11:44:27,283:INFO:  Epoch 9/500:  train Loss: 100.3824   val Loss: 100.3806   time: 0.23s   best: 95.1917
2023-10-29 11:44:27,540:INFO:  Epoch 10/500:  train Loss: 100.3590   val Loss: 100.3662   time: 0.26s   best: 95.1917
2023-10-29 11:44:27,787:INFO:  Epoch 11/500:  train Loss: 100.3330   val Loss: 100.3472   time: 0.24s   best: 95.1917
2023-10-29 11:44:28,066:INFO:  Epoch 12/500:  train Loss: 100.2977   val Loss: 100.3245   time: 0.28s   best: 95.1917
2023-10-29 11:44:28,320:INFO:  Epoch 13/500:  train Loss: 100.2489   val Loss: 100.2963   time: 0.24s   best: 95.1917
2023-10-29 11:44:28,576:INFO:  Epoch 14/500:  train Loss: 100.1004   val Loss: 100.2630   time: 0.25s   best: 95.1917
2023-10-29 11:44:28,815:INFO:  Epoch 15/500:  train Loss: 99.7784   val Loss: 99.6821   time: 0.24s   best: 95.1917
2023-10-29 11:44:29,071:INFO:  Epoch 16/500:  train Loss: 98.7250   val Loss: 97.8797   time: 0.25s   best: 95.1917
2023-10-29 11:44:29,308:INFO:  Epoch 17/500:  train Loss: 96.4817   val Loss: 95.2704   time: 0.23s   best: 95.1917
2023-10-29 11:44:29,566:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:29,587:INFO:  Epoch 18/500:  train Loss: 94.7611   val Loss: 94.4416   time: 0.25s   best: 94.4416
2023-10-29 11:44:29,835:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:29,858:INFO:  Epoch 19/500:  train Loss: 93.8534   val Loss: 94.1670   time: 0.24s   best: 94.1670
2023-10-29 11:44:30,148:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:30,169:INFO:  Epoch 20/500:  train Loss: 93.5574   val Loss: 94.0157   time: 0.28s   best: 94.0157
2023-10-29 11:44:30,408:INFO:  Epoch 21/500:  train Loss: 93.6999   val Loss: 94.0459   time: 0.24s   best: 94.0157
2023-10-29 11:44:30,664:INFO:  Epoch 22/500:  train Loss: 93.3977   val Loss: 94.1771   time: 0.25s   best: 94.0157
2023-10-29 11:44:30,903:INFO:  Epoch 23/500:  train Loss: 93.7537   val Loss: 94.1723   time: 0.24s   best: 94.0157
2023-10-29 11:44:31,161:INFO:  Epoch 24/500:  train Loss: 94.1301   val Loss: 94.2042   time: 0.26s   best: 94.0157
2023-10-29 11:44:31,399:INFO:  Epoch 25/500:  train Loss: 94.3890   val Loss: 94.2515   time: 0.24s   best: 94.0157
2023-10-29 11:44:31,657:INFO:  Epoch 26/500:  train Loss: 94.3145   val Loss: 94.3048   time: 0.26s   best: 94.0157
2023-10-29 11:44:31,904:INFO:  Epoch 27/500:  train Loss: 93.7264   val Loss: 94.3545   time: 0.24s   best: 94.0157
2023-10-29 11:44:32,179:INFO:  Epoch 28/500:  train Loss: 94.2438   val Loss: 94.2468   time: 0.27s   best: 94.0157
2023-10-29 11:44:32,436:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:32,457:INFO:  Epoch 29/500:  train Loss: 94.0982   val Loss: 93.9872   time: 0.25s   best: 93.9872
2023-10-29 11:44:32,713:INFO:  Epoch 30/500:  train Loss: 93.7387   val Loss: 94.0496   time: 0.25s   best: 93.9872
2023-10-29 11:44:32,953:INFO:  Epoch 31/500:  train Loss: 93.6322   val Loss: 94.0814   time: 0.24s   best: 93.9872
2023-10-29 11:44:33,209:INFO:  Epoch 32/500:  train Loss: 93.9751   val Loss: 94.1219   time: 0.25s   best: 93.9872
2023-10-29 11:44:33,448:INFO:  Epoch 33/500:  train Loss: 93.7227   val Loss: 94.1392   time: 0.24s   best: 93.9872
2023-10-29 11:44:33,707:INFO:  Epoch 34/500:  train Loss: 94.2726   val Loss: 94.1318   time: 0.26s   best: 93.9872
2023-10-29 11:44:33,955:INFO:  Epoch 35/500:  train Loss: 93.8389   val Loss: 94.1817   time: 0.24s   best: 93.9872
2023-10-29 11:44:34,213:INFO:  Epoch 36/500:  train Loss: 94.2778   val Loss: 94.1730   time: 0.25s   best: 93.9872
2023-10-29 11:44:34,484:INFO:  Epoch 37/500:  train Loss: 94.1769   val Loss: 94.0158   time: 0.27s   best: 93.9872
2023-10-29 11:44:34,739:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:34,760:INFO:  Epoch 38/500:  train Loss: 94.1555   val Loss: 93.9076   time: 0.25s   best: 93.9076
2023-10-29 11:44:35,002:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:35,023:INFO:  Epoch 39/500:  train Loss: 93.8949   val Loss: 93.7984   time: 0.24s   best: 93.7984
2023-10-29 11:44:35,281:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:35,302:INFO:  Epoch 40/500:  train Loss: 93.9047   val Loss: 93.6944   time: 0.25s   best: 93.6944
2023-10-29 11:44:35,542:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:35,563:INFO:  Epoch 41/500:  train Loss: 93.4632   val Loss: 93.5714   time: 0.24s   best: 93.5714
2023-10-29 11:44:35,829:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:35,849:INFO:  Epoch 42/500:  train Loss: 93.5639   val Loss: 93.2200   time: 0.26s   best: 93.2200
2023-10-29 11:44:36,089:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:36,112:INFO:  Epoch 43/500:  train Loss: 92.8396   val Loss: 92.7406   time: 0.24s   best: 92.7406
2023-10-29 11:44:36,405:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:36,425:INFO:  Epoch 44/500:  train Loss: 92.5858   val Loss: 92.2193   time: 0.29s   best: 92.2193
2023-10-29 11:44:36,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:36,686:INFO:  Epoch 45/500:  train Loss: 91.9177   val Loss: 91.6120   time: 0.24s   best: 91.6120
2023-10-29 11:44:36,945:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:36,966:INFO:  Epoch 46/500:  train Loss: 91.5098   val Loss: 90.8159   time: 0.25s   best: 90.8159
2023-10-29 11:44:37,206:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:37,227:INFO:  Epoch 47/500:  train Loss: 90.6913   val Loss: 89.5904   time: 0.24s   best: 89.5904
2023-10-29 11:44:37,486:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:37,507:INFO:  Epoch 48/500:  train Loss: 89.0891   val Loss: 89.0009   time: 0.25s   best: 89.0009
2023-10-29 11:44:37,747:INFO:  Epoch 49/500:  train Loss: 89.0361   val Loss: 89.5967   time: 0.24s   best: 89.0009
2023-10-29 11:44:38,009:INFO:  Epoch 50/500:  train Loss: 89.7604   val Loss: 89.7483   time: 0.26s   best: 89.0009
2023-10-29 11:44:38,248:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:38,272:INFO:  Epoch 51/500:  train Loss: 89.3561   val Loss: 88.2313   time: 0.23s   best: 88.2313
2023-10-29 11:44:38,565:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:38,586:INFO:  Epoch 52/500:  train Loss: 88.1457   val Loss: 87.7074   time: 0.29s   best: 87.7074
2023-10-29 11:44:38,825:INFO:  Epoch 53/500:  train Loss: 87.8032   val Loss: 88.1519   time: 0.24s   best: 87.7074
2023-10-29 11:44:39,166:INFO:  Epoch 54/500:  train Loss: 88.5013   val Loss: 88.5314   time: 0.34s   best: 87.7074
2023-10-29 11:44:39,402:INFO:  Epoch 55/500:  train Loss: 88.8275   val Loss: 87.9484   time: 0.23s   best: 87.7074
2023-10-29 11:44:39,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:39,687:INFO:  Epoch 56/500:  train Loss: 87.7636   val Loss: 87.2544   time: 0.26s   best: 87.2544
2023-10-29 11:44:39,933:INFO:  Epoch 57/500:  train Loss: 87.6269   val Loss: 87.3690   time: 0.24s   best: 87.2544
2023-10-29 11:44:40,189:INFO:  Epoch 58/500:  train Loss: 87.9384   val Loss: 88.0179   time: 0.25s   best: 87.2544
2023-10-29 11:44:40,430:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:40,453:INFO:  Epoch 59/500:  train Loss: 87.6161   val Loss: 87.0157   time: 0.24s   best: 87.0157
2023-10-29 11:44:40,738:INFO:  Epoch 60/500:  train Loss: 90.7188   val Loss: 88.2934   time: 0.28s   best: 87.0157
2023-10-29 11:44:40,978:INFO:  Epoch 61/500:  train Loss: 89.0125   val Loss: 90.5796   time: 0.24s   best: 87.0157
2023-10-29 11:44:41,237:INFO:  Epoch 62/500:  train Loss: 91.2662   val Loss: 90.7856   time: 0.26s   best: 87.0157
2023-10-29 11:44:41,476:INFO:  Epoch 63/500:  train Loss: 89.9916   val Loss: 88.8605   time: 0.24s   best: 87.0157
2023-10-29 11:44:41,732:INFO:  Epoch 64/500:  train Loss: 88.7480   val Loss: 88.0378   time: 0.25s   best: 87.0157
2023-10-29 11:44:41,978:INFO:  Epoch 65/500:  train Loss: 88.8580   val Loss: 87.8607   time: 0.24s   best: 87.0157
2023-10-29 11:44:42,216:INFO:  Epoch 66/500:  train Loss: 87.6342   val Loss: 87.3057   time: 0.23s   best: 87.0157
2023-10-29 11:44:42,476:INFO:  Epoch 67/500:  train Loss: 87.6302   val Loss: 87.1498   time: 0.26s   best: 87.0157
2023-10-29 11:44:42,768:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:42,789:INFO:  Epoch 68/500:  train Loss: 87.7219   val Loss: 86.5685   time: 0.27s   best: 86.5685
2023-10-29 11:44:43,028:INFO:  Epoch 69/500:  train Loss: 87.8173   val Loss: 86.9938   time: 0.24s   best: 86.5685
2023-10-29 11:44:43,284:INFO:  Epoch 70/500:  train Loss: 87.5007   val Loss: 87.4613   time: 0.23s   best: 86.5685
2023-10-29 11:44:43,522:INFO:  Epoch 71/500:  train Loss: 88.0267   val Loss: 87.2972   time: 0.24s   best: 86.5685
2023-10-29 11:44:43,768:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:43,790:INFO:  Epoch 72/500:  train Loss: 86.7819   val Loss: 86.4883   time: 0.24s   best: 86.4883
2023-10-29 11:44:44,049:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:44,069:INFO:  Epoch 73/500:  train Loss: 86.5269   val Loss: 86.1840   time: 0.25s   best: 86.1840
2023-10-29 11:44:44,310:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:44,331:INFO:  Epoch 74/500:  train Loss: 86.4406   val Loss: 86.0207   time: 0.24s   best: 86.0207
2023-10-29 11:44:44,590:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:44,644:INFO:  Epoch 75/500:  train Loss: 86.1369   val Loss: 86.0022   time: 0.24s   best: 86.0022
2023-10-29 11:44:44,903:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:44,924:INFO:  Epoch 76/500:  train Loss: 85.9332   val Loss: 85.5286   time: 0.25s   best: 85.5286
2023-10-29 11:44:45,165:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:45,185:INFO:  Epoch 77/500:  train Loss: 85.8552   val Loss: 85.0216   time: 0.24s   best: 85.0216
2023-10-29 11:44:45,443:INFO:  Epoch 78/500:  train Loss: 85.6692   val Loss: 85.3558   time: 0.26s   best: 85.0216
2023-10-29 11:44:45,681:INFO:  Epoch 79/500:  train Loss: 86.1725   val Loss: 85.5582   time: 0.23s   best: 85.0216
2023-10-29 11:44:45,945:INFO:  Epoch 80/500:  train Loss: 86.2932   val Loss: 86.1091   time: 0.26s   best: 85.0216
2023-10-29 11:44:46,182:INFO:  Epoch 81/500:  train Loss: 86.1411   val Loss: 85.1062   time: 0.24s   best: 85.0216
2023-10-29 11:44:46,421:INFO:  Epoch 82/500:  train Loss: 86.2297   val Loss: 85.7989   time: 0.24s   best: 85.0216
2023-10-29 11:44:46,712:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:46,735:INFO:  Epoch 83/500:  train Loss: 85.2640   val Loss: 84.6585   time: 0.29s   best: 84.6585
2023-10-29 11:44:46,994:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:47,016:INFO:  Epoch 84/500:  train Loss: 86.0717   val Loss: 84.5251   time: 0.25s   best: 84.5251
2023-10-29 11:44:47,253:INFO:  Epoch 85/500:  train Loss: 84.7827   val Loss: 84.5949   time: 0.23s   best: 84.5251
2023-10-29 11:44:47,513:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:47,534:INFO:  Epoch 86/500:  train Loss: 84.8981   val Loss: 84.2845   time: 0.24s   best: 84.2845
2023-10-29 11:44:47,776:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:47,801:INFO:  Epoch 87/500:  train Loss: 84.7411   val Loss: 83.7324   time: 0.24s   best: 83.7324
2023-10-29 11:44:48,056:INFO:  Epoch 88/500:  train Loss: 83.9708   val Loss: 83.7326   time: 0.25s   best: 83.7324
2023-10-29 11:44:48,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:48,316:INFO:  Epoch 89/500:  train Loss: 84.9347   val Loss: 83.2243   time: 0.23s   best: 83.2243
2023-10-29 11:44:48,574:INFO:  Epoch 90/500:  train Loss: 84.0430   val Loss: 83.3928   time: 0.25s   best: 83.2243
2023-10-29 11:44:48,847:INFO:  Epoch 91/500:  train Loss: 83.8551   val Loss: 84.2337   time: 0.27s   best: 83.2243
2023-10-29 11:44:49,102:INFO:  Epoch 92/500:  train Loss: 88.6679   val Loss: 91.1578   time: 0.25s   best: 83.2243
2023-10-29 11:44:49,342:INFO:  Epoch 93/500:  train Loss: 91.7423   val Loss: 90.9875   time: 0.24s   best: 83.2243
2023-10-29 11:44:49,601:INFO:  Epoch 94/500:  train Loss: 89.8992   val Loss: 88.6986   time: 0.24s   best: 83.2243
2023-10-29 11:44:49,846:INFO:  Epoch 95/500:  train Loss: 89.2084   val Loss: 89.2032   time: 0.24s   best: 83.2243
2023-10-29 11:44:50,084:INFO:  Epoch 96/500:  train Loss: 89.5584   val Loss: 88.6252   time: 0.24s   best: 83.2243
2023-10-29 11:44:50,341:INFO:  Epoch 97/500:  train Loss: 87.8443   val Loss: 86.6058   time: 0.25s   best: 83.2243
2023-10-29 11:44:50,579:INFO:  Epoch 98/500:  train Loss: 86.1521   val Loss: 85.0719   time: 0.23s   best: 83.2243
2023-10-29 11:44:50,905:INFO:  Epoch 99/500:  train Loss: 85.0710   val Loss: 84.4417   time: 0.32s   best: 83.2243
2023-10-29 11:44:51,231:INFO:  Epoch 100/500:  train Loss: 84.3943   val Loss: 84.1576   time: 0.30s   best: 83.2243
2023-10-29 11:44:51,478:INFO:  Epoch 101/500:  train Loss: 84.3375   val Loss: 83.7574   time: 0.24s   best: 83.2243
2023-10-29 11:44:51,743:INFO:  Epoch 102/500:  train Loss: 84.5327   val Loss: 83.2760   time: 0.26s   best: 83.2243
2023-10-29 11:44:52,017:INFO:  Epoch 103/500:  train Loss: 83.7695   val Loss: 83.2427   time: 0.26s   best: 83.2243
2023-10-29 11:44:52,275:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:52,304:INFO:  Epoch 104/500:  train Loss: 83.3279   val Loss: 82.7229   time: 0.25s   best: 82.7229
2023-10-29 11:44:52,552:INFO:  Epoch 105/500:  train Loss: 83.8312   val Loss: 82.9605   time: 0.24s   best: 82.7229
2023-10-29 11:44:52,809:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:52,839:INFO:  Epoch 106/500:  train Loss: 83.9686   val Loss: 82.7183   time: 0.25s   best: 82.7183
2023-10-29 11:44:53,112:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:53,144:INFO:  Epoch 107/500:  train Loss: 83.1918   val Loss: 82.4446   time: 0.27s   best: 82.4446
2023-10-29 11:44:53,406:INFO:  Epoch 108/500:  train Loss: 83.1169   val Loss: 83.2611   time: 0.25s   best: 82.4446
2023-10-29 11:44:53,646:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:53,666:INFO:  Epoch 109/500:  train Loss: 83.2138   val Loss: 81.6949   time: 0.24s   best: 81.6949
2023-10-29 11:44:53,957:INFO:  Epoch 110/500:  train Loss: 82.4638   val Loss: 81.7572   time: 0.26s   best: 81.6949
2023-10-29 11:44:54,203:INFO:  Epoch 111/500:  train Loss: 82.0870   val Loss: 81.7441   time: 0.23s   best: 81.6949
2023-10-29 11:44:54,474:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:54,497:INFO:  Epoch 112/500:  train Loss: 82.6154   val Loss: 81.4662   time: 0.27s   best: 81.4662
2023-10-29 11:44:54,737:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:54,760:INFO:  Epoch 113/500:  train Loss: 81.7103   val Loss: 80.9371   time: 0.24s   best: 80.9371
2023-10-29 11:44:55,074:INFO:  Epoch 114/500:  train Loss: 82.1296   val Loss: 81.0210   time: 0.30s   best: 80.9371
2023-10-29 11:44:55,313:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:55,345:INFO:  Epoch 115/500:  train Loss: 82.8774   val Loss: 80.4977   time: 0.23s   best: 80.4977
2023-10-29 11:44:55,609:INFO:  Epoch 116/500:  train Loss: 81.0689   val Loss: 80.9082   time: 0.24s   best: 80.4977
2023-10-29 11:44:55,863:INFO:  Epoch 117/500:  train Loss: 82.0688   val Loss: 80.9431   time: 0.24s   best: 80.4977
2023-10-29 11:44:56,129:INFO:  Epoch 118/500:  train Loss: 81.5936   val Loss: 80.6082   time: 0.26s   best: 80.4977
2023-10-29 11:44:56,369:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:56,398:INFO:  Epoch 119/500:  train Loss: 81.0918   val Loss: 80.4233   time: 0.24s   best: 80.4233
2023-10-29 11:44:56,657:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:44:56,686:INFO:  Epoch 120/500:  train Loss: 82.8085   val Loss: 79.8056   time: 0.25s   best: 79.8056
2023-10-29 11:44:56,934:INFO:  Epoch 121/500:  train Loss: 81.4532   val Loss: 80.8012   time: 0.24s   best: 79.8056
2023-10-29 11:44:57,231:INFO:  Epoch 122/500:  train Loss: 82.2182   val Loss: 80.4555   time: 0.29s   best: 79.8056
2023-10-29 11:44:57,499:INFO:  Epoch 123/500:  train Loss: 81.4283   val Loss: 81.3751   time: 0.24s   best: 79.8056
2023-10-29 11:44:57,744:INFO:  Epoch 124/500:  train Loss: 81.6867   val Loss: 80.7689   time: 0.23s   best: 79.8056
2023-10-29 11:44:57,998:INFO:  Epoch 125/500:  train Loss: 81.6006   val Loss: 80.4063   time: 0.24s   best: 79.8056
2023-10-29 11:44:58,263:INFO:  Epoch 126/500:  train Loss: 81.3557   val Loss: 80.3487   time: 0.25s   best: 79.8056
2023-10-29 11:44:58,510:INFO:  Epoch 127/500:  train Loss: 81.0125   val Loss: 80.0775   time: 0.24s   best: 79.8056
2023-10-29 11:44:58,775:INFO:  Epoch 128/500:  train Loss: 82.3430   val Loss: 80.1744   time: 0.25s   best: 79.8056
2023-10-29 11:44:59,044:INFO:  Epoch 129/500:  train Loss: 81.1796   val Loss: 80.5934   time: 0.24s   best: 79.8056
2023-10-29 11:44:59,342:INFO:  Epoch 130/500:  train Loss: 80.2716   val Loss: 80.8708   time: 0.29s   best: 79.8056
2023-10-29 11:44:59,591:INFO:  Epoch 131/500:  train Loss: 83.1609   val Loss: 83.0086   time: 0.24s   best: 79.8056
2023-10-29 11:44:59,863:INFO:  Epoch 132/500:  train Loss: 83.0962   val Loss: 82.6617   time: 0.26s   best: 79.8056
2023-10-29 11:45:00,109:INFO:  Epoch 133/500:  train Loss: 84.0286   val Loss: 82.9484   time: 0.24s   best: 79.8056
2023-10-29 11:45:00,374:INFO:  Epoch 134/500:  train Loss: 82.0733   val Loss: 81.0661   time: 0.25s   best: 79.8056
2023-10-29 11:45:00,620:INFO:  Epoch 135/500:  train Loss: 81.2806   val Loss: 80.1678   time: 0.24s   best: 79.8056
2023-10-29 11:45:00,880:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:00,910:INFO:  Epoch 136/500:  train Loss: 80.3811   val Loss: 79.7161   time: 0.26s   best: 79.7161
2023-10-29 11:45:01,149:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:01,178:INFO:  Epoch 137/500:  train Loss: 79.9516   val Loss: 79.6916   time: 0.23s   best: 79.6916
2023-10-29 11:45:01,469:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:01,498:INFO:  Epoch 138/500:  train Loss: 80.1749   val Loss: 79.2710   time: 0.29s   best: 79.2710
2023-10-29 11:45:01,763:INFO:  Epoch 139/500:  train Loss: 80.3079   val Loss: 79.6135   time: 0.25s   best: 79.2710
2023-10-29 11:45:02,011:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:02,040:INFO:  Epoch 140/500:  train Loss: 81.6474   val Loss: 79.1841   time: 0.24s   best: 79.1841
2023-10-29 11:45:02,305:INFO:  Epoch 141/500:  train Loss: 79.6295   val Loss: 79.6014   time: 0.25s   best: 79.1841
2023-10-29 11:45:02,552:INFO:  Epoch 142/500:  train Loss: 80.6799   val Loss: 79.5080   time: 0.24s   best: 79.1841
2023-10-29 11:45:02,812:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:02,841:INFO:  Epoch 143/500:  train Loss: 79.7730   val Loss: 78.9645   time: 0.26s   best: 78.9645
2023-10-29 11:45:03,083:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:03,112:INFO:  Epoch 144/500:  train Loss: 79.5919   val Loss: 78.6056   time: 0.24s   best: 78.6056
2023-10-29 11:45:03,406:INFO:  Epoch 145/500:  train Loss: 79.3877   val Loss: 78.6527   time: 0.28s   best: 78.6056
2023-10-29 11:45:03,653:INFO:  Epoch 146/500:  train Loss: 79.3769   val Loss: 78.6159   time: 0.24s   best: 78.6056
2023-10-29 11:45:03,918:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:03,947:INFO:  Epoch 147/500:  train Loss: 80.1508   val Loss: 78.1535   time: 0.26s   best: 78.1535
2023-10-29 11:45:04,193:INFO:  Epoch 148/500:  train Loss: 79.2187   val Loss: 78.2361   time: 0.24s   best: 78.1535
2023-10-29 11:45:04,452:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:04,481:INFO:  Epoch 149/500:  train Loss: 79.1013   val Loss: 78.0641   time: 0.25s   best: 78.0641
2023-10-29 11:45:04,729:INFO:  Epoch 150/500:  train Loss: 79.3292   val Loss: 78.5181   time: 0.24s   best: 78.0641
2023-10-29 11:45:04,997:INFO:  Epoch 151/500:  train Loss: 80.4538   val Loss: 78.7093   time: 0.26s   best: 78.0641
2023-10-29 11:45:05,243:INFO:  Epoch 152/500:  train Loss: 82.2481   val Loss: 82.1389   time: 0.24s   best: 78.0641
2023-10-29 11:45:05,540:INFO:  Epoch 153/500:  train Loss: 81.0155   val Loss: 80.8338   time: 0.29s   best: 78.0641
2023-10-29 11:45:05,787:INFO:  Epoch 154/500:  train Loss: 80.6929   val Loss: 81.2010   time: 0.24s   best: 78.0641
2023-10-29 11:45:06,058:INFO:  Epoch 155/500:  train Loss: 92.0025   val Loss: 95.1331   time: 0.26s   best: 78.0641
2023-10-29 11:45:06,303:INFO:  Epoch 156/500:  train Loss: 96.3909   val Loss: 96.0367   time: 0.23s   best: 78.0641
2023-10-29 11:45:06,571:INFO:  Epoch 157/500:  train Loss: 94.9342   val Loss: 93.5835   time: 0.26s   best: 78.0641
2023-10-29 11:45:06,819:INFO:  Epoch 158/500:  train Loss: 92.3243   val Loss: 90.9604   time: 0.24s   best: 78.0641
2023-10-29 11:45:07,086:INFO:  Epoch 159/500:  train Loss: 90.6077   val Loss: 90.2351   time: 0.26s   best: 78.0641
2023-10-29 11:45:07,332:INFO:  Epoch 160/500:  train Loss: 90.5994   val Loss: 90.2325   time: 0.23s   best: 78.0641
2023-10-29 11:45:07,626:INFO:  Epoch 161/500:  train Loss: 90.4139   val Loss: 89.8971   time: 0.28s   best: 78.0641
2023-10-29 11:45:07,880:INFO:  Epoch 162/500:  train Loss: 89.7609   val Loss: 89.3700   time: 0.24s   best: 78.0641
2023-10-29 11:45:08,146:INFO:  Epoch 163/500:  train Loss: 89.2281   val Loss: 89.0364   time: 0.25s   best: 78.0641
2023-10-29 11:45:08,393:INFO:  Epoch 164/500:  train Loss: 88.8932   val Loss: 88.5537   time: 0.24s   best: 78.0641
2023-10-29 11:45:08,659:INFO:  Epoch 165/500:  train Loss: 88.0272   val Loss: 87.9796   time: 0.26s   best: 78.0641
2023-10-29 11:45:08,907:INFO:  Epoch 166/500:  train Loss: 87.8039   val Loss: 87.3408   time: 0.24s   best: 78.0641
2023-10-29 11:45:09,173:INFO:  Epoch 167/500:  train Loss: 86.8737   val Loss: 86.6188   time: 0.26s   best: 78.0641
2023-10-29 11:45:09,419:INFO:  Epoch 168/500:  train Loss: 86.3589   val Loss: 85.7841   time: 0.24s   best: 78.0641
2023-10-29 11:45:09,713:INFO:  Epoch 169/500:  train Loss: 85.6213   val Loss: 84.6979   time: 0.28s   best: 78.0641
2023-10-29 11:45:09,974:INFO:  Epoch 170/500:  train Loss: 84.2178   val Loss: 83.3333   time: 0.26s   best: 78.0641
2023-10-29 11:45:10,245:INFO:  Epoch 171/500:  train Loss: 82.9692   val Loss: 81.9312   time: 0.26s   best: 78.0641
2023-10-29 11:45:10,491:INFO:  Epoch 172/500:  train Loss: 81.4199   val Loss: 80.5599   time: 0.24s   best: 78.0641
2023-10-29 11:45:10,759:INFO:  Epoch 173/500:  train Loss: 81.1573   val Loss: 79.9626   time: 0.26s   best: 78.0641
2023-10-29 11:45:11,010:INFO:  Epoch 174/500:  train Loss: 80.0719   val Loss: 79.3275   time: 0.24s   best: 78.0641
2023-10-29 11:45:11,274:INFO:  Epoch 175/500:  train Loss: 79.6980   val Loss: 78.7805   time: 0.25s   best: 78.0641
2023-10-29 11:45:11,520:INFO:  Epoch 176/500:  train Loss: 78.8551   val Loss: 78.3208   time: 0.24s   best: 78.0641
2023-10-29 11:45:11,812:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:11,844:INFO:  Epoch 177/500:  train Loss: 78.4649   val Loss: 77.6947   time: 0.29s   best: 77.6947
2023-10-29 11:45:12,092:INFO:  Epoch 178/500:  train Loss: 79.7627   val Loss: 78.0102   time: 0.24s   best: 77.6947
2023-10-29 11:45:12,357:INFO:  Epoch 179/500:  train Loss: 79.1292   val Loss: 78.7666   time: 0.25s   best: 77.6947
2023-10-29 11:45:12,604:INFO:  Epoch 180/500:  train Loss: 78.9147   val Loss: 77.8730   time: 0.24s   best: 77.6947
2023-10-29 11:45:12,865:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:12,895:INFO:  Epoch 181/500:  train Loss: 79.0179   val Loss: 77.5355   time: 0.26s   best: 77.5355
2023-10-29 11:45:13,143:INFO:  Epoch 182/500:  train Loss: 78.6555   val Loss: 77.6490   time: 0.24s   best: 77.5355
2023-10-29 11:45:13,397:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:13,427:INFO:  Epoch 183/500:  train Loss: 77.9926   val Loss: 77.0846   time: 0.25s   best: 77.0846
2023-10-29 11:45:13,674:INFO:  Epoch 184/500:  train Loss: 78.4360   val Loss: 77.3422   time: 0.24s   best: 77.0846
2023-10-29 11:45:14,004:INFO:  Epoch 185/500:  train Loss: 78.4098   val Loss: 77.3551   time: 0.32s   best: 77.0846
2023-10-29 11:45:14,250:INFO:  Epoch 186/500:  train Loss: 78.1993   val Loss: 77.6922   time: 0.24s   best: 77.0846
2023-10-29 11:45:14,518:INFO:  Epoch 187/500:  train Loss: 78.7079   val Loss: 77.4982   time: 0.25s   best: 77.0846
2023-10-29 11:45:14,764:INFO:  Epoch 188/500:  train Loss: 78.1172   val Loss: 77.4868   time: 0.24s   best: 77.0846
2023-10-29 11:45:15,031:INFO:  Epoch 189/500:  train Loss: 78.1491   val Loss: 77.5454   time: 0.26s   best: 77.0846
2023-10-29 11:45:15,278:INFO:  Epoch 190/500:  train Loss: 78.9644   val Loss: 77.4673   time: 0.24s   best: 77.0846
2023-10-29 11:45:15,537:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:15,567:INFO:  Epoch 191/500:  train Loss: 78.2376   val Loss: 76.5920   time: 0.26s   best: 76.5920
2023-10-29 11:45:15,821:INFO:  Epoch 192/500:  train Loss: 79.4639   val Loss: 77.3284   time: 0.24s   best: 76.5920
2023-10-29 11:45:16,115:INFO:  Epoch 193/500:  train Loss: 77.5425   val Loss: 77.3881   time: 0.28s   best: 76.5920
2023-10-29 11:45:16,362:INFO:  Epoch 194/500:  train Loss: 78.5483   val Loss: 77.3507   time: 0.24s   best: 76.5920
2023-10-29 11:45:16,630:INFO:  Epoch 195/500:  train Loss: 78.0783   val Loss: 77.0273   time: 0.26s   best: 76.5920
2023-10-29 11:45:16,878:INFO:  Epoch 196/500:  train Loss: 77.6213   val Loss: 76.9301   time: 0.24s   best: 76.5920
2023-10-29 11:45:17,143:INFO:  Epoch 197/500:  train Loss: 78.3376   val Loss: 76.7630   time: 0.25s   best: 76.5920
2023-10-29 11:45:17,383:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:17,412:INFO:  Epoch 198/500:  train Loss: 77.6478   val Loss: 76.2755   time: 0.24s   best: 76.2755
2023-10-29 11:45:17,679:INFO:  Epoch 199/500:  train Loss: 78.3446   val Loss: 76.3369   time: 0.26s   best: 76.2755
2023-10-29 11:45:18,022:INFO:  Epoch 200/500:  train Loss: 77.3286   val Loss: 77.0361   time: 0.32s   best: 76.2755
2023-10-29 11:45:18,285:INFO:  Epoch 201/500:  train Loss: 78.4586   val Loss: 78.0479   time: 0.25s   best: 76.2755
2023-10-29 11:45:18,532:INFO:  Epoch 202/500:  train Loss: 78.0057   val Loss: 77.7942   time: 0.24s   best: 76.2755
2023-10-29 11:45:18,799:INFO:  Epoch 203/500:  train Loss: 78.0754   val Loss: 77.3440   time: 0.25s   best: 76.2755
2023-10-29 11:45:19,048:INFO:  Epoch 204/500:  train Loss: 78.3834   val Loss: 77.1586   time: 0.24s   best: 76.2755
2023-10-29 11:45:19,306:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:19,335:INFO:  Epoch 205/500:  train Loss: 76.6276   val Loss: 76.0531   time: 0.25s   best: 76.0531
2023-10-29 11:45:19,577:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:19,624:INFO:  Epoch 206/500:  train Loss: 77.9511   val Loss: 75.8512   time: 0.24s   best: 75.8512
2023-10-29 11:45:19,868:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:19,900:INFO:  Epoch 207/500:  train Loss: 76.7299   val Loss: 75.7800   time: 0.24s   best: 75.7800
2023-10-29 11:45:20,195:INFO:  Epoch 208/500:  train Loss: 76.8689   val Loss: 76.3748   time: 0.28s   best: 75.7800
2023-10-29 11:45:20,442:INFO:  Epoch 209/500:  train Loss: 78.5575   val Loss: 76.6466   time: 0.24s   best: 75.7800
2023-10-29 11:45:20,708:INFO:  Epoch 210/500:  train Loss: 78.3976   val Loss: 77.4958   time: 0.26s   best: 75.7800
2023-10-29 11:45:20,958:INFO:  Epoch 211/500:  train Loss: 77.8807   val Loss: 76.8497   time: 0.24s   best: 75.7800
2023-10-29 11:45:21,222:INFO:  Epoch 212/500:  train Loss: 79.2148   val Loss: 77.7013   time: 0.25s   best: 75.7800
2023-10-29 11:45:21,469:INFO:  Epoch 213/500:  train Loss: 77.0390   val Loss: 77.2957   time: 0.24s   best: 75.7800
2023-10-29 11:45:21,734:INFO:  Epoch 214/500:  train Loss: 77.5918   val Loss: 76.3312   time: 0.25s   best: 75.7800
2023-10-29 11:45:21,978:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:22,007:INFO:  Epoch 215/500:  train Loss: 78.3390   val Loss: 75.5371   time: 0.24s   best: 75.5371
2023-10-29 11:45:22,333:INFO:  Epoch 216/500:  train Loss: 76.9153   val Loss: 75.9300   time: 0.32s   best: 75.5371
2023-10-29 11:45:22,580:INFO:  Epoch 217/500:  train Loss: 80.9501   val Loss: 76.5406   time: 0.24s   best: 75.5371
2023-10-29 11:45:22,843:INFO:  Epoch 218/500:  train Loss: 76.6577   val Loss: 77.0710   time: 0.25s   best: 75.5371
2023-10-29 11:45:23,092:INFO:  Epoch 219/500:  train Loss: 77.5065   val Loss: 76.3809   time: 0.24s   best: 75.5371
2023-10-29 11:45:23,351:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:23,380:INFO:  Epoch 220/500:  train Loss: 76.8964   val Loss: 75.0591   time: 0.25s   best: 75.0591
2023-10-29 11:45:23,621:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:23,650:INFO:  Epoch 221/500:  train Loss: 75.4804   val Loss: 74.7620   time: 0.24s   best: 74.7620
2023-10-29 11:45:23,919:INFO:  Epoch 222/500:  train Loss: 76.5077   val Loss: 76.1601   time: 0.26s   best: 74.7620
2023-10-29 11:45:24,166:INFO:  Epoch 223/500:  train Loss: 77.3289   val Loss: 76.5968   time: 0.24s   best: 74.7620
2023-10-29 11:45:24,463:INFO:  Epoch 224/500:  train Loss: 77.0665   val Loss: 75.8334   time: 0.29s   best: 74.7620
2023-10-29 11:45:24,704:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:24,734:INFO:  Epoch 225/500:  train Loss: 75.3462   val Loss: 74.6022   time: 0.24s   best: 74.6022
2023-10-29 11:45:25,001:INFO:  Epoch 226/500:  train Loss: 76.5476   val Loss: 74.9425   time: 0.26s   best: 74.6022
2023-10-29 11:45:25,254:INFO:  Epoch 227/500:  train Loss: 76.0151   val Loss: 75.7293   time: 0.25s   best: 74.6022
2023-10-29 11:45:25,527:INFO:  Epoch 228/500:  train Loss: 78.6528   val Loss: 80.6165   time: 0.26s   best: 74.6022
2023-10-29 11:45:25,774:INFO:  Epoch 229/500:  train Loss: 88.9151   val Loss: 91.5520   time: 0.24s   best: 74.6022
2023-10-29 11:45:26,044:INFO:  Epoch 230/500:  train Loss: 91.0086   val Loss: 89.5093   time: 0.26s   best: 74.6022
2023-10-29 11:45:26,291:INFO:  Epoch 231/500:  train Loss: 89.0996   val Loss: 88.0863   time: 0.24s   best: 74.6022
2023-10-29 11:45:26,593:INFO:  Epoch 232/500:  train Loss: 88.7999   val Loss: 88.4125   time: 0.29s   best: 74.6022
2023-10-29 11:45:26,840:INFO:  Epoch 233/500:  train Loss: 88.4571   val Loss: 87.9562   time: 0.24s   best: 74.6022
2023-10-29 11:45:27,106:INFO:  Epoch 234/500:  train Loss: 88.1899   val Loss: 88.0316   time: 0.26s   best: 74.6022
2023-10-29 11:45:27,352:INFO:  Epoch 235/500:  train Loss: 87.7587   val Loss: 87.1648   time: 0.24s   best: 74.6022
2023-10-29 11:45:27,618:INFO:  Epoch 236/500:  train Loss: 86.6558   val Loss: 85.8860   time: 0.25s   best: 74.6022
2023-10-29 11:45:27,866:INFO:  Epoch 237/500:  train Loss: 85.6318   val Loss: 84.3449   time: 0.24s   best: 74.6022
2023-10-29 11:45:28,135:INFO:  Epoch 238/500:  train Loss: 83.7218   val Loss: 82.2703   time: 0.26s   best: 74.6022
2023-10-29 11:45:28,382:INFO:  Epoch 239/500:  train Loss: 81.5968   val Loss: 80.2830   time: 0.24s   best: 74.6022
2023-10-29 11:45:28,679:INFO:  Epoch 240/500:  train Loss: 79.6546   val Loss: 78.1829   time: 0.29s   best: 74.6022
2023-10-29 11:45:28,928:INFO:  Epoch 241/500:  train Loss: 77.6801   val Loss: 76.3498   time: 0.24s   best: 74.6022
2023-10-29 11:45:29,194:INFO:  Epoch 242/500:  train Loss: 76.8236   val Loss: 74.7056   time: 0.26s   best: 74.6022
2023-10-29 11:45:29,439:INFO:  Epoch 243/500:  train Loss: 76.6755   val Loss: 75.8584   time: 0.23s   best: 74.6022
2023-10-29 11:45:29,705:INFO:  Epoch 244/500:  train Loss: 77.1146   val Loss: 75.4326   time: 0.25s   best: 74.6022
2023-10-29 11:45:29,949:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:29,978:INFO:  Epoch 245/500:  train Loss: 76.2518   val Loss: 74.3427   time: 0.24s   best: 74.3427
2023-10-29 11:45:30,243:INFO:  Epoch 246/500:  train Loss: 75.6151   val Loss: 75.6369   time: 0.25s   best: 74.3427
2023-10-29 11:45:30,508:INFO:  Epoch 247/500:  train Loss: 76.9819   val Loss: 76.0322   time: 0.26s   best: 74.3427
2023-10-29 11:45:30,789:INFO:  Epoch 248/500:  train Loss: 76.1858   val Loss: 74.9375   time: 0.27s   best: 74.3427
2023-10-29 11:45:31,038:INFO:  Epoch 249/500:  train Loss: 76.9262   val Loss: 74.5066   time: 0.24s   best: 74.3427
2023-10-29 11:45:31,302:INFO:  Epoch 250/500:  train Loss: 75.1431   val Loss: 74.4140   time: 0.25s   best: 74.3427
2023-10-29 11:45:31,550:INFO:  Epoch 251/500:  train Loss: 75.4618   val Loss: 74.4615   time: 0.24s   best: 74.3427
2023-10-29 11:45:31,808:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:31,838:INFO:  Epoch 252/500:  train Loss: 75.0994   val Loss: 73.6633   time: 0.25s   best: 73.6633
2023-10-29 11:45:32,090:INFO:  Epoch 253/500:  train Loss: 75.3646   val Loss: 74.1730   time: 0.24s   best: 73.6633
2023-10-29 11:45:32,348:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:32,377:INFO:  Epoch 254/500:  train Loss: 76.5924   val Loss: 73.6556   time: 0.25s   best: 73.6556
2023-10-29 11:45:32,655:INFO:  Epoch 255/500:  train Loss: 75.1097   val Loss: 73.8207   time: 0.27s   best: 73.6556
2023-10-29 11:45:32,920:INFO:  Epoch 256/500:  train Loss: 76.6551   val Loss: 75.4136   time: 0.25s   best: 73.6556
2023-10-29 11:45:33,167:INFO:  Epoch 257/500:  train Loss: 76.0088   val Loss: 76.1606   time: 0.24s   best: 73.6556
2023-10-29 11:45:33,433:INFO:  Epoch 258/500:  train Loss: 76.6482   val Loss: 76.7139   time: 0.25s   best: 73.6556
2023-10-29 11:45:33,679:INFO:  Epoch 259/500:  train Loss: 77.4385   val Loss: 74.9674   time: 0.24s   best: 73.6556
2023-10-29 11:45:33,949:INFO:  Epoch 260/500:  train Loss: 74.9597   val Loss: 73.9763   time: 0.26s   best: 73.6556
2023-10-29 11:45:34,189:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:34,218:INFO:  Epoch 261/500:  train Loss: 73.7400   val Loss: 72.8435   time: 0.24s   best: 72.8435
2023-10-29 11:45:34,484:INFO:  Epoch 262/500:  train Loss: 75.1080   val Loss: 74.5275   time: 0.25s   best: 72.8435
2023-10-29 11:45:34,763:INFO:  Epoch 263/500:  train Loss: 74.9867   val Loss: 74.3319   time: 0.27s   best: 72.8435
2023-10-29 11:45:35,030:INFO:  Epoch 264/500:  train Loss: 74.0827   val Loss: 72.9870   time: 0.26s   best: 72.8435
2023-10-29 11:45:35,269:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:35,298:INFO:  Epoch 265/500:  train Loss: 75.0648   val Loss: 72.1801   time: 0.23s   best: 72.1801
2023-10-29 11:45:35,565:INFO:  Epoch 266/500:  train Loss: 73.7984   val Loss: 73.2068   time: 0.26s   best: 72.1801
2023-10-29 11:45:35,811:INFO:  Epoch 267/500:  train Loss: 75.8388   val Loss: 73.6421   time: 0.24s   best: 72.1801
2023-10-29 11:45:36,082:INFO:  Epoch 268/500:  train Loss: 74.9790   val Loss: 72.3377   time: 0.26s   best: 72.1801
2023-10-29 11:45:36,329:INFO:  Epoch 269/500:  train Loss: 74.3402   val Loss: 73.5922   time: 0.24s   best: 72.1801
2023-10-29 11:45:36,595:INFO:  Epoch 270/500:  train Loss: 76.0566   val Loss: 74.2150   time: 0.26s   best: 72.1801
2023-10-29 11:45:36,876:INFO:  Epoch 271/500:  train Loss: 74.2374   val Loss: 73.1313   time: 0.27s   best: 72.1801
2023-10-29 11:45:37,143:INFO:  Epoch 272/500:  train Loss: 74.5373   val Loss: 72.7961   time: 0.26s   best: 72.1801
2023-10-29 11:45:37,389:INFO:  Epoch 273/500:  train Loss: 73.6818   val Loss: 73.1697   time: 0.24s   best: 72.1801
2023-10-29 11:45:37,655:INFO:  Epoch 274/500:  train Loss: 74.0198   val Loss: 72.6008   time: 0.25s   best: 72.1801
2023-10-29 11:45:37,898:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:37,929:INFO:  Epoch 275/500:  train Loss: 72.3563   val Loss: 71.8080   time: 0.24s   best: 71.8080
2023-10-29 11:45:38,188:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:38,217:INFO:  Epoch 276/500:  train Loss: 72.3263   val Loss: 71.5326   time: 0.25s   best: 71.5326
2023-10-29 11:45:38,463:INFO:  Epoch 277/500:  train Loss: 72.6298   val Loss: 71.6043   time: 0.24s   best: 71.5326
2023-10-29 11:45:38,722:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:38,751:INFO:  Epoch 278/500:  train Loss: 73.4549   val Loss: 71.1029   time: 0.25s   best: 71.1029
2023-10-29 11:45:39,026:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:39,058:INFO:  Epoch 279/500:  train Loss: 71.1259   val Loss: 70.2592   time: 0.27s   best: 70.2592
2023-10-29 11:45:39,321:INFO:  Epoch 280/500:  train Loss: 71.7241   val Loss: 71.3033   time: 0.25s   best: 70.2592
2023-10-29 11:45:39,568:INFO:  Epoch 281/500:  train Loss: 72.6233   val Loss: 71.4917   time: 0.24s   best: 70.2592
2023-10-29 11:45:39,829:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:39,858:INFO:  Epoch 282/500:  train Loss: 71.6980   val Loss: 69.9515   time: 0.26s   best: 69.9515
2023-10-29 11:45:40,109:INFO:  Epoch 283/500:  train Loss: 70.9842   val Loss: 69.9689   time: 0.24s   best: 69.9515
2023-10-29 11:45:40,375:INFO:  Epoch 284/500:  train Loss: 72.7264   val Loss: 72.2137   time: 0.25s   best: 69.9515
2023-10-29 11:45:40,629:INFO:  Epoch 285/500:  train Loss: 89.4240   val Loss: 91.7580   time: 0.25s   best: 69.9515
2023-10-29 11:45:40,936:INFO:  Epoch 286/500:  train Loss: 89.1930   val Loss: 86.2411   time: 0.30s   best: 69.9515
2023-10-29 11:45:41,183:INFO:  Epoch 287/500:  train Loss: 87.1653   val Loss: 86.3373   time: 0.24s   best: 69.9515
2023-10-29 11:45:41,447:INFO:  Epoch 288/500:  train Loss: 86.3811   val Loss: 85.4671   time: 0.23s   best: 69.9515
2023-10-29 11:45:41,694:INFO:  Epoch 289/500:  train Loss: 84.7525   val Loss: 83.9870   time: 0.24s   best: 69.9515
2023-10-29 11:45:41,963:INFO:  Epoch 290/500:  train Loss: 83.3506   val Loss: 81.8417   time: 0.26s   best: 69.9515
2023-10-29 11:45:42,210:INFO:  Epoch 291/500:  train Loss: 82.1278   val Loss: 80.0038   time: 0.24s   best: 69.9515
2023-10-29 11:45:42,475:INFO:  Epoch 292/500:  train Loss: 78.7357   val Loss: 77.0869   time: 0.25s   best: 69.9515
2023-10-29 11:45:42,721:INFO:  Epoch 293/500:  train Loss: 76.6467   val Loss: 75.3703   time: 0.24s   best: 69.9515
2023-10-29 11:45:43,021:INFO:  Epoch 294/500:  train Loss: 75.1319   val Loss: 73.7254   time: 0.29s   best: 69.9515
2023-10-29 11:45:43,268:INFO:  Epoch 295/500:  train Loss: 73.5986   val Loss: 72.0556   time: 0.24s   best: 69.9515
2023-10-29 11:45:43,532:INFO:  Epoch 296/500:  train Loss: 73.2072   val Loss: 71.4652   time: 0.25s   best: 69.9515
2023-10-29 11:45:43,778:INFO:  Epoch 297/500:  train Loss: 73.6011   val Loss: 73.1031   time: 0.23s   best: 69.9515
2023-10-29 11:45:44,047:INFO:  Epoch 298/500:  train Loss: 74.3959   val Loss: 72.3137   time: 0.26s   best: 69.9515
2023-10-29 11:45:44,294:INFO:  Epoch 299/500:  train Loss: 72.3443   val Loss: 71.6566   time: 0.24s   best: 69.9515
2023-10-29 11:45:44,624:INFO:  Epoch 300/500:  train Loss: 73.8163   val Loss: 74.6830   time: 0.31s   best: 69.9515
2023-10-29 11:45:44,887:INFO:  Epoch 301/500:  train Loss: 78.1090   val Loss: 71.2034   time: 0.25s   best: 69.9515
2023-10-29 11:45:45,167:INFO:  Epoch 302/500:  train Loss: 72.5235   val Loss: 71.7654   time: 0.27s   best: 69.9515
2023-10-29 11:45:45,431:INFO:  Epoch 303/500:  train Loss: 72.8157   val Loss: 71.1664   time: 0.25s   best: 69.9515
2023-10-29 11:45:45,678:INFO:  Epoch 304/500:  train Loss: 72.1055   val Loss: 71.1605   time: 0.24s   best: 69.9515
2023-10-29 11:45:45,947:INFO:  Epoch 305/500:  train Loss: 73.8405   val Loss: 71.1535   time: 0.26s   best: 69.9515
2023-10-29 11:45:46,194:INFO:  Epoch 306/500:  train Loss: 73.0262   val Loss: 72.3742   time: 0.24s   best: 69.9515
2023-10-29 11:45:46,460:INFO:  Epoch 307/500:  train Loss: 73.2873   val Loss: 72.3101   time: 0.26s   best: 69.9515
2023-10-29 11:45:46,707:INFO:  Epoch 308/500:  train Loss: 73.7979   val Loss: 73.4202   time: 0.24s   best: 69.9515
2023-10-29 11:45:46,973:INFO:  Epoch 309/500:  train Loss: 74.2058   val Loss: 72.8766   time: 0.25s   best: 69.9515
2023-10-29 11:45:47,329:INFO:  Epoch 310/500:  train Loss: 72.9864   val Loss: 72.4675   time: 0.34s   best: 69.9515
2023-10-29 11:45:47,587:INFO:  Epoch 311/500:  train Loss: 73.8091   val Loss: 71.9971   time: 0.25s   best: 69.9515
2023-10-29 11:45:47,833:INFO:  Epoch 312/500:  train Loss: 72.8943   val Loss: 71.9058   time: 0.24s   best: 69.9515
2023-10-29 11:45:48,104:INFO:  Epoch 313/500:  train Loss: 73.9848   val Loss: 71.0055   time: 0.26s   best: 69.9515
2023-10-29 11:45:48,350:INFO:  Epoch 314/500:  train Loss: 72.2381   val Loss: 70.8217   time: 0.24s   best: 69.9515
2023-10-29 11:45:48,617:INFO:  Epoch 315/500:  train Loss: 72.1806   val Loss: 71.8534   time: 0.26s   best: 69.9515
2023-10-29 11:45:48,863:INFO:  Epoch 316/500:  train Loss: 72.0028   val Loss: 70.3057   time: 0.24s   best: 69.9515
2023-10-29 11:45:49,132:INFO:  Epoch 317/500:  train Loss: 71.7993   val Loss: 70.6314   time: 0.26s   best: 69.9515
2023-10-29 11:45:49,411:INFO:  Epoch 318/500:  train Loss: 72.1738   val Loss: 71.7101   time: 0.27s   best: 69.9515
2023-10-29 11:45:49,675:INFO:  Epoch 319/500:  train Loss: 73.4365   val Loss: 72.6953   time: 0.25s   best: 69.9515
2023-10-29 11:45:49,924:INFO:  Epoch 320/500:  train Loss: 72.4652   val Loss: 71.1365   time: 0.24s   best: 69.9515
2023-10-29 11:45:50,186:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:50,215:INFO:  Epoch 321/500:  train Loss: 71.3747   val Loss: 69.9002   time: 0.26s   best: 69.9002
2023-10-29 11:45:50,455:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:50,484:INFO:  Epoch 322/500:  train Loss: 70.8415   val Loss: 69.8861   time: 0.24s   best: 69.8861
2023-10-29 11:45:50,781:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:50,810:INFO:  Epoch 323/500:  train Loss: 71.3539   val Loss: 69.2504   time: 0.25s   best: 69.2504
2023-10-29 11:45:51,059:INFO:  Epoch 324/500:  train Loss: 71.8513   val Loss: 70.0705   time: 0.24s   best: 69.2504
2023-10-29 11:45:51,355:INFO:  Epoch 325/500:  train Loss: 73.9626   val Loss: 71.2430   time: 0.28s   best: 69.2504
2023-10-29 11:45:51,596:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:51,625:INFO:  Epoch 326/500:  train Loss: 70.7863   val Loss: 68.9648   time: 0.24s   best: 68.9648
2023-10-29 11:45:51,883:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:51,916:INFO:  Epoch 327/500:  train Loss: 71.7813   val Loss: 68.8535   time: 0.25s   best: 68.8535
2023-10-29 11:45:52,165:INFO:  Epoch 328/500:  train Loss: 72.1503   val Loss: 70.9744   time: 0.24s   best: 68.8535
2023-10-29 11:45:52,429:INFO:  Epoch 329/500:  train Loss: 76.2751   val Loss: 74.8523   time: 0.25s   best: 68.8535
2023-10-29 11:45:52,676:INFO:  Epoch 330/500:  train Loss: 81.5838   val Loss: 78.2083   time: 0.24s   best: 68.8535
2023-10-29 11:45:52,943:INFO:  Epoch 331/500:  train Loss: 80.2588   val Loss: 79.2873   time: 0.26s   best: 68.8535
2023-10-29 11:45:53,190:INFO:  Epoch 332/500:  train Loss: 77.1861   val Loss: 74.8548   time: 0.24s   best: 68.8535
2023-10-29 11:45:53,521:INFO:  Epoch 333/500:  train Loss: 73.9809   val Loss: 72.4821   time: 0.32s   best: 68.8535
2023-10-29 11:45:53,768:INFO:  Epoch 334/500:  train Loss: 74.7108   val Loss: 71.5576   time: 0.24s   best: 68.8535
2023-10-29 11:45:54,036:INFO:  Epoch 335/500:  train Loss: 74.3138   val Loss: 71.7634   time: 0.26s   best: 68.8535
2023-10-29 11:45:54,282:INFO:  Epoch 336/500:  train Loss: 77.2297   val Loss: 71.0413   time: 0.24s   best: 68.8535
2023-10-29 11:45:54,547:INFO:  Epoch 337/500:  train Loss: 77.7723   val Loss: 75.1469   time: 0.25s   best: 68.8535
2023-10-29 11:45:54,794:INFO:  Epoch 338/500:  train Loss: 74.9491   val Loss: 73.9385   time: 0.24s   best: 68.8535
2023-10-29 11:45:55,061:INFO:  Epoch 339/500:  train Loss: 71.7369   val Loss: 70.6097   time: 0.26s   best: 68.8535
2023-10-29 11:45:55,307:INFO:  Epoch 340/500:  train Loss: 71.2658   val Loss: 70.3937   time: 0.24s   best: 68.8535
2023-10-29 11:45:55,607:INFO:  Epoch 341/500:  train Loss: 71.3813   val Loss: 69.4251   time: 0.29s   best: 68.8535
2023-10-29 11:45:55,853:INFO:  Epoch 342/500:  train Loss: 71.1531   val Loss: 69.6987   time: 0.24s   best: 68.8535
2023-10-29 11:45:56,129:INFO:  Epoch 343/500:  train Loss: 72.1996   val Loss: 69.4879   time: 0.27s   best: 68.8535
2023-10-29 11:45:56,377:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:56,406:INFO:  Epoch 344/500:  train Loss: 72.0482   val Loss: 68.4726   time: 0.24s   best: 68.4726
2023-10-29 11:45:56,671:INFO:  Epoch 345/500:  train Loss: 73.5642   val Loss: 69.8364   time: 0.25s   best: 68.4726
2023-10-29 11:45:56,917:INFO:  Epoch 346/500:  train Loss: 73.2712   val Loss: 70.3846   time: 0.24s   best: 68.4726
2023-10-29 11:45:57,184:INFO:  Epoch 347/500:  train Loss: 74.0930   val Loss: 69.5084   time: 0.26s   best: 68.4726
2023-10-29 11:45:57,430:INFO:  Epoch 348/500:  train Loss: 70.9088   val Loss: 69.6260   time: 0.24s   best: 68.4726
2023-10-29 11:45:57,729:INFO:  Epoch 349/500:  train Loss: 70.7291   val Loss: 69.9119   time: 0.29s   best: 68.4726
2023-10-29 11:45:57,972:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:58,001:INFO:  Epoch 350/500:  train Loss: 69.6730   val Loss: 68.2825   time: 0.24s   best: 68.2825
2023-10-29 11:45:58,259:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:58,288:INFO:  Epoch 351/500:  train Loss: 70.0582   val Loss: 67.6941   time: 0.25s   best: 67.6941
2023-10-29 11:45:58,555:INFO:  Epoch 352/500:  train Loss: 69.8415   val Loss: 68.9711   time: 0.24s   best: 67.6941
2023-10-29 11:45:58,801:INFO:  Epoch 353/500:  train Loss: 70.3590   val Loss: 67.7048   time: 0.24s   best: 67.6941
2023-10-29 11:45:59,048:INFO:  Epoch 354/500:  train Loss: 69.3152   val Loss: 68.2714   time: 0.24s   best: 67.6941
2023-10-29 11:45:59,317:INFO:  Epoch 355/500:  train Loss: 71.5151   val Loss: 68.6500   time: 0.26s   best: 67.6941
2023-10-29 11:45:59,558:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:45:59,587:INFO:  Epoch 356/500:  train Loss: 69.7974   val Loss: 67.4495   time: 0.24s   best: 67.4495
2023-10-29 11:45:59,886:INFO:  Epoch 357/500:  train Loss: 69.9492   val Loss: 69.2942   time: 0.29s   best: 67.4495
2023-10-29 11:46:00,154:INFO:  Epoch 358/500:  train Loss: 70.0122   val Loss: 68.9162   time: 0.26s   best: 67.4495
2023-10-29 11:46:00,401:INFO:  Epoch 359/500:  train Loss: 70.0916   val Loss: 68.7406   time: 0.24s   best: 67.4495
2023-10-29 11:46:00,649:INFO:  Epoch 360/500:  train Loss: 70.2314   val Loss: 69.7702   time: 0.24s   best: 67.4495
2023-10-29 11:46:00,915:INFO:  Epoch 361/500:  train Loss: 70.4045   val Loss: 68.5702   time: 0.25s   best: 67.4495
2023-10-29 11:46:01,163:INFO:  Epoch 362/500:  train Loss: 69.8379   val Loss: 69.3939   time: 0.24s   best: 67.4495
2023-10-29 11:46:01,427:INFO:  Epoch 363/500:  train Loss: 73.0189   val Loss: 69.4669   time: 0.25s   best: 67.4495
2023-10-29 11:46:01,674:INFO:  Epoch 364/500:  train Loss: 70.5825   val Loss: 67.9923   time: 0.24s   best: 67.4495
2023-10-29 11:46:01,970:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:01,999:INFO:  Epoch 365/500:  train Loss: 69.6492   val Loss: 66.8864   time: 0.29s   best: 66.8864
2023-10-29 11:46:02,246:INFO:  Epoch 366/500:  train Loss: 77.1649   val Loss: 73.5599   time: 0.24s   best: 66.8864
2023-10-29 11:46:02,511:INFO:  Epoch 367/500:  train Loss: 75.1755   val Loss: 72.1768   time: 0.25s   best: 66.8864
2023-10-29 11:46:02,758:INFO:  Epoch 368/500:  train Loss: 70.2583   val Loss: 69.9704   time: 0.24s   best: 66.8864
2023-10-29 11:46:03,023:INFO:  Epoch 369/500:  train Loss: 70.4078   val Loss: 68.9432   time: 0.25s   best: 66.8864
2023-10-29 11:46:03,269:INFO:  Epoch 370/500:  train Loss: 71.4373   val Loss: 70.5089   time: 0.24s   best: 66.8864
2023-10-29 11:46:03,534:INFO:  Epoch 371/500:  train Loss: 70.2289   val Loss: 68.7944   time: 0.25s   best: 66.8864
2023-10-29 11:46:03,781:INFO:  Epoch 372/500:  train Loss: 71.0310   val Loss: 69.3470   time: 0.24s   best: 66.8864
2023-10-29 11:46:04,083:INFO:  Epoch 373/500:  train Loss: 70.4684   val Loss: 69.7386   time: 0.29s   best: 66.8864
2023-10-29 11:46:04,330:INFO:  Epoch 374/500:  train Loss: 70.2829   val Loss: 68.9289   time: 0.24s   best: 66.8864
2023-10-29 11:46:04,596:INFO:  Epoch 375/500:  train Loss: 70.2426   val Loss: 67.3663   time: 0.25s   best: 66.8864
2023-10-29 11:46:04,843:INFO:  Epoch 376/500:  train Loss: 69.6760   val Loss: 67.2748   time: 0.24s   best: 66.8864
2023-10-29 11:46:05,102:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:05,132:INFO:  Epoch 377/500:  train Loss: 69.0414   val Loss: 66.6085   time: 0.25s   best: 66.6085
2023-10-29 11:46:05,378:INFO:  Epoch 378/500:  train Loss: 71.0404   val Loss: 68.2765   time: 0.24s   best: 66.6085
2023-10-29 11:46:05,643:INFO:  Epoch 379/500:  train Loss: 72.1613   val Loss: 68.3730   time: 0.25s   best: 66.6085
2023-10-29 11:46:05,924:INFO:  Epoch 380/500:  train Loss: 75.0757   val Loss: 67.0664   time: 0.27s   best: 66.6085
2023-10-29 11:46:06,193:INFO:  Epoch 381/500:  train Loss: 73.4141   val Loss: 71.4285   time: 0.26s   best: 66.6085
2023-10-29 11:46:06,440:INFO:  Epoch 382/500:  train Loss: 70.6631   val Loss: 70.6606   time: 0.24s   best: 66.6085
2023-10-29 11:46:06,706:INFO:  Epoch 383/500:  train Loss: 70.1680   val Loss: 69.0014   time: 0.25s   best: 66.6085
2023-10-29 11:46:06,952:INFO:  Epoch 384/500:  train Loss: 71.1987   val Loss: 67.3348   time: 0.24s   best: 66.6085
2023-10-29 11:46:07,218:INFO:  Epoch 385/500:  train Loss: 68.7281   val Loss: 67.2217   time: 0.25s   best: 66.6085
2023-10-29 11:46:07,464:INFO:  Epoch 386/500:  train Loss: 68.9165   val Loss: 68.5356   time: 0.24s   best: 66.6085
2023-10-29 11:46:07,731:INFO:  Epoch 387/500:  train Loss: 69.0839   val Loss: 68.3399   time: 0.26s   best: 66.6085
2023-10-29 11:46:08,008:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:08,038:INFO:  Epoch 388/500:  train Loss: 68.2665   val Loss: 66.4300   time: 0.27s   best: 66.4300
2023-10-29 11:46:08,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:08,325:INFO:  Epoch 389/500:  train Loss: 67.0503   val Loss: 65.8732   time: 0.25s   best: 65.8732
2023-10-29 11:46:08,572:INFO:  Epoch 390/500:  train Loss: 67.9510   val Loss: 68.6330   time: 0.24s   best: 65.8732
2023-10-29 11:46:08,837:INFO:  Epoch 391/500:  train Loss: 69.9525   val Loss: 68.3699   time: 0.25s   best: 65.8732
2023-10-29 11:46:09,084:INFO:  Epoch 392/500:  train Loss: 69.3434   val Loss: 68.3806   time: 0.24s   best: 65.8732
2023-10-29 11:46:09,352:INFO:  Epoch 393/500:  train Loss: 69.2209   val Loss: 67.8206   time: 0.25s   best: 65.8732
2023-10-29 11:46:09,599:INFO:  Epoch 394/500:  train Loss: 67.7079   val Loss: 66.1118   time: 0.24s   best: 65.8732
2023-10-29 11:46:09,865:INFO:  Epoch 395/500:  train Loss: 68.2416   val Loss: 67.2016   time: 0.26s   best: 65.8732
2023-10-29 11:46:10,142:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:10,205:INFO:  Epoch 396/500:  train Loss: 68.8939   val Loss: 65.4384   time: 0.27s   best: 65.4384
2023-10-29 11:46:10,451:INFO:  Epoch 397/500:  train Loss: 69.1680   val Loss: 66.2669   time: 0.24s   best: 65.4384
2023-10-29 11:46:10,714:INFO:  Epoch 398/500:  train Loss: 67.7694   val Loss: 68.3909   time: 0.25s   best: 65.4384
2023-10-29 11:46:10,961:INFO:  Epoch 399/500:  train Loss: 68.3285   val Loss: 66.5595   time: 0.24s   best: 65.4384
2023-10-29 11:46:11,290:INFO:  Epoch 400/500:  train Loss: 68.8281   val Loss: 66.2416   time: 0.32s   best: 65.4384
2023-10-29 11:46:11,549:INFO:  Epoch 401/500:  train Loss: 68.1910   val Loss: 65.7756   time: 0.25s   best: 65.4384
2023-10-29 11:46:11,815:INFO:  Epoch 402/500:  train Loss: 66.4981   val Loss: 66.0275   time: 0.26s   best: 65.4384
2023-10-29 11:46:12,059:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:12,108:INFO:  Epoch 403/500:  train Loss: 67.0067   val Loss: 64.8154   time: 0.24s   best: 64.8154
2023-10-29 11:46:12,386:INFO:  Epoch 404/500:  train Loss: 66.4718   val Loss: 64.9052   time: 0.27s   best: 64.8154
2023-10-29 11:46:12,632:INFO:  Epoch 405/500:  train Loss: 66.5116   val Loss: 65.4971   time: 0.24s   best: 64.8154
2023-10-29 11:46:12,898:INFO:  Epoch 406/500:  train Loss: 66.8032   val Loss: 65.6827   time: 0.26s   best: 64.8154
2023-10-29 11:46:13,146:INFO:  Epoch 407/500:  train Loss: 67.0165   val Loss: 65.1875   time: 0.24s   best: 64.8154
2023-10-29 11:46:13,409:INFO:  Epoch 408/500:  train Loss: 66.2543   val Loss: 64.9668   time: 0.25s   best: 64.8154
2023-10-29 11:46:13,656:INFO:  Epoch 409/500:  train Loss: 68.7837   val Loss: 65.9763   time: 0.24s   best: 64.8154
2023-10-29 11:46:13,922:INFO:  Epoch 410/500:  train Loss: 69.4990   val Loss: 66.3262   time: 0.26s   best: 64.8154
2023-10-29 11:46:14,175:INFO:  Epoch 411/500:  train Loss: 71.5679   val Loss: 68.8288   time: 0.24s   best: 64.8154
2023-10-29 11:46:14,468:INFO:  Epoch 412/500:  train Loss: 71.1729   val Loss: 69.4547   time: 0.28s   best: 64.8154
2023-10-29 11:46:14,715:INFO:  Epoch 413/500:  train Loss: 69.3911   val Loss: 68.8216   time: 0.24s   best: 64.8154
2023-10-29 11:46:14,981:INFO:  Epoch 414/500:  train Loss: 74.3798   val Loss: 68.5423   time: 0.26s   best: 64.8154
2023-10-29 11:46:15,229:INFO:  Epoch 415/500:  train Loss: 74.9822   val Loss: 69.7887   time: 0.24s   best: 64.8154
2023-10-29 11:46:15,495:INFO:  Epoch 416/500:  train Loss: 78.2994   val Loss: 72.8671   time: 0.26s   best: 64.8154
2023-10-29 11:46:15,742:INFO:  Epoch 417/500:  train Loss: 78.5230   val Loss: 78.8397   time: 0.24s   best: 64.8154
2023-10-29 11:46:16,011:INFO:  Epoch 418/500:  train Loss: 76.5945   val Loss: 74.5750   time: 0.26s   best: 64.8154
2023-10-29 11:46:16,272:INFO:  Epoch 419/500:  train Loss: 72.9698   val Loss: 71.1672   time: 0.24s   best: 64.8154
2023-10-29 11:46:16,553:INFO:  Epoch 420/500:  train Loss: 71.0726   val Loss: 70.1007   time: 0.27s   best: 64.8154
2023-10-29 11:46:16,800:INFO:  Epoch 421/500:  train Loss: 71.9084   val Loss: 69.4156   time: 0.24s   best: 64.8154
2023-10-29 11:46:17,066:INFO:  Epoch 422/500:  train Loss: 69.9286   val Loss: 68.0862   time: 0.26s   best: 64.8154
2023-10-29 11:46:17,313:INFO:  Epoch 423/500:  train Loss: 69.8871   val Loss: 68.8045   time: 0.24s   best: 64.8154
2023-10-29 11:46:17,578:INFO:  Epoch 424/500:  train Loss: 68.7694   val Loss: 66.6712   time: 0.25s   best: 64.8154
2023-10-29 11:46:17,825:INFO:  Epoch 425/500:  train Loss: 69.4469   val Loss: 68.3329   time: 0.24s   best: 64.8154
2023-10-29 11:46:18,096:INFO:  Epoch 426/500:  train Loss: 69.0746   val Loss: 68.4664   time: 0.26s   best: 64.8154
2023-10-29 11:46:18,352:INFO:  Epoch 427/500:  train Loss: 69.0503   val Loss: 67.2267   time: 0.25s   best: 64.8154
2023-10-29 11:46:18,640:INFO:  Epoch 428/500:  train Loss: 68.4254   val Loss: 66.9978   time: 0.28s   best: 64.8154
2023-10-29 11:46:18,888:INFO:  Epoch 429/500:  train Loss: 67.7765   val Loss: 65.8420   time: 0.24s   best: 64.8154
2023-10-29 11:46:19,155:INFO:  Epoch 430/500:  train Loss: 67.9303   val Loss: 67.3746   time: 0.26s   best: 64.8154
2023-10-29 11:46:19,404:INFO:  Epoch 431/500:  train Loss: 71.7106   val Loss: 67.1721   time: 0.24s   best: 64.8154
2023-10-29 11:46:19,672:INFO:  Epoch 432/500:  train Loss: 69.4043   val Loss: 67.3668   time: 0.26s   best: 64.8154
2023-10-29 11:46:19,913:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:19,944:INFO:  Epoch 433/500:  train Loss: 72.2374   val Loss: 64.5851   time: 0.24s   best: 64.5851
2023-10-29 11:46:20,211:INFO:  Epoch 434/500:  train Loss: 69.4007   val Loss: 64.7276   time: 0.26s   best: 64.5851
2023-10-29 11:46:20,492:INFO:  Epoch 435/500:  train Loss: 69.8719   val Loss: 68.8036   time: 0.27s   best: 64.5851
2023-10-29 11:46:20,757:INFO:  Epoch 436/500:  train Loss: 75.5936   val Loss: 70.8670   time: 0.25s   best: 64.5851
2023-10-29 11:46:21,003:INFO:  Epoch 437/500:  train Loss: 75.3261   val Loss: 70.0513   time: 0.24s   best: 64.5851
2023-10-29 11:46:21,270:INFO:  Epoch 438/500:  train Loss: 68.2445   val Loss: 67.0770   time: 0.26s   best: 64.5851
2023-10-29 11:46:21,516:INFO:  Epoch 439/500:  train Loss: 71.2393   val Loss: 64.8482   time: 0.24s   best: 64.5851
2023-10-29 11:46:21,781:INFO:  Epoch 440/500:  train Loss: 73.4743   val Loss: 71.3779   time: 0.25s   best: 64.5851
2023-10-29 11:46:22,030:INFO:  Epoch 441/500:  train Loss: 74.6012   val Loss: 70.4423   time: 0.24s   best: 64.5851
2023-10-29 11:46:22,295:INFO:  Epoch 442/500:  train Loss: 69.6216   val Loss: 68.1097   time: 0.25s   best: 64.5851
2023-10-29 11:46:22,574:INFO:  Epoch 443/500:  train Loss: 68.6561   val Loss: 66.4873   time: 0.27s   best: 64.5851
2023-10-29 11:46:22,840:INFO:  Epoch 444/500:  train Loss: 69.1469   val Loss: 66.0931   time: 0.25s   best: 64.5851
2023-10-29 11:46:23,086:INFO:  Epoch 445/500:  train Loss: 69.1413   val Loss: 65.2970   time: 0.24s   best: 64.5851
2023-10-29 11:46:23,352:INFO:  Epoch 446/500:  train Loss: 67.0553   val Loss: 66.4903   time: 0.25s   best: 64.5851
2023-10-29 11:46:23,598:INFO:  Epoch 447/500:  train Loss: 79.1212   val Loss: 79.4106   time: 0.24s   best: 64.5851
2023-10-29 11:46:23,865:INFO:  Epoch 448/500:  train Loss: 81.9555   val Loss: 81.5866   time: 0.24s   best: 64.5851
2023-10-29 11:46:24,116:INFO:  Epoch 449/500:  train Loss: 81.1709   val Loss: 80.7263   time: 0.24s   best: 64.5851
2023-10-29 11:46:24,363:INFO:  Epoch 450/500:  train Loss: 79.0277   val Loss: 76.7986   time: 0.24s   best: 64.5851
2023-10-29 11:46:24,693:INFO:  Epoch 451/500:  train Loss: 75.3759   val Loss: 73.0683   time: 0.32s   best: 64.5851
2023-10-29 11:46:24,956:INFO:  Epoch 452/500:  train Loss: 72.1391   val Loss: 70.7714   time: 0.25s   best: 64.5851
2023-10-29 11:46:25,204:INFO:  Epoch 453/500:  train Loss: 71.1102   val Loss: 69.2799   time: 0.24s   best: 64.5851
2023-10-29 11:46:25,471:INFO:  Epoch 454/500:  train Loss: 70.0064   val Loss: 68.6739   time: 0.26s   best: 64.5851
2023-10-29 11:46:25,718:INFO:  Epoch 455/500:  train Loss: 69.6990   val Loss: 68.2369   time: 0.24s   best: 64.5851
2023-10-29 11:46:25,980:INFO:  Epoch 456/500:  train Loss: 70.4490   val Loss: 67.2259   time: 0.24s   best: 64.5851
2023-10-29 11:46:26,234:INFO:  Epoch 457/500:  train Loss: 70.1248   val Loss: 67.3887   time: 0.24s   best: 64.5851
2023-10-29 11:46:26,480:INFO:  Epoch 458/500:  train Loss: 69.9304   val Loss: 66.6275   time: 0.24s   best: 64.5851
2023-10-29 11:46:26,785:INFO:  Epoch 459/500:  train Loss: 69.0033   val Loss: 68.8814   time: 0.30s   best: 64.5851
2023-10-29 11:46:27,059:INFO:  Epoch 460/500:  train Loss: 70.7369   val Loss: 68.0425   time: 0.24s   best: 64.5851
2023-10-29 11:46:27,304:INFO:  Epoch 461/500:  train Loss: 72.0405   val Loss: 66.4943   time: 0.23s   best: 64.5851
2023-10-29 11:46:27,551:INFO:  Epoch 462/500:  train Loss: 68.4125   val Loss: 67.4401   time: 0.24s   best: 64.5851
2023-10-29 11:46:27,817:INFO:  Epoch 463/500:  train Loss: 71.4557   val Loss: 68.5608   time: 0.26s   best: 64.5851
2023-10-29 11:46:28,067:INFO:  Epoch 464/500:  train Loss: 69.1965   val Loss: 68.4482   time: 0.24s   best: 64.5851
2023-10-29 11:46:28,332:INFO:  Epoch 465/500:  train Loss: 69.8031   val Loss: 67.5208   time: 0.25s   best: 64.5851
2023-10-29 11:46:28,579:INFO:  Epoch 466/500:  train Loss: 67.8508   val Loss: 66.0718   time: 0.24s   best: 64.5851
2023-10-29 11:46:28,877:INFO:  Epoch 467/500:  train Loss: 68.1645   val Loss: 65.1500   time: 0.29s   best: 64.5851
2023-10-29 11:46:29,125:INFO:  Epoch 468/500:  train Loss: 67.4999   val Loss: 66.1599   time: 0.24s   best: 64.5851
2023-10-29 11:46:29,390:INFO:  Epoch 469/500:  train Loss: 67.9881   val Loss: 65.4351   time: 0.25s   best: 64.5851
2023-10-29 11:46:29,630:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:29,662:INFO:  Epoch 470/500:  train Loss: 66.8056   val Loss: 63.9956   time: 0.23s   best: 63.9956
2023-10-29 11:46:29,922:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:29,953:INFO:  Epoch 471/500:  train Loss: 68.9455   val Loss: 63.9721   time: 0.26s   best: 63.9721
2023-10-29 11:46:30,202:INFO:  Epoch 472/500:  train Loss: 68.7760   val Loss: 64.4307   time: 0.24s   best: 63.9721
2023-10-29 11:46:30,466:INFO:  Epoch 473/500:  train Loss: 69.9509   val Loss: 64.7653   time: 0.25s   best: 63.9721
2023-10-29 11:46:30,707:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:30,755:INFO:  Epoch 474/500:  train Loss: 68.7633   val Loss: 63.8410   time: 0.24s   best: 63.8410
2023-10-29 11:46:31,034:INFO:  Epoch 475/500:  train Loss: 68.9652   val Loss: 64.6486   time: 0.27s   best: 63.8410
2023-10-29 11:46:31,297:INFO:  Epoch 476/500:  train Loss: 67.9088   val Loss: 64.3228   time: 0.25s   best: 63.8410
2023-10-29 11:46:31,544:INFO:  Epoch 477/500:  train Loss: 70.6718   val Loss: 64.8856   time: 0.24s   best: 63.8410
2023-10-29 11:46:31,810:INFO:  Epoch 478/500:  train Loss: 68.3608   val Loss: 65.7039   time: 0.26s   best: 63.8410
2023-10-29 11:46:32,060:INFO:  Epoch 479/500:  train Loss: 68.2746   val Loss: 66.2646   time: 0.24s   best: 63.8410
2023-10-29 11:46:32,324:INFO:  Epoch 480/500:  train Loss: 67.8690   val Loss: 63.9695   time: 0.24s   best: 63.8410
2023-10-29 11:46:32,574:INFO:  Epoch 481/500:  train Loss: 66.6026   val Loss: 65.7698   time: 0.24s   best: 63.8410
2023-10-29 11:46:32,821:INFO:  Epoch 482/500:  train Loss: 66.3187   val Loss: 65.3330   time: 0.24s   best: 63.8410
2023-10-29 11:46:33,118:INFO:  Epoch 483/500:  train Loss: 65.9028   val Loss: 65.6882   time: 0.29s   best: 63.8410
2023-10-29 11:46:33,384:INFO:  Epoch 484/500:  train Loss: 66.4743   val Loss: 65.2695   time: 0.24s   best: 63.8410
2023-10-29 11:46:33,629:INFO:  Epoch 485/500:  train Loss: 65.3487   val Loss: 65.1991   time: 0.23s   best: 63.8410
2023-10-29 11:46:33,876:INFO:  Epoch 486/500:  train Loss: 65.6588   val Loss: 66.5489   time: 0.24s   best: 63.8410
2023-10-29 11:46:34,147:INFO:  Epoch 487/500:  train Loss: 66.3932   val Loss: 66.5354   time: 0.26s   best: 63.8410
2023-10-29 11:46:34,393:INFO:  Epoch 488/500:  train Loss: 68.0159   val Loss: 65.7880   time: 0.24s   best: 63.8410
2023-10-29 11:46:34,651:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:34,681:INFO:  Epoch 489/500:  train Loss: 65.8783   val Loss: 63.6349   time: 0.25s   best: 63.6349
2023-10-29 11:46:34,922:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:34,951:INFO:  Epoch 490/500:  train Loss: 65.1740   val Loss: 63.3545   time: 0.24s   best: 63.3545
2023-10-29 11:46:35,245:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:35,274:INFO:  Epoch 491/500:  train Loss: 64.2107   val Loss: 63.3066   time: 0.29s   best: 63.3066
2023-10-29 11:46:35,531:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_5da1.pt
2023-10-29 11:46:35,561:INFO:  Epoch 492/500:  train Loss: 64.1547   val Loss: 62.6924   time: 0.25s   best: 62.6924
2023-10-29 11:46:35,808:INFO:  Epoch 493/500:  train Loss: 65.0384   val Loss: 66.3393   time: 0.24s   best: 62.6924
2023-10-29 11:46:36,077:INFO:  Epoch 494/500:  train Loss: 65.7135   val Loss: 65.1084   time: 0.26s   best: 62.6924
2023-10-29 11:46:36,324:INFO:  Epoch 495/500:  train Loss: 66.4924   val Loss: 65.9057   time: 0.24s   best: 62.6924
2023-10-29 11:46:36,589:INFO:  Epoch 496/500:  train Loss: 67.7933   val Loss: 64.8424   time: 0.25s   best: 62.6924
2023-10-29 11:46:36,838:INFO:  Epoch 497/500:  train Loss: 67.5767   val Loss: 66.3994   time: 0.24s   best: 62.6924
2023-10-29 11:46:37,128:INFO:  Epoch 498/500:  train Loss: 72.8120   val Loss: 73.9176   time: 0.28s   best: 62.6924
2023-10-29 11:46:37,379:INFO:  Epoch 499/500:  train Loss: 73.9175   val Loss: 71.2351   time: 0.24s   best: 62.6924
2023-10-29 11:46:37,706:INFO:  Epoch 500/500:  train Loss: 70.1288   val Loss: 68.8051   time: 0.30s   best: 62.6924
2023-10-29 11:46:37,706:INFO:  -----> Training complete in 2m 14s   best validation loss: 62.6924
 
2023-10-29 11:48:00,395:INFO:  Epoch 352/500:  train Loss: 17.6556   val Loss: 23.3404   time: 436.44s   best: 22.3835
2023-10-29 11:55:18,498:INFO:  Epoch 353/500:  train Loss: 17.8528   val Loss: 22.9315   time: 438.08s   best: 22.3835
2023-10-29 12:02:31,446:INFO:  Epoch 354/500:  train Loss: 17.6993   val Loss: 23.8092   time: 432.93s   best: 22.3835
2023-10-29 12:09:45,282:INFO:  Epoch 355/500:  train Loss: 17.4891   val Loss: 23.7392   time: 433.81s   best: 22.3835
2023-10-29 12:17:02,350:INFO:  Epoch 356/500:  train Loss: 17.5750   val Loss: 23.6887   time: 437.05s   best: 22.3835
2023-10-29 12:24:15,190:INFO:  Epoch 357/500:  train Loss: 17.5407   val Loss: 23.4330   time: 432.82s   best: 22.3835
2023-10-29 12:31:28,335:INFO:  Epoch 358/500:  train Loss: 17.6204   val Loss: 22.5806   time: 433.12s   best: 22.3835
2023-10-29 12:38:41,953:INFO:  Epoch 359/500:  train Loss: 17.5927   val Loss: 24.0894   time: 433.57s   best: 22.3835
2023-10-29 12:45:57,892:INFO:  Epoch 360/500:  train Loss: 17.5449   val Loss: 23.4417   time: 435.92s   best: 22.3835
2023-10-29 12:53:13,518:INFO:  Epoch 361/500:  train Loss: 17.6423   val Loss: 23.3396   time: 435.62s   best: 22.3835
2023-10-29 13:00:26,090:INFO:  Epoch 362/500:  train Loss: 17.8566   val Loss: 23.6007   time: 432.55s   best: 22.3835
2023-10-29 13:07:42,340:INFO:  Epoch 363/500:  train Loss: 17.4432   val Loss: 23.7800   time: 436.24s   best: 22.3835
2023-10-29 13:14:57,270:INFO:  Epoch 364/500:  train Loss: 17.6313   val Loss: 22.9058   time: 434.92s   best: 22.3835
2023-10-29 13:22:10,604:INFO:  Epoch 365/500:  train Loss: 17.5547   val Loss: 27.1498   time: 433.31s   best: 22.3835
2023-10-29 13:29:26,938:INFO:  Epoch 366/500:  train Loss: 17.6489   val Loss: 24.1886   time: 436.30s   best: 22.3835
2023-10-29 13:36:40,022:INFO:  Epoch 367/500:  train Loss: 17.5858   val Loss: 23.1896   time: 433.07s   best: 22.3835
2023-10-29 13:43:56,921:INFO:  Epoch 368/500:  train Loss: 17.4448   val Loss: 23.1117   time: 436.87s   best: 22.3835
2023-10-29 13:51:12,147:INFO:  Epoch 369/500:  train Loss: 17.6419   val Loss: 25.5947   time: 435.21s   best: 22.3835
2023-10-29 13:58:28,001:INFO:  Epoch 370/500:  train Loss: 17.9086   val Loss: 23.1088   time: 435.82s   best: 22.3835
2023-10-29 14:05:42,403:INFO:  Epoch 371/500:  train Loss: 17.7038   val Loss: 23.4743   time: 434.39s   best: 22.3835
2023-10-29 14:12:54,076:INFO:  Epoch 372/500:  train Loss: 17.5027   val Loss: 23.1313   time: 431.65s   best: 22.3835
2023-10-29 14:20:10,710:INFO:  Epoch 373/500:  train Loss: 17.7746   val Loss: 22.7550   time: 436.59s   best: 22.3835
2023-10-29 14:27:23,824:INFO:  Epoch 374/500:  train Loss: 17.6843   val Loss: 22.8059   time: 433.10s   best: 22.3835
2023-10-29 14:34:37,165:INFO:  Epoch 375/500:  train Loss: 17.4248   val Loss: 25.1662   time: 433.31s   best: 22.3835
2023-10-29 14:41:51,696:INFO:  Epoch 376/500:  train Loss: 17.5749   val Loss: 23.0293   time: 434.51s   best: 22.3835
2023-10-29 14:49:04,820:INFO:  Epoch 377/500:  train Loss: 17.6667   val Loss: 22.8066   time: 433.10s   best: 22.3835
2023-10-29 14:56:21,511:INFO:  Epoch 378/500:  train Loss: 17.5128   val Loss: 23.0641   time: 436.67s   best: 22.3835
2023-10-29 15:03:33,464:INFO:  Epoch 379/500:  train Loss: 17.5419   val Loss: 23.0148   time: 431.94s   best: 22.3835
2023-10-29 15:10:47,704:INFO:  Epoch 380/500:  train Loss: 17.5456   val Loss: 22.8616   time: 434.22s   best: 22.3835
2023-10-29 15:18:00,147:INFO:  Epoch 381/500:  train Loss: 17.6425   val Loss: 23.3371   time: 432.42s   best: 22.3835
2023-10-29 15:25:15,879:INFO:  Epoch 382/500:  train Loss: 17.4944   val Loss: 22.6017   time: 435.71s   best: 22.3835
2023-10-29 15:32:28,093:INFO:  Epoch 383/500:  train Loss: 17.5728   val Loss: 23.1116   time: 432.20s   best: 22.3835
2023-10-29 15:39:45,522:INFO:  Epoch 384/500:  train Loss: 17.3959   val Loss: 23.2386   time: 437.41s   best: 22.3835
2023-10-29 15:47:01,921:INFO:  Epoch 385/500:  train Loss: 17.4782   val Loss: 23.9509   time: 436.37s   best: 22.3835
2023-10-29 15:54:17,901:INFO:  Epoch 386/500:  train Loss: 17.6184   val Loss: 23.0102   time: 435.95s   best: 22.3835
2023-10-29 16:01:29,944:INFO:  Epoch 387/500:  train Loss: 17.5218   val Loss: 23.2896   time: 432.02s   best: 22.3835
2023-10-29 16:08:42,086:INFO:  Epoch 388/500:  train Loss: 17.4404   val Loss: 23.1235   time: 432.13s   best: 22.3835
2023-10-29 16:15:54,122:INFO:  Epoch 389/500:  train Loss: 17.4996   val Loss: 22.8119   time: 432.03s   best: 22.3835
2023-10-29 16:23:06,380:INFO:  Epoch 390/500:  train Loss: 17.3928   val Loss: 23.7063   time: 432.24s   best: 22.3835
2023-10-29 16:30:18,157:INFO:  Epoch 391/500:  train Loss: 17.4420   val Loss: 22.9781   time: 431.75s   best: 22.3835
2023-10-29 16:37:31,178:INFO:  Epoch 392/500:  train Loss: 17.5299   val Loss: 23.1138   time: 433.00s   best: 22.3835
2023-10-29 16:44:46,334:INFO:  Epoch 393/500:  train Loss: 17.3211   val Loss: 23.6053   time: 435.12s   best: 22.3835
2023-10-29 16:51:58,788:INFO:  Epoch 394/500:  train Loss: 17.4940   val Loss: 23.3004   time: 432.43s   best: 22.3835
2023-10-29 16:59:15,032:INFO:  Epoch 395/500:  train Loss: 17.5276   val Loss: 22.8616   time: 436.21s   best: 22.3835
2023-10-29 17:06:28,647:INFO:  Epoch 396/500:  train Loss: 17.5846   val Loss: 22.7409   time: 433.59s   best: 22.3835
2023-10-29 17:13:45,731:INFO:  Epoch 397/500:  train Loss: 17.4837   val Loss: 23.0198   time: 437.04s   best: 22.3835
2023-10-29 17:21:01,236:INFO:  Epoch 398/500:  train Loss: 17.4251   val Loss: 24.9158   time: 435.46s   best: 22.3835
2023-10-29 17:28:15,030:INFO:  Epoch 399/500:  train Loss: 17.6273   val Loss: 25.5944   time: 433.76s   best: 22.3835
2023-10-29 17:35:30,912:INFO:  Epoch 400/500:  train Loss: 17.4163   val Loss: 23.1900   time: 435.86s   best: 22.3835
2023-10-29 17:42:43,382:INFO:  Epoch 401/500:  train Loss: 17.5584   val Loss: 23.2098   time: 432.43s   best: 22.3835
2023-10-29 17:49:59,911:INFO:  Epoch 402/500:  train Loss: 17.3559   val Loss: 23.5449   time: 436.49s   best: 22.3835
2023-10-29 17:57:14,020:INFO:  Epoch 403/500:  train Loss: 17.3897   val Loss: 23.0600   time: 434.08s   best: 22.3835
2023-10-29 18:04:31,112:INFO:  Epoch 404/500:  train Loss: 17.4946   val Loss: 23.2467   time: 437.06s   best: 22.3835
2023-10-29 18:11:47,893:INFO:  Epoch 405/500:  train Loss: 17.5882   val Loss: 23.0812   time: 436.75s   best: 22.3835
2023-10-29 18:19:00,867:INFO:  Epoch 406/500:  train Loss: 17.6032   val Loss: 23.1701   time: 432.95s   best: 22.3835
2023-10-29 18:26:15,106:INFO:  Epoch 407/500:  train Loss: 17.3704   val Loss: 23.1268   time: 434.20s   best: 22.3835
2023-10-29 18:33:27,386:INFO:  Epoch 408/500:  train Loss: 17.6121   val Loss: 23.4827   time: 432.27s   best: 22.3835
2023-10-29 18:40:39,460:INFO:  Epoch 409/500:  train Loss: 17.3441   val Loss: 23.2131   time: 432.06s   best: 22.3835
2023-10-29 18:47:52,543:INFO:  Epoch 410/500:  train Loss: 17.4023   val Loss: 23.7782   time: 433.06s   best: 22.3835
2023-10-29 18:55:06,942:INFO:  Epoch 411/500:  train Loss: 17.4208   val Loss: 23.0672   time: 434.36s   best: 22.3835
2023-10-29 19:02:24,428:INFO:  Epoch 412/500:  train Loss: 17.4595   val Loss: 23.4020   time: 437.46s   best: 22.3835
2023-10-29 19:09:39,570:INFO:  Epoch 413/500:  train Loss: 17.3867   val Loss: 23.4795   time: 435.11s   best: 22.3835
2023-10-29 19:16:52,572:INFO:  Epoch 414/500:  train Loss: 17.3327   val Loss: 23.3344   time: 432.98s   best: 22.3835
2023-10-29 19:24:08,639:INFO:  Epoch 415/500:  train Loss: 17.3381   val Loss: 23.0299   time: 436.06s   best: 22.3835
2023-10-29 19:31:22,427:INFO:  Epoch 416/500:  train Loss: 17.3569   val Loss: 23.2904   time: 433.77s   best: 22.3835
2023-10-29 19:38:40,084:INFO:  Epoch 417/500:  train Loss: 17.7302   val Loss: 23.2147   time: 437.63s   best: 22.3835
2023-10-29 19:45:53,140:INFO:  Epoch 418/500:  train Loss: 17.7191   val Loss: 23.8005   time: 433.04s   best: 22.3835
2023-10-29 19:53:07,045:INFO:  Epoch 419/500:  train Loss: 17.8626   val Loss: 22.9278   time: 433.90s   best: 22.3835
2023-10-29 20:00:19,567:INFO:  Epoch 420/500:  train Loss: 17.5758   val Loss: 22.9076   time: 432.49s   best: 22.3835
2023-10-29 20:07:31,656:INFO:  Epoch 421/500:  train Loss: 17.2341   val Loss: 23.0660   time: 432.06s   best: 22.3835
2023-10-29 20:14:48,503:INFO:  Epoch 422/500:  train Loss: 17.4040   val Loss: 22.9299   time: 436.82s   best: 22.3835
2023-10-29 20:22:05,070:INFO:  Epoch 423/500:  train Loss: 17.4549   val Loss: 23.5564   time: 436.41s   best: 22.3835
2023-10-29 20:29:22,470:INFO:  Epoch 424/500:  train Loss: 17.4025   val Loss: 23.1590   time: 437.39s   best: 22.3835
2023-10-29 20:36:37,870:INFO:  Epoch 425/500:  train Loss: 17.2454   val Loss: 23.8181   time: 435.39s   best: 22.3835
2023-10-29 20:43:52,469:INFO:  Epoch 426/500:  train Loss: 17.8191   val Loss: 22.7645   time: 434.59s   best: 22.3835
2023-10-29 20:51:07,772:INFO:  Epoch 427/500:  train Loss: 17.4005   val Loss: 23.2763   time: 435.29s   best: 22.3835
2023-10-29 20:58:22,444:INFO:  Epoch 428/500:  train Loss: 17.3526   val Loss: 23.0695   time: 434.63s   best: 22.3835
2023-10-29 21:05:37,277:INFO:  Epoch 429/500:  train Loss: 17.2919   val Loss: 22.6921   time: 434.81s   best: 22.3835
2023-10-29 21:12:54,103:INFO:  Epoch 430/500:  train Loss: 17.3225   val Loss: 23.1101   time: 436.81s   best: 22.3835
2023-10-29 21:20:05,961:INFO:  Epoch 431/500:  train Loss: 17.3928   val Loss: 22.5877   time: 431.83s   best: 22.3835
2023-10-29 21:27:21,120:INFO:  Epoch 432/500:  train Loss: 17.3259   val Loss: 23.5689   time: 435.12s   best: 22.3835
2023-10-29 21:34:34,085:INFO:  Epoch 433/500:  train Loss: 17.4450   val Loss: 23.0900   time: 432.95s   best: 22.3835
2023-10-29 21:41:49,016:INFO:  Epoch 434/500:  train Loss: 17.2698   val Loss: 23.1809   time: 434.91s   best: 22.3835
2023-10-29 21:49:06,889:INFO:  Epoch 435/500:  train Loss: 17.2734   val Loss: 23.3445   time: 437.83s   best: 22.3835
2023-10-29 21:56:20,176:INFO:  Epoch 436/500:  train Loss: 17.3202   val Loss: 23.0542   time: 433.26s   best: 22.3835
2023-10-29 22:03:35,389:INFO:  Epoch 437/500:  train Loss: 17.4240   val Loss: 28.1293   time: 435.17s   best: 22.3835
2023-10-29 22:10:48,285:INFO:  Epoch 438/500:  train Loss: 17.2739   val Loss: 23.2329   time: 432.89s   best: 22.3835
2023-10-29 22:18:00,052:INFO:  Epoch 439/500:  train Loss: 17.4269   val Loss: 22.8396   time: 431.75s   best: 22.3835
2023-10-29 22:25:14,635:INFO:  Epoch 440/500:  train Loss: 17.4180   val Loss: 22.5519   time: 434.56s   best: 22.3835
2023-10-29 22:32:29,611:INFO:  Epoch 441/500:  train Loss: 17.2949   val Loss: 22.9395   time: 434.93s   best: 22.3835
2023-10-29 22:39:46,328:INFO:  Epoch 442/500:  train Loss: 17.2861   val Loss: 23.5065   time: 436.69s   best: 22.3835
2023-10-29 22:47:03,481:INFO:  Epoch 443/500:  train Loss: 17.3348   val Loss: 22.8241   time: 437.12s   best: 22.3835
2023-10-29 22:54:17,121:INFO:  Epoch 444/500:  train Loss: 17.2036   val Loss: 23.1822   time: 433.62s   best: 22.3835
2023-10-29 23:01:34,194:INFO:  Epoch 445/500:  train Loss: 17.4831   val Loss: 22.8657   time: 437.04s   best: 22.3835
2023-10-29 23:08:49,066:INFO:  Epoch 446/500:  train Loss: 17.3334   val Loss: 23.0791   time: 434.82s   best: 22.3835
2023-10-29 23:12:56,340:INFO:  Starting experiment lstm autoencoder (2 layer + hidden)
2023-10-29 23:12:56,349:INFO:  Defining the model
2023-10-29 23:12:56,406:INFO:  Reading the dataset
2023-10-29 23:16:00,792:INFO:  Epoch 447/500:  train Loss: 17.1664   val Loss: 23.2296   time: 431.69s   best: 22.3835
2023-10-29 23:23:14,215:INFO:  Epoch 448/500:  train Loss: 17.3204   val Loss: 22.9042   time: 433.39s   best: 22.3835
2023-10-29 23:30:26,848:INFO:  Epoch 449/500:  train Loss: 17.3715   val Loss: 22.6295   time: 432.61s   best: 22.3835
2023-10-29 23:37:39,927:INFO:  Epoch 450/500:  train Loss: 17.3181   val Loss: 23.2774   time: 433.07s   best: 22.3835
2023-10-29 23:41:21,098:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-29 23:41:21,144:INFO:  Epoch 1/500:  train Loss: 74.3910   val Loss: 66.3783   time: 439.68s   best: 66.3783
2023-10-29 23:44:54,632:INFO:  Epoch 451/500:  train Loss: 17.3387   val Loss: 22.9039   time: 434.68s   best: 22.3835
2023-10-29 23:48:36,204:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-29 23:48:36,236:INFO:  Epoch 2/500:  train Loss: 63.1209   val Loss: 59.2145   time: 435.05s   best: 59.2145
2023-10-29 23:52:08,104:INFO:  Epoch 452/500:  train Loss: 17.3551   val Loss: 23.6547   time: 433.42s   best: 22.3835
2023-10-29 23:55:51,497:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-29 23:55:51,523:INFO:  Epoch 3/500:  train Loss: 56.6051   val Loss: 52.9017   time: 435.25s   best: 52.9017
2023-10-29 23:59:23,717:INFO:  Epoch 453/500:  train Loss: 17.2556   val Loss: 23.0590   time: 435.59s   best: 22.3835
2023-10-30 00:03:06,947:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 00:03:06,976:INFO:  Epoch 4/500:  train Loss: 50.6692   val Loss: 49.6421   time: 435.41s   best: 49.6421
2023-10-30 00:06:40,733:INFO:  Epoch 454/500:  train Loss: 17.3092   val Loss: 23.0707   time: 436.99s   best: 22.3835
2023-10-30 00:10:23,277:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 00:10:23,320:INFO:  Epoch 5/500:  train Loss: 46.5302   val Loss: 45.1667   time: 436.26s   best: 45.1667
2023-10-30 00:13:56,549:INFO:  Epoch 455/500:  train Loss: 17.1628   val Loss: 24.2226   time: 435.78s   best: 22.3835
2023-10-30 00:17:40,836:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 00:17:40,864:INFO:  Epoch 6/500:  train Loss: 43.2983   val Loss: 42.2861   time: 437.50s   best: 42.2861
2023-10-30 00:21:13,772:INFO:  Epoch 456/500:  train Loss: 17.2010   val Loss: 23.8633   time: 437.19s   best: 22.3835
2023-10-30 00:25:01,029:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 00:25:01,058:INFO:  Epoch 7/500:  train Loss: 41.0663   val Loss: 41.1751   time: 440.14s   best: 41.1751
2023-10-30 00:28:30,860:INFO:  Epoch 457/500:  train Loss: 17.2588   val Loss: 22.9944   time: 437.06s   best: 22.3835
2023-10-30 00:32:19,158:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 00:32:19,185:INFO:  Epoch 8/500:  train Loss: 38.9638   val Loss: 39.4965   time: 438.08s   best: 39.4965
2023-10-30 00:35:46,948:INFO:  Epoch 458/500:  train Loss: 17.2789   val Loss: 23.2823   time: 436.08s   best: 22.3835
2023-10-30 00:39:39,604:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 00:39:39,637:INFO:  Epoch 9/500:  train Loss: 37.3606   val Loss: 37.3252   time: 440.41s   best: 37.3252
2023-10-30 00:43:02,202:INFO:  Epoch 459/500:  train Loss: 17.2124   val Loss: 22.5571   time: 435.24s   best: 22.3835
2023-10-30 00:46:55,563:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 00:46:55,590:INFO:  Epoch 10/500:  train Loss: 36.0001   val Loss: 35.9490   time: 435.92s   best: 35.9490
2023-10-30 00:50:14,935:INFO:  Epoch 460/500:  train Loss: 17.3972   val Loss: 23.2414   time: 432.72s   best: 22.3835
2023-10-30 00:54:14,880:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 00:54:14,909:INFO:  Epoch 11/500:  train Loss: 34.8769   val Loss: 35.5814   time: 439.29s   best: 35.5814
2023-10-30 00:57:28,005:INFO:  Epoch 461/500:  train Loss: 17.3219   val Loss: 23.4284   time: 433.04s   best: 22.3835
2023-10-30 01:01:31,256:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 01:01:31,287:INFO:  Epoch 12/500:  train Loss: 34.0145   val Loss: 34.9500   time: 436.34s   best: 34.9500
2023-10-30 01:04:44,287:INFO:  Epoch 462/500:  train Loss: 17.3506   val Loss: 23.5722   time: 436.24s   best: 22.3835
2023-10-30 01:08:48,511:INFO:  Epoch 13/500:  train Loss: 33.1122   val Loss: 35.3985   time: 437.22s   best: 34.9500
2023-10-30 01:11:57,488:INFO:  Epoch 463/500:  train Loss: 17.3672   val Loss: 24.0081   time: 433.17s   best: 22.3835
2023-10-30 01:16:04,375:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 01:16:04,417:INFO:  Epoch 14/500:  train Loss: 32.4607   val Loss: 32.2422   time: 435.82s   best: 32.2422
2023-10-30 01:19:15,166:INFO:  Epoch 464/500:  train Loss: 17.1835   val Loss: 22.7322   time: 437.67s   best: 22.3835
2023-10-30 01:23:20,242:INFO:  Epoch 15/500:  train Loss: 31.8665   val Loss: 32.4485   time: 435.82s   best: 32.2422
2023-10-30 01:26:31,395:INFO:  Epoch 465/500:  train Loss: 17.3106   val Loss: 23.5256   time: 436.22s   best: 22.3835
2023-10-30 01:30:39,665:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 01:30:39,688:INFO:  Epoch 16/500:  train Loss: 30.8985   val Loss: 32.2366   time: 439.41s   best: 32.2366
2023-10-30 01:33:46,127:INFO:  Epoch 466/500:  train Loss: 17.1174   val Loss: 23.2010   time: 434.72s   best: 22.3835
2023-10-30 01:37:55,521:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 01:37:55,574:INFO:  Epoch 17/500:  train Loss: 30.4900   val Loss: 31.5266   time: 435.83s   best: 31.5266
2023-10-30 01:41:01,633:INFO:  Epoch 467/500:  train Loss: 17.2092   val Loss: 23.5865   time: 435.48s   best: 22.3835
2023-10-30 01:45:16,988:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 01:45:17,018:INFO:  Epoch 18/500:  train Loss: 30.0614   val Loss: 30.7042   time: 441.39s   best: 30.7042
2023-10-30 01:48:17,938:INFO:  Epoch 468/500:  train Loss: 17.1738   val Loss: 22.9766   time: 436.28s   best: 22.3835
2023-10-30 01:52:35,989:INFO:  Epoch 19/500:  train Loss: 29.5424   val Loss: 31.2998   time: 438.97s   best: 30.7042
2023-10-30 01:55:33,012:INFO:  Epoch 469/500:  train Loss: 17.0961   val Loss: 22.6494   time: 435.05s   best: 22.3835
2023-10-30 01:59:56,626:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 01:59:56,655:INFO:  Epoch 20/500:  train Loss: 28.9880   val Loss: 30.4856   time: 440.61s   best: 30.4856
2023-10-30 02:02:46,529:INFO:  Epoch 470/500:  train Loss: 17.3103   val Loss: 24.3248   time: 433.48s   best: 22.3835
2023-10-30 02:07:15,380:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 02:07:15,408:INFO:  Epoch 21/500:  train Loss: 28.8536   val Loss: 29.4630   time: 438.72s   best: 29.4630
2023-10-30 02:10:03,524:INFO:  Epoch 471/500:  train Loss: 17.1823   val Loss: 22.9677   time: 436.97s   best: 22.3835
2023-10-30 02:14:32,309:INFO:  Epoch 22/500:  train Loss: 28.3610   val Loss: 30.1107   time: 436.89s   best: 29.4630
2023-10-30 02:17:18,598:INFO:  Epoch 472/500:  train Loss: 17.4763   val Loss: 23.0970   time: 435.05s   best: 22.3835
2023-10-30 02:21:53,174:INFO:  Epoch 23/500:  train Loss: 28.0493   val Loss: 29.5230   time: 440.84s   best: 29.4630
2023-10-30 02:24:30,861:INFO:  Epoch 473/500:  train Loss: 17.1441   val Loss: 23.3231   time: 432.21s   best: 22.3835
2023-10-30 02:29:10,068:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 02:29:10,098:INFO:  Epoch 24/500:  train Loss: 27.6052   val Loss: 28.7680   time: 436.86s   best: 28.7680
2023-10-30 02:31:43,684:INFO:  Epoch 474/500:  train Loss: 17.2501   val Loss: 23.0107   time: 432.81s   best: 22.3835
2023-10-30 02:36:30,466:INFO:  Epoch 25/500:  train Loss: 27.3270   val Loss: 29.7382   time: 440.35s   best: 28.7680
2023-10-30 02:38:57,126:INFO:  Epoch 475/500:  train Loss: 17.2390   val Loss: 23.1685   time: 433.42s   best: 22.3835
2023-10-30 02:43:46,238:INFO:  Epoch 26/500:  train Loss: 27.0378   val Loss: 29.7462   time: 435.76s   best: 28.7680
2023-10-30 02:46:15,277:INFO:  Epoch 476/500:  train Loss: 17.1379   val Loss: 23.3960   time: 438.12s   best: 22.3835
2023-10-30 02:51:06,103:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 02:51:06,130:INFO:  Epoch 27/500:  train Loss: 26.7704   val Loss: 28.1506   time: 439.85s   best: 28.1506
2023-10-30 02:53:31,093:INFO:  Epoch 477/500:  train Loss: 17.1568   val Loss: 22.8725   time: 435.81s   best: 22.3835
2023-10-30 02:58:26,708:INFO:  Epoch 28/500:  train Loss: 26.5041   val Loss: 28.6358   time: 440.58s   best: 28.1506
2023-10-30 03:00:47,558:INFO:  Epoch 478/500:  train Loss: 17.1719   val Loss: 22.4065   time: 436.44s   best: 22.3835
2023-10-30 03:05:42,557:INFO:  Epoch 29/500:  train Loss: 26.3102   val Loss: 30.5296   time: 435.82s   best: 28.1506
2023-10-30 03:07:59,590:INFO:  Epoch 479/500:  train Loss: 17.1177   val Loss: 22.7309   time: 432.02s   best: 22.3835
2023-10-30 03:12:57,989:INFO:  Epoch 30/500:  train Loss: 26.0574   val Loss: 29.3410   time: 435.40s   best: 28.1506
2023-10-30 03:15:16,840:INFO:  Epoch 480/500:  train Loss: 17.0791   val Loss: 23.3280   time: 437.24s   best: 22.3835
2023-10-30 03:20:14,464:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 03:20:14,490:INFO:  Epoch 31/500:  train Loss: 25.8093   val Loss: 28.0327   time: 436.45s   best: 28.0327
2023-10-30 03:22:30,949:INFO:  Epoch 481/500:  train Loss: 17.2422   val Loss: 22.6020   time: 434.07s   best: 22.3835
2023-10-30 03:27:31,538:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 03:27:31,559:INFO:  Epoch 32/500:  train Loss: 25.9854   val Loss: 27.1712   time: 437.03s   best: 27.1712
2023-10-30 03:29:46,191:INFO:  Epoch 482/500:  train Loss: 17.4104   val Loss: 22.8892   time: 435.22s   best: 22.3835
2023-10-30 03:34:50,782:INFO:  Epoch 33/500:  train Loss: 25.3888   val Loss: 27.8215   time: 439.22s   best: 27.1712
2023-10-30 03:37:04,514:INFO:  Epoch 483/500:  train Loss: 17.2659   val Loss: 23.1637   time: 438.30s   best: 22.3835
2023-10-30 03:42:07,740:INFO:  Epoch 34/500:  train Loss: 25.3370   val Loss: 27.8167   time: 436.91s   best: 27.1712
2023-10-30 03:44:21,254:INFO:  Epoch 484/500:  train Loss: 17.0998   val Loss: 22.7259   time: 436.71s   best: 22.3835
2023-10-30 03:49:28,485:INFO:  Epoch 35/500:  train Loss: 25.1898   val Loss: 27.3637   time: 440.71s   best: 27.1712
2023-10-30 03:51:37,774:INFO:  Epoch 485/500:  train Loss: 17.2470   val Loss: 22.8815   time: 436.51s   best: 22.3835
2023-10-30 03:56:43,397:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 03:56:43,427:INFO:  Epoch 36/500:  train Loss: 25.1261   val Loss: 26.8688   time: 434.89s   best: 26.8688
2023-10-30 03:58:53,672:INFO:  Epoch 486/500:  train Loss: 17.0941   val Loss: 27.4257   time: 435.87s   best: 22.3835
2023-10-30 04:03:59,746:INFO:  Epoch 37/500:  train Loss: 24.8966   val Loss: 28.6004   time: 436.32s   best: 26.8688
2023-10-30 04:06:07,989:INFO:  Epoch 487/500:  train Loss: 17.6601   val Loss: 22.9082   time: 434.28s   best: 22.3835
2023-10-30 04:11:18,884:INFO:  Epoch 38/500:  train Loss: 24.7068   val Loss: 27.0686   time: 439.13s   best: 26.8688
2023-10-30 04:13:23,627:INFO:  Epoch 488/500:  train Loss: 17.0689   val Loss: 22.9877   time: 435.61s   best: 22.3835
2023-10-30 04:18:35,505:INFO:  Epoch 39/500:  train Loss: 24.7413   val Loss: 27.3988   time: 436.61s   best: 26.8688
2023-10-30 04:20:40,913:INFO:  Epoch 489/500:  train Loss: 17.0403   val Loss: 22.8079   time: 437.26s   best: 22.3835
2023-10-30 04:25:52,861:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 04:25:52,912:INFO:  Epoch 40/500:  train Loss: 24.5269   val Loss: 26.6543   time: 437.32s   best: 26.6543
2023-10-30 04:27:55,356:INFO:  Epoch 490/500:  train Loss: 17.3267   val Loss: 23.1429   time: 434.43s   best: 22.3835
2023-10-30 04:33:09,370:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 04:33:09,398:INFO:  Epoch 41/500:  train Loss: 24.4080   val Loss: 26.3955   time: 436.45s   best: 26.3955
2023-10-30 04:35:12,576:INFO:  Epoch 491/500:  train Loss: 17.1826   val Loss: 22.8107   time: 437.21s   best: 22.3835
2023-10-30 04:40:26,920:INFO:  Epoch 42/500:  train Loss: 24.0907   val Loss: 27.1963   time: 437.52s   best: 26.3955
2023-10-30 04:42:30,342:INFO:  Epoch 492/500:  train Loss: 17.2076   val Loss: 22.5504   time: 437.73s   best: 22.3835
2023-10-30 04:47:47,792:INFO:  Epoch 43/500:  train Loss: 24.3665   val Loss: 26.6573   time: 440.82s   best: 26.3955
2023-10-30 04:49:46,891:INFO:  Epoch 493/500:  train Loss: 17.1190   val Loss: 23.8637   time: 436.54s   best: 22.3835
2023-10-30 04:55:04,628:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 04:55:04,655:INFO:  Epoch 44/500:  train Loss: 23.8433   val Loss: 26.3306   time: 436.80s   best: 26.3306
2023-10-30 04:57:01,455:INFO:  Epoch 494/500:  train Loss: 17.1036   val Loss: 23.7527   time: 434.54s   best: 22.3835
2023-10-30 05:02:20,335:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 05:02:20,367:INFO:  Epoch 45/500:  train Loss: 23.6734   val Loss: 25.7408   time: 435.66s   best: 25.7408
2023-10-30 05:04:19,308:INFO:  Epoch 495/500:  train Loss: 17.0749   val Loss: 23.3076   time: 437.83s   best: 22.3835
2023-10-30 05:09:41,479:INFO:  Epoch 46/500:  train Loss: 23.7585   val Loss: 27.8859   time: 441.10s   best: 25.7408
2023-10-30 05:11:31,698:INFO:  Epoch 496/500:  train Loss: 17.2507   val Loss: 22.8232   time: 432.36s   best: 22.3835
2023-10-30 05:16:58,424:INFO:  Epoch 47/500:  train Loss: 23.5649   val Loss: 28.2064   time: 436.91s   best: 25.7408
2023-10-30 05:18:44,611:INFO:  Epoch 497/500:  train Loss: 17.2419   val Loss: 22.8839   time: 432.88s   best: 22.3835
2023-10-30 05:24:14,900:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 05:24:14,940:INFO:  Epoch 48/500:  train Loss: 23.4386   val Loss: 25.6981   time: 436.46s   best: 25.6981
2023-10-30 05:25:58,885:INFO:  Epoch 498/500:  train Loss: 17.0708   val Loss: 22.9577   time: 434.26s   best: 22.3835
2023-10-30 05:31:31,475:INFO:  Epoch 49/500:  train Loss: 23.3454   val Loss: 26.0266   time: 436.53s   best: 25.6981
2023-10-30 05:33:14,677:INFO:  Epoch 499/500:  train Loss: 17.0724   val Loss: 22.4333   time: 435.75s   best: 22.3835
2023-10-30 05:38:51,489:INFO:  Epoch 50/500:  train Loss: 23.2709   val Loss: 25.9348   time: 439.97s   best: 25.6981
2023-10-30 05:40:29,340:INFO:  Epoch 500/500:  train Loss: 17.2160   val Loss: 23.3609   time: 434.66s   best: 22.3835
2023-10-30 05:40:29,365:INFO:  -----> Training complete in 3626m 24s   best validation loss: 22.3835
 
2023-10-30 05:46:07,690:INFO:  Epoch 51/500:  train Loss: 23.0715   val Loss: 31.3038   time: 436.17s   best: 25.6981
2023-10-30 05:53:27,028:INFO:  Epoch 52/500:  train Loss: 23.2361   val Loss: 26.9871   time: 439.31s   best: 25.6981
2023-10-30 06:00:47,137:INFO:  Epoch 53/500:  train Loss: 23.0648   val Loss: 30.2680   time: 440.07s   best: 25.6981
2023-10-30 06:08:06,120:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 06:08:06,146:INFO:  Epoch 54/500:  train Loss: 22.8893   val Loss: 25.4274   time: 438.94s   best: 25.4274
2023-10-30 06:15:26,145:INFO:  Epoch 55/500:  train Loss: 22.7353   val Loss: 25.7311   time: 440.00s   best: 25.4274
2023-10-30 06:22:43,925:INFO:  Epoch 56/500:  train Loss: 22.8667   val Loss: 26.4827   time: 437.74s   best: 25.4274
2023-10-30 06:30:00,021:INFO:  Epoch 57/500:  train Loss: 22.6719   val Loss: 25.5714   time: 436.05s   best: 25.4274
2023-10-30 06:37:17,206:INFO:  Epoch 58/500:  train Loss: 22.5341   val Loss: 25.9873   time: 437.16s   best: 25.4274
2023-10-30 06:44:37,225:INFO:  Epoch 59/500:  train Loss: 22.5622   val Loss: 25.7932   time: 440.01s   best: 25.4274
2023-10-30 06:51:52,543:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 06:51:52,569:INFO:  Epoch 60/500:  train Loss: 22.3305   val Loss: 25.1864   time: 435.29s   best: 25.1864
2023-10-30 06:59:07,850:INFO:  Epoch 61/500:  train Loss: 22.2368   val Loss: 25.3093   time: 435.26s   best: 25.1864
2023-10-30 07:06:26,055:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 07:06:26,091:INFO:  Epoch 62/500:  train Loss: 22.3682   val Loss: 24.9685   time: 438.18s   best: 24.9685
2023-10-30 07:13:41,590:INFO:  Epoch 63/500:  train Loss: 22.1167   val Loss: 25.9707   time: 435.49s   best: 24.9685
2023-10-30 07:21:00,703:INFO:  Epoch 64/500:  train Loss: 22.1480   val Loss: 26.6890   time: 439.08s   best: 24.9685
2023-10-30 07:28:17,238:INFO:  Epoch 65/500:  train Loss: 22.0630   val Loss: 25.1888   time: 436.49s   best: 24.9685
2023-10-30 07:35:33,430:INFO:  Epoch 66/500:  train Loss: 21.9476   val Loss: 26.0424   time: 436.18s   best: 24.9685
2023-10-30 07:42:48,884:INFO:  Epoch 67/500:  train Loss: 21.9997   val Loss: 25.0252   time: 435.43s   best: 24.9685
2023-10-30 07:50:07,203:INFO:  Epoch 68/500:  train Loss: 21.9603   val Loss: 25.2729   time: 438.31s   best: 24.9685
2023-10-30 07:57:23,397:INFO:  Epoch 69/500:  train Loss: 21.8299   val Loss: 28.2480   time: 436.18s   best: 24.9685
2023-10-30 08:04:38,900:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 08:04:38,929:INFO:  Epoch 70/500:  train Loss: 21.8233   val Loss: 24.5868   time: 435.46s   best: 24.5868
2023-10-30 08:11:54,782:INFO:  Epoch 71/500:  train Loss: 21.7193   val Loss: 25.1393   time: 435.84s   best: 24.5868
2023-10-30 08:19:12,116:INFO:  Epoch 72/500:  train Loss: 21.5245   val Loss: 25.3194   time: 437.33s   best: 24.5868
2023-10-30 08:26:31,036:INFO:  Epoch 73/500:  train Loss: 21.7121   val Loss: 25.0365   time: 438.89s   best: 24.5868
2023-10-30 08:33:46,066:INFO:  Epoch 74/500:  train Loss: 21.4908   val Loss: 26.5181   time: 435.00s   best: 24.5868
2023-10-30 08:41:02,106:INFO:  Epoch 75/500:  train Loss: 21.4371   val Loss: 24.8135   time: 436.03s   best: 24.5868
2023-10-30 08:48:20,650:INFO:  Epoch 76/500:  train Loss: 21.4199   val Loss: 24.7864   time: 438.52s   best: 24.5868
2023-10-30 08:55:35,456:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 08:55:35,488:INFO:  Epoch 77/500:  train Loss: 21.3079   val Loss: 24.5781   time: 434.78s   best: 24.5781
2023-10-30 09:02:53,080:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 09:02:53,131:INFO:  Epoch 78/500:  train Loss: 21.2254   val Loss: 24.1293   time: 437.58s   best: 24.1293
2023-10-30 09:10:08,959:INFO:  Epoch 79/500:  train Loss: 21.1986   val Loss: 24.5641   time: 435.83s   best: 24.1293
2023-10-30 09:17:25,157:INFO:  Epoch 80/500:  train Loss: 21.1919   val Loss: 25.1061   time: 436.17s   best: 24.1293
2023-10-30 09:24:40,946:INFO:  Epoch 81/500:  train Loss: 21.1139   val Loss: 24.5283   time: 435.77s   best: 24.1293
2023-10-30 09:31:57,723:INFO:  Epoch 82/500:  train Loss: 21.1625   val Loss: 24.8030   time: 436.74s   best: 24.1293
2023-10-30 09:39:14,891:INFO:  Epoch 83/500:  train Loss: 21.3367   val Loss: 24.5993   time: 437.16s   best: 24.1293
2023-10-30 09:46:30,480:INFO:  Epoch 84/500:  train Loss: 21.0300   val Loss: 24.5437   time: 435.55s   best: 24.1293
2023-10-30 09:53:46,082:INFO:  Epoch 85/500:  train Loss: 21.2320   val Loss: 25.7673   time: 435.59s   best: 24.1293
2023-10-30 10:01:00,177:INFO:  Epoch 86/500:  train Loss: 20.9464   val Loss: 24.1960   time: 434.08s   best: 24.1293
2023-10-30 10:08:14,414:INFO:  Epoch 87/500:  train Loss: 21.0370   val Loss: 25.2010   time: 434.23s   best: 24.1293
2023-10-30 10:15:33,285:INFO:  Epoch 88/500:  train Loss: 20.7964   val Loss: 25.0426   time: 438.84s   best: 24.1293
2023-10-30 10:22:50,069:INFO:  Epoch 89/500:  train Loss: 20.7544   val Loss: 24.5021   time: 436.76s   best: 24.1293
2023-10-30 10:30:05,997:INFO:  Epoch 90/500:  train Loss: 20.7660   val Loss: 24.3943   time: 435.92s   best: 24.1293
2023-10-30 10:37:22,616:INFO:  Epoch 91/500:  train Loss: 20.7232   val Loss: 25.0338   time: 436.59s   best: 24.1293
2023-10-30 10:44:42,377:INFO:  Epoch 92/500:  train Loss: 20.6515   val Loss: 28.9414   time: 439.73s   best: 24.1293
2023-10-30 10:52:00,220:INFO:  Epoch 93/500:  train Loss: 20.6819   val Loss: 24.5676   time: 437.82s   best: 24.1293
2023-10-30 10:59:17,771:INFO:  Epoch 94/500:  train Loss: 20.8181   val Loss: 24.4185   time: 437.54s   best: 24.1293
2023-10-30 11:06:37,426:INFO:  Epoch 95/500:  train Loss: 20.8835   val Loss: 24.3241   time: 439.62s   best: 24.1293
2023-10-30 11:13:56,515:INFO:  Epoch 96/500:  train Loss: 20.5020   val Loss: 24.3191   time: 439.08s   best: 24.1293
2023-10-30 11:21:12,516:INFO:  Epoch 97/500:  train Loss: 20.5053   val Loss: 25.2270   time: 435.99s   best: 24.1293
2023-10-30 11:28:30,586:INFO:  Epoch 98/500:  train Loss: 20.5896   val Loss: 26.1008   time: 438.04s   best: 24.1293
2023-10-30 11:35:46,419:INFO:  Epoch 99/500:  train Loss: 20.4339   val Loss: 24.1489   time: 435.83s   best: 24.1293
2023-10-30 11:43:05,071:INFO:  Epoch 100/500:  train Loss: 20.5480   val Loss: 24.1602   time: 438.61s   best: 24.1293
2023-10-30 11:50:24,545:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 11:50:24,576:INFO:  Epoch 101/500:  train Loss: 20.5152   val Loss: 24.1009   time: 439.46s   best: 24.1009
2023-10-30 11:57:40,005:INFO:  Epoch 102/500:  train Loss: 20.2831   val Loss: 24.3199   time: 435.42s   best: 24.1009
2023-10-30 12:04:56,649:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 12:04:56,673:INFO:  Epoch 103/500:  train Loss: 20.4952   val Loss: 23.8194   time: 436.61s   best: 23.8194
2023-10-30 12:12:12,570:INFO:  Epoch 104/500:  train Loss: 20.3389   val Loss: 23.9185   time: 435.88s   best: 23.8194
2023-10-30 12:19:28,269:INFO:  Epoch 105/500:  train Loss: 20.2734   val Loss: 24.6006   time: 435.69s   best: 23.8194
2023-10-30 12:26:45,236:INFO:  Epoch 106/500:  train Loss: 20.6320   val Loss: 23.9592   time: 436.93s   best: 23.8194
2023-10-30 12:34:03,475:INFO:  Epoch 107/500:  train Loss: 20.3714   val Loss: 23.8969   time: 438.21s   best: 23.8194
2023-10-30 12:41:20,747:INFO:  Epoch 108/500:  train Loss: 20.4074   val Loss: 24.3956   time: 437.27s   best: 23.8194
2023-10-30 12:48:37,448:INFO:  Epoch 109/500:  train Loss: 20.2598   val Loss: 24.0093   time: 436.67s   best: 23.8194
2023-10-30 12:55:55,092:INFO:  Epoch 110/500:  train Loss: 20.1434   val Loss: 23.8230   time: 437.63s   best: 23.8194
2023-10-30 13:03:11,034:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 13:03:11,102:INFO:  Epoch 111/500:  train Loss: 20.0139   val Loss: 23.8087   time: 435.90s   best: 23.8087
2023-10-30 13:10:30,423:INFO:  Epoch 112/500:  train Loss: 20.1265   val Loss: 23.9579   time: 439.30s   best: 23.8087
2023-10-30 13:17:49,547:INFO:  Epoch 113/500:  train Loss: 20.0206   val Loss: 24.5388   time: 439.10s   best: 23.8087
2023-10-30 13:25:10,091:INFO:  Epoch 114/500:  train Loss: 20.0755   val Loss: 24.4618   time: 440.49s   best: 23.8087
2023-10-30 13:32:26,858:INFO:  Epoch 115/500:  train Loss: 19.9664   val Loss: 24.1539   time: 436.74s   best: 23.8087
2023-10-30 13:39:43,584:INFO:  Epoch 116/500:  train Loss: 19.9823   val Loss: 24.0429   time: 436.68s   best: 23.8087
2023-10-30 13:47:00,543:INFO:  Epoch 117/500:  train Loss: 19.9993   val Loss: 24.1470   time: 436.94s   best: 23.8087
2023-10-30 13:54:15,028:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 13:54:15,070:INFO:  Epoch 118/500:  train Loss: 19.9153   val Loss: 23.8014   time: 434.46s   best: 23.8014
2023-10-30 14:01:30,293:INFO:  Epoch 119/500:  train Loss: 19.8962   val Loss: 24.0802   time: 435.20s   best: 23.8014
2023-10-30 14:08:44,476:INFO:  Epoch 120/500:  train Loss: 19.9832   val Loss: 24.1878   time: 434.16s   best: 23.8014
2023-10-30 14:16:03,526:INFO:  Epoch 121/500:  train Loss: 20.0198   val Loss: 24.3404   time: 439.04s   best: 23.8014
2023-10-30 14:23:19,744:INFO:  Epoch 122/500:  train Loss: 19.8016   val Loss: 24.6576   time: 436.18s   best: 23.8014
2023-10-30 14:30:37,519:INFO:  Epoch 123/500:  train Loss: 19.9278   val Loss: 26.0221   time: 437.76s   best: 23.8014
2023-10-30 14:37:54,216:INFO:  Epoch 124/500:  train Loss: 19.9280   val Loss: 24.9406   time: 436.69s   best: 23.8014
2023-10-30 14:45:13,495:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 14:45:13,539:INFO:  Epoch 125/500:  train Loss: 19.7691   val Loss: 23.6882   time: 439.22s   best: 23.6882
2023-10-30 14:52:31,441:INFO:  Epoch 126/500:  train Loss: 19.6713   val Loss: 23.8139   time: 437.89s   best: 23.6882
2023-10-30 14:59:52,805:INFO:  Epoch 127/500:  train Loss: 19.9295   val Loss: 24.0794   time: 441.36s   best: 23.6882
2023-10-30 15:07:10,190:INFO:  Epoch 128/500:  train Loss: 19.7159   val Loss: 24.1293   time: 437.37s   best: 23.6882
2023-10-30 15:14:31,504:INFO:  Epoch 129/500:  train Loss: 19.7553   val Loss: 26.4853   time: 441.29s   best: 23.6882
2023-10-30 15:21:50,117:INFO:  Epoch 130/500:  train Loss: 19.7334   val Loss: 23.6884   time: 438.59s   best: 23.6882
2023-10-30 15:29:09,419:INFO:  Epoch 131/500:  train Loss: 19.6830   val Loss: 24.2783   time: 439.29s   best: 23.6882
2023-10-30 15:36:27,854:INFO:  Epoch 132/500:  train Loss: 19.6215   val Loss: 24.4755   time: 438.42s   best: 23.6882
2023-10-30 15:43:48,268:INFO:  Epoch 133/500:  train Loss: 19.5770   val Loss: 23.8692   time: 440.40s   best: 23.6882
2023-10-30 15:51:05,393:INFO:  Epoch 134/500:  train Loss: 19.5355   val Loss: 23.8461   time: 437.11s   best: 23.6882
2023-10-30 15:58:25,156:INFO:  Epoch 135/500:  train Loss: 19.4442   val Loss: 24.2863   time: 439.74s   best: 23.6882
2023-10-30 16:05:44,334:INFO:  Epoch 136/500:  train Loss: 19.4587   val Loss: 24.7259   time: 439.17s   best: 23.6882
2023-10-30 16:13:03,633:INFO:  Epoch 137/500:  train Loss: 19.5174   val Loss: 23.7224   time: 439.28s   best: 23.6882
2023-10-30 16:20:22,486:INFO:  Epoch 138/500:  train Loss: 19.5473   val Loss: 25.8409   time: 438.84s   best: 23.6882
2023-10-30 16:27:41,388:INFO:  Epoch 139/500:  train Loss: 19.4744   val Loss: 23.7084   time: 438.87s   best: 23.6882
2023-10-30 16:35:02,333:INFO:  Epoch 140/500:  train Loss: 19.4849   val Loss: 24.1581   time: 440.91s   best: 23.6882
2023-10-30 16:42:19,282:INFO:  Epoch 141/500:  train Loss: 19.4743   val Loss: 24.3954   time: 436.93s   best: 23.6882
2023-10-30 16:49:39,266:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 16:49:39,308:INFO:  Epoch 142/500:  train Loss: 19.4303   val Loss: 23.5542   time: 439.93s   best: 23.5542
2023-10-30 16:56:56,843:INFO:  Epoch 143/500:  train Loss: 19.7400   val Loss: 24.1089   time: 437.52s   best: 23.5542
2023-10-30 17:04:14,382:INFO:  Epoch 144/500:  train Loss: 19.4764   val Loss: 24.6464   time: 437.51s   best: 23.5542
2023-10-30 17:11:33,504:INFO:  Epoch 145/500:  train Loss: 19.3830   val Loss: 23.6631   time: 439.09s   best: 23.5542
2023-10-30 17:18:50,088:INFO:  Epoch 146/500:  train Loss: 19.2885   val Loss: 24.4323   time: 436.57s   best: 23.5542
2023-10-30 17:26:09,313:INFO:  Epoch 147/500:  train Loss: 19.7945   val Loss: 23.5629   time: 439.20s   best: 23.5542
2023-10-30 17:33:29,602:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 17:33:29,683:INFO:  Epoch 148/500:  train Loss: 19.5147   val Loss: 23.3037   time: 440.28s   best: 23.3037
2023-10-30 17:40:50,202:INFO:  Epoch 149/500:  train Loss: 19.4013   val Loss: 24.1772   time: 440.50s   best: 23.3037
2023-10-30 17:48:07,299:INFO:  Epoch 150/500:  train Loss: 19.2588   val Loss: 24.2218   time: 437.08s   best: 23.3037
2023-10-30 17:55:28,337:INFO:  Epoch 151/500:  train Loss: 19.5448   val Loss: 25.3065   time: 441.03s   best: 23.3037
2023-10-30 18:02:45,380:INFO:  Epoch 152/500:  train Loss: 19.4699   val Loss: 25.5424   time: 437.02s   best: 23.3037
2023-10-30 18:10:01,676:INFO:  Epoch 153/500:  train Loss: 19.3974   val Loss: 23.9116   time: 436.27s   best: 23.3037
2023-10-30 18:17:18,670:INFO:  Epoch 154/500:  train Loss: 19.1875   val Loss: 23.8772   time: 436.94s   best: 23.3037
2023-10-30 18:24:38,500:INFO:  Epoch 155/500:  train Loss: 19.3435   val Loss: 23.8252   time: 439.82s   best: 23.3037
2023-10-30 18:31:55,589:INFO:  Epoch 156/500:  train Loss: 19.3624   val Loss: 25.0523   time: 437.06s   best: 23.3037
2023-10-30 18:39:16,182:INFO:  Epoch 157/500:  train Loss: 19.2101   val Loss: 23.7189   time: 440.58s   best: 23.3037
2023-10-30 18:46:35,741:INFO:  Epoch 158/500:  train Loss: 19.4702   val Loss: 25.6796   time: 439.53s   best: 23.3037
2023-10-30 18:53:55,999:INFO:  Epoch 159/500:  train Loss: 19.2151   val Loss: 24.1149   time: 440.23s   best: 23.3037
2023-10-30 19:01:14,103:INFO:  Epoch 160/500:  train Loss: 19.4736   val Loss: 23.6266   time: 438.08s   best: 23.3037
2023-10-30 19:08:30,537:INFO:  Epoch 161/500:  train Loss: 19.1207   val Loss: 24.1395   time: 436.41s   best: 23.3037
2023-10-30 19:15:47,188:INFO:  Epoch 162/500:  train Loss: 19.1209   val Loss: 23.3125   time: 436.63s   best: 23.3037
2023-10-30 19:23:06,936:INFO:  Epoch 163/500:  train Loss: 19.1527   val Loss: 23.3938   time: 439.71s   best: 23.3037
2023-10-30 19:30:25,444:INFO:  Epoch 164/500:  train Loss: 19.0514   val Loss: 23.5996   time: 438.49s   best: 23.3037
2023-10-30 19:37:45,820:INFO:  Epoch 165/500:  train Loss: 19.0199   val Loss: 23.6976   time: 440.36s   best: 23.3037
2023-10-30 19:45:03,314:INFO:  Epoch 166/500:  train Loss: 19.1169   val Loss: 24.8262   time: 437.47s   best: 23.3037
2023-10-30 19:52:23,158:INFO:  Epoch 167/500:  train Loss: 18.9313   val Loss: 25.1673   time: 439.83s   best: 23.3037
2023-10-30 19:59:40,053:INFO:  Epoch 168/500:  train Loss: 19.1287   val Loss: 24.1551   time: 436.87s   best: 23.3037
2023-10-30 20:06:57,864:INFO:  Epoch 169/500:  train Loss: 19.3230   val Loss: 23.6575   time: 437.80s   best: 23.3037
2023-10-30 20:14:14,829:INFO:  Epoch 170/500:  train Loss: 19.2610   val Loss: 23.7067   time: 436.94s   best: 23.3037
2023-10-30 20:21:33,013:INFO:  Epoch 171/500:  train Loss: 19.0522   val Loss: 23.9476   time: 438.15s   best: 23.3037
2023-10-30 20:28:49,467:INFO:  Epoch 172/500:  train Loss: 18.9331   val Loss: 23.8834   time: 436.43s   best: 23.3037
2023-10-30 20:36:05,900:INFO:  Epoch 173/500:  train Loss: 19.0554   val Loss: 23.8736   time: 436.41s   best: 23.3037
2023-10-30 20:43:26,564:INFO:  Epoch 174/500:  train Loss: 18.9095   val Loss: 23.8103   time: 440.62s   best: 23.3037
2023-10-30 20:50:43,783:INFO:  Epoch 175/500:  train Loss: 19.0300   val Loss: 24.3001   time: 437.21s   best: 23.3037
2023-10-30 20:58:01,341:INFO:  Epoch 176/500:  train Loss: 19.0073   val Loss: 23.5275   time: 437.53s   best: 23.3037
2023-10-30 21:05:21,492:INFO:  Epoch 177/500:  train Loss: 18.9772   val Loss: 24.0347   time: 440.14s   best: 23.3037
2023-10-30 21:12:39,830:INFO:  Epoch 178/500:  train Loss: 19.4548   val Loss: 24.4551   time: 438.31s   best: 23.3037
2023-10-30 21:20:00,176:INFO:  Epoch 179/500:  train Loss: 18.8430   val Loss: 23.5539   time: 440.34s   best: 23.3037
2023-10-30 21:27:17,282:INFO:  Epoch 180/500:  train Loss: 19.0407   val Loss: 23.9155   time: 437.09s   best: 23.3037
2023-10-30 21:34:38,311:INFO:  Epoch 181/500:  train Loss: 18.9711   val Loss: 23.8159   time: 441.02s   best: 23.3037
2023-10-30 21:41:59,504:INFO:  Epoch 182/500:  train Loss: 18.8725   val Loss: 24.2136   time: 441.17s   best: 23.3037
2023-10-30 21:49:18,494:INFO:  Epoch 183/500:  train Loss: 18.8679   val Loss: 23.4366   time: 438.97s   best: 23.3037
2023-10-30 21:56:38,307:INFO:  Epoch 184/500:  train Loss: 18.7750   val Loss: 23.7873   time: 439.79s   best: 23.3037
2023-10-30 22:03:54,955:INFO:  Epoch 185/500:  train Loss: 19.0864   val Loss: 24.2022   time: 436.61s   best: 23.3037
2023-10-30 22:11:11,206:INFO:  Epoch 186/500:  train Loss: 18.7570   val Loss: 23.7573   time: 436.23s   best: 23.3037
2023-10-30 22:18:27,940:INFO:  Epoch 187/500:  train Loss: 18.8510   val Loss: 23.8343   time: 436.71s   best: 23.3037
2023-10-30 22:25:48,479:INFO:  Epoch 188/500:  train Loss: 19.1159   val Loss: 23.9065   time: 440.53s   best: 23.3037
2023-10-30 22:33:07,168:INFO:  Epoch 189/500:  train Loss: 18.9676   val Loss: 23.3962   time: 438.66s   best: 23.3037
2023-10-30 22:40:23,698:INFO:  Epoch 190/500:  train Loss: 18.8997   val Loss: 24.3720   time: 436.52s   best: 23.3037
2023-10-30 22:47:43,937:INFO:  Epoch 191/500:  train Loss: 18.8310   val Loss: 23.4770   time: 440.23s   best: 23.3037
2023-10-30 22:55:04,592:INFO:  Epoch 192/500:  train Loss: 18.9754   val Loss: 23.5316   time: 440.65s   best: 23.3037
2023-10-30 23:02:25,024:INFO:  Epoch 193/500:  train Loss: 18.6990   val Loss: 24.9196   time: 440.42s   best: 23.3037
2023-10-30 23:09:43,339:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-30 23:09:43,366:INFO:  Epoch 194/500:  train Loss: 18.6580   val Loss: 23.2613   time: 438.16s   best: 23.2613
2023-10-30 23:17:03,558:INFO:  Epoch 195/500:  train Loss: 18.6164   val Loss: 23.8067   time: 440.18s   best: 23.2613
2023-10-30 23:24:23,605:INFO:  Epoch 196/500:  train Loss: 18.8195   val Loss: 28.8968   time: 440.04s   best: 23.2613
2023-10-30 23:31:41,620:INFO:  Epoch 197/500:  train Loss: 18.7730   val Loss: 24.5421   time: 437.99s   best: 23.2613
2023-10-30 23:38:58,858:INFO:  Epoch 198/500:  train Loss: 18.7577   val Loss: 23.6117   time: 437.23s   best: 23.2613
2023-10-30 23:46:19,419:INFO:  Epoch 199/500:  train Loss: 18.8691   val Loss: 23.8188   time: 440.54s   best: 23.2613
2023-10-30 23:53:40,354:INFO:  Epoch 200/500:  train Loss: 18.6503   val Loss: 23.9390   time: 440.91s   best: 23.2613
2023-10-31 00:00:58,193:INFO:  Epoch 201/500:  train Loss: 19.1019   val Loss: 23.6660   time: 437.81s   best: 23.2613
2023-10-31 00:08:17,501:INFO:  Epoch 202/500:  train Loss: 18.5897   val Loss: 23.3129   time: 439.29s   best: 23.2613
2023-10-31 00:15:36,510:INFO:  Epoch 203/500:  train Loss: 18.6404   val Loss: 25.2713   time: 438.99s   best: 23.2613
2023-10-31 00:22:55,685:INFO:  Epoch 204/500:  train Loss: 18.6805   val Loss: 23.7515   time: 439.17s   best: 23.2613
2023-10-31 00:30:15,479:INFO:  Epoch 205/500:  train Loss: 18.5374   val Loss: 24.6297   time: 439.78s   best: 23.2613
2023-10-31 00:37:36,064:INFO:  Epoch 206/500:  train Loss: 18.8365   val Loss: 24.5252   time: 440.56s   best: 23.2613
2023-10-31 00:44:54,629:INFO:  Epoch 207/500:  train Loss: 18.6874   val Loss: 23.9400   time: 438.55s   best: 23.2613
2023-10-31 00:52:14,188:INFO:  Epoch 208/500:  train Loss: 18.7984   val Loss: 24.1948   time: 439.54s   best: 23.2613
2023-10-31 00:59:34,184:INFO:  Epoch 209/500:  train Loss: 18.5557   val Loss: 23.8073   time: 439.97s   best: 23.2613
2023-10-31 01:06:51,443:INFO:  Epoch 210/500:  train Loss: 18.6598   val Loss: 23.9386   time: 437.24s   best: 23.2613
2023-10-31 01:14:11,201:INFO:  Epoch 211/500:  train Loss: 18.6584   val Loss: 23.7802   time: 439.74s   best: 23.2613
2023-10-31 01:21:31,341:INFO:  Epoch 212/500:  train Loss: 18.6537   val Loss: 23.8131   time: 440.11s   best: 23.2613
2023-10-31 01:28:48,818:INFO:  Epoch 213/500:  train Loss: 18.5542   val Loss: 23.9388   time: 437.45s   best: 23.2613
2023-10-31 01:36:05,420:INFO:  Epoch 214/500:  train Loss: 18.6502   val Loss: 23.7944   time: 436.59s   best: 23.2613
2023-10-31 01:43:21,957:INFO:  Epoch 215/500:  train Loss: 18.6557   val Loss: 23.9040   time: 436.52s   best: 23.2613
2023-10-31 01:50:38,272:INFO:  Epoch 216/500:  train Loss: 18.6850   val Loss: 24.4238   time: 436.29s   best: 23.2613
2023-10-31 01:57:58,082:INFO:  Epoch 217/500:  train Loss: 18.6337   val Loss: 23.5633   time: 439.78s   best: 23.2613
2023-10-31 02:05:18,166:INFO:  Epoch 218/500:  train Loss: 18.4682   val Loss: 23.6159   time: 440.07s   best: 23.2613
2023-10-31 02:12:36,016:INFO:  Epoch 219/500:  train Loss: 18.5472   val Loss: 23.7843   time: 437.84s   best: 23.2613
2023-10-31 02:19:52,671:INFO:  Epoch 220/500:  train Loss: 18.4320   val Loss: 23.9861   time: 436.63s   best: 23.2613
2023-10-31 02:27:10,543:INFO:  Epoch 221/500:  train Loss: 18.4265   val Loss: 23.6195   time: 437.85s   best: 23.2613
2023-10-31 02:34:27,333:INFO:  Epoch 222/500:  train Loss: 18.3453   val Loss: 23.6577   time: 436.78s   best: 23.2613
2023-10-31 02:41:43,429:INFO:  Epoch 223/500:  train Loss: 18.6658   val Loss: 23.5758   time: 436.08s   best: 23.2613
2023-10-31 02:49:03,940:INFO:  Epoch 224/500:  train Loss: 18.4836   val Loss: 24.7459   time: 440.49s   best: 23.2613
2023-10-31 02:56:23,884:INFO:  Epoch 225/500:  train Loss: 18.4438   val Loss: 23.2802   time: 439.92s   best: 23.2613
2023-10-31 03:03:43,524:INFO:  Epoch 226/500:  train Loss: 18.3956   val Loss: 23.2879   time: 439.61s   best: 23.2613
2023-10-31 03:11:00,788:INFO:  Epoch 227/500:  train Loss: 18.3886   val Loss: 23.8063   time: 437.25s   best: 23.2613
2023-10-31 03:18:16,651:INFO:  Epoch 228/500:  train Loss: 18.7467   val Loss: 23.4802   time: 435.83s   best: 23.2613
2023-10-31 03:25:34,197:INFO:  Epoch 229/500:  train Loss: 18.3850   val Loss: 23.9432   time: 437.52s   best: 23.2613
2023-10-31 03:32:55,267:INFO:  Epoch 230/500:  train Loss: 18.2850   val Loss: 24.0444   time: 441.03s   best: 23.2613
2023-10-31 03:40:14,795:INFO:  Epoch 231/500:  train Loss: 18.6470   val Loss: 23.5186   time: 439.52s   best: 23.2613
2023-10-31 03:47:32,256:INFO:  Epoch 232/500:  train Loss: 18.3372   val Loss: 24.3511   time: 437.43s   best: 23.2613
2023-10-31 03:54:48,626:INFO:  Epoch 233/500:  train Loss: 18.2413   val Loss: 24.4313   time: 436.36s   best: 23.2613
2023-10-31 04:02:09,537:INFO:  Epoch 234/500:  train Loss: 18.3433   val Loss: 24.2647   time: 440.88s   best: 23.2613
2023-10-31 04:09:26,734:INFO:  Epoch 235/500:  train Loss: 18.5065   val Loss: 23.6783   time: 437.18s   best: 23.2613
2023-10-31 04:16:43,559:INFO:  Epoch 236/500:  train Loss: 18.3668   val Loss: 23.6895   time: 436.80s   best: 23.2613
2023-10-31 04:23:59,440:INFO:  Epoch 237/500:  train Loss: 18.3359   val Loss: 23.7159   time: 435.86s   best: 23.2613
2023-10-31 04:31:16,886:INFO:  Epoch 238/500:  train Loss: 18.3305   val Loss: 23.6698   time: 437.43s   best: 23.2613
2023-10-31 04:38:33,221:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-31 04:38:33,256:INFO:  Epoch 239/500:  train Loss: 18.3894   val Loss: 23.2111   time: 436.31s   best: 23.2111
2023-10-31 04:45:53,876:INFO:  Epoch 240/500:  train Loss: 18.3584   val Loss: 24.8783   time: 440.60s   best: 23.2111
2023-10-31 04:53:13,429:INFO:  Epoch 241/500:  train Loss: 18.3765   val Loss: 23.6176   time: 439.53s   best: 23.2111
2023-10-31 05:00:34,789:INFO:  Epoch 242/500:  train Loss: 18.3066   val Loss: 24.1774   time: 441.34s   best: 23.2111
2023-10-31 05:07:54,978:INFO:  Epoch 243/500:  train Loss: 18.6498   val Loss: 24.0466   time: 440.18s   best: 23.2111
2023-10-31 05:15:15,037:INFO:  Epoch 244/500:  train Loss: 18.3199   val Loss: 24.0691   time: 440.04s   best: 23.2111
2023-10-31 05:22:36,509:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-31 05:22:36,543:INFO:  Epoch 245/500:  train Loss: 18.3229   val Loss: 22.9898   time: 441.44s   best: 22.9898
2023-10-31 05:29:56,499:INFO:  Epoch 246/500:  train Loss: 18.2828   val Loss: 23.7857   time: 439.94s   best: 22.9898
2023-10-31 05:37:12,948:INFO:  Epoch 247/500:  train Loss: 18.3625   val Loss: 23.2697   time: 436.43s   best: 22.9898
2023-10-31 05:44:30,013:INFO:  Epoch 248/500:  train Loss: 18.1892   val Loss: 23.7937   time: 437.06s   best: 22.9898
2023-10-31 05:51:46,944:INFO:  Epoch 249/500:  train Loss: 18.3239   val Loss: 24.0969   time: 436.89s   best: 22.9898
2023-10-31 05:59:03,537:INFO:  Epoch 250/500:  train Loss: 18.4035   val Loss: 23.6288   time: 436.58s   best: 22.9898
2023-10-31 06:06:21,528:INFO:  Epoch 251/500:  train Loss: 18.3289   val Loss: 23.2588   time: 437.97s   best: 22.9898
2023-10-31 06:13:38,380:INFO:  Epoch 252/500:  train Loss: 18.1297   val Loss: 23.6439   time: 436.84s   best: 22.9898
2023-10-31 06:20:55,888:INFO:  Epoch 253/500:  train Loss: 18.3007   val Loss: 23.5257   time: 437.48s   best: 22.9898
2023-10-31 06:28:14,029:INFO:  Epoch 254/500:  train Loss: 18.1268   val Loss: 23.7514   time: 438.12s   best: 22.9898
2023-10-31 06:35:35,847:INFO:  Epoch 255/500:  train Loss: 18.2203   val Loss: 23.6374   time: 441.79s   best: 22.9898
2023-10-31 06:42:56,609:INFO:  Epoch 256/500:  train Loss: 18.1929   val Loss: 27.2858   time: 440.73s   best: 22.9898
2023-10-31 06:50:16,507:INFO:  Epoch 257/500:  train Loss: 18.1053   val Loss: 24.6646   time: 439.89s   best: 22.9898
2023-10-31 06:57:33,552:INFO:  Epoch 258/500:  train Loss: 18.0711   val Loss: 23.4176   time: 437.00s   best: 22.9898
2023-10-31 07:04:54,971:INFO:  Epoch 259/500:  train Loss: 18.1837   val Loss: 25.9079   time: 441.41s   best: 22.9898
2023-10-31 07:12:11,660:INFO:  Epoch 260/500:  train Loss: 18.0586   val Loss: 23.3245   time: 436.67s   best: 22.9898
2023-10-31 07:19:32,216:INFO:  Epoch 261/500:  train Loss: 18.2548   val Loss: 25.3457   time: 440.55s   best: 22.9898
2023-10-31 07:26:51,038:INFO:  Epoch 262/500:  train Loss: 18.6841   val Loss: 34.7440   time: 438.82s   best: 22.9898
2023-10-31 07:34:07,883:INFO:  Epoch 263/500:  train Loss: 18.2735   val Loss: 25.7046   time: 436.82s   best: 22.9898
2023-10-31 07:41:27,575:INFO:  Epoch 264/500:  train Loss: 18.2191   val Loss: 23.2692   time: 439.67s   best: 22.9898
2023-10-31 07:48:48,585:INFO:  Epoch 265/500:  train Loss: 18.1696   val Loss: 24.2733   time: 440.99s   best: 22.9898
2023-10-31 07:56:08,934:INFO:  Epoch 266/500:  train Loss: 18.1010   val Loss: 23.5874   time: 440.33s   best: 22.9898
2023-10-31 08:03:29,053:INFO:  Epoch 267/500:  train Loss: 18.1327   val Loss: 28.8629   time: 440.09s   best: 22.9898
2023-10-31 08:10:48,665:INFO:  Epoch 268/500:  train Loss: 18.1418   val Loss: 24.2510   time: 439.60s   best: 22.9898
2023-10-31 08:18:07,195:INFO:  Epoch 269/500:  train Loss: 18.0495   val Loss: 23.5476   time: 438.51s   best: 22.9898
2023-10-31 08:25:24,186:INFO:  Epoch 270/500:  train Loss: 18.1488   val Loss: 23.5175   time: 436.97s   best: 22.9898
2023-10-31 08:32:39,263:INFO:  Epoch 271/500:  train Loss: 18.0528   val Loss: 23.4212   time: 435.04s   best: 22.9898
2023-10-31 08:39:59,501:INFO:  Epoch 272/500:  train Loss: 18.5010   val Loss: 23.7907   time: 440.21s   best: 22.9898
2023-10-31 08:47:20,711:INFO:  Epoch 273/500:  train Loss: 18.2005   val Loss: 24.6640   time: 441.20s   best: 22.9898
2023-10-31 08:54:40,069:INFO:  Epoch 274/500:  train Loss: 18.0550   val Loss: 23.5914   time: 439.35s   best: 22.9898
2023-10-31 09:01:56,998:INFO:  Epoch 275/500:  train Loss: 17.9911   val Loss: 23.2828   time: 436.91s   best: 22.9898
2023-10-31 09:09:17,305:INFO:  Epoch 276/500:  train Loss: 18.1211   val Loss: 24.9285   time: 440.29s   best: 22.9898
2023-10-31 09:16:36,374:INFO:  Epoch 277/500:  train Loss: 18.1438   val Loss: 23.3724   time: 439.05s   best: 22.9898
2023-10-31 09:23:52,065:INFO:  Epoch 278/500:  train Loss: 18.0604   val Loss: 23.7985   time: 435.68s   best: 22.9898
2023-10-31 09:31:09,497:INFO:  Epoch 279/500:  train Loss: 18.1571   val Loss: 23.7892   time: 437.41s   best: 22.9898
2023-10-31 09:38:29,625:INFO:  Epoch 280/500:  train Loss: 18.0769   val Loss: 24.7682   time: 440.10s   best: 22.9898
2023-10-31 09:45:46,212:INFO:  Epoch 281/500:  train Loss: 18.0578   val Loss: 23.6401   time: 436.55s   best: 22.9898
2023-10-31 09:53:03,206:INFO:  Epoch 282/500:  train Loss: 18.0561   val Loss: 23.8558   time: 436.98s   best: 22.9898
2023-10-31 10:00:22,317:INFO:  Epoch 283/500:  train Loss: 18.0409   val Loss: 23.2104   time: 439.09s   best: 22.9898
2023-10-31 10:07:42,885:INFO:  Epoch 284/500:  train Loss: 17.9090   val Loss: 23.7867   time: 440.54s   best: 22.9898
2023-10-31 10:15:01,550:INFO:  Epoch 285/500:  train Loss: 17.9991   val Loss: 23.8248   time: 438.63s   best: 22.9898
2023-10-31 10:22:19,236:INFO:  Epoch 286/500:  train Loss: 17.9935   val Loss: 23.8509   time: 437.64s   best: 22.9898
2023-10-31 10:29:36,811:INFO:  Epoch 287/500:  train Loss: 18.2766   val Loss: 23.1729   time: 437.57s   best: 22.9898
2023-10-31 10:36:53,958:INFO:  Epoch 288/500:  train Loss: 18.1022   val Loss: 23.5704   time: 437.12s   best: 22.9898
2023-10-31 10:44:10,630:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-31 10:44:10,670:INFO:  Epoch 289/500:  train Loss: 18.5900   val Loss: 22.9039   time: 436.63s   best: 22.9039
2023-10-31 10:51:28,557:INFO:  Epoch 290/500:  train Loss: 18.1006   val Loss: 23.2666   time: 437.89s   best: 22.9039
2023-10-31 10:58:43,565:INFO:  Epoch 291/500:  train Loss: 18.0647   val Loss: 23.5046   time: 435.00s   best: 22.9039
2023-10-31 11:06:02,045:INFO:  Epoch 292/500:  train Loss: 17.9442   val Loss: 23.7323   time: 438.47s   best: 22.9039
2023-10-31 11:13:18,951:INFO:  Epoch 293/500:  train Loss: 18.0163   val Loss: 24.8150   time: 436.86s   best: 22.9039
2023-10-31 11:20:36,853:INFO:  Epoch 294/500:  train Loss: 18.4149   val Loss: 23.3788   time: 437.87s   best: 22.9039
2023-10-31 11:27:52,902:INFO:  Epoch 295/500:  train Loss: 17.9549   val Loss: 23.3512   time: 436.03s   best: 22.9039
2023-10-31 11:35:08,211:INFO:  Epoch 296/500:  train Loss: 17.9576   val Loss: 23.0333   time: 435.28s   best: 22.9039
2023-10-31 11:42:23,625:INFO:  Epoch 297/500:  train Loss: 17.9184   val Loss: 24.7627   time: 435.40s   best: 22.9039
2023-10-31 11:49:39,132:INFO:  Epoch 298/500:  train Loss: 17.8455   val Loss: 23.0491   time: 435.50s   best: 22.9039
2023-10-31 11:56:54,024:INFO:  Epoch 299/500:  train Loss: 17.9422   val Loss: 23.5689   time: 434.87s   best: 22.9039
2023-10-31 12:04:12,357:INFO:  Epoch 300/500:  train Loss: 17.8377   val Loss: 23.4531   time: 438.32s   best: 22.9039
2023-10-31 12:11:31,053:INFO:  Epoch 301/500:  train Loss: 18.2675   val Loss: 27.0542   time: 438.68s   best: 22.9039
2023-10-31 12:18:46,148:INFO:  Epoch 302/500:  train Loss: 18.5884   val Loss: 23.7028   time: 435.09s   best: 22.9039
2023-10-31 12:26:03,449:INFO:  Epoch 303/500:  train Loss: 18.0530   val Loss: 23.1604   time: 437.28s   best: 22.9039
2023-10-31 12:33:21,190:INFO:  Epoch 304/500:  train Loss: 17.9214   val Loss: 24.1054   time: 437.72s   best: 22.9039
2023-10-31 12:40:39,829:INFO:  Epoch 305/500:  train Loss: 17.8403   val Loss: 23.0698   time: 438.61s   best: 22.9039
2023-10-31 12:47:57,373:INFO:  Epoch 306/500:  train Loss: 17.7684   val Loss: 23.3826   time: 437.52s   best: 22.9039
2023-10-31 12:55:13,679:INFO:  Epoch 307/500:  train Loss: 18.3230   val Loss: 23.3860   time: 436.28s   best: 22.9039
2023-10-31 13:02:29,108:INFO:  Epoch 308/500:  train Loss: 17.8617   val Loss: 24.3306   time: 435.42s   best: 22.9039
2023-10-31 13:09:47,222:INFO:  Epoch 309/500:  train Loss: 17.8922   val Loss: 25.0283   time: 438.09s   best: 22.9039
2023-10-31 13:17:06,016:INFO:  Epoch 310/500:  train Loss: 17.8831   val Loss: 23.9712   time: 438.77s   best: 22.9039
2023-10-31 13:24:26,213:INFO:  Epoch 311/500:  train Loss: 17.9971   val Loss: 23.9757   time: 440.19s   best: 22.9039
2023-10-31 13:31:47,336:INFO:  Epoch 312/500:  train Loss: 17.9976   val Loss: 23.3552   time: 441.09s   best: 22.9039
2023-10-31 13:39:04,909:INFO:  Epoch 313/500:  train Loss: 17.8678   val Loss: 22.9466   time: 437.56s   best: 22.9039
2023-10-31 13:46:25,053:INFO:  Epoch 314/500:  train Loss: 17.8931   val Loss: 23.3197   time: 440.14s   best: 22.9039
2023-10-31 13:53:42,892:INFO:  Epoch 315/500:  train Loss: 17.8338   val Loss: 23.1597   time: 437.80s   best: 22.9039
2023-10-31 14:00:59,393:INFO:  Epoch 316/500:  train Loss: 17.9380   val Loss: 25.4081   time: 436.49s   best: 22.9039
2023-10-31 14:08:15,435:INFO:  Epoch 317/500:  train Loss: 18.0694   val Loss: 23.1892   time: 436.02s   best: 22.9039
2023-10-31 14:15:32,506:INFO:  Epoch 318/500:  train Loss: 17.7563   val Loss: 23.8357   time: 437.06s   best: 22.9039
2023-10-31 14:22:52,231:INFO:  Epoch 319/500:  train Loss: 17.8226   val Loss: 26.3857   time: 439.71s   best: 22.9039
2023-10-31 14:30:09,534:INFO:  Epoch 320/500:  train Loss: 17.9653   val Loss: 23.6275   time: 437.28s   best: 22.9039
2023-10-31 14:37:30,752:INFO:  Epoch 321/500:  train Loss: 17.9018   val Loss: 24.6550   time: 441.19s   best: 22.9039
2023-10-31 14:44:51,640:INFO:  Epoch 322/500:  train Loss: 17.8772   val Loss: 23.2371   time: 440.88s   best: 22.9039
2023-10-31 14:52:11,429:INFO:  Epoch 323/500:  train Loss: 17.7842   val Loss: 23.4932   time: 439.76s   best: 22.9039
2023-10-31 14:59:31,828:INFO:  Epoch 324/500:  train Loss: 17.7174   val Loss: 22.9771   time: 440.38s   best: 22.9039
2023-10-31 15:06:51,560:INFO:  Epoch 325/500:  train Loss: 17.6786   val Loss: 23.7740   time: 439.72s   best: 22.9039
2023-10-31 15:14:12,468:INFO:  Epoch 326/500:  train Loss: 18.0236   val Loss: 23.5062   time: 440.88s   best: 22.9039
2023-10-31 15:21:32,887:INFO:  Epoch 327/500:  train Loss: 17.8879   val Loss: 25.6681   time: 440.41s   best: 22.9039
2023-10-31 15:28:51,311:INFO:  Epoch 328/500:  train Loss: 17.7652   val Loss: 24.7809   time: 438.40s   best: 22.9039
2023-10-31 15:36:11,768:INFO:  Epoch 329/500:  train Loss: 17.8550   val Loss: 23.6620   time: 440.43s   best: 22.9039
2023-10-31 15:43:29,950:INFO:  Epoch 330/500:  train Loss: 17.9516   val Loss: 24.7270   time: 438.15s   best: 22.9039
2023-10-31 15:50:49,697:INFO:  Epoch 331/500:  train Loss: 17.9814   val Loss: 24.7219   time: 439.74s   best: 22.9039
2023-10-31 15:58:09,404:INFO:  Epoch 332/500:  train Loss: 17.8932   val Loss: 23.5893   time: 439.68s   best: 22.9039
2023-10-31 16:05:29,465:INFO:  Epoch 333/500:  train Loss: 17.7895   val Loss: 23.5237   time: 440.03s   best: 22.9039
2023-10-31 16:12:47,285:INFO:  Epoch 334/500:  train Loss: 17.8329   val Loss: 23.5151   time: 437.81s   best: 22.9039
2023-10-31 16:20:07,175:INFO:  Epoch 335/500:  train Loss: 17.8628   val Loss: 23.3726   time: 439.85s   best: 22.9039
2023-10-31 16:27:28,908:INFO:  Epoch 336/500:  train Loss: 17.9896   val Loss: 23.3347   time: 441.71s   best: 22.9039
2023-10-31 16:34:48,903:INFO:  Epoch 337/500:  train Loss: 17.8486   val Loss: 23.1145   time: 439.99s   best: 22.9039
2023-10-31 16:42:09,155:INFO:  Epoch 338/500:  train Loss: 17.7156   val Loss: 23.3528   time: 440.20s   best: 22.9039
2023-10-31 16:49:26,110:INFO:  Epoch 339/500:  train Loss: 18.2411   val Loss: 28.1310   time: 436.95s   best: 22.9039
2023-10-31 16:56:45,511:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-31 16:56:45,551:INFO:  Epoch 340/500:  train Loss: 18.1674   val Loss: 22.8507   time: 439.38s   best: 22.8507
2023-10-31 17:04:06,049:INFO:  Epoch 341/500:  train Loss: 17.7435   val Loss: 23.2423   time: 440.50s   best: 22.8507
2023-10-31 17:11:27,317:INFO:  Epoch 342/500:  train Loss: 18.0884   val Loss: 25.0660   time: 441.24s   best: 22.8507
2023-10-31 17:18:43,618:INFO:  Epoch 343/500:  train Loss: 17.8081   val Loss: 23.2291   time: 436.28s   best: 22.8507
2023-10-31 17:26:04,905:INFO:  Epoch 344/500:  train Loss: 17.8828   val Loss: 23.7780   time: 441.26s   best: 22.8507
2023-10-31 17:33:24,524:INFO:  Epoch 345/500:  train Loss: 17.7183   val Loss: 23.6630   time: 439.59s   best: 22.8507
2023-10-31 17:40:41,625:INFO:  Epoch 346/500:  train Loss: 17.8184   val Loss: 24.1229   time: 437.07s   best: 22.8507
2023-10-31 17:48:02,250:INFO:  Epoch 347/500:  train Loss: 18.0556   val Loss: 22.9498   time: 440.60s   best: 22.8507
2023-10-31 17:55:16,963:INFO:  Epoch 348/500:  train Loss: 18.0089   val Loss: 23.1147   time: 434.70s   best: 22.8507
2023-10-31 18:02:33,522:INFO:  Epoch 349/500:  train Loss: 18.0795   val Loss: 23.7450   time: 436.54s   best: 22.8507
2023-10-31 18:09:53,623:INFO:  Epoch 350/500:  train Loss: 18.0392   val Loss: 23.7126   time: 440.08s   best: 22.8507
2023-10-31 18:17:11,663:INFO:  Epoch 351/500:  train Loss: 17.9524   val Loss: 22.8828   time: 438.02s   best: 22.8507
2023-10-31 18:24:27,807:INFO:  Epoch 352/500:  train Loss: 17.7471   val Loss: 23.3784   time: 436.13s   best: 22.8507
2023-10-31 18:31:44,338:INFO:  Epoch 353/500:  train Loss: 18.0117   val Loss: 30.1423   time: 436.51s   best: 22.8507
2023-10-31 18:39:02,328:INFO:  Epoch 354/500:  train Loss: 17.9199   val Loss: 22.8821   time: 437.96s   best: 22.8507
2023-10-31 18:46:19,309:INFO:  Epoch 355/500:  train Loss: 17.6531   val Loss: 23.1873   time: 436.97s   best: 22.8507
2023-10-31 18:53:36,144:INFO:  Epoch 356/500:  train Loss: 17.8191   val Loss: 23.1199   time: 436.81s   best: 22.8507
2023-10-31 19:00:53,741:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-10-31 19:00:53,768:INFO:  Epoch 357/500:  train Loss: 17.5792   val Loss: 22.6034   time: 437.58s   best: 22.6034
2023-10-31 19:08:13,607:INFO:  Epoch 358/500:  train Loss: 17.6648   val Loss: 23.6211   time: 439.84s   best: 22.6034
2023-10-31 19:15:30,967:INFO:  Epoch 359/500:  train Loss: 18.0312   val Loss: 23.3385   time: 437.34s   best: 22.6034
2023-10-31 19:22:48,363:INFO:  Epoch 360/500:  train Loss: 17.7831   val Loss: 24.0352   time: 437.39s   best: 22.6034
2023-10-31 19:30:05,223:INFO:  Epoch 361/500:  train Loss: 17.6278   val Loss: 23.0218   time: 436.83s   best: 22.6034
2023-10-31 19:37:22,312:INFO:  Epoch 362/500:  train Loss: 17.6262   val Loss: 23.5954   time: 437.04s   best: 22.6034
2023-10-31 19:44:39,052:INFO:  Epoch 363/500:  train Loss: 17.7396   val Loss: 22.9773   time: 436.73s   best: 22.6034
2023-10-31 19:51:55,205:INFO:  Epoch 364/500:  train Loss: 17.6125   val Loss: 23.5721   time: 436.14s   best: 22.6034
2023-10-31 19:59:15,176:INFO:  Epoch 365/500:  train Loss: 17.7233   val Loss: 23.3954   time: 439.95s   best: 22.6034
2023-10-31 20:06:36,110:INFO:  Epoch 366/500:  train Loss: 17.5660   val Loss: 23.0816   time: 440.91s   best: 22.6034
2023-10-31 20:13:55,458:INFO:  Epoch 367/500:  train Loss: 17.6132   val Loss: 23.2101   time: 439.32s   best: 22.6034
2023-10-31 20:21:14,842:INFO:  Epoch 368/500:  train Loss: 17.7591   val Loss: 23.8292   time: 439.37s   best: 22.6034
2023-10-31 20:28:32,117:INFO:  Epoch 369/500:  train Loss: 17.9386   val Loss: 24.2095   time: 437.25s   best: 22.6034
2023-10-31 20:35:51,499:INFO:  Epoch 370/500:  train Loss: 18.2491   val Loss: 23.6054   time: 439.36s   best: 22.6034
2023-10-31 20:43:11,206:INFO:  Epoch 371/500:  train Loss: 17.6955   val Loss: 23.8331   time: 439.68s   best: 22.6034
2023-10-31 20:50:29,199:INFO:  Epoch 372/500:  train Loss: 17.6719   val Loss: 23.2832   time: 437.97s   best: 22.6034
2023-10-31 20:57:45,770:INFO:  Epoch 373/500:  train Loss: 17.6381   val Loss: 23.5504   time: 436.55s   best: 22.6034
2023-10-31 21:05:02,972:INFO:  Epoch 374/500:  train Loss: 17.6418   val Loss: 25.0370   time: 437.18s   best: 22.6034
2023-10-31 21:12:20,748:INFO:  Epoch 375/500:  train Loss: 17.6841   val Loss: 23.2102   time: 437.75s   best: 22.6034
2023-10-31 21:19:37,201:INFO:  Epoch 376/500:  train Loss: 17.5011   val Loss: 23.3944   time: 436.43s   best: 22.6034
2023-10-31 21:26:54,838:INFO:  Epoch 377/500:  train Loss: 17.5575   val Loss: 23.1218   time: 437.61s   best: 22.6034
2023-10-31 21:34:12,143:INFO:  Epoch 378/500:  train Loss: 17.8184   val Loss: 23.2319   time: 437.29s   best: 22.6034
2023-10-31 21:41:31,216:INFO:  Epoch 379/500:  train Loss: 17.5908   val Loss: 23.4991   time: 439.05s   best: 22.6034
2023-10-31 21:48:51,215:INFO:  Epoch 380/500:  train Loss: 17.5820   val Loss: 23.9838   time: 439.97s   best: 22.6034
2023-10-31 21:56:09,063:INFO:  Epoch 381/500:  train Loss: 17.6016   val Loss: 23.5275   time: 437.82s   best: 22.6034
2023-10-31 22:03:29,413:INFO:  Epoch 382/500:  train Loss: 17.6529   val Loss: 25.5218   time: 440.33s   best: 22.6034
2023-10-31 22:10:50,325:INFO:  Epoch 383/500:  train Loss: 17.7506   val Loss: 23.7374   time: 440.88s   best: 22.6034
2023-10-31 22:18:07,157:INFO:  Epoch 384/500:  train Loss: 17.5773   val Loss: 23.3922   time: 436.82s   best: 22.6034
2023-10-31 22:25:23,512:INFO:  Epoch 385/500:  train Loss: 17.6952   val Loss: 23.0841   time: 436.33s   best: 22.6034
2023-10-31 22:32:42,426:INFO:  Epoch 386/500:  train Loss: 17.6633   val Loss: 23.7392   time: 438.89s   best: 22.6034
2023-10-31 22:40:02,863:INFO:  Epoch 387/500:  train Loss: 17.5516   val Loss: 23.3694   time: 440.43s   best: 22.6034
2023-10-31 22:47:20,212:INFO:  Epoch 388/500:  train Loss: 17.5652   val Loss: 23.2500   time: 437.33s   best: 22.6034
2023-10-31 22:54:39,116:INFO:  Epoch 389/500:  train Loss: 17.6730   val Loss: 24.1200   time: 438.88s   best: 22.6034
2023-10-31 23:01:55,450:INFO:  Epoch 390/500:  train Loss: 17.5534   val Loss: 23.7583   time: 436.31s   best: 22.6034
2023-10-31 23:09:12,520:INFO:  Epoch 391/500:  train Loss: 17.7743   val Loss: 22.9273   time: 437.03s   best: 22.6034
2023-10-31 23:16:32,830:INFO:  Epoch 392/500:  train Loss: 17.4286   val Loss: 22.9894   time: 440.30s   best: 22.6034
2023-10-31 23:23:51,601:INFO:  Epoch 393/500:  train Loss: 17.4756   val Loss: 23.3561   time: 438.75s   best: 22.6034
2023-10-31 23:31:07,564:INFO:  Epoch 394/500:  train Loss: 17.7413   val Loss: 23.0263   time: 435.92s   best: 22.6034
2023-10-31 23:38:25,423:INFO:  Epoch 395/500:  train Loss: 17.4952   val Loss: 23.4997   time: 437.85s   best: 22.6034
2023-10-31 23:45:42,113:INFO:  Epoch 396/500:  train Loss: 17.7741   val Loss: 23.5697   time: 436.66s   best: 22.6034
2023-10-31 23:53:01,902:INFO:  Epoch 397/500:  train Loss: 17.6064   val Loss: 22.9271   time: 439.77s   best: 22.6034
2023-11-01 00:00:23,024:INFO:  Epoch 398/500:  train Loss: 17.4969   val Loss: 23.3635   time: 441.11s   best: 22.6034
2023-11-01 00:07:39,291:INFO:  Epoch 399/500:  train Loss: 17.7065   val Loss: 22.8322   time: 436.26s   best: 22.6034
2023-11-01 00:15:00,135:INFO:  Epoch 400/500:  train Loss: 17.6318   val Loss: 23.1288   time: 440.83s   best: 22.6034
2023-11-01 00:22:18,605:INFO:  Epoch 401/500:  train Loss: 17.4748   val Loss: 23.1538   time: 438.45s   best: 22.6034
2023-11-01 00:29:35,429:INFO:  Epoch 402/500:  train Loss: 17.4755   val Loss: 22.9325   time: 436.79s   best: 22.6034
2023-11-01 00:36:51,624:INFO:  Epoch 403/500:  train Loss: 17.5261   val Loss: 23.6060   time: 436.17s   best: 22.6034
2023-11-01 00:44:11,244:INFO:  Epoch 404/500:  train Loss: 17.7971   val Loss: 24.5108   time: 439.58s   best: 22.6034
2023-11-01 00:51:29,148:INFO:  Epoch 405/500:  train Loss: 17.5371   val Loss: 23.0124   time: 437.89s   best: 22.6034
2023-11-01 00:58:50,173:INFO:  Epoch 406/500:  train Loss: 17.8790   val Loss: 23.4896   time: 441.00s   best: 22.6034
2023-11-01 01:06:06,115:INFO:  Epoch 407/500:  train Loss: 17.6716   val Loss: 22.8007   time: 435.93s   best: 22.6034
2023-11-01 01:13:21,502:INFO:  Epoch 408/500:  train Loss: 17.5274   val Loss: 22.7087   time: 435.15s   best: 22.6034
2023-11-01 01:20:37,718:INFO:  Epoch 409/500:  train Loss: 17.4456   val Loss: 22.9975   time: 436.21s   best: 22.6034
2023-11-01 01:27:57,498:INFO:  Epoch 410/500:  train Loss: 17.4131   val Loss: 22.7679   time: 439.75s   best: 22.6034
2023-11-01 01:35:12,780:INFO:  Epoch 411/500:  train Loss: 17.4390   val Loss: 23.6597   time: 435.26s   best: 22.6034
2023-11-01 01:42:32,989:INFO:  Epoch 412/500:  train Loss: 17.6660   val Loss: 23.6361   time: 440.20s   best: 22.6034
2023-11-01 01:49:47,526:INFO:  Epoch 413/500:  train Loss: 17.5657   val Loss: 23.4926   time: 434.52s   best: 22.6034
2023-11-01 01:57:05,017:INFO:  Epoch 414/500:  train Loss: 17.4551   val Loss: 23.4701   time: 437.47s   best: 22.6034
2023-11-01 02:04:20,950:INFO:  Epoch 415/500:  train Loss: 17.3750   val Loss: 23.2547   time: 435.91s   best: 22.6034
2023-11-01 02:11:37,618:INFO:  Epoch 416/500:  train Loss: 17.4063   val Loss: 22.7045   time: 436.64s   best: 22.6034
2023-11-01 02:18:54,457:INFO:  Epoch 417/500:  train Loss: 17.5331   val Loss: 22.6928   time: 436.80s   best: 22.6034
2023-11-01 02:26:10,924:INFO:  Epoch 418/500:  train Loss: 17.5616   val Loss: 23.2315   time: 436.45s   best: 22.6034
2023-11-01 02:33:31,842:INFO:  Epoch 419/500:  train Loss: 17.4721   val Loss: 23.0240   time: 440.89s   best: 22.6034
2023-11-01 02:40:50,976:INFO:  Epoch 420/500:  train Loss: 17.4064   val Loss: 22.8260   time: 439.12s   best: 22.6034
2023-11-01 02:48:07,794:INFO:  Epoch 421/500:  train Loss: 17.4514   val Loss: 23.8757   time: 436.79s   best: 22.6034
2023-11-01 02:55:24,663:INFO:  Epoch 422/500:  train Loss: 17.6257   val Loss: 25.1111   time: 436.85s   best: 22.6034
2023-11-01 03:02:42,477:INFO:  Epoch 423/500:  train Loss: 17.7700   val Loss: 23.4132   time: 437.78s   best: 22.6034
2023-11-01 03:10:01,700:INFO:  Epoch 424/500:  train Loss: 17.4859   val Loss: 23.2465   time: 439.20s   best: 22.6034
2023-11-01 03:17:23,010:INFO:  Epoch 425/500:  train Loss: 17.3830   val Loss: 22.9699   time: 441.28s   best: 22.6034
2023-11-01 03:24:41,804:INFO:  Epoch 426/500:  train Loss: 17.3801   val Loss: 24.5115   time: 438.77s   best: 22.6034
2023-11-01 03:31:59,519:INFO:  Epoch 427/500:  train Loss: 17.4456   val Loss: 23.1002   time: 437.68s   best: 22.6034
2023-11-01 03:39:15,544:INFO:  Epoch 428/500:  train Loss: 17.6653   val Loss: 22.9851   time: 436.00s   best: 22.6034
2023-11-01 03:46:36,747:INFO:  Epoch 429/500:  train Loss: 17.5317   val Loss: 24.7475   time: 441.19s   best: 22.6034
2023-11-01 03:53:56,398:INFO:  Epoch 430/500:  train Loss: 17.5453   val Loss: 23.1191   time: 439.62s   best: 22.6034
2023-11-01 04:01:17,340:INFO:  Epoch 431/500:  train Loss: 17.5510   val Loss: 22.9535   time: 440.93s   best: 22.6034
2023-11-01 04:08:34,037:INFO:  Epoch 432/500:  train Loss: 17.4543   val Loss: 23.2923   time: 436.67s   best: 22.6034
2023-11-01 04:15:53,252:INFO:  Epoch 433/500:  train Loss: 17.2485   val Loss: 22.8689   time: 439.20s   best: 22.6034
2023-11-01 04:23:12,955:INFO:  Epoch 434/500:  train Loss: 17.6158   val Loss: 22.9460   time: 439.67s   best: 22.6034
2023-11-01 04:30:32,692:INFO:  Epoch 435/500:  train Loss: 17.4981   val Loss: 23.0645   time: 439.73s   best: 22.6034
2023-11-01 04:37:52,679:INFO:  Epoch 436/500:  train Loss: 17.4348   val Loss: 22.8644   time: 439.96s   best: 22.6034
2023-11-01 04:45:10,804:INFO:  Epoch 437/500:  train Loss: 17.2382   val Loss: 22.8521   time: 438.08s   best: 22.6034
2023-11-01 04:52:31,787:INFO:  Epoch 438/500:  train Loss: 17.2547   val Loss: 22.8743   time: 440.96s   best: 22.6034
2023-11-01 04:59:53,187:INFO:  Epoch 439/500:  train Loss: 17.3803   val Loss: 22.8989   time: 441.37s   best: 22.6034
2023-11-01 05:07:12,581:INFO:  Epoch 440/500:  train Loss: 17.4092   val Loss: 22.8489   time: 439.38s   best: 22.6034
2023-11-01 05:14:32,625:INFO:  Epoch 441/500:  train Loss: 17.2741   val Loss: 22.7528   time: 440.03s   best: 22.6034
2023-11-01 05:21:53,231:INFO:  Epoch 442/500:  train Loss: 17.3892   val Loss: 24.1092   time: 440.56s   best: 22.6034
2023-11-01 05:29:12,712:INFO:  Epoch 443/500:  train Loss: 17.5847   val Loss: 22.6841   time: 439.46s   best: 22.6034
2023-11-01 05:36:30,527:INFO:  Epoch 444/500:  train Loss: 17.3545   val Loss: 22.9241   time: 437.78s   best: 22.6034
2023-11-01 05:43:51,304:INFO:  Epoch 445/500:  train Loss: 17.6389   val Loss: 23.4881   time: 440.76s   best: 22.6034
2023-11-01 05:51:08,086:INFO:  Epoch 446/500:  train Loss: 17.4257   val Loss: 23.0758   time: 436.77s   best: 22.6034
2023-11-01 05:58:28,817:INFO:  Epoch 447/500:  train Loss: 17.3474   val Loss: 23.3281   time: 440.70s   best: 22.6034
2023-11-01 06:05:46,069:INFO:  Epoch 448/500:  train Loss: 17.2586   val Loss: 24.2798   time: 437.22s   best: 22.6034
2023-11-01 06:13:02,278:INFO:  Epoch 449/500:  train Loss: 17.4115   val Loss: 22.8349   time: 436.19s   best: 22.6034
2023-11-01 06:20:21,303:INFO:  Epoch 450/500:  train Loss: 17.3448   val Loss: 23.3692   time: 438.99s   best: 22.6034
2023-11-01 06:27:40,331:INFO:  Epoch 451/500:  train Loss: 17.2319   val Loss: 24.0916   time: 438.99s   best: 22.6034
2023-11-01 06:34:59,973:INFO:  Epoch 452/500:  train Loss: 17.4557   val Loss: 38.2634   time: 439.63s   best: 22.6034
2023-11-01 06:42:16,768:INFO:  Epoch 453/500:  train Loss: 17.5114   val Loss: 23.4125   time: 436.77s   best: 22.6034
2023-11-01 06:49:35,078:INFO:  Epoch 454/500:  train Loss: 17.3121   val Loss: 24.5905   time: 438.30s   best: 22.6034
2023-11-01 06:56:51,363:INFO:  Epoch 455/500:  train Loss: 17.5443   val Loss: 22.7360   time: 436.26s   best: 22.6034
2023-11-01 07:04:11,812:INFO:  Epoch 456/500:  train Loss: 17.4248   val Loss: 23.3956   time: 440.41s   best: 22.6034
2023-11-01 07:11:28,285:INFO:  Epoch 457/500:  train Loss: 17.2719   val Loss: 22.9054   time: 436.47s   best: 22.6034
2023-11-01 07:18:48,120:INFO:  Epoch 458/500:  train Loss: 17.2781   val Loss: 23.4140   time: 439.82s   best: 22.6034
2023-11-01 07:26:05,539:INFO:  Epoch 459/500:  train Loss: 17.2576   val Loss: 22.8151   time: 437.39s   best: 22.6034
2023-11-01 07:33:22,922:INFO:  Epoch 460/500:  train Loss: 17.3777   val Loss: 22.9410   time: 437.35s   best: 22.6034
2023-11-01 07:40:44,018:INFO:  Epoch 461/500:  train Loss: 17.3111   val Loss: 23.3170   time: 441.06s   best: 22.6034
2023-11-01 07:48:03,244:INFO:  Epoch 462/500:  train Loss: 17.5991   val Loss: 23.0901   time: 439.22s   best: 22.6034
2023-11-01 07:55:20,551:INFO:  Epoch 463/500:  train Loss: 17.2958   val Loss: 22.9640   time: 437.28s   best: 22.6034
2023-11-01 08:02:37,244:INFO:  Epoch 464/500:  train Loss: 17.5191   val Loss: 23.5558   time: 436.68s   best: 22.6034
2023-11-01 08:09:57,913:INFO:  Epoch 465/500:  train Loss: 17.2562   val Loss: 22.8417   time: 440.64s   best: 22.6034
2023-11-01 08:17:13,369:INFO:  Epoch 466/500:  train Loss: 17.3931   val Loss: 23.9333   time: 435.34s   best: 22.6034
2023-11-01 08:24:34,373:INFO:  Epoch 467/500:  train Loss: 17.2404   val Loss: 23.3084   time: 440.98s   best: 22.6034
2023-11-01 08:31:50,096:INFO:  Epoch 468/500:  train Loss: 17.2862   val Loss: 22.6899   time: 435.71s   best: 22.6034
2023-11-01 08:39:09,141:INFO:  Epoch 469/500:  train Loss: 17.2315   val Loss: 24.3830   time: 439.03s   best: 22.6034
2023-11-01 08:46:27,893:INFO:  Epoch 470/500:  train Loss: 17.2874   val Loss: 23.7940   time: 438.73s   best: 22.6034
2023-11-01 08:53:49,461:INFO:  Epoch 471/500:  train Loss: 17.3602   val Loss: 23.7614   time: 441.56s   best: 22.6034
2023-11-01 09:01:05,998:INFO:  Epoch 472/500:  train Loss: 17.3718   val Loss: 23.3106   time: 436.49s   best: 22.6034
2023-11-01 09:08:25,648:INFO:  Epoch 473/500:  train Loss: 17.3540   val Loss: 23.2391   time: 439.62s   best: 22.6034
2023-11-01 09:15:42,665:INFO:  Epoch 474/500:  train Loss: 17.4729   val Loss: 23.3362   time: 437.01s   best: 22.6034
2023-11-01 09:23:00,305:INFO:  Epoch 475/500:  train Loss: 17.2609   val Loss: 22.8770   time: 437.62s   best: 22.6034
2023-11-01 09:30:16,116:INFO:  Epoch 476/500:  train Loss: 17.3995   val Loss: 22.6048   time: 435.80s   best: 22.6034
2023-11-01 09:37:33,018:INFO:  Epoch 477/500:  train Loss: 17.3354   val Loss: 23.3264   time: 436.89s   best: 22.6034
2023-11-01 09:44:49,289:INFO:  Epoch 478/500:  train Loss: 17.1460   val Loss: 23.0591   time: 436.25s   best: 22.6034
2023-11-01 09:52:11,051:INFO:  Epoch 479/500:  train Loss: 17.4747   val Loss: 25.7136   time: 441.74s   best: 22.6034
2023-11-01 09:59:32,347:INFO:  Epoch 480/500:  train Loss: 17.4227   val Loss: 23.3425   time: 441.27s   best: 22.6034
2023-11-01 10:06:53,947:INFO:  Epoch 481/500:  train Loss: 17.2705   val Loss: 23.4802   time: 441.57s   best: 22.6034
2023-11-01 10:14:12,903:INFO:  Epoch 482/500:  train Loss: 17.1928   val Loss: 23.1011   time: 438.93s   best: 22.6034
2023-11-01 10:21:31,208:INFO:  Epoch 483/500:  train Loss: 17.2532   val Loss: 24.8257   time: 438.28s   best: 22.6034
2023-11-01 10:28:50,432:INFO:  Epoch 484/500:  train Loss: 17.3507   val Loss: 22.6393   time: 439.20s   best: 22.6034
2023-11-01 10:36:10,870:INFO:  Epoch 485/500:  train Loss: 17.2047   val Loss: 23.5202   time: 440.41s   best: 22.6034
2023-11-01 10:43:28,950:INFO:  Epoch 486/500:  train Loss: 17.5746   val Loss: 23.0240   time: 438.04s   best: 22.6034
2023-11-01 10:50:48,462:INFO:  Epoch 487/500:  train Loss: 17.1957   val Loss: 22.8740   time: 439.48s   best: 22.6034
2023-11-01 10:58:04,886:INFO:  Epoch 488/500:  train Loss: 17.4462   val Loss: 23.3774   time: 436.38s   best: 22.6034
2023-11-01 11:05:24,381:INFO:  Epoch 489/500:  train Loss: 17.2398   val Loss: 22.9436   time: 439.47s   best: 22.6034
2023-11-01 11:12:45,095:INFO:  Epoch 490/500:  train Loss: 17.4335   val Loss: 24.2542   time: 440.70s   best: 22.6034
2023-11-01 11:20:04,432:INFO:  Epoch 491/500:  train Loss: 17.2701   val Loss: 24.9387   time: 439.29s   best: 22.6034
2023-11-01 11:27:21,941:INFO:  Epoch 492/500:  train Loss: 17.4475   val Loss: 22.8828   time: 437.48s   best: 22.6034
2023-11-01 11:34:38,102:INFO:  Epoch 493/500:  train Loss: 17.1220   val Loss: 23.1603   time: 436.12s   best: 22.6034
2023-11-01 11:41:59,545:INFO:  Epoch 494/500:  train Loss: 17.1793   val Loss: 22.7944   time: 441.42s   best: 22.6034
2023-11-01 11:49:14,944:INFO:  Epoch 495/500:  train Loss: 17.1972   val Loss: 22.9983   time: 435.39s   best: 22.6034
2023-11-01 11:56:34,084:INFO:  Epoch 496/500:  train Loss: 17.3045   val Loss: 23.6739   time: 439.11s   best: 22.6034
2023-11-01 12:03:49,401:INFO:  Epoch 497/500:  train Loss: 17.2612   val Loss: 24.9527   time: 435.30s   best: 22.6034
2023-11-01 12:11:10,729:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder (2 layer + hidden)_74f1.pt
2023-11-01 12:11:10,840:INFO:  Epoch 498/500:  train Loss: 17.1861   val Loss: 22.4598   time: 441.31s   best: 22.4598
2023-11-01 12:18:27,148:INFO:  Epoch 499/500:  train Loss: 17.6130   val Loss: 23.2789   time: 436.28s   best: 22.4598
2023-11-01 12:25:43,975:INFO:  Epoch 500/500:  train Loss: 17.4123   val Loss: 23.1107   time: 436.77s   best: 22.4598
2023-11-01 12:25:43,990:INFO:  -----> Training complete in 3651m 43s   best validation loss: 22.4598
 
2023-11-02 14:01:40,873:INFO:  Starting experiment lstm autoencoder debug (2 layer + hidden)
2023-11-02 14:01:40,888:INFO:  Defining the model
2023-11-02 14:01:40,936:INFO:  Reading the dataset
2023-11-02 14:02:11,053:INFO:  Starting experiment lstm autoencoder debug (2 layer + hidden)
2023-11-02 14:02:11,054:INFO:  Defining the model
2023-11-02 14:02:11,097:INFO:  Reading the dataset
2023-11-02 14:02:18,290:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:18,310:INFO:  Epoch 1/500:  train Loss: 99.7017   val Loss: 100.0926   time: 1.56s   best: 100.0926
2023-11-02 14:02:18,463:INFO:  Epoch 2/500:  train Loss: 99.5808   val Loss: 100.1197   time: 0.15s   best: 100.0926
2023-11-02 14:02:18,634:INFO:  Epoch 3/500:  train Loss: 99.7827   val Loss: 100.1241   time: 0.17s   best: 100.0926
2023-11-02 14:02:18,792:INFO:  Epoch 4/500:  train Loss: 99.8443   val Loss: 100.1139   time: 0.16s   best: 100.0926
2023-11-02 14:02:18,923:INFO:  Epoch 5/500:  train Loss: 99.5024   val Loss: 100.0961   time: 0.13s   best: 100.0926
2023-11-02 14:02:19,056:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:19,153:INFO:  Epoch 6/500:  train Loss: 99.8161   val Loss: 100.0650   time: 0.13s   best: 100.0650
2023-11-02 14:02:19,306:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:19,392:INFO:  Epoch 7/500:  train Loss: 98.5923   val Loss: 100.0177   time: 0.15s   best: 100.0177
2023-11-02 14:02:19,523:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:19,614:INFO:  Epoch 8/500:  train Loss: 98.1314   val Loss: 95.9870   time: 0.13s   best: 95.9870
2023-11-02 14:02:19,746:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:19,843:INFO:  Epoch 9/500:  train Loss: 99.3991   val Loss: 95.3491   time: 0.13s   best: 95.3491
2023-11-02 14:02:19,974:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:19,994:INFO:  Epoch 10/500:  train Loss: 98.4532   val Loss: 95.3041   time: 0.13s   best: 95.3041
2023-11-02 14:02:20,126:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:20,232:INFO:  Epoch 11/500:  train Loss: 100.0331   val Loss: 94.9397   time: 0.13s   best: 94.9397
2023-11-02 14:02:20,379:INFO:  Epoch 12/500:  train Loss: 99.4364   val Loss: 96.5020   time: 0.15s   best: 94.9397
2023-11-02 14:02:20,510:INFO:  Epoch 13/500:  train Loss: 97.9189   val Loss: 98.6956   time: 0.13s   best: 94.9397
2023-11-02 14:02:20,656:INFO:  Epoch 14/500:  train Loss: 98.3733   val Loss: 99.0168   time: 0.14s   best: 94.9397
2023-11-02 14:02:20,813:INFO:  Epoch 15/500:  train Loss: 97.5874   val Loss: 99.0270   time: 0.16s   best: 94.9397
2023-11-02 14:02:20,965:INFO:  Epoch 16/500:  train Loss: 99.3452   val Loss: 98.9282   time: 0.15s   best: 94.9397
2023-11-02 14:02:21,093:INFO:  Epoch 17/500:  train Loss: 98.3472   val Loss: 98.6254   time: 0.13s   best: 94.9397
2023-11-02 14:02:21,223:INFO:  Epoch 18/500:  train Loss: 97.5432   val Loss: 97.9535   time: 0.13s   best: 94.9397
2023-11-02 14:02:21,353:INFO:  Epoch 19/500:  train Loss: 98.4444   val Loss: 97.1732   time: 0.13s   best: 94.9397
2023-11-02 14:02:21,507:INFO:  Epoch 20/500:  train Loss: 95.6792   val Loss: 96.5163   time: 0.15s   best: 94.9397
2023-11-02 14:02:21,636:INFO:  Epoch 21/500:  train Loss: 96.4144   val Loss: 96.0518   time: 0.13s   best: 94.9397
2023-11-02 14:02:21,767:INFO:  Epoch 22/500:  train Loss: 94.5736   val Loss: 95.4625   time: 0.13s   best: 94.9397
2023-11-02 14:02:21,898:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:22,003:INFO:  Epoch 23/500:  train Loss: 93.9237   val Loss: 94.8323   time: 0.13s   best: 94.8323
2023-11-02 14:02:22,134:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:22,181:INFO:  Epoch 24/500:  train Loss: 92.1574   val Loss: 94.3440   time: 0.13s   best: 94.3440
2023-11-02 14:02:22,313:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:22,403:INFO:  Epoch 25/500:  train Loss: 91.0524   val Loss: 94.0074   time: 0.13s   best: 94.0074
2023-11-02 14:02:22,547:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:22,596:INFO:  Epoch 26/500:  train Loss: 90.0699   val Loss: 93.7473   time: 0.14s   best: 93.7473
2023-11-02 14:02:22,774:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:22,881:INFO:  Epoch 27/500:  train Loss: 90.4112   val Loss: 93.5964   time: 0.17s   best: 93.5964
2023-11-02 14:02:23,027:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:23,146:INFO:  Epoch 28/500:  train Loss: 89.0770   val Loss: 93.5000   time: 0.14s   best: 93.5000
2023-11-02 14:02:23,277:INFO:  Epoch 29/500:  train Loss: 88.8266   val Loss: 93.5816   time: 0.13s   best: 93.5000
2023-11-02 14:02:23,408:INFO:  Epoch 30/500:  train Loss: 87.5473   val Loss: 93.7627   time: 0.13s   best: 93.5000
2023-11-02 14:02:23,556:INFO:  Epoch 31/500:  train Loss: 86.5930   val Loss: 94.0991   time: 0.15s   best: 93.5000
2023-11-02 14:02:23,685:INFO:  Epoch 32/500:  train Loss: 86.1603   val Loss: 94.6388   time: 0.13s   best: 93.5000
2023-11-02 14:02:23,815:INFO:  Epoch 33/500:  train Loss: 84.9777   val Loss: 95.3766   time: 0.13s   best: 93.5000
2023-11-02 14:02:23,944:INFO:  Epoch 34/500:  train Loss: 85.4433   val Loss: 96.3827   time: 0.13s   best: 93.5000
2023-11-02 14:02:24,098:INFO:  Epoch 35/500:  train Loss: 84.7906   val Loss: 97.5254   time: 0.15s   best: 93.5000
2023-11-02 14:02:24,227:INFO:  Epoch 36/500:  train Loss: 84.9526   val Loss: 98.2342   time: 0.13s   best: 93.5000
2023-11-02 14:02:24,358:INFO:  Epoch 37/500:  train Loss: 84.7610   val Loss: 97.4836   time: 0.13s   best: 93.5000
2023-11-02 14:02:24,489:INFO:  Epoch 38/500:  train Loss: 86.1862   val Loss: 97.6241   time: 0.13s   best: 93.5000
2023-11-02 14:02:24,643:INFO:  Epoch 39/500:  train Loss: 87.4774   val Loss: 100.4966   time: 0.15s   best: 93.5000
2023-11-02 14:02:24,807:INFO:  Epoch 40/500:  train Loss: 86.1135   val Loss: 101.6234   time: 0.15s   best: 93.5000
2023-11-02 14:02:24,947:INFO:  Epoch 41/500:  train Loss: 86.9166   val Loss: 101.3073   time: 0.14s   best: 93.5000
2023-11-02 14:02:25,100:INFO:  Epoch 42/500:  train Loss: 85.8981   val Loss: 100.6177   time: 0.15s   best: 93.5000
2023-11-02 14:02:25,229:INFO:  Epoch 43/500:  train Loss: 85.4990   val Loss: 99.7944   time: 0.13s   best: 93.5000
2023-11-02 14:02:25,359:INFO:  Epoch 44/500:  train Loss: 86.2933   val Loss: 98.9589   time: 0.13s   best: 93.5000
2023-11-02 14:02:25,490:INFO:  Epoch 45/500:  train Loss: 85.7632   val Loss: 98.1846   time: 0.13s   best: 93.5000
2023-11-02 14:02:25,642:INFO:  Epoch 46/500:  train Loss: 85.2938   val Loss: 97.5561   time: 0.15s   best: 93.5000
2023-11-02 14:02:25,771:INFO:  Epoch 47/500:  train Loss: 85.6355   val Loss: 97.0808   time: 0.13s   best: 93.5000
2023-11-02 14:02:25,900:INFO:  Epoch 48/500:  train Loss: 85.4062   val Loss: 96.7497   time: 0.13s   best: 93.5000
2023-11-02 14:02:26,030:INFO:  Epoch 49/500:  train Loss: 85.4498   val Loss: 96.5650   time: 0.13s   best: 93.5000
2023-11-02 14:02:26,184:INFO:  Epoch 50/500:  train Loss: 85.4031   val Loss: 96.5137   time: 0.15s   best: 93.5000
2023-11-02 14:02:26,314:INFO:  Epoch 51/500:  train Loss: 85.7213   val Loss: 96.5802   time: 0.13s   best: 93.5000
2023-11-02 14:02:26,445:INFO:  Epoch 52/500:  train Loss: 85.1629   val Loss: 96.7066   time: 0.13s   best: 93.5000
2023-11-02 14:02:26,574:INFO:  Epoch 53/500:  train Loss: 85.7198   val Loss: 96.9339   time: 0.13s   best: 93.5000
2023-11-02 14:02:26,729:INFO:  Epoch 54/500:  train Loss: 84.9546   val Loss: 97.2360   time: 0.15s   best: 93.5000
2023-11-02 14:02:26,982:INFO:  Epoch 55/500:  train Loss: 85.1969   val Loss: 97.5310   time: 0.25s   best: 93.5000
2023-11-02 14:02:27,113:INFO:  Epoch 56/500:  train Loss: 85.1677   val Loss: 97.7103   time: 0.13s   best: 93.5000
2023-11-02 14:02:27,261:INFO:  Epoch 57/500:  train Loss: 84.9610   val Loss: 97.8034   time: 0.15s   best: 93.5000
2023-11-02 14:02:27,392:INFO:  Epoch 58/500:  train Loss: 84.7832   val Loss: 97.7919   time: 0.13s   best: 93.5000
2023-11-02 14:02:27,521:INFO:  Epoch 59/500:  train Loss: 84.3639   val Loss: 97.6250   time: 0.13s   best: 93.5000
2023-11-02 14:02:27,651:INFO:  Epoch 60/500:  train Loss: 84.1957   val Loss: 97.3576   time: 0.13s   best: 93.5000
2023-11-02 14:02:27,805:INFO:  Epoch 61/500:  train Loss: 83.3262   val Loss: 97.0008   time: 0.15s   best: 93.5000
2023-11-02 14:02:27,935:INFO:  Epoch 62/500:  train Loss: 83.1321   val Loss: 93.7321   time: 0.13s   best: 93.5000
2023-11-02 14:02:28,066:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:28,156:INFO:  Epoch 63/500:  train Loss: 83.2424   val Loss: 92.9450   time: 0.13s   best: 92.9450
2023-11-02 14:02:28,304:INFO:  Epoch 64/500:  train Loss: 83.8344   val Loss: 95.3345   time: 0.14s   best: 92.9450
2023-11-02 14:02:28,434:INFO:  Epoch 65/500:  train Loss: 82.7571   val Loss: 94.9908   time: 0.13s   best: 92.9450
2023-11-02 14:02:28,563:INFO:  Epoch 66/500:  train Loss: 83.8642   val Loss: 94.6119   time: 0.13s   best: 92.9450
2023-11-02 14:02:28,692:INFO:  Epoch 67/500:  train Loss: 84.0207   val Loss: 94.3822   time: 0.13s   best: 92.9450
2023-11-02 14:02:28,858:INFO:  Epoch 68/500:  train Loss: 83.9878   val Loss: 94.2759   time: 0.16s   best: 92.9450
2023-11-02 14:02:29,025:INFO:  Epoch 69/500:  train Loss: 83.5839   val Loss: 94.2788   time: 0.16s   best: 92.9450
2023-11-02 14:02:29,155:INFO:  Epoch 70/500:  train Loss: 83.3534   val Loss: 94.4282   time: 0.13s   best: 92.9450
2023-11-02 14:02:29,285:INFO:  Epoch 71/500:  train Loss: 83.4950   val Loss: 94.7178   time: 0.13s   best: 92.9450
2023-11-02 14:02:29,437:INFO:  Epoch 72/500:  train Loss: 83.1448   val Loss: 95.0633   time: 0.15s   best: 92.9450
2023-11-02 14:02:29,566:INFO:  Epoch 73/500:  train Loss: 83.7497   val Loss: 95.2974   time: 0.13s   best: 92.9450
2023-11-02 14:02:29,694:INFO:  Epoch 74/500:  train Loss: 83.8613   val Loss: 95.3568   time: 0.13s   best: 92.9450
2023-11-02 14:02:29,825:INFO:  Epoch 75/500:  train Loss: 83.9866   val Loss: 95.2710   time: 0.13s   best: 92.9450
2023-11-02 14:02:29,978:INFO:  Epoch 76/500:  train Loss: 83.3230   val Loss: 94.8835   time: 0.15s   best: 92.9450
2023-11-02 14:02:30,107:INFO:  Epoch 77/500:  train Loss: 83.2593   val Loss: 94.2973   time: 0.13s   best: 92.9450
2023-11-02 14:02:30,236:INFO:  Epoch 78/500:  train Loss: 83.1369   val Loss: 93.8255   time: 0.13s   best: 92.9450
2023-11-02 14:02:30,366:INFO:  Epoch 79/500:  train Loss: 82.5481   val Loss: 93.3772   time: 0.13s   best: 92.9450
2023-11-02 14:02:30,519:INFO:  Epoch 80/500:  train Loss: 82.6570   val Loss: 93.0073   time: 0.15s   best: 92.9450
2023-11-02 14:02:30,651:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:30,725:INFO:  Epoch 81/500:  train Loss: 81.6494   val Loss: 92.8186   time: 0.13s   best: 92.8186
2023-11-02 14:02:30,869:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:30,957:INFO:  Epoch 82/500:  train Loss: 82.5131   val Loss: 92.7363   time: 0.14s   best: 92.7363
2023-11-02 14:02:31,126:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:31,432:INFO:  Epoch 83/500:  train Loss: 81.3426   val Loss: 92.6952   time: 0.16s   best: 92.6952
2023-11-02 14:02:31,570:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:31,618:INFO:  Epoch 84/500:  train Loss: 83.5884   val Loss: 92.6137   time: 0.13s   best: 92.6137
2023-11-02 14:02:31,748:INFO:  Epoch 85/500:  train Loss: 82.1670   val Loss: 93.0816   time: 0.13s   best: 92.6137
2023-11-02 14:02:31,880:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:31,921:INFO:  Epoch 86/500:  train Loss: 82.4928   val Loss: 92.1967   time: 0.13s   best: 92.1967
2023-11-02 14:02:32,073:INFO:  Epoch 87/500:  train Loss: 81.6596   val Loss: 92.2932   time: 0.15s   best: 92.1967
2023-11-02 14:02:32,210:INFO:  Epoch 88/500:  train Loss: 82.4649   val Loss: 92.3325   time: 0.14s   best: 92.1967
2023-11-02 14:02:32,339:INFO:  Epoch 89/500:  train Loss: 81.7671   val Loss: 92.7610   time: 0.13s   best: 92.1967
2023-11-02 14:02:32,475:INFO:  Epoch 90/500:  train Loss: 81.9351   val Loss: 93.4463   time: 0.13s   best: 92.1967
2023-11-02 14:02:32,627:INFO:  Epoch 91/500:  train Loss: 81.4519   val Loss: 94.3319   time: 0.15s   best: 92.1967
2023-11-02 14:02:32,756:INFO:  Epoch 92/500:  train Loss: 82.9474   val Loss: 95.0334   time: 0.13s   best: 92.1967
2023-11-02 14:02:32,896:INFO:  Epoch 93/500:  train Loss: 81.5485   val Loss: 94.5016   time: 0.14s   best: 92.1967
2023-11-02 14:02:33,042:INFO:  Epoch 94/500:  train Loss: 81.2358   val Loss: 93.9414   time: 0.14s   best: 92.1967
2023-11-02 14:02:33,215:INFO:  Epoch 95/500:  train Loss: 81.5443   val Loss: 93.7339   time: 0.17s   best: 92.1967
2023-11-02 14:02:33,346:INFO:  Epoch 96/500:  train Loss: 81.8780   val Loss: 93.6677   time: 0.13s   best: 92.1967
2023-11-02 14:02:33,476:INFO:  Epoch 97/500:  train Loss: 81.1105   val Loss: 93.6348   time: 0.13s   best: 92.1967
2023-11-02 14:02:33,628:INFO:  Epoch 98/500:  train Loss: 80.8918   val Loss: 93.7218   time: 0.15s   best: 92.1967
2023-11-02 14:02:33,757:INFO:  Epoch 99/500:  train Loss: 80.8281   val Loss: 94.3844   time: 0.13s   best: 92.1967
2023-11-02 14:02:33,887:INFO:  Epoch 100/500:  train Loss: 81.6552   val Loss: 94.6218   time: 0.13s   best: 92.1967
2023-11-02 14:02:34,016:INFO:  Epoch 101/500:  train Loss: 80.2349   val Loss: 94.3249   time: 0.13s   best: 92.1967
2023-11-02 14:02:34,166:INFO:  Epoch 102/500:  train Loss: 80.6685   val Loss: 94.0076   time: 0.15s   best: 92.1967
2023-11-02 14:02:34,295:INFO:  Epoch 103/500:  train Loss: 80.0865   val Loss: 94.2210   time: 0.13s   best: 92.1967
2023-11-02 14:02:34,426:INFO:  Epoch 104/500:  train Loss: 80.7620   val Loss: 95.7005   time: 0.13s   best: 92.1967
2023-11-02 14:02:34,555:INFO:  Epoch 105/500:  train Loss: 79.8241   val Loss: 97.2332   time: 0.13s   best: 92.1967
2023-11-02 14:02:34,707:INFO:  Epoch 106/500:  train Loss: 82.0888   val Loss: 92.9353   time: 0.15s   best: 92.1967
2023-11-02 14:02:34,842:INFO:  Epoch 107/500:  train Loss: 80.4234   val Loss: 92.4908   time: 0.13s   best: 92.1967
2023-11-02 14:02:34,976:INFO:  Epoch 108/500:  train Loss: 80.5395   val Loss: 94.2416   time: 0.13s   best: 92.1967
2023-11-02 14:02:35,108:INFO:  Epoch 109/500:  train Loss: 79.5023   val Loss: 99.3044   time: 0.13s   best: 92.1967
2023-11-02 14:02:35,297:INFO:  Epoch 110/500:  train Loss: 80.2153   val Loss: 106.7799   time: 0.19s   best: 92.1967
2023-11-02 14:02:35,428:INFO:  Epoch 111/500:  train Loss: 80.9134   val Loss: 107.5338   time: 0.13s   best: 92.1967
2023-11-02 14:02:35,557:INFO:  Epoch 112/500:  train Loss: 78.7299   val Loss: 103.4559   time: 0.13s   best: 92.1967
2023-11-02 14:02:35,686:INFO:  Epoch 113/500:  train Loss: 76.2561   val Loss: 102.7001   time: 0.13s   best: 92.1967
2023-11-02 14:02:35,839:INFO:  Epoch 114/500:  train Loss: 75.1874   val Loss: 116.9602   time: 0.15s   best: 92.1967
2023-11-02 14:02:35,968:INFO:  Epoch 115/500:  train Loss: 99.3461   val Loss: 93.3525   time: 0.13s   best: 92.1967
2023-11-02 14:02:36,099:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:36,201:INFO:  Epoch 116/500:  train Loss: 86.1121   val Loss: 91.1246   time: 0.13s   best: 91.1246
2023-11-02 14:02:36,351:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:36,446:INFO:  Epoch 117/500:  train Loss: 88.1457   val Loss: 90.6122   time: 0.15s   best: 90.6122
2023-11-02 14:02:36,578:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:02:36,642:INFO:  Epoch 118/500:  train Loss: 87.1930   val Loss: 90.2904   time: 0.13s   best: 90.2904
2023-11-02 14:02:36,777:INFO:  Epoch 119/500:  train Loss: 84.3651   val Loss: 91.3639   time: 0.13s   best: 90.2904
2023-11-02 14:02:36,930:INFO:  Epoch 120/500:  train Loss: 84.7234   val Loss: 94.3709   time: 0.15s   best: 90.2904
2023-11-02 14:02:37,062:INFO:  Epoch 121/500:  train Loss: 85.4820   val Loss: 97.5734   time: 0.13s   best: 90.2904
2023-11-02 14:02:37,191:INFO:  Epoch 122/500:  train Loss: 85.5699   val Loss: 98.6701   time: 0.13s   best: 90.2904
2023-11-02 14:02:37,378:INFO:  Epoch 123/500:  train Loss: 85.3253   val Loss: 97.5212   time: 0.18s   best: 90.2904
2023-11-02 14:02:37,510:INFO:  Epoch 124/500:  train Loss: 84.6333   val Loss: 95.4795   time: 0.13s   best: 90.2904
2023-11-02 14:02:37,639:INFO:  Epoch 125/500:  train Loss: 83.4809   val Loss: 93.9236   time: 0.13s   best: 90.2904
2023-11-02 14:02:37,769:INFO:  Epoch 126/500:  train Loss: 83.9956   val Loss: 93.1165   time: 0.13s   best: 90.2904
2023-11-02 14:02:37,923:INFO:  Epoch 127/500:  train Loss: 84.0543   val Loss: 92.9249   time: 0.15s   best: 90.2904
2023-11-02 14:02:38,052:INFO:  Epoch 128/500:  train Loss: 84.0688   val Loss: 93.1145   time: 0.13s   best: 90.2904
2023-11-02 14:02:38,181:INFO:  Epoch 129/500:  train Loss: 83.9574   val Loss: 93.5383   time: 0.13s   best: 90.2904
2023-11-02 14:02:38,311:INFO:  Epoch 130/500:  train Loss: 83.6394   val Loss: 94.1080   time: 0.13s   best: 90.2904
2023-11-02 14:02:38,466:INFO:  Epoch 131/500:  train Loss: 83.4633   val Loss: 94.8376   time: 0.15s   best: 90.2904
2023-11-02 14:02:38,595:INFO:  Epoch 132/500:  train Loss: 83.7482   val Loss: 95.4451   time: 0.13s   best: 90.2904
2023-11-02 14:02:38,724:INFO:  Epoch 133/500:  train Loss: 83.8195   val Loss: 95.6638   time: 0.13s   best: 90.2904
2023-11-02 14:02:38,860:INFO:  Epoch 134/500:  train Loss: 83.7384   val Loss: 95.4513   time: 0.13s   best: 90.2904
2023-11-02 14:02:39,016:INFO:  Epoch 135/500:  train Loss: 83.1755   val Loss: 94.9644   time: 0.15s   best: 90.2904
2023-11-02 14:02:39,147:INFO:  Epoch 136/500:  train Loss: 83.5134   val Loss: 94.3923   time: 0.13s   best: 90.2904
2023-11-02 14:02:39,276:INFO:  Epoch 137/500:  train Loss: 83.2753   val Loss: 93.7844   time: 0.13s   best: 90.2904
2023-11-02 14:02:39,463:INFO:  Epoch 138/500:  train Loss: 83.0811   val Loss: 93.3956   time: 0.18s   best: 90.2904
2023-11-02 14:02:39,597:INFO:  Epoch 139/500:  train Loss: 82.5191   val Loss: 93.2956   time: 0.13s   best: 90.2904
2023-11-02 14:02:39,727:INFO:  Epoch 140/500:  train Loss: 82.9788   val Loss: 93.4388   time: 0.13s   best: 90.2904
2023-11-02 14:02:39,857:INFO:  Epoch 141/500:  train Loss: 82.6563   val Loss: 93.7684   time: 0.13s   best: 90.2904
2023-11-02 14:02:40,009:INFO:  Epoch 142/500:  train Loss: 82.5764   val Loss: 94.0992   time: 0.15s   best: 90.2904
2023-11-02 14:02:40,138:INFO:  Epoch 143/500:  train Loss: 82.5538   val Loss: 94.1417   time: 0.13s   best: 90.2904
2023-11-02 14:02:40,267:INFO:  Epoch 144/500:  train Loss: 82.2637   val Loss: 93.8944   time: 0.13s   best: 90.2904
2023-11-02 14:02:40,398:INFO:  Epoch 145/500:  train Loss: 82.3056   val Loss: 93.7762   time: 0.13s   best: 90.2904
2023-11-02 14:02:40,551:INFO:  Epoch 146/500:  train Loss: 81.5745   val Loss: 93.0498   time: 0.15s   best: 90.2904
2023-11-02 14:02:40,681:INFO:  Epoch 147/500:  train Loss: 81.8111   val Loss: 93.3848   time: 0.13s   best: 90.2904
2023-11-02 14:02:40,816:INFO:  Epoch 148/500:  train Loss: 82.1881   val Loss: 93.5643   time: 0.13s   best: 90.2904
2023-11-02 14:02:40,951:INFO:  Epoch 149/500:  train Loss: 81.9139   val Loss: 93.5458   time: 0.13s   best: 90.2904
2023-11-02 14:02:41,106:INFO:  Epoch 150/500:  train Loss: 82.0087   val Loss: 93.5978   time: 0.15s   best: 90.2904
2023-11-02 14:02:41,235:INFO:  Epoch 151/500:  train Loss: 81.5801   val Loss: 93.4254   time: 0.13s   best: 90.2904
2023-11-02 14:02:41,366:INFO:  Epoch 152/500:  train Loss: 81.4958   val Loss: 93.3623   time: 0.13s   best: 90.2904
2023-11-02 14:02:41,533:INFO:  Epoch 153/500:  train Loss: 81.4655   val Loss: 92.4875   time: 0.16s   best: 90.2904
2023-11-02 14:02:41,682:INFO:  Epoch 154/500:  train Loss: 80.8925   val Loss: 91.4494   time: 0.15s   best: 90.2904
2023-11-02 14:02:41,811:INFO:  Epoch 155/500:  train Loss: 83.1527   val Loss: 94.8994   time: 0.13s   best: 90.2904
2023-11-02 14:02:41,942:INFO:  Epoch 156/500:  train Loss: 81.4015   val Loss: 95.6161   time: 0.13s   best: 90.2904
2023-11-02 14:02:42,071:INFO:  Epoch 157/500:  train Loss: 81.6967   val Loss: 95.3905   time: 0.13s   best: 90.2904
2023-11-02 14:02:42,224:INFO:  Epoch 158/500:  train Loss: 81.7895   val Loss: 94.6295   time: 0.15s   best: 90.2904
2023-11-02 14:02:42,354:INFO:  Epoch 159/500:  train Loss: 82.6741   val Loss: 94.1654   time: 0.13s   best: 90.2904
2023-11-02 14:02:42,486:INFO:  Epoch 160/500:  train Loss: 82.1891   val Loss: 94.1168   time: 0.13s   best: 90.2904
2023-11-02 14:02:42,615:INFO:  Epoch 161/500:  train Loss: 82.3536   val Loss: 94.3160   time: 0.13s   best: 90.2904
2023-11-02 14:02:42,807:INFO:  Epoch 162/500:  train Loss: 81.6669   val Loss: 93.9019   time: 0.19s   best: 90.2904
2023-11-02 14:02:42,944:INFO:  Epoch 163/500:  train Loss: 81.0633   val Loss: 93.7317   time: 0.13s   best: 90.2904
2023-11-02 14:02:43,075:INFO:  Epoch 164/500:  train Loss: 81.3547   val Loss: 94.9775   time: 0.13s   best: 90.2904
2023-11-02 14:02:43,227:INFO:  Epoch 165/500:  train Loss: 81.6373   val Loss: 95.8172   time: 0.15s   best: 90.2904
2023-11-02 14:02:43,357:INFO:  Epoch 166/500:  train Loss: 81.4820   val Loss: 95.1904   time: 0.13s   best: 90.2904
2023-11-02 14:02:43,527:INFO:  Epoch 167/500:  train Loss: 81.2406   val Loss: 95.1535   time: 0.17s   best: 90.2904
2023-11-02 14:02:43,690:INFO:  Epoch 168/500:  train Loss: 80.9211   val Loss: 96.7507   time: 0.16s   best: 90.2904
2023-11-02 14:02:43,840:INFO:  Epoch 169/500:  train Loss: 81.2603   val Loss: 99.3604   time: 0.15s   best: 90.2904
2023-11-02 14:02:43,974:INFO:  Epoch 170/500:  train Loss: 80.6610   val Loss: 101.0084   time: 0.13s   best: 90.2904
2023-11-02 14:02:44,112:INFO:  Epoch 171/500:  train Loss: 81.6132   val Loss: 98.9067   time: 0.14s   best: 90.2904
2023-11-02 14:02:44,263:INFO:  Epoch 172/500:  train Loss: 80.3119   val Loss: 94.7118   time: 0.15s   best: 90.2904
2023-11-02 14:02:44,400:INFO:  Epoch 173/500:  train Loss: 79.4683   val Loss: 94.3079   time: 0.13s   best: 90.2904
2023-11-02 14:02:44,534:INFO:  Epoch 174/500:  train Loss: 78.7807   val Loss: 98.1934   time: 0.13s   best: 90.2904
2023-11-02 14:02:44,664:INFO:  Epoch 175/500:  train Loss: 78.4309   val Loss: 101.2697   time: 0.13s   best: 90.2904
2023-11-02 14:02:44,823:INFO:  Epoch 176/500:  train Loss: 77.9974   val Loss: 101.7106   time: 0.16s   best: 90.2904
2023-11-02 14:02:44,960:INFO:  Epoch 177/500:  train Loss: 77.8003   val Loss: 101.4841   time: 0.13s   best: 90.2904
2023-11-02 14:02:45,090:INFO:  Epoch 178/500:  train Loss: 77.1153   val Loss: 100.2533   time: 0.13s   best: 90.2904
2023-11-02 14:02:45,221:INFO:  Epoch 179/500:  train Loss: 76.8946   val Loss: 101.0871   time: 0.13s   best: 90.2904
2023-11-02 14:02:45,375:INFO:  Epoch 180/500:  train Loss: 77.6985   val Loss: 101.4149   time: 0.15s   best: 90.2904
2023-11-02 14:02:45,506:INFO:  Epoch 181/500:  train Loss: 78.4186   val Loss: 103.1543   time: 0.13s   best: 90.2904
2023-11-02 14:02:45,657:INFO:  Epoch 182/500:  train Loss: 76.4890   val Loss: 106.3601   time: 0.15s   best: 90.2904
2023-11-02 14:02:45,804:INFO:  Epoch 183/500:  train Loss: 77.2900   val Loss: 104.8338   time: 0.14s   best: 90.2904
2023-11-02 14:02:45,956:INFO:  Epoch 184/500:  train Loss: 76.1771   val Loss: 105.5395   time: 0.15s   best: 90.2904
2023-11-02 14:02:46,085:INFO:  Epoch 185/500:  train Loss: 76.1561   val Loss: 110.7120   time: 0.13s   best: 90.2904
2023-11-02 14:02:46,214:INFO:  Epoch 186/500:  train Loss: 74.8904   val Loss: 111.6460   time: 0.13s   best: 90.2904
2023-11-02 14:02:46,345:INFO:  Epoch 187/500:  train Loss: 75.6363   val Loss: 111.1003   time: 0.13s   best: 90.2904
2023-11-02 14:02:46,498:INFO:  Epoch 188/500:  train Loss: 75.9290   val Loss: 114.0111   time: 0.15s   best: 90.2904
2023-11-02 14:02:46,627:INFO:  Epoch 189/500:  train Loss: 74.5371   val Loss: 112.3116   time: 0.13s   best: 90.2904
2023-11-02 14:02:46,756:INFO:  Epoch 190/500:  train Loss: 73.8051   val Loss: 137.8470   time: 0.13s   best: 90.2904
2023-11-02 14:02:46,893:INFO:  Epoch 191/500:  train Loss: 232.9698   val Loss: 100.1846   time: 0.13s   best: 90.2904
2023-11-02 14:02:47,050:INFO:  Epoch 192/500:  train Loss: 100.4001   val Loss: 100.4250   time: 0.15s   best: 90.2904
2023-11-02 14:02:47,182:INFO:  Epoch 193/500:  train Loss: 100.5118   val Loss: 100.5458   time: 0.13s   best: 90.2904
2023-11-02 14:02:47,311:INFO:  Epoch 194/500:  train Loss: 100.6409   val Loss: 100.6198   time: 0.13s   best: 90.2904
2023-11-02 14:02:47,479:INFO:  Epoch 195/500:  train Loss: 100.7223   val Loss: 100.6570   time: 0.17s   best: 90.2904
2023-11-02 14:02:47,607:INFO:  Epoch 196/500:  train Loss: 100.8176   val Loss: 100.6679   time: 0.13s   best: 90.2904
2023-11-02 14:02:47,776:INFO:  Epoch 197/500:  train Loss: 100.7909   val Loss: 100.6588   time: 0.17s   best: 90.2904
2023-11-02 14:02:47,908:INFO:  Epoch 198/500:  train Loss: 100.7591   val Loss: 100.6254   time: 0.13s   best: 90.2904
2023-11-02 14:02:48,060:INFO:  Epoch 199/500:  train Loss: 100.1965   val Loss: 98.0544   time: 0.15s   best: 90.2904
2023-11-02 14:02:48,190:INFO:  Epoch 200/500:  train Loss: 95.0203   val Loss: 94.3247   time: 0.13s   best: 90.2904
2023-11-02 14:02:48,319:INFO:  Epoch 201/500:  train Loss: 87.3114   val Loss: 103.1928   time: 0.13s   best: 90.2904
2023-11-02 14:02:48,451:INFO:  Epoch 202/500:  train Loss: 91.2111   val Loss: 112.1682   time: 0.13s   best: 90.2904
2023-11-02 14:02:48,605:INFO:  Epoch 203/500:  train Loss: 89.6891   val Loss: 102.0357   time: 0.15s   best: 90.2904
2023-11-02 14:02:48,734:INFO:  Epoch 204/500:  train Loss: 84.8592   val Loss: 95.5515   time: 0.13s   best: 90.2904
2023-11-02 14:02:48,870:INFO:  Epoch 205/500:  train Loss: 86.5687   val Loss: 94.5002   time: 0.13s   best: 90.2904
2023-11-02 14:02:49,005:INFO:  Epoch 206/500:  train Loss: 86.4066   val Loss: 94.8648   time: 0.13s   best: 90.2904
2023-11-02 14:02:49,159:INFO:  Epoch 207/500:  train Loss: 85.5029   val Loss: 96.4742   time: 0.15s   best: 90.2904
2023-11-02 14:02:49,288:INFO:  Epoch 208/500:  train Loss: 85.4714   val Loss: 99.5819   time: 0.13s   best: 90.2904
2023-11-02 14:02:49,419:INFO:  Epoch 209/500:  train Loss: 85.9095   val Loss: 101.5701   time: 0.13s   best: 90.2904
2023-11-02 14:02:49,550:INFO:  Epoch 210/500:  train Loss: 86.5445   val Loss: 100.3965   time: 0.13s   best: 90.2904
2023-11-02 14:02:49,701:INFO:  Epoch 211/500:  train Loss: 85.2663   val Loss: 98.2163   time: 0.15s   best: 90.2904
2023-11-02 14:02:49,865:INFO:  Epoch 212/500:  train Loss: 85.4883   val Loss: 97.2821   time: 0.16s   best: 90.2904
2023-11-02 14:02:49,994:INFO:  Epoch 213/500:  train Loss: 85.4982   val Loss: 97.6312   time: 0.13s   best: 90.2904
2023-11-02 14:02:50,146:INFO:  Epoch 214/500:  train Loss: 85.2958   val Loss: 98.4395   time: 0.13s   best: 90.2904
2023-11-02 14:02:50,275:INFO:  Epoch 215/500:  train Loss: 84.9656   val Loss: 98.7170   time: 0.13s   best: 90.2904
2023-11-02 14:02:50,406:INFO:  Epoch 216/500:  train Loss: 84.5511   val Loss: 97.7511   time: 0.13s   best: 90.2904
2023-11-02 14:02:50,536:INFO:  Epoch 217/500:  train Loss: 84.9035   val Loss: 96.9940   time: 0.13s   best: 90.2904
2023-11-02 14:02:50,666:INFO:  Epoch 218/500:  train Loss: 84.6141   val Loss: 97.4272   time: 0.13s   best: 90.2904
2023-11-02 14:02:50,825:INFO:  Epoch 219/500:  train Loss: 85.4920   val Loss: 98.9474   time: 0.16s   best: 90.2904
2023-11-02 14:02:50,962:INFO:  Epoch 220/500:  train Loss: 85.1537   val Loss: 99.7174   time: 0.13s   best: 90.2904
2023-11-02 14:02:51,094:INFO:  Epoch 221/500:  train Loss: 84.5083   val Loss: 106.9225   time: 0.13s   best: 90.2904
2023-11-02 14:02:51,245:INFO:  Epoch 222/500:  train Loss: 93.7268   val Loss: 95.4492   time: 0.15s   best: 90.2904
2023-11-02 14:02:51,375:INFO:  Epoch 223/500:  train Loss: 86.8581   val Loss: 94.7683   time: 0.13s   best: 90.2904
2023-11-02 14:02:51,506:INFO:  Epoch 224/500:  train Loss: 87.7909   val Loss: 96.5862   time: 0.13s   best: 90.2904
2023-11-02 14:02:51,635:INFO:  Epoch 225/500:  train Loss: 87.0723   val Loss: 101.4789   time: 0.13s   best: 90.2904
2023-11-02 14:02:51,789:INFO:  Epoch 226/500:  train Loss: 88.0796   val Loss: 102.1540   time: 0.15s   best: 90.2904
2023-11-02 14:02:51,956:INFO:  Epoch 227/500:  train Loss: 87.1358   val Loss: 98.4251   time: 0.16s   best: 90.2904
2023-11-02 14:02:52,085:INFO:  Epoch 228/500:  train Loss: 87.1014   val Loss: 96.8866   time: 0.13s   best: 90.2904
2023-11-02 14:02:52,214:INFO:  Epoch 229/500:  train Loss: 87.0123   val Loss: 98.5573   time: 0.13s   best: 90.2904
2023-11-02 14:02:52,367:INFO:  Epoch 230/500:  train Loss: 86.8106   val Loss: 99.8273   time: 0.15s   best: 90.2904
2023-11-02 14:02:52,497:INFO:  Epoch 231/500:  train Loss: 86.2954   val Loss: 98.7711   time: 0.13s   best: 90.2904
2023-11-02 14:02:52,627:INFO:  Epoch 232/500:  train Loss: 86.0326   val Loss: 97.6231   time: 0.13s   best: 90.2904
2023-11-02 14:02:52,758:INFO:  Epoch 233/500:  train Loss: 86.4859   val Loss: 97.3831   time: 0.13s   best: 90.2904
2023-11-02 14:02:52,916:INFO:  Epoch 234/500:  train Loss: 85.6827   val Loss: 98.1083   time: 0.16s   best: 90.2904
2023-11-02 14:02:53,052:INFO:  Epoch 235/500:  train Loss: 86.8408   val Loss: 98.7916   time: 0.13s   best: 90.2904
2023-11-02 14:02:53,184:INFO:  Epoch 236/500:  train Loss: 85.3707   val Loss: 99.2030   time: 0.13s   best: 90.2904
2023-11-02 14:02:53,313:INFO:  Epoch 237/500:  train Loss: 85.2186   val Loss: 98.7056   time: 0.13s   best: 90.2904
2023-11-02 14:02:53,469:INFO:  Epoch 238/500:  train Loss: 85.2783   val Loss: 97.6764   time: 0.15s   best: 90.2904
2023-11-02 14:02:53,598:INFO:  Epoch 239/500:  train Loss: 85.1026   val Loss: 97.5499   time: 0.13s   best: 90.2904
2023-11-02 14:02:53,727:INFO:  Epoch 240/500:  train Loss: 84.9062   val Loss: 98.3401   time: 0.13s   best: 90.2904
2023-11-02 14:02:53,857:INFO:  Epoch 241/500:  train Loss: 84.9515   val Loss: 98.5908   time: 0.13s   best: 90.2904
2023-11-02 14:02:54,046:INFO:  Epoch 242/500:  train Loss: 84.6523   val Loss: 97.5359   time: 0.19s   best: 90.2904
2023-11-02 14:02:54,176:INFO:  Epoch 243/500:  train Loss: 84.4204   val Loss: 96.6406   time: 0.13s   best: 90.2904
2023-11-02 14:02:54,305:INFO:  Epoch 244/500:  train Loss: 83.7814   val Loss: 96.9450   time: 0.13s   best: 90.2904
2023-11-02 14:02:54,458:INFO:  Epoch 245/500:  train Loss: 83.6091   val Loss: 97.7242   time: 0.15s   best: 90.2904
2023-11-02 14:02:54,588:INFO:  Epoch 246/500:  train Loss: 83.3422   val Loss: 97.1768   time: 0.13s   best: 90.2904
2023-11-02 14:02:54,717:INFO:  Epoch 247/500:  train Loss: 82.3723   val Loss: 95.7377   time: 0.13s   best: 90.2904
2023-11-02 14:02:54,852:INFO:  Epoch 248/500:  train Loss: 81.9231   val Loss: 95.1312   time: 0.13s   best: 90.2904
2023-11-02 14:02:55,011:INFO:  Epoch 249/500:  train Loss: 82.2312   val Loss: 95.1367   time: 0.16s   best: 90.2904
2023-11-02 14:02:55,143:INFO:  Epoch 250/500:  train Loss: 81.2370   val Loss: 94.9379   time: 0.13s   best: 90.2904
2023-11-02 14:02:55,272:INFO:  Epoch 251/500:  train Loss: 80.6431   val Loss: 94.8003   time: 0.13s   best: 90.2904
2023-11-02 14:02:55,403:INFO:  Epoch 252/500:  train Loss: 79.7853   val Loss: 93.8842   time: 0.13s   best: 90.2904
2023-11-02 14:02:55,559:INFO:  Epoch 253/500:  train Loss: 79.8325   val Loss: 92.8576   time: 0.15s   best: 90.2904
2023-11-02 14:02:55,689:INFO:  Epoch 254/500:  train Loss: 78.7714   val Loss: 95.5022   time: 0.13s   best: 90.2904
2023-11-02 14:02:55,818:INFO:  Epoch 255/500:  train Loss: 82.4288   val Loss: 96.0366   time: 0.13s   best: 90.2904
2023-11-02 14:02:55,948:INFO:  Epoch 256/500:  train Loss: 79.5008   val Loss: 93.5372   time: 0.13s   best: 90.2904
2023-11-02 14:02:56,134:INFO:  Epoch 257/500:  train Loss: 80.1010   val Loss: 94.9490   time: 0.18s   best: 90.2904
2023-11-02 14:02:56,264:INFO:  Epoch 258/500:  train Loss: 81.7206   val Loss: 97.2150   time: 0.13s   best: 90.2904
2023-11-02 14:02:56,393:INFO:  Epoch 259/500:  train Loss: 81.3205   val Loss: 93.6539   time: 0.13s   best: 90.2904
2023-11-02 14:02:56,523:INFO:  Epoch 260/500:  train Loss: 80.6404   val Loss: 94.4842   time: 0.13s   best: 90.2904
2023-11-02 14:02:56,675:INFO:  Epoch 261/500:  train Loss: 80.4471   val Loss: 91.5641   time: 0.15s   best: 90.2904
2023-11-02 14:02:56,805:INFO:  Epoch 262/500:  train Loss: 83.8610   val Loss: 95.3086   time: 0.13s   best: 90.2904
2023-11-02 14:02:56,940:INFO:  Epoch 263/500:  train Loss: 81.5459   val Loss: 110.3320   time: 0.13s   best: 90.2904
2023-11-02 14:02:57,075:INFO:  Epoch 264/500:  train Loss: 81.9653   val Loss: 98.1989   time: 0.13s   best: 90.2904
2023-11-02 14:02:57,231:INFO:  Epoch 265/500:  train Loss: 83.7596   val Loss: 93.9168   time: 0.15s   best: 90.2904
2023-11-02 14:02:57,360:INFO:  Epoch 266/500:  train Loss: 79.9662   val Loss: 104.2562   time: 0.13s   best: 90.2904
2023-11-02 14:02:57,494:INFO:  Epoch 267/500:  train Loss: 83.5492   val Loss: 113.1096   time: 0.13s   best: 90.2904
2023-11-02 14:02:57,623:INFO:  Epoch 268/500:  train Loss: 79.9231   val Loss: 100.7871   time: 0.13s   best: 90.2904
2023-11-02 14:02:57,777:INFO:  Epoch 269/500:  train Loss: 78.0466   val Loss: 97.7549   time: 0.15s   best: 90.2904
2023-11-02 14:02:57,907:INFO:  Epoch 270/500:  train Loss: 79.5625   val Loss: 105.7397   time: 0.13s   best: 90.2904
2023-11-02 14:02:58,036:INFO:  Epoch 271/500:  train Loss: 81.6851   val Loss: 104.2541   time: 0.13s   best: 90.2904
2023-11-02 14:02:58,222:INFO:  Epoch 272/500:  train Loss: 79.7388   val Loss: 97.4242   time: 0.18s   best: 90.2904
2023-11-02 14:02:58,351:INFO:  Epoch 273/500:  train Loss: 79.4867   val Loss: 98.8013   time: 0.13s   best: 90.2904
2023-11-02 14:02:58,483:INFO:  Epoch 274/500:  train Loss: 77.6794   val Loss: 106.6694   time: 0.13s   best: 90.2904
2023-11-02 14:02:58,612:INFO:  Epoch 275/500:  train Loss: 83.9398   val Loss: 144.3843   time: 0.13s   best: 90.2904
2023-11-02 14:02:58,765:INFO:  Epoch 276/500:  train Loss: 210.8460   val Loss: 100.6913   time: 0.15s   best: 90.2904
2023-11-02 14:02:58,901:INFO:  Epoch 277/500:  train Loss: 100.8969   val Loss: 100.9087   time: 0.13s   best: 90.2904
2023-11-02 14:02:59,036:INFO:  Epoch 278/500:  train Loss: 101.1277   val Loss: 101.0664   time: 0.13s   best: 90.2904
2023-11-02 14:02:59,168:INFO:  Epoch 279/500:  train Loss: 101.2843   val Loss: 101.1459   time: 0.13s   best: 90.2904
2023-11-02 14:02:59,322:INFO:  Epoch 280/500:  train Loss: 101.3436   val Loss: 101.1656   time: 0.15s   best: 90.2904
2023-11-02 14:02:59,454:INFO:  Epoch 281/500:  train Loss: 101.3690   val Loss: 101.1444   time: 0.13s   best: 90.2904
2023-11-02 14:02:59,583:INFO:  Epoch 282/500:  train Loss: 101.3378   val Loss: 101.0952   time: 0.13s   best: 90.2904
2023-11-02 14:02:59,713:INFO:  Epoch 283/500:  train Loss: 101.2638   val Loss: 101.0261   time: 0.13s   best: 90.2904
2023-11-02 14:02:59,865:INFO:  Epoch 284/500:  train Loss: 101.1937   val Loss: 100.9424   time: 0.15s   best: 90.2904
2023-11-02 14:02:59,996:INFO:  Epoch 285/500:  train Loss: 101.1013   val Loss: 100.8475   time: 0.13s   best: 90.2904
2023-11-02 14:03:00,125:INFO:  Epoch 286/500:  train Loss: 100.9643   val Loss: 100.7439   time: 0.13s   best: 90.2904
2023-11-02 14:03:00,291:INFO:  Epoch 287/500:  train Loss: 100.8495   val Loss: 100.6328   time: 0.16s   best: 90.2904
2023-11-02 14:03:00,443:INFO:  Epoch 288/500:  train Loss: 100.7217   val Loss: 100.5155   time: 0.15s   best: 90.2904
2023-11-02 14:03:00,573:INFO:  Epoch 289/500:  train Loss: 100.5748   val Loss: 100.3922   time: 0.13s   best: 90.2904
2023-11-02 14:03:00,702:INFO:  Epoch 290/500:  train Loss: 100.3390   val Loss: 99.8010   time: 0.13s   best: 90.2904
2023-11-02 14:03:00,837:INFO:  Epoch 291/500:  train Loss: 97.9343   val Loss: 93.7022   time: 0.13s   best: 90.2904
2023-11-02 14:03:00,997:INFO:  Epoch 292/500:  train Loss: 100.0788   val Loss: 97.7986   time: 0.16s   best: 90.2904
2023-11-02 14:03:01,129:INFO:  Epoch 293/500:  train Loss: 96.9734   val Loss: 95.5067   time: 0.13s   best: 90.2904
2023-11-02 14:03:01,258:INFO:  Epoch 294/500:  train Loss: 90.3220   val Loss: 99.5572   time: 0.13s   best: 90.2904
2023-11-02 14:03:01,410:INFO:  Epoch 295/500:  train Loss: 90.9094   val Loss: 111.1563   time: 0.15s   best: 90.2904
2023-11-02 14:03:01,541:INFO:  Epoch 296/500:  train Loss: 89.5745   val Loss: 100.4413   time: 0.13s   best: 90.2904
2023-11-02 14:03:01,671:INFO:  Epoch 297/500:  train Loss: 85.7714   val Loss: 95.5507   time: 0.13s   best: 90.2904
2023-11-02 14:03:01,801:INFO:  Epoch 298/500:  train Loss: 85.6435   val Loss: 111.1991   time: 0.13s   best: 90.2904
2023-11-02 14:03:01,953:INFO:  Epoch 299/500:  train Loss: 101.7497   val Loss: 95.6587   time: 0.15s   best: 90.2904
2023-11-02 14:03:02,083:INFO:  Epoch 300/500:  train Loss: 97.7146   val Loss: 98.1048   time: 0.13s   best: 90.2904
2023-11-02 14:03:02,212:INFO:  Epoch 301/500:  train Loss: 98.5999   val Loss: 97.2369   time: 0.13s   best: 90.2904
2023-11-02 14:03:02,377:INFO:  Epoch 302/500:  train Loss: 95.7599   val Loss: 95.6160   time: 0.16s   best: 90.2904
2023-11-02 14:03:02,532:INFO:  Epoch 303/500:  train Loss: 90.0652   val Loss: 99.5555   time: 0.15s   best: 90.2904
2023-11-02 14:03:02,661:INFO:  Epoch 304/500:  train Loss: 90.3203   val Loss: 108.1209   time: 0.13s   best: 90.2904
2023-11-02 14:03:02,803:INFO:  Epoch 305/500:  train Loss: 89.9474   val Loss: 102.5539   time: 0.14s   best: 90.2904
2023-11-02 14:03:02,935:INFO:  Epoch 306/500:  train Loss: 86.8556   val Loss: 97.0191   time: 0.13s   best: 90.2904
2023-11-02 14:03:03,098:INFO:  Epoch 307/500:  train Loss: 86.6570   val Loss: 96.0556   time: 0.16s   best: 90.2904
2023-11-02 14:03:03,228:INFO:  Epoch 308/500:  train Loss: 86.9156   val Loss: 97.1758   time: 0.13s   best: 90.2904
2023-11-02 14:03:03,359:INFO:  Epoch 309/500:  train Loss: 86.4234   val Loss: 99.8616   time: 0.13s   best: 90.2904
2023-11-02 14:03:03,492:INFO:  Epoch 310/500:  train Loss: 86.7810   val Loss: 100.7782   time: 0.13s   best: 90.2904
2023-11-02 14:03:03,645:INFO:  Epoch 311/500:  train Loss: 85.8491   val Loss: 99.2309   time: 0.15s   best: 90.2904
2023-11-02 14:03:03,774:INFO:  Epoch 312/500:  train Loss: 85.9689   val Loss: 98.0668   time: 0.13s   best: 90.2904
2023-11-02 14:03:03,904:INFO:  Epoch 313/500:  train Loss: 85.6909   val Loss: 98.2868   time: 0.13s   best: 90.2904
2023-11-02 14:03:04,034:INFO:  Epoch 314/500:  train Loss: 85.8129   val Loss: 99.0278   time: 0.13s   best: 90.2904
2023-11-02 14:03:04,188:INFO:  Epoch 315/500:  train Loss: 85.7233   val Loss: 99.5447   time: 0.15s   best: 90.2904
2023-11-02 14:03:04,351:INFO:  Epoch 316/500:  train Loss: 85.7793   val Loss: 98.5050   time: 0.16s   best: 90.2904
2023-11-02 14:03:04,482:INFO:  Epoch 317/500:  train Loss: 85.7379   val Loss: 97.9267   time: 0.13s   best: 90.2904
2023-11-02 14:03:04,633:INFO:  Epoch 318/500:  train Loss: 84.9845   val Loss: 98.3239   time: 0.15s   best: 90.2904
2023-11-02 14:03:04,762:INFO:  Epoch 319/500:  train Loss: 85.1759   val Loss: 95.1401   time: 0.13s   best: 90.2904
2023-11-02 14:03:04,895:INFO:  Epoch 320/500:  train Loss: 87.4337   val Loss: 94.7747   time: 0.13s   best: 90.2904
2023-11-02 14:03:05,033:INFO:  Epoch 321/500:  train Loss: 87.0261   val Loss: 96.3148   time: 0.14s   best: 90.2904
2023-11-02 14:03:05,188:INFO:  Epoch 322/500:  train Loss: 85.7482   val Loss: 100.0633   time: 0.15s   best: 90.2904
2023-11-02 14:03:05,318:INFO:  Epoch 323/500:  train Loss: 86.5735   val Loss: 101.7777   time: 0.13s   best: 90.2904
2023-11-02 14:03:05,448:INFO:  Epoch 324/500:  train Loss: 86.2352   val Loss: 99.5920   time: 0.13s   best: 90.2904
2023-11-02 14:03:05,579:INFO:  Epoch 325/500:  train Loss: 85.6927   val Loss: 97.5883   time: 0.13s   best: 90.2904
2023-11-02 14:03:05,732:INFO:  Epoch 326/500:  train Loss: 85.8904   val Loss: 97.7339   time: 0.15s   best: 90.2904
2023-11-02 14:03:05,862:INFO:  Epoch 327/500:  train Loss: 85.9778   val Loss: 98.6014   time: 0.13s   best: 90.2904
2023-11-02 14:03:05,993:INFO:  Epoch 328/500:  train Loss: 85.6659   val Loss: 98.7443   time: 0.13s   best: 90.2904
2023-11-02 14:03:06,122:INFO:  Epoch 329/500:  train Loss: 85.4212   val Loss: 98.5953   time: 0.13s   best: 90.2904
2023-11-02 14:03:06,276:INFO:  Epoch 330/500:  train Loss: 85.6997   val Loss: 98.1750   time: 0.15s   best: 90.2904
2023-11-02 14:03:06,443:INFO:  Epoch 331/500:  train Loss: 85.2626   val Loss: 97.9533   time: 0.16s   best: 90.2904
2023-11-02 14:03:06,649:INFO:  Epoch 332/500:  train Loss: 85.2636   val Loss: 98.4793   time: 0.20s   best: 90.2904
2023-11-02 14:03:06,796:INFO:  Epoch 333/500:  train Loss: 85.0337   val Loss: 97.5864   time: 0.15s   best: 90.2904
2023-11-02 14:03:06,999:INFO:  Epoch 334/500:  train Loss: 85.6690   val Loss: 96.5213   time: 0.18s   best: 90.2904
2023-11-02 14:03:07,145:INFO:  Epoch 335/500:  train Loss: 84.3030   val Loss: 98.3122   time: 0.13s   best: 90.2904
2023-11-02 14:03:07,306:INFO:  Epoch 336/500:  train Loss: 87.3023   val Loss: 94.8486   time: 0.15s   best: 90.2904
2023-11-02 14:03:07,446:INFO:  Epoch 337/500:  train Loss: 87.4223   val Loss: 94.9432   time: 0.13s   best: 90.2904
2023-11-02 14:03:07,585:INFO:  Epoch 338/500:  train Loss: 87.4093   val Loss: 97.3725   time: 0.13s   best: 90.2904
2023-11-02 14:03:07,724:INFO:  Epoch 339/500:  train Loss: 86.6303   val Loss: 101.0473   time: 0.13s   best: 90.2904
2023-11-02 14:03:07,885:INFO:  Epoch 340/500:  train Loss: 87.0237   val Loss: 100.6790   time: 0.15s   best: 90.2904
2023-11-02 14:03:08,025:INFO:  Epoch 341/500:  train Loss: 86.3052   val Loss: 97.9107   time: 0.13s   best: 90.2904
2023-11-02 14:03:08,165:INFO:  Epoch 342/500:  train Loss: 85.8209   val Loss: 96.4298   time: 0.13s   best: 90.2904
2023-11-02 14:03:08,303:INFO:  Epoch 343/500:  train Loss: 85.8482   val Loss: 97.0948   time: 0.13s   best: 90.2904
2023-11-02 14:03:08,501:INFO:  Epoch 344/500:  train Loss: 85.8784   val Loss: 98.8066   time: 0.19s   best: 90.2904
2023-11-02 14:03:08,640:INFO:  Epoch 345/500:  train Loss: 86.1356   val Loss: 98.4378   time: 0.13s   best: 90.2904
2023-11-02 14:03:08,778:INFO:  Epoch 346/500:  train Loss: 85.8786   val Loss: 97.2985   time: 0.13s   best: 90.2904
2023-11-02 14:03:08,941:INFO:  Epoch 347/500:  train Loss: 85.3194   val Loss: 97.1634   time: 0.15s   best: 90.2904
2023-11-02 14:03:09,086:INFO:  Epoch 348/500:  train Loss: 86.0677   val Loss: 97.6073   time: 0.13s   best: 90.2904
2023-11-02 14:03:09,224:INFO:  Epoch 349/500:  train Loss: 85.7199   val Loss: 98.0134   time: 0.13s   best: 90.2904
2023-11-02 14:03:09,365:INFO:  Epoch 350/500:  train Loss: 85.5632   val Loss: 97.9969   time: 0.13s   best: 90.2904
2023-11-02 14:03:09,527:INFO:  Epoch 351/500:  train Loss: 85.2749   val Loss: 97.4151   time: 0.15s   best: 90.2904
2023-11-02 14:03:09,666:INFO:  Epoch 352/500:  train Loss: 85.0382   val Loss: 96.7937   time: 0.13s   best: 90.2904
2023-11-02 14:03:09,804:INFO:  Epoch 353/500:  train Loss: 85.1400   val Loss: 97.2519   time: 0.13s   best: 90.2904
2023-11-02 14:03:09,963:INFO:  Epoch 354/500:  train Loss: 85.8794   val Loss: 98.0359   time: 0.15s   best: 90.2904
2023-11-02 14:03:10,105:INFO:  Epoch 355/500:  train Loss: 84.8321   val Loss: 98.7549   time: 0.13s   best: 90.2904
2023-11-02 14:03:10,243:INFO:  Epoch 356/500:  train Loss: 90.4207   val Loss: 94.8214   time: 0.13s   best: 90.2904
2023-11-02 14:03:10,383:INFO:  Epoch 357/500:  train Loss: 86.1050   val Loss: 93.6021   time: 0.13s   best: 90.2904
2023-11-02 14:03:10,578:INFO:  Epoch 358/500:  train Loss: 88.4328   val Loss: 93.5988   time: 0.18s   best: 90.2904
2023-11-02 14:03:10,717:INFO:  Epoch 359/500:  train Loss: 87.6308   val Loss: 95.1776   time: 0.13s   best: 90.2904
2023-11-02 14:03:10,858:INFO:  Epoch 360/500:  train Loss: 85.8347   val Loss: 99.9265   time: 0.13s   best: 90.2904
2023-11-02 14:03:10,997:INFO:  Epoch 361/500:  train Loss: 85.3203   val Loss: 98.3054   time: 0.13s   best: 90.2904
2023-11-02 14:03:11,162:INFO:  Epoch 362/500:  train Loss: 85.7756   val Loss: 95.6723   time: 0.15s   best: 90.2904
2023-11-02 14:03:11,300:INFO:  Epoch 363/500:  train Loss: 86.4373   val Loss: 96.6525   time: 0.13s   best: 90.2904
2023-11-02 14:03:11,442:INFO:  Epoch 364/500:  train Loss: 85.9895   val Loss: 99.6334   time: 0.13s   best: 90.2904
2023-11-02 14:03:11,605:INFO:  Epoch 365/500:  train Loss: 86.8305   val Loss: 100.1629   time: 0.15s   best: 90.2904
2023-11-02 14:03:11,743:INFO:  Epoch 366/500:  train Loss: 86.2312   val Loss: 97.8491   time: 0.13s   best: 90.2904
2023-11-02 14:03:11,882:INFO:  Epoch 367/500:  train Loss: 86.1651   val Loss: 96.7991   time: 0.13s   best: 90.2904
2023-11-02 14:03:12,022:INFO:  Epoch 368/500:  train Loss: 86.2405   val Loss: 97.7819   time: 0.13s   best: 90.2904
2023-11-02 14:03:12,182:INFO:  Epoch 369/500:  train Loss: 85.6850   val Loss: 99.3057   time: 0.15s   best: 90.2904
2023-11-02 14:03:12,322:INFO:  Epoch 370/500:  train Loss: 85.8347   val Loss: 99.0236   time: 0.13s   best: 90.2904
2023-11-02 14:03:12,461:INFO:  Epoch 371/500:  train Loss: 85.3043   val Loss: 97.6209   time: 0.13s   best: 90.2904
2023-11-02 14:03:12,658:INFO:  Epoch 372/500:  train Loss: 85.5828   val Loss: 97.2297   time: 0.19s   best: 90.2904
2023-11-02 14:03:12,797:INFO:  Epoch 373/500:  train Loss: 85.6595   val Loss: 97.4996   time: 0.13s   best: 90.2904
2023-11-02 14:03:12,938:INFO:  Epoch 374/500:  train Loss: 85.5956   val Loss: 97.8075   time: 0.13s   best: 90.2904
2023-11-02 14:03:13,084:INFO:  Epoch 375/500:  train Loss: 85.4510   val Loss: 97.5733   time: 0.13s   best: 90.2904
2023-11-02 14:03:13,244:INFO:  Epoch 376/500:  train Loss: 84.4488   val Loss: 97.6692   time: 0.15s   best: 90.2904
2023-11-02 14:03:13,383:INFO:  Epoch 377/500:  train Loss: 84.1672   val Loss: 97.6922   time: 0.13s   best: 90.2904
2023-11-02 14:03:13,524:INFO:  Epoch 378/500:  train Loss: 86.3322   val Loss: 94.8607   time: 0.13s   best: 90.2904
2023-11-02 14:03:13,662:INFO:  Epoch 379/500:  train Loss: 86.7103   val Loss: 94.4519   time: 0.13s   best: 90.2904
2023-11-02 14:03:13,822:INFO:  Epoch 380/500:  train Loss: 86.8345   val Loss: 96.2844   time: 0.15s   best: 90.2904
2023-11-02 14:03:13,962:INFO:  Epoch 381/500:  train Loss: 85.4366   val Loss: 100.1399   time: 0.13s   best: 90.2904
2023-11-02 14:03:14,101:INFO:  Epoch 382/500:  train Loss: 86.4521   val Loss: 100.2752   time: 0.13s   best: 90.2904
2023-11-02 14:03:14,262:INFO:  Epoch 383/500:  train Loss: 85.5037   val Loss: 97.9671   time: 0.15s   best: 90.2904
2023-11-02 14:03:14,400:INFO:  Epoch 384/500:  train Loss: 85.2916   val Loss: 96.6613   time: 0.13s   best: 90.2904
2023-11-02 14:03:14,542:INFO:  Epoch 385/500:  train Loss: 85.5207   val Loss: 97.0738   time: 0.13s   best: 90.2904
2023-11-02 14:03:14,713:INFO:  Epoch 386/500:  train Loss: 85.2433   val Loss: 98.3198   time: 0.16s   best: 90.2904
2023-11-02 14:03:14,914:INFO:  Epoch 387/500:  train Loss: 85.3204   val Loss: 98.3348   time: 0.19s   best: 90.2904
2023-11-02 14:03:15,059:INFO:  Epoch 388/500:  train Loss: 84.8873   val Loss: 97.1026   time: 0.13s   best: 90.2904
2023-11-02 14:03:15,197:INFO:  Epoch 389/500:  train Loss: 85.1583   val Loss: 96.4393   time: 0.13s   best: 90.2904
2023-11-02 14:03:15,358:INFO:  Epoch 390/500:  train Loss: 84.9257   val Loss: 96.6298   time: 0.15s   best: 90.2904
2023-11-02 14:03:15,498:INFO:  Epoch 391/500:  train Loss: 84.6991   val Loss: 95.0398   time: 0.13s   best: 90.2904
2023-11-02 14:03:15,637:INFO:  Epoch 392/500:  train Loss: 83.6559   val Loss: 95.2949   time: 0.13s   best: 90.2904
2023-11-02 14:03:15,775:INFO:  Epoch 393/500:  train Loss: 85.2190   val Loss: 95.7689   time: 0.13s   best: 90.2904
2023-11-02 14:03:15,935:INFO:  Epoch 394/500:  train Loss: 85.1052   val Loss: 97.7384   time: 0.15s   best: 90.2904
2023-11-02 14:03:16,075:INFO:  Epoch 395/500:  train Loss: 84.9730   val Loss: 97.9958   time: 0.13s   best: 90.2904
2023-11-02 14:03:16,213:INFO:  Epoch 396/500:  train Loss: 84.9003   val Loss: 96.8073   time: 0.13s   best: 90.2904
2023-11-02 14:03:16,375:INFO:  Epoch 397/500:  train Loss: 84.2732   val Loss: 95.9019   time: 0.13s   best: 90.2904
2023-11-02 14:03:16,515:INFO:  Epoch 398/500:  train Loss: 84.0871   val Loss: 94.2713   time: 0.13s   best: 90.2904
2023-11-02 14:03:16,654:INFO:  Epoch 399/500:  train Loss: 85.2016   val Loss: 93.7108   time: 0.13s   best: 90.2904
2023-11-02 14:03:16,793:INFO:  Epoch 400/500:  train Loss: 87.4691   val Loss: 93.4679   time: 0.13s   best: 90.2904
2023-11-02 14:03:16,992:INFO:  Epoch 401/500:  train Loss: 87.3046   val Loss: 96.0732   time: 0.19s   best: 90.2904
2023-11-02 14:03:17,136:INFO:  Epoch 402/500:  train Loss: 85.5298   val Loss: 100.6360   time: 0.13s   best: 90.2904
2023-11-02 14:03:17,275:INFO:  Epoch 403/500:  train Loss: 85.7664   val Loss: 98.6168   time: 0.13s   best: 90.2904
2023-11-02 14:03:17,414:INFO:  Epoch 404/500:  train Loss: 84.8890   val Loss: 96.6210   time: 0.13s   best: 90.2904
2023-11-02 14:03:17,578:INFO:  Epoch 405/500:  train Loss: 84.9707   val Loss: 96.4545   time: 0.15s   best: 90.2904
2023-11-02 14:03:17,717:INFO:  Epoch 406/500:  train Loss: 84.6616   val Loss: 96.8026   time: 0.13s   best: 90.2904
2023-11-02 14:03:17,855:INFO:  Epoch 407/500:  train Loss: 84.5270   val Loss: 96.6970   time: 0.13s   best: 90.2904
2023-11-02 14:03:18,017:INFO:  Epoch 408/500:  train Loss: 84.5875   val Loss: 95.5156   time: 0.15s   best: 90.2904
2023-11-02 14:03:18,163:INFO:  Epoch 409/500:  train Loss: 84.4009   val Loss: 94.9720   time: 0.14s   best: 90.2904
2023-11-02 14:03:18,295:INFO:  Epoch 410/500:  train Loss: 84.3603   val Loss: 94.9628   time: 0.13s   best: 90.2904
2023-11-02 14:03:18,434:INFO:  Epoch 411/500:  train Loss: 84.3742   val Loss: 94.6613   time: 0.13s   best: 90.2904
2023-11-02 14:03:18,610:INFO:  Epoch 412/500:  train Loss: 84.0421   val Loss: 94.0785   time: 0.16s   best: 90.2904
2023-11-02 14:03:18,749:INFO:  Epoch 413/500:  train Loss: 83.6263   val Loss: 94.5789   time: 0.13s   best: 90.2904
2023-11-02 14:03:18,890:INFO:  Epoch 414/500:  train Loss: 83.6775   val Loss: 94.8142   time: 0.13s   best: 90.2904
2023-11-02 14:03:19,092:INFO:  Epoch 415/500:  train Loss: 83.1235   val Loss: 91.9090   time: 0.19s   best: 90.2904
2023-11-02 14:03:19,223:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:03:19,291:INFO:  Epoch 416/500:  train Loss: 84.8237   val Loss: 89.9466   time: 0.13s   best: 89.9466
2023-11-02 14:03:19,430:INFO:  Epoch 417/500:  train Loss: 82.1792   val Loss: 95.3298   time: 0.13s   best: 89.9466
2023-11-02 14:03:19,571:INFO:  Epoch 418/500:  train Loss: 89.0302   val Loss: 92.9952   time: 0.13s   best: 89.9466
2023-11-02 14:03:19,727:INFO:  Epoch 419/500:  train Loss: 84.9821   val Loss: 97.5055   time: 0.15s   best: 89.9466
2023-11-02 14:03:19,865:INFO:  Epoch 420/500:  train Loss: 87.0644   val Loss: 100.2426   time: 0.13s   best: 89.9466
2023-11-02 14:03:20,004:INFO:  Epoch 421/500:  train Loss: 85.0700   val Loss: 95.8298   time: 0.13s   best: 89.9466
2023-11-02 14:03:20,166:INFO:  Epoch 422/500:  train Loss: 85.1847   val Loss: 94.6564   time: 0.15s   best: 89.9466
2023-11-02 14:03:20,305:INFO:  Epoch 423/500:  train Loss: 84.4397   val Loss: 96.3665   time: 0.13s   best: 89.9466
2023-11-02 14:03:20,443:INFO:  Epoch 424/500:  train Loss: 84.8172   val Loss: 96.7834   time: 0.13s   best: 89.9466
2023-11-02 14:03:20,584:INFO:  Epoch 425/500:  train Loss: 84.8316   val Loss: 94.4330   time: 0.13s   best: 89.9466
2023-11-02 14:03:20,744:INFO:  Epoch 426/500:  train Loss: 84.5207   val Loss: 93.8550   time: 0.15s   best: 89.9466
2023-11-02 14:03:20,885:INFO:  Epoch 427/500:  train Loss: 84.6285   val Loss: 95.0339   time: 0.13s   best: 89.9466
2023-11-02 14:03:21,045:INFO:  Epoch 428/500:  train Loss: 84.2349   val Loss: 94.9777   time: 0.13s   best: 89.9466
2023-11-02 14:03:21,222:INFO:  Epoch 429/500:  train Loss: 84.3802   val Loss: 93.5978   time: 0.17s   best: 89.9466
2023-11-02 14:03:21,361:INFO:  Epoch 430/500:  train Loss: 84.0426   val Loss: 92.9673   time: 0.13s   best: 89.9466
2023-11-02 14:03:21,504:INFO:  Epoch 431/500:  train Loss: 83.5898   val Loss: 93.0220   time: 0.13s   best: 89.9466
2023-11-02 14:03:21,643:INFO:  Epoch 432/500:  train Loss: 83.1503   val Loss: 92.5656   time: 0.13s   best: 89.9466
2023-11-02 14:03:21,802:INFO:  Epoch 433/500:  train Loss: 82.6985   val Loss: 92.4714   time: 0.15s   best: 89.9466
2023-11-02 14:03:21,941:INFO:  Epoch 434/500:  train Loss: 83.4947   val Loss: 92.5769   time: 0.13s   best: 89.9466
2023-11-02 14:03:22,080:INFO:  Epoch 435/500:  train Loss: 82.5631   val Loss: 91.0263   time: 0.13s   best: 89.9466
2023-11-02 14:03:22,218:INFO:  Epoch 436/500:  train Loss: 82.4763   val Loss: 90.2726   time: 0.13s   best: 89.9466
2023-11-02 14:03:22,379:INFO:  Epoch 437/500:  train Loss: 82.2331   val Loss: 92.2520   time: 0.15s   best: 89.9466
2023-11-02 14:03:22,519:INFO:  Epoch 438/500:  train Loss: 82.3963   val Loss: 91.7016   time: 0.13s   best: 89.9466
2023-11-02 14:03:22,651:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_10ce.pt
2023-11-02 14:03:22,830:INFO:  Epoch 439/500:  train Loss: 81.3419   val Loss: 86.2841   time: 0.13s   best: 86.2841
2023-11-02 14:03:22,983:INFO:  Epoch 440/500:  train Loss: 81.4539   val Loss: 88.0711   time: 0.14s   best: 86.2841
2023-11-02 14:03:23,156:INFO:  Epoch 441/500:  train Loss: 86.5868   val Loss: 112.5989   time: 0.15s   best: 86.2841
2023-11-02 14:03:23,318:INFO:  Epoch 442/500:  train Loss: 98.3729   val Loss: 90.5430   time: 0.15s   best: 86.2841
2023-11-02 14:03:23,457:INFO:  Epoch 443/500:  train Loss: 89.5889   val Loss: 90.9236   time: 0.13s   best: 86.2841
2023-11-02 14:03:23,598:INFO:  Epoch 444/500:  train Loss: 83.9864   val Loss: 97.9742   time: 0.13s   best: 86.2841
2023-11-02 14:03:23,737:INFO:  Epoch 445/500:  train Loss: 86.7704   val Loss: 99.4561   time: 0.13s   best: 86.2841
2023-11-02 14:03:23,897:INFO:  Epoch 446/500:  train Loss: 84.3600   val Loss: 93.5247   time: 0.15s   best: 86.2841
2023-11-02 14:03:24,036:INFO:  Epoch 447/500:  train Loss: 84.5498   val Loss: 92.8980   time: 0.13s   best: 86.2841
2023-11-02 14:03:24,174:INFO:  Epoch 448/500:  train Loss: 84.2254   val Loss: 94.6917   time: 0.13s   best: 86.2841
2023-11-02 14:03:24,312:INFO:  Epoch 449/500:  train Loss: 82.7935   val Loss: 95.5554   time: 0.13s   best: 86.2841
2023-11-02 14:03:24,472:INFO:  Epoch 450/500:  train Loss: 83.1556   val Loss: 93.5019   time: 0.15s   best: 86.2841
2023-11-02 14:03:24,613:INFO:  Epoch 451/500:  train Loss: 83.1191   val Loss: 92.5007   time: 0.13s   best: 86.2841
2023-11-02 14:03:24,751:INFO:  Epoch 452/500:  train Loss: 83.0851   val Loss: 92.2769   time: 0.13s   best: 86.2841
2023-11-02 14:03:24,914:INFO:  Epoch 453/500:  train Loss: 83.4356   val Loss: 92.1925   time: 0.15s   best: 86.2841
2023-11-02 14:03:25,057:INFO:  Epoch 454/500:  train Loss: 82.5707   val Loss: 92.2207   time: 0.13s   best: 86.2841
2023-11-02 14:03:25,213:INFO:  Epoch 455/500:  train Loss: 82.3961   val Loss: 91.9015   time: 0.14s   best: 86.2841
2023-11-02 14:03:25,372:INFO:  Epoch 456/500:  train Loss: 81.9933   val Loss: 91.0824   time: 0.15s   best: 86.2841
2023-11-02 14:03:25,532:INFO:  Epoch 457/500:  train Loss: 81.7831   val Loss: 90.4409   time: 0.15s   best: 86.2841
2023-11-02 14:03:25,670:INFO:  Epoch 458/500:  train Loss: 81.6660   val Loss: 89.4360   time: 0.13s   best: 86.2841
2023-11-02 14:03:25,809:INFO:  Epoch 459/500:  train Loss: 81.0007   val Loss: 89.5715   time: 0.13s   best: 86.2841
2023-11-02 14:03:25,969:INFO:  Epoch 460/500:  train Loss: 81.3593   val Loss: 88.3672   time: 0.15s   best: 86.2841
2023-11-02 14:03:26,113:INFO:  Epoch 461/500:  train Loss: 79.9100   val Loss: 86.3374   time: 0.13s   best: 86.2841
2023-11-02 14:03:26,252:INFO:  Epoch 462/500:  train Loss: 80.7983   val Loss: 97.6599   time: 0.13s   best: 86.2841
2023-11-02 14:03:26,390:INFO:  Epoch 463/500:  train Loss: 93.7004   val Loss: 88.2238   time: 0.13s   best: 86.2841
2023-11-02 14:03:26,551:INFO:  Epoch 464/500:  train Loss: 84.2965   val Loss: 92.5439   time: 0.15s   best: 86.2841
2023-11-02 14:03:26,690:INFO:  Epoch 465/500:  train Loss: 84.6947   val Loss: 95.4922   time: 0.13s   best: 86.2841
2023-11-02 14:03:26,828:INFO:  Epoch 466/500:  train Loss: 82.6697   val Loss: 90.6583   time: 0.13s   best: 86.2841
2023-11-02 14:03:26,969:INFO:  Epoch 467/500:  train Loss: 84.0299   val Loss: 90.1921   time: 0.13s   best: 86.2841
2023-11-02 14:03:27,135:INFO:  Epoch 468/500:  train Loss: 82.7480   val Loss: 92.2977   time: 0.15s   best: 86.2841
2023-11-02 14:03:27,273:INFO:  Epoch 469/500:  train Loss: 83.1366   val Loss: 92.3284   time: 0.13s   best: 86.2841
2023-11-02 14:03:27,448:INFO:  Epoch 470/500:  train Loss: 82.5611   val Loss: 90.0846   time: 0.16s   best: 86.2841
2023-11-02 14:03:27,609:INFO:  Epoch 471/500:  train Loss: 82.5361   val Loss: 89.5472   time: 0.15s   best: 86.2841
2023-11-02 14:03:27,748:INFO:  Epoch 472/500:  train Loss: 82.1213   val Loss: 89.7122   time: 0.13s   best: 86.2841
2023-11-02 14:03:27,886:INFO:  Epoch 473/500:  train Loss: 81.6556   val Loss: 89.4946   time: 0.13s   best: 86.2841
2023-11-02 14:03:28,025:INFO:  Epoch 474/500:  train Loss: 82.0742   val Loss: 88.9828   time: 0.13s   best: 86.2841
2023-11-02 14:03:28,186:INFO:  Epoch 475/500:  train Loss: 81.3220   val Loss: 89.3460   time: 0.15s   best: 86.2841
2023-11-02 14:03:28,325:INFO:  Epoch 476/500:  train Loss: 81.4556   val Loss: 90.0651   time: 0.13s   best: 86.2841
2023-11-02 14:03:28,463:INFO:  Epoch 477/500:  train Loss: 81.3227   val Loss: 88.1348   time: 0.13s   best: 86.2841
2023-11-02 14:03:28,626:INFO:  Epoch 478/500:  train Loss: 81.1928   val Loss: 87.5770   time: 0.15s   best: 86.2841
2023-11-02 14:03:28,765:INFO:  Epoch 479/500:  train Loss: 80.7063   val Loss: 89.4792   time: 0.13s   best: 86.2841
2023-11-02 14:03:28,906:INFO:  Epoch 480/500:  train Loss: 81.2208   val Loss: 89.7623   time: 0.13s   best: 86.2841
2023-11-02 14:03:29,045:INFO:  Epoch 481/500:  train Loss: 80.9345   val Loss: 89.1328   time: 0.13s   best: 86.2841
2023-11-02 14:03:29,207:INFO:  Epoch 482/500:  train Loss: 81.1327   val Loss: 88.9373   time: 0.15s   best: 86.2841
2023-11-02 14:03:29,345:INFO:  Epoch 483/500:  train Loss: 80.9246   val Loss: 88.3008   time: 0.13s   best: 86.2841
2023-11-02 14:03:29,520:INFO:  Epoch 484/500:  train Loss: 81.0674   val Loss: 88.0271   time: 0.16s   best: 86.2841
2023-11-02 14:03:29,680:INFO:  Epoch 485/500:  train Loss: 80.5216   val Loss: 89.2280   time: 0.15s   best: 86.2841
2023-11-02 14:03:29,819:INFO:  Epoch 486/500:  train Loss: 81.1047   val Loss: 89.7117   time: 0.13s   best: 86.2841
2023-11-02 14:03:29,957:INFO:  Epoch 487/500:  train Loss: 80.7700   val Loss: 88.4148   time: 0.13s   best: 86.2841
2023-11-02 14:03:30,097:INFO:  Epoch 488/500:  train Loss: 81.6977   val Loss: 88.5801   time: 0.13s   best: 86.2841
2023-11-02 14:03:30,257:INFO:  Epoch 489/500:  train Loss: 81.1125   val Loss: 90.1141   time: 0.15s   best: 86.2841
2023-11-02 14:03:30,395:INFO:  Epoch 490/500:  train Loss: 80.4896   val Loss: 90.2450   time: 0.13s   best: 86.2841
2023-11-02 14:03:30,534:INFO:  Epoch 491/500:  train Loss: 80.3205   val Loss: 88.5407   time: 0.13s   best: 86.2841
2023-11-02 14:03:30,673:INFO:  Epoch 492/500:  train Loss: 80.7110   val Loss: 88.0861   time: 0.13s   best: 86.2841
2023-11-02 14:03:30,826:INFO:  Epoch 493/500:  train Loss: 80.1851   val Loss: 88.3537   time: 0.15s   best: 86.2841
2023-11-02 14:03:30,960:INFO:  Epoch 494/500:  train Loss: 80.5102   val Loss: 87.8321   time: 0.13s   best: 86.2841
2023-11-02 14:03:31,096:INFO:  Epoch 495/500:  train Loss: 81.0124   val Loss: 87.7607   time: 0.13s   best: 86.2841
2023-11-02 14:03:31,228:INFO:  Epoch 496/500:  train Loss: 82.4355   val Loss: 92.0049   time: 0.13s   best: 86.2841
2023-11-02 14:03:31,415:INFO:  Epoch 497/500:  train Loss: 83.5609   val Loss: 91.1686   time: 0.18s   best: 86.2841
2023-11-02 14:03:31,594:INFO:  Epoch 498/500:  train Loss: 81.4173   val Loss: 89.4296   time: 0.17s   best: 86.2841
2023-11-02 14:03:31,733:INFO:  Epoch 499/500:  train Loss: 82.2647   val Loss: 90.3222   time: 0.13s   best: 86.2841
2023-11-02 14:03:31,892:INFO:  Epoch 500/500:  train Loss: 81.7557   val Loss: 91.1858   time: 0.15s   best: 86.2841
2023-11-02 14:03:31,893:INFO:  -----> Training complete in 1m 15s   best validation loss: 86.2841
 
2023-11-02 14:03:55,469:INFO:  Starting experiment lstm autoencoder debug (2 layer + hidden)
2023-11-02 14:03:55,470:INFO:  Defining the model
2023-11-02 14:03:55,513:INFO:  Reading the dataset
2023-11-02 14:04:01,929:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:01,960:INFO:  Epoch 1/500:  train Loss: 100.2875   val Loss: 100.1569   time: 1.46s   best: 100.1569
2023-11-02 14:04:02,204:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:04,260:INFO:  Epoch 2/500:  train Loss: 99.2221   val Loss: 96.1266   time: 0.24s   best: 96.1266
2023-11-02 14:04:04,579:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:04,598:INFO:  Epoch 3/500:  train Loss: 98.4241   val Loss: 94.8055   time: 0.31s   best: 94.8055
2023-11-02 14:04:04,838:INFO:  Epoch 4/500:  train Loss: 97.7476   val Loss: 95.6939   time: 0.24s   best: 94.8055
2023-11-02 14:04:05,096:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:05,200:INFO:  Epoch 5/500:  train Loss: 98.2495   val Loss: 94.4978   time: 0.25s   best: 94.4978
2023-11-02 14:04:05,439:INFO:  Epoch 6/500:  train Loss: 97.1976   val Loss: 95.8824   time: 0.24s   best: 94.4978
2023-11-02 14:04:05,696:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:05,723:INFO:  Epoch 7/500:  train Loss: 96.7139   val Loss: 94.1414   time: 0.25s   best: 94.1414
2023-11-02 14:04:05,965:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:06,359:INFO:  Epoch 8/500:  train Loss: 96.1447   val Loss: 93.7062   time: 0.24s   best: 93.7062
2023-11-02 14:04:06,646:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:06,915:INFO:  Epoch 9/500:  train Loss: 96.1517   val Loss: 92.3619   time: 0.28s   best: 92.3619
2023-11-02 14:04:07,165:INFO:  Epoch 10/500:  train Loss: 95.1600   val Loss: 94.1111   time: 0.25s   best: 92.3619
2023-11-02 14:04:07,403:INFO:  Epoch 11/500:  train Loss: 95.3328   val Loss: 95.7733   time: 0.24s   best: 92.3619
2023-11-02 14:04:07,661:INFO:  Epoch 12/500:  train Loss: 95.2552   val Loss: 95.5329   time: 0.26s   best: 92.3619
2023-11-02 14:04:07,899:INFO:  Epoch 13/500:  train Loss: 94.0706   val Loss: 94.6213   time: 0.24s   best: 92.3619
2023-11-02 14:04:08,161:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:08,207:INFO:  Epoch 14/500:  train Loss: 94.1830   val Loss: 91.8470   time: 0.26s   best: 91.8470
2023-11-02 14:04:08,446:INFO:  Epoch 15/500:  train Loss: 93.9349   val Loss: 94.4792   time: 0.24s   best: 91.8470
2023-11-02 14:04:08,740:INFO:  Epoch 16/500:  train Loss: 93.8957   val Loss: 94.6263   time: 0.29s   best: 91.8470
2023-11-02 14:04:08,978:INFO:  Epoch 17/500:  train Loss: 94.5780   val Loss: 94.5445   time: 0.24s   best: 91.8470
2023-11-02 14:04:09,235:INFO:  Epoch 18/500:  train Loss: 94.3670   val Loss: 94.4427   time: 0.25s   best: 91.8470
2023-11-02 14:04:09,474:INFO:  Epoch 19/500:  train Loss: 94.0431   val Loss: 94.2599   time: 0.24s   best: 91.8470
2023-11-02 14:04:09,731:INFO:  Epoch 20/500:  train Loss: 94.0292   val Loss: 94.3706   time: 0.25s   best: 91.8470
2023-11-02 14:04:09,989:INFO:  Epoch 21/500:  train Loss: 94.1603   val Loss: 94.3613   time: 0.26s   best: 91.8470
2023-11-02 14:04:10,249:INFO:  Epoch 22/500:  train Loss: 94.3195   val Loss: 94.5210   time: 0.26s   best: 91.8470
2023-11-02 14:04:10,494:INFO:  Epoch 23/500:  train Loss: 94.4983   val Loss: 94.4984   time: 0.24s   best: 91.8470
2023-11-02 14:04:10,785:INFO:  Epoch 24/500:  train Loss: 94.4452   val Loss: 94.2512   time: 0.29s   best: 91.8470
2023-11-02 14:04:11,025:INFO:  Epoch 25/500:  train Loss: 93.7493   val Loss: 94.1414   time: 0.24s   best: 91.8470
2023-11-02 14:04:11,286:INFO:  Epoch 26/500:  train Loss: 94.1619   val Loss: 94.1554   time: 0.26s   best: 91.8470
2023-11-02 14:04:11,526:INFO:  Epoch 27/500:  train Loss: 93.7501   val Loss: 94.1513   time: 0.24s   best: 91.8470
2023-11-02 14:04:11,782:INFO:  Epoch 28/500:  train Loss: 93.8023   val Loss: 94.1953   time: 0.25s   best: 91.8470
2023-11-02 14:04:12,024:INFO:  Epoch 29/500:  train Loss: 93.7292   val Loss: 94.1913   time: 0.24s   best: 91.8470
2023-11-02 14:04:12,284:INFO:  Epoch 30/500:  train Loss: 93.7059   val Loss: 93.8617   time: 0.26s   best: 91.8470
2023-11-02 14:04:12,531:INFO:  Epoch 31/500:  train Loss: 93.4791   val Loss: 92.1311   time: 0.24s   best: 91.8470
2023-11-02 14:04:12,824:INFO:  Epoch 32/500:  train Loss: 93.0628   val Loss: 93.2881   time: 0.29s   best: 91.8470
2023-11-02 14:04:13,063:INFO:  Epoch 33/500:  train Loss: 93.2595   val Loss: 93.9875   time: 0.24s   best: 91.8470
2023-11-02 14:04:13,304:INFO:  Epoch 34/500:  train Loss: 93.3284   val Loss: 93.9900   time: 0.24s   best: 91.8470
2023-11-02 14:04:13,563:INFO:  Epoch 35/500:  train Loss: 93.3135   val Loss: 93.9222   time: 0.26s   best: 91.8470
2023-11-02 14:04:13,802:INFO:  Epoch 36/500:  train Loss: 93.3589   val Loss: 93.8318   time: 0.24s   best: 91.8470
2023-11-02 14:04:14,062:INFO:  Epoch 37/500:  train Loss: 94.0282   val Loss: 94.1405   time: 0.26s   best: 91.8470
2023-11-02 14:04:14,302:INFO:  Epoch 38/500:  train Loss: 93.7600   val Loss: 94.2356   time: 0.24s   best: 91.8470
2023-11-02 14:04:14,565:INFO:  Epoch 39/500:  train Loss: 93.7990   val Loss: 94.0059   time: 0.26s   best: 91.8470
2023-11-02 14:04:14,841:INFO:  Epoch 40/500:  train Loss: 93.1603   val Loss: 93.6547   time: 0.27s   best: 91.8470
2023-11-02 14:04:15,098:INFO:  Epoch 41/500:  train Loss: 93.1491   val Loss: 93.3182   time: 0.26s   best: 91.8470
2023-11-02 14:04:15,338:INFO:  Epoch 42/500:  train Loss: 92.7976   val Loss: 91.9897   time: 0.24s   best: 91.8470
2023-11-02 14:04:15,597:INFO:  Epoch 43/500:  train Loss: 92.8124   val Loss: 93.3337   time: 0.26s   best: 91.8470
2023-11-02 14:04:15,849:INFO:  Epoch 44/500:  train Loss: 93.1286   val Loss: 93.6455   time: 0.25s   best: 91.8470
2023-11-02 14:04:16,105:INFO:  Epoch 45/500:  train Loss: 93.1628   val Loss: 93.6739   time: 0.25s   best: 91.8470
2023-11-02 14:04:16,351:INFO:  Epoch 46/500:  train Loss: 93.4743   val Loss: 93.4617   time: 0.24s   best: 91.8470
2023-11-02 14:04:16,615:INFO:  Epoch 47/500:  train Loss: 92.9272   val Loss: 93.0690   time: 0.26s   best: 91.8470
2023-11-02 14:04:16,890:INFO:  Epoch 48/500:  train Loss: 92.7098   val Loss: 91.8550   time: 0.27s   best: 91.8470
2023-11-02 14:04:17,148:INFO:  Epoch 49/500:  train Loss: 92.0382   val Loss: 91.9246   time: 0.26s   best: 91.8470
2023-11-02 14:04:17,389:INFO:  Epoch 50/500:  train Loss: 91.9009   val Loss: 91.9658   time: 0.24s   best: 91.8470
2023-11-02 14:04:17,647:INFO:  Epoch 51/500:  train Loss: 91.4571   val Loss: 92.5182   time: 0.26s   best: 91.8470
2023-11-02 14:04:17,886:INFO:  Epoch 52/500:  train Loss: 92.6541   val Loss: 92.4719   time: 0.24s   best: 91.8470
2023-11-02 14:04:18,144:INFO:  Epoch 53/500:  train Loss: 92.0819   val Loss: 92.4227   time: 0.26s   best: 91.8470
2023-11-02 14:04:18,386:INFO:  Epoch 54/500:  train Loss: 91.7966   val Loss: 91.8916   time: 0.24s   best: 91.8470
2023-11-02 14:04:18,654:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:20,347:INFO:  Epoch 55/500:  train Loss: 90.9575   val Loss: 90.0186   time: 0.26s   best: 90.0186
2023-11-02 14:04:20,648:INFO:  Epoch 56/500:  train Loss: 90.6373   val Loss: 90.2387   time: 0.30s   best: 90.0186
2023-11-02 14:04:20,888:INFO:  Epoch 57/500:  train Loss: 90.3014   val Loss: 90.2286   time: 0.24s   best: 90.0186
2023-11-02 14:04:21,256:INFO:  Epoch 58/500:  train Loss: 90.2521   val Loss: 90.1717   time: 0.37s   best: 90.0186
2023-11-02 14:04:21,496:INFO:  Epoch 59/500:  train Loss: 90.3085   val Loss: 90.4281   time: 0.24s   best: 90.0186
2023-11-02 14:04:21,738:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:21,911:INFO:  Epoch 60/500:  train Loss: 90.2519   val Loss: 89.7835   time: 0.24s   best: 89.7835
2023-11-02 14:04:22,153:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:22,174:INFO:  Epoch 61/500:  train Loss: 89.8506   val Loss: 88.3188   time: 0.24s   best: 88.3188
2023-11-02 14:04:22,425:INFO:  Epoch 62/500:  train Loss: 89.2780   val Loss: 88.9300   time: 0.25s   best: 88.3188
2023-11-02 14:04:22,672:INFO:  Epoch 63/500:  train Loss: 90.1043   val Loss: 90.0927   time: 0.24s   best: 88.3188
2023-11-02 14:04:22,933:INFO:  Epoch 64/500:  train Loss: 90.9434   val Loss: 90.8256   time: 0.26s   best: 88.3188
2023-11-02 14:04:23,212:INFO:  Epoch 65/500:  train Loss: 90.7246   val Loss: 89.7466   time: 0.28s   best: 88.3188
2023-11-02 14:04:23,467:INFO:  Epoch 66/500:  train Loss: 88.9094   val Loss: 88.4387   time: 0.25s   best: 88.3188
2023-11-02 14:04:23,707:INFO:  Epoch 67/500:  train Loss: 88.6063   val Loss: 88.3365   time: 0.24s   best: 88.3188
2023-11-02 14:04:23,966:INFO:  Epoch 68/500:  train Loss: 89.1635   val Loss: 89.0064   time: 0.26s   best: 88.3188
2023-11-02 14:04:24,207:INFO:  Epoch 69/500:  train Loss: 89.2750   val Loss: 89.3156   time: 0.24s   best: 88.3188
2023-11-02 14:04:24,463:INFO:  Epoch 70/500:  train Loss: 89.1784   val Loss: 88.7278   time: 0.25s   best: 88.3188
2023-11-02 14:04:24,711:INFO:  Epoch 71/500:  train Loss: 89.0937   val Loss: 88.5225   time: 0.25s   best: 88.3188
2023-11-02 14:04:24,975:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:25,095:INFO:  Epoch 72/500:  train Loss: 88.4276   val Loss: 88.1502   time: 0.26s   best: 88.1502
2023-11-02 14:04:25,358:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:25,383:INFO:  Epoch 73/500:  train Loss: 89.1553   val Loss: 87.8661   time: 0.26s   best: 87.8661
2023-11-02 14:04:25,636:INFO:  Epoch 74/500:  train Loss: 88.8050   val Loss: 89.5197   time: 0.25s   best: 87.8661
2023-11-02 14:04:25,875:INFO:  Epoch 75/500:  train Loss: 89.4482   val Loss: 88.7674   time: 0.24s   best: 87.8661
2023-11-02 14:04:26,135:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:26,233:INFO:  Epoch 76/500:  train Loss: 88.4280   val Loss: 87.5363   time: 0.26s   best: 87.5363
2023-11-02 14:04:26,489:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:26,917:INFO:  Epoch 77/500:  train Loss: 87.7554   val Loss: 87.4064   time: 0.25s   best: 87.4064
2023-11-02 14:04:27,185:INFO:  Epoch 78/500:  train Loss: 87.8505   val Loss: 87.9732   time: 0.27s   best: 87.4064
2023-11-02 14:04:27,473:INFO:  Epoch 79/500:  train Loss: 88.6451   val Loss: 88.4867   time: 0.29s   best: 87.4064
2023-11-02 14:04:27,730:INFO:  Epoch 80/500:  train Loss: 87.9794   val Loss: 87.5386   time: 0.25s   best: 87.4064
2023-11-02 14:04:27,971:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:28,040:INFO:  Epoch 81/500:  train Loss: 87.6530   val Loss: 86.8366   time: 0.24s   best: 86.8366
2023-11-02 14:04:28,289:INFO:  Epoch 82/500:  train Loss: 87.6980   val Loss: 87.7171   time: 0.25s   best: 86.8366
2023-11-02 14:04:28,561:INFO:  Epoch 83/500:  train Loss: 87.3495   val Loss: 86.9069   time: 0.27s   best: 86.8366
2023-11-02 14:04:28,802:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:28,858:INFO:  Epoch 84/500:  train Loss: 87.3355   val Loss: 86.3255   time: 0.24s   best: 86.3255
2023-11-02 14:04:29,115:INFO:  Epoch 85/500:  train Loss: 86.8120   val Loss: 86.5923   time: 0.25s   best: 86.3255
2023-11-02 14:04:29,394:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:29,420:INFO:  Epoch 86/500:  train Loss: 86.7824   val Loss: 85.6472   time: 0.27s   best: 85.6472
2023-11-02 14:04:29,674:INFO:  Epoch 87/500:  train Loss: 86.8535   val Loss: 85.8037   time: 0.25s   best: 85.6472
2023-11-02 14:04:29,913:INFO:  Epoch 88/500:  train Loss: 86.1205   val Loss: 85.8618   time: 0.24s   best: 85.6472
2023-11-02 14:04:30,174:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:30,291:INFO:  Epoch 89/500:  train Loss: 86.1801   val Loss: 85.5948   time: 0.26s   best: 85.5948
2023-11-02 14:04:30,536:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:30,968:INFO:  Epoch 90/500:  train Loss: 85.9641   val Loss: 85.2676   time: 0.24s   best: 85.2676
2023-11-02 14:04:31,228:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:31,267:INFO:  Epoch 91/500:  train Loss: 85.9048   val Loss: 84.9007   time: 0.25s   best: 84.9007
2023-11-02 14:04:31,548:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:31,614:INFO:  Epoch 92/500:  train Loss: 84.7509   val Loss: 84.7621   time: 0.28s   best: 84.7621
2023-11-02 14:04:31,871:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:31,969:INFO:  Epoch 93/500:  train Loss: 84.9515   val Loss: 84.4798   time: 0.25s   best: 84.4798
2023-11-02 14:04:32,227:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:32,253:INFO:  Epoch 94/500:  train Loss: 84.4645   val Loss: 83.9036   time: 0.25s   best: 83.9036
2023-11-02 14:04:32,493:INFO:  Epoch 95/500:  train Loss: 84.5795   val Loss: 84.0522   time: 0.24s   best: 83.9036
2023-11-02 14:04:32,758:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:32,816:INFO:  Epoch 96/500:  train Loss: 84.2447   val Loss: 83.4181   time: 0.26s   best: 83.4181
2023-11-02 14:04:33,057:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:33,976:INFO:  Epoch 97/500:  train Loss: 84.6407   val Loss: 83.2289   time: 0.24s   best: 83.2289
2023-11-02 14:04:34,224:INFO:  Epoch 98/500:  train Loss: 83.5343   val Loss: 83.5477   time: 0.24s   best: 83.2289
2023-11-02 14:04:34,463:INFO:  Epoch 99/500:  train Loss: 86.0410   val Loss: 84.1271   time: 0.24s   best: 83.2289
2023-11-02 14:04:34,797:INFO:  Epoch 100/500:  train Loss: 84.8413   val Loss: 84.2762   time: 0.31s   best: 83.2289
2023-11-02 14:04:35,049:INFO:  Epoch 101/500:  train Loss: 85.2791   val Loss: 83.8804   time: 0.24s   best: 83.2289
2023-11-02 14:04:35,316:INFO:  Epoch 102/500:  train Loss: 84.4090   val Loss: 83.6210   time: 0.26s   best: 83.2289
2023-11-02 14:04:35,595:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:35,828:INFO:  Epoch 103/500:  train Loss: 83.8586   val Loss: 83.1858   time: 0.27s   best: 83.1858
2023-11-02 14:04:36,069:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:36,133:INFO:  Epoch 104/500:  train Loss: 84.3959   val Loss: 82.5452   time: 0.24s   best: 82.5452
2023-11-02 14:04:36,388:INFO:  Epoch 105/500:  train Loss: 83.5244   val Loss: 82.9172   time: 0.25s   best: 82.5452
2023-11-02 14:04:36,633:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:38,089:INFO:  Epoch 106/500:  train Loss: 82.9032   val Loss: 82.1368   time: 0.24s   best: 82.1368
2023-11-02 14:04:38,338:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:40,317:INFO:  Epoch 107/500:  train Loss: 82.4773   val Loss: 81.6318   time: 0.24s   best: 81.6318
2023-11-02 14:04:40,594:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:40,627:INFO:  Epoch 108/500:  train Loss: 82.9282   val Loss: 81.4257   time: 0.27s   best: 81.4257
2023-11-02 14:04:40,881:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:41,038:INFO:  Epoch 109/500:  train Loss: 82.0273   val Loss: 80.6964   time: 0.25s   best: 80.6964
2023-11-02 14:04:41,282:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:41,357:INFO:  Epoch 110/500:  train Loss: 82.4375   val Loss: 80.5744   time: 0.24s   best: 80.5744
2023-11-02 14:04:41,605:INFO:  Epoch 111/500:  train Loss: 82.1600   val Loss: 81.3219   time: 0.24s   best: 80.5744
2023-11-02 14:04:41,904:INFO:  Epoch 112/500:  train Loss: 85.0482   val Loss: 82.0160   time: 0.29s   best: 80.5744
2023-11-02 14:04:42,152:INFO:  Epoch 113/500:  train Loss: 93.0833   val Loss: 97.3431   time: 0.24s   best: 80.5744
2023-11-02 14:04:42,411:INFO:  Epoch 114/500:  train Loss: 99.0697   val Loss: 99.4836   time: 0.25s   best: 80.5744
2023-11-02 14:04:42,655:INFO:  Epoch 115/500:  train Loss: 99.1238   val Loss: 98.4961   time: 0.24s   best: 80.5744
2023-11-02 14:04:42,946:INFO:  Epoch 116/500:  train Loss: 97.1690   val Loss: 95.7356   time: 0.29s   best: 80.5744
2023-11-02 14:04:43,188:INFO:  Epoch 117/500:  train Loss: 94.1221   val Loss: 92.7859   time: 0.24s   best: 80.5744
2023-11-02 14:04:43,470:INFO:  Epoch 118/500:  train Loss: 92.1756   val Loss: 91.1195   time: 0.28s   best: 80.5744
2023-11-02 14:04:43,730:INFO:  Epoch 119/500:  train Loss: 91.5556   val Loss: 91.1808   time: 0.24s   best: 80.5744
2023-11-02 14:04:44,029:INFO:  Epoch 120/500:  train Loss: 91.0559   val Loss: 90.5749   time: 0.29s   best: 80.5744
2023-11-02 14:04:44,277:INFO:  Epoch 121/500:  train Loss: 90.2986   val Loss: 89.5108   time: 0.24s   best: 80.5744
2023-11-02 14:04:44,545:INFO:  Epoch 122/500:  train Loss: 89.4351   val Loss: 88.8890   time: 0.26s   best: 80.5744
2023-11-02 14:04:44,796:INFO:  Epoch 123/500:  train Loss: 89.0651   val Loss: 88.1179   time: 0.24s   best: 80.5744
2023-11-02 14:04:45,060:INFO:  Epoch 124/500:  train Loss: 87.9215   val Loss: 87.3453   time: 0.25s   best: 80.5744
2023-11-02 14:04:45,310:INFO:  Epoch 125/500:  train Loss: 86.9162   val Loss: 86.3416   time: 0.24s   best: 80.5744
2023-11-02 14:04:45,579:INFO:  Epoch 126/500:  train Loss: 85.7834   val Loss: 85.0966   time: 0.26s   best: 80.5744
2023-11-02 14:04:45,827:INFO:  Epoch 127/500:  train Loss: 84.8618   val Loss: 83.5575   time: 0.24s   best: 80.5744
2023-11-02 14:04:46,128:INFO:  Epoch 128/500:  train Loss: 83.3572   val Loss: 82.4752   time: 0.29s   best: 80.5744
2023-11-02 14:04:46,377:INFO:  Epoch 129/500:  train Loss: 82.5864   val Loss: 81.2144   time: 0.24s   best: 80.5744
2023-11-02 14:04:46,655:INFO:  Epoch 130/500:  train Loss: 81.9391   val Loss: 81.2155   time: 0.27s   best: 80.5744
2023-11-02 14:04:46,911:INFO:  Epoch 131/500:  train Loss: 81.7252   val Loss: 80.8741   time: 0.25s   best: 80.5744
2023-11-02 14:04:47,170:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:47,234:INFO:  Epoch 132/500:  train Loss: 81.2879   val Loss: 80.3285   time: 0.25s   best: 80.3285
2023-11-02 14:04:47,478:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:47,542:INFO:  Epoch 133/500:  train Loss: 81.0269   val Loss: 79.8885   time: 0.24s   best: 79.8885
2023-11-02 14:04:47,800:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:47,860:INFO:  Epoch 134/500:  train Loss: 80.2375   val Loss: 79.4704   time: 0.25s   best: 79.4704
2023-11-02 14:04:48,157:INFO:  Epoch 135/500:  train Loss: 80.3610   val Loss: 79.5081   time: 0.29s   best: 79.4704
2023-11-02 14:04:48,405:INFO:  Epoch 136/500:  train Loss: 80.5040   val Loss: 79.6248   time: 0.24s   best: 79.4704
2023-11-02 14:04:48,675:INFO:  Epoch 137/500:  train Loss: 80.5437   val Loss: 79.5515   time: 0.26s   best: 79.4704
2023-11-02 14:04:48,915:INFO:  Epoch 138/500:  train Loss: 79.8449   val Loss: 80.1124   time: 0.24s   best: 79.4704
2023-11-02 14:04:49,223:INFO:  Epoch 139/500:  train Loss: 81.8434   val Loss: 80.2102   time: 0.25s   best: 79.4704
2023-11-02 14:04:49,473:INFO:  Epoch 140/500:  train Loss: 81.4204   val Loss: 80.7372   time: 0.24s   best: 79.4704
2023-11-02 14:04:49,734:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:49,762:INFO:  Epoch 141/500:  train Loss: 80.7232   val Loss: 79.4447   time: 0.26s   best: 79.4447
2023-11-02 14:04:50,014:INFO:  Epoch 142/500:  train Loss: 79.9178   val Loss: 79.6349   time: 0.24s   best: 79.4447
2023-11-02 14:04:50,313:INFO:  Epoch 143/500:  train Loss: 80.0098   val Loss: 79.5016   time: 0.29s   best: 79.4447
2023-11-02 14:04:50,558:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:50,953:INFO:  Epoch 144/500:  train Loss: 80.0794   val Loss: 78.7847   time: 0.24s   best: 78.7847
2023-11-02 14:04:51,196:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:51,233:INFO:  Epoch 145/500:  train Loss: 79.1416   val Loss: 78.5731   time: 0.24s   best: 78.5731
2023-11-02 14:04:51,477:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:51,588:INFO:  Epoch 146/500:  train Loss: 81.3592   val Loss: 78.0585   time: 0.24s   best: 78.0585
2023-11-02 14:04:51,850:INFO:  Epoch 147/500:  train Loss: 79.6823   val Loss: 79.0746   time: 0.25s   best: 78.0585
2023-11-02 14:04:52,100:INFO:  Epoch 148/500:  train Loss: 79.6011   val Loss: 79.0428   time: 0.24s   best: 78.0585
2023-11-02 14:04:52,402:INFO:  Epoch 149/500:  train Loss: 79.6441   val Loss: 78.1585   time: 0.29s   best: 78.0585
2023-11-02 14:04:52,654:INFO:  Epoch 150/500:  train Loss: 79.5975   val Loss: 78.8355   time: 0.24s   best: 78.0585
2023-11-02 14:04:52,919:INFO:  Epoch 151/500:  train Loss: 79.3154   val Loss: 78.0622   time: 0.25s   best: 78.0585
2023-11-02 14:04:53,161:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:53,253:INFO:  Epoch 152/500:  train Loss: 78.9103   val Loss: 77.8383   time: 0.24s   best: 77.8383
2023-11-02 14:04:53,513:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:53,623:INFO:  Epoch 153/500:  train Loss: 78.4291   val Loss: 77.2276   time: 0.25s   best: 77.2276
2023-11-02 14:04:53,878:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:53,937:INFO:  Epoch 154/500:  train Loss: 78.6047   val Loss: 76.9871   time: 0.25s   best: 76.9871
2023-11-02 14:04:54,180:INFO:  Epoch 155/500:  train Loss: 78.3593   val Loss: 77.2944   time: 0.24s   best: 76.9871
2023-11-02 14:04:54,494:INFO:  Epoch 156/500:  train Loss: 77.8750   val Loss: 77.2324   time: 0.30s   best: 76.9871
2023-11-02 14:04:54,740:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:54,821:INFO:  Epoch 157/500:  train Loss: 78.4545   val Loss: 76.9046   time: 0.24s   best: 76.9046
2023-11-02 14:04:55,086:INFO:  Epoch 158/500:  train Loss: 77.9075   val Loss: 76.9518   time: 0.25s   best: 76.9046
2023-11-02 14:04:55,329:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:55,391:INFO:  Epoch 159/500:  train Loss: 77.8192   val Loss: 76.5864   time: 0.24s   best: 76.5864
2023-11-02 14:04:55,634:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:55,678:INFO:  Epoch 160/500:  train Loss: 77.6577   val Loss: 76.1659   time: 0.24s   best: 76.1659
2023-11-02 14:04:55,940:INFO:  Epoch 161/500:  train Loss: 77.4161   val Loss: 76.2899   time: 0.25s   best: 76.1659
2023-11-02 14:04:56,189:INFO:  Epoch 162/500:  train Loss: 79.0514   val Loss: 76.5001   time: 0.24s   best: 76.1659
2023-11-02 14:04:56,493:INFO:  Epoch 163/500:  train Loss: 77.4971   val Loss: 76.6567   time: 0.29s   best: 76.1659
2023-11-02 14:04:56,745:INFO:  Epoch 164/500:  train Loss: 77.4627   val Loss: 76.4673   time: 0.24s   best: 76.1659
2023-11-02 14:04:57,008:INFO:  Epoch 165/500:  train Loss: 77.6073   val Loss: 76.4577   time: 0.25s   best: 76.1659
2023-11-02 14:04:57,251:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:57,699:INFO:  Epoch 166/500:  train Loss: 77.0712   val Loss: 75.8943   time: 0.24s   best: 75.8943
2023-11-02 14:04:57,950:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:58,040:INFO:  Epoch 167/500:  train Loss: 77.1668   val Loss: 75.3551   time: 0.24s   best: 75.3551
2023-11-02 14:04:58,289:INFO:  Epoch 168/500:  train Loss: 77.3748   val Loss: 76.7328   time: 0.24s   best: 75.3551
2023-11-02 14:04:58,627:INFO:  Epoch 169/500:  train Loss: 77.4506   val Loss: 76.6311   time: 0.33s   best: 75.3551
2023-11-02 14:04:58,875:INFO:  Epoch 170/500:  train Loss: 77.8958   val Loss: 76.7701   time: 0.24s   best: 75.3551
2023-11-02 14:04:59,141:INFO:  Epoch 171/500:  train Loss: 76.8252   val Loss: 75.8435   time: 0.26s   best: 75.3551
2023-11-02 14:04:59,383:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:59,433:INFO:  Epoch 172/500:  train Loss: 77.2655   val Loss: 75.3376   time: 0.24s   best: 75.3376
2023-11-02 14:04:59,696:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:04:59,766:INFO:  Epoch 173/500:  train Loss: 76.5996   val Loss: 74.8356   time: 0.26s   best: 74.8356
2023-11-02 14:05:00,010:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:00,092:INFO:  Epoch 174/500:  train Loss: 76.2282   val Loss: 74.7170   time: 0.24s   best: 74.7170
2023-11-02 14:05:00,334:INFO:  Epoch 175/500:  train Loss: 76.7122   val Loss: 75.9149   time: 0.24s   best: 74.7170
2023-11-02 14:05:00,633:INFO:  Epoch 176/500:  train Loss: 76.7733   val Loss: 75.8558   time: 0.29s   best: 74.7170
2023-11-02 14:05:00,891:INFO:  Epoch 177/500:  train Loss: 76.3721   val Loss: 75.4010   time: 0.25s   best: 74.7170
2023-11-02 14:05:01,154:INFO:  Epoch 178/500:  train Loss: 76.4868   val Loss: 74.7706   time: 0.25s   best: 74.7170
2023-11-02 14:05:01,396:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:03,844:INFO:  Epoch 179/500:  train Loss: 74.8431   val Loss: 74.2234   time: 0.24s   best: 74.2234
2023-11-02 14:05:04,136:INFO:  Epoch 180/500:  train Loss: 76.8400   val Loss: 74.6292   time: 0.28s   best: 74.2234
2023-11-02 14:05:04,385:INFO:  Epoch 181/500:  train Loss: 77.8614   val Loss: 75.8973   time: 0.24s   best: 74.2234
2023-11-02 14:05:04,655:INFO:  Epoch 182/500:  train Loss: 78.6009   val Loss: 77.0744   time: 0.26s   best: 74.2234
2023-11-02 14:05:04,940:INFO:  Epoch 183/500:  train Loss: 76.7282   val Loss: 76.4577   time: 0.27s   best: 74.2234
2023-11-02 14:05:05,205:INFO:  Epoch 184/500:  train Loss: 75.7173   val Loss: 74.9354   time: 0.25s   best: 74.2234
2023-11-02 14:05:05,453:INFO:  Epoch 185/500:  train Loss: 75.7979   val Loss: 74.5789   time: 0.24s   best: 74.2234
2023-11-02 14:05:05,714:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:05,878:INFO:  Epoch 186/500:  train Loss: 75.1563   val Loss: 74.2137   time: 0.26s   best: 74.2137
2023-11-02 14:05:06,121:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:06,189:INFO:  Epoch 187/500:  train Loss: 79.7387   val Loss: 74.1541   time: 0.24s   best: 74.1541
2023-11-02 14:05:06,438:INFO:  Epoch 188/500:  train Loss: 76.4381   val Loss: 75.5206   time: 0.24s   best: 74.1541
2023-11-02 14:05:06,711:INFO:  Epoch 189/500:  train Loss: 76.8136   val Loss: 75.5902   time: 0.24s   best: 74.1541
2023-11-02 14:05:06,988:INFO:  Epoch 190/500:  train Loss: 76.9365   val Loss: 76.4979   time: 0.27s   best: 74.1541
2023-11-02 14:05:07,258:INFO:  Epoch 191/500:  train Loss: 76.9228   val Loss: 76.3071   time: 0.25s   best: 74.1541
2023-11-02 14:05:07,508:INFO:  Epoch 192/500:  train Loss: 77.5092   val Loss: 74.6279   time: 0.24s   best: 74.1541
2023-11-02 14:05:07,775:INFO:  Epoch 193/500:  train Loss: 75.4487   val Loss: 74.1984   time: 0.26s   best: 74.1541
2023-11-02 14:05:08,016:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:08,091:INFO:  Epoch 194/500:  train Loss: 75.5234   val Loss: 73.6804   time: 0.24s   best: 73.6804
2023-11-02 14:05:08,355:INFO:  Epoch 195/500:  train Loss: 74.8362   val Loss: 73.8690   time: 0.25s   best: 73.6804
2023-11-02 14:05:08,600:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:10,726:INFO:  Epoch 196/500:  train Loss: 74.1852   val Loss: 73.3375   time: 0.24s   best: 73.3375
2023-11-02 14:05:11,052:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:11,114:INFO:  Epoch 197/500:  train Loss: 74.0176   val Loss: 72.9034   time: 0.32s   best: 72.9034
2023-11-02 14:05:11,376:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:11,532:INFO:  Epoch 198/500:  train Loss: 73.8763   val Loss: 72.8079   time: 0.26s   best: 72.8079
2023-11-02 14:05:11,781:INFO:  Epoch 199/500:  train Loss: 73.8786   val Loss: 72.9387   time: 0.24s   best: 72.8079
2023-11-02 14:05:12,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:12,151:INFO:  Epoch 200/500:  train Loss: 73.8380   val Loss: 72.6691   time: 0.30s   best: 72.6691
2023-11-02 14:05:12,408:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:12,530:INFO:  Epoch 201/500:  train Loss: 74.3216   val Loss: 72.5402   time: 0.25s   best: 72.5402
2023-11-02 14:05:12,775:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:13,416:INFO:  Epoch 202/500:  train Loss: 73.4577   val Loss: 72.4846   time: 0.24s   best: 72.4846
2023-11-02 14:05:13,666:INFO:  Epoch 203/500:  train Loss: 73.9484   val Loss: 72.5731   time: 0.24s   best: 72.4846
2023-11-02 14:05:13,924:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:13,987:INFO:  Epoch 204/500:  train Loss: 73.7209   val Loss: 72.4705   time: 0.25s   best: 72.4705
2023-11-02 14:05:14,230:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:14,363:INFO:  Epoch 205/500:  train Loss: 74.7465   val Loss: 72.0764   time: 0.24s   best: 72.0764
2023-11-02 14:05:14,618:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:14,761:INFO:  Epoch 206/500:  train Loss: 73.4677   val Loss: 71.7659   time: 0.25s   best: 71.7659
2023-11-02 14:05:15,040:INFO:  Epoch 207/500:  train Loss: 74.9150   val Loss: 72.7365   time: 0.27s   best: 71.7659
2023-11-02 14:05:15,307:INFO:  Epoch 208/500:  train Loss: 73.0655   val Loss: 72.3773   time: 0.26s   best: 71.7659
2023-11-02 14:05:15,573:INFO:  Epoch 209/500:  train Loss: 74.0173   val Loss: 72.3653   time: 0.26s   best: 71.7659
2023-11-02 14:05:15,816:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:15,961:INFO:  Epoch 210/500:  train Loss: 72.7109   val Loss: 71.0372   time: 0.24s   best: 71.0372
2023-11-02 14:05:16,204:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:16,239:INFO:  Epoch 211/500:  train Loss: 72.8468   val Loss: 70.9903   time: 0.24s   best: 70.9903
2023-11-02 14:05:16,501:INFO:  Epoch 212/500:  train Loss: 72.1658   val Loss: 71.0492   time: 0.25s   best: 70.9903
2023-11-02 14:05:16,757:INFO:  Epoch 213/500:  train Loss: 72.9017   val Loss: 71.0659   time: 0.25s   best: 70.9903
2023-11-02 14:05:17,022:INFO:  Epoch 214/500:  train Loss: 71.8708   val Loss: 71.1446   time: 0.25s   best: 70.9903
2023-11-02 14:05:17,314:INFO:  Epoch 215/500:  train Loss: 73.3863   val Loss: 71.4382   time: 0.29s   best: 70.9903
2023-11-02 14:05:17,587:INFO:  Epoch 216/500:  train Loss: 72.6240   val Loss: 71.1886   time: 0.26s   best: 70.9903
2023-11-02 14:05:17,837:INFO:  Epoch 217/500:  train Loss: 72.6700   val Loss: 71.3181   time: 0.24s   best: 70.9903
2023-11-02 14:05:18,096:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:18,137:INFO:  Epoch 218/500:  train Loss: 71.8339   val Loss: 70.5030   time: 0.25s   best: 70.5030
2023-11-02 14:05:18,386:INFO:  Epoch 219/500:  train Loss: 71.0998   val Loss: 70.7035   time: 0.24s   best: 70.5030
2023-11-02 14:05:18,655:INFO:  Epoch 220/500:  train Loss: 72.6307   val Loss: 71.0096   time: 0.26s   best: 70.5030
2023-11-02 14:05:18,904:INFO:  Epoch 221/500:  train Loss: 71.7030   val Loss: 71.1622   time: 0.24s   best: 70.5030
2023-11-02 14:05:19,169:INFO:  Epoch 222/500:  train Loss: 72.0716   val Loss: 70.5320   time: 0.25s   best: 70.5030
2023-11-02 14:05:19,446:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:19,531:INFO:  Epoch 223/500:  train Loss: 70.9899   val Loss: 69.9474   time: 0.27s   best: 69.9474
2023-11-02 14:05:19,797:INFO:  Epoch 224/500:  train Loss: 72.8297   val Loss: 70.4918   time: 0.25s   best: 69.9474
2023-11-02 14:05:20,045:INFO:  Epoch 225/500:  train Loss: 71.6683   val Loss: 70.7944   time: 0.24s   best: 69.9474
2023-11-02 14:05:20,311:INFO:  Epoch 226/500:  train Loss: 73.7987   val Loss: 70.2907   time: 0.26s   best: 69.9474
2023-11-02 14:05:20,560:INFO:  Epoch 227/500:  train Loss: 73.8863   val Loss: 70.9596   time: 0.24s   best: 69.9474
2023-11-02 14:05:20,831:INFO:  Epoch 228/500:  train Loss: 73.3582   val Loss: 71.3924   time: 0.26s   best: 69.9474
2023-11-02 14:05:21,078:INFO:  Epoch 229/500:  train Loss: 71.1991   val Loss: 70.5986   time: 0.24s   best: 69.9474
2023-11-02 14:05:21,379:INFO:  Epoch 230/500:  train Loss: 72.1389   val Loss: 70.6618   time: 0.29s   best: 69.9474
2023-11-02 14:05:21,629:INFO:  Epoch 231/500:  train Loss: 71.8911   val Loss: 70.7063   time: 0.24s   best: 69.9474
2023-11-02 14:05:21,895:INFO:  Epoch 232/500:  train Loss: 71.6521   val Loss: 70.0197   time: 0.25s   best: 69.9474
2023-11-02 14:05:22,136:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:22,230:INFO:  Epoch 233/500:  train Loss: 71.0020   val Loss: 69.8808   time: 0.24s   best: 69.8808
2023-11-02 14:05:22,471:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:22,563:INFO:  Epoch 234/500:  train Loss: 73.2365   val Loss: 69.7357   time: 0.24s   best: 69.7357
2023-11-02 14:05:22,828:INFO:  Epoch 235/500:  train Loss: 73.6001   val Loss: 71.2742   time: 0.25s   best: 69.7357
2023-11-02 14:05:23,076:INFO:  Epoch 236/500:  train Loss: 74.5941   val Loss: 72.2358   time: 0.24s   best: 69.7357
2023-11-02 14:05:23,355:INFO:  Epoch 237/500:  train Loss: 72.6050   val Loss: 72.5244   time: 0.26s   best: 69.7357
2023-11-02 14:05:23,627:INFO:  Epoch 238/500:  train Loss: 72.7514   val Loss: 71.4169   time: 0.26s   best: 69.7357
2023-11-02 14:05:23,892:INFO:  Epoch 239/500:  train Loss: 74.4301   val Loss: 71.2446   time: 0.25s   best: 69.7357
2023-11-02 14:05:24,140:INFO:  Epoch 240/500:  train Loss: 71.6253   val Loss: 69.8834   time: 0.24s   best: 69.7357
2023-11-02 14:05:24,400:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:24,457:INFO:  Epoch 241/500:  train Loss: 73.1446   val Loss: 69.3482   time: 0.26s   best: 69.3482
2023-11-02 14:05:24,708:INFO:  Epoch 242/500:  train Loss: 71.5351   val Loss: 70.3990   time: 0.24s   best: 69.3482
2023-11-02 14:05:24,973:INFO:  Epoch 243/500:  train Loss: 73.7772   val Loss: 70.3370   time: 0.25s   best: 69.3482
2023-11-02 14:05:25,221:INFO:  Epoch 244/500:  train Loss: 72.0647   val Loss: 70.1512   time: 0.24s   best: 69.3482
2023-11-02 14:05:25,518:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:25,547:INFO:  Epoch 245/500:  train Loss: 71.3489   val Loss: 69.1472   time: 0.29s   best: 69.1472
2023-11-02 14:05:25,790:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:25,925:INFO:  Epoch 246/500:  train Loss: 69.4112   val Loss: 68.2611   time: 0.24s   best: 68.2611
2023-11-02 14:05:26,166:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:26,194:INFO:  Epoch 247/500:  train Loss: 69.6849   val Loss: 68.2287   time: 0.24s   best: 68.2287
2023-11-02 14:05:26,454:INFO:  Epoch 248/500:  train Loss: 70.8113   val Loss: 70.2398   time: 0.25s   best: 68.2287
2023-11-02 14:05:26,709:INFO:  Epoch 249/500:  train Loss: 72.5232   val Loss: 70.1259   time: 0.24s   best: 68.2287
2023-11-02 14:05:26,974:INFO:  Epoch 250/500:  train Loss: 72.1398   val Loss: 70.4252   time: 0.25s   best: 68.2287
2023-11-02 14:05:27,221:INFO:  Epoch 251/500:  train Loss: 70.6063   val Loss: 69.7549   time: 0.24s   best: 68.2287
2023-11-02 14:05:27,490:INFO:  Epoch 252/500:  train Loss: 71.3722   val Loss: 70.4399   time: 0.26s   best: 68.2287
2023-11-02 14:05:27,776:INFO:  Epoch 253/500:  train Loss: 72.5386   val Loss: 70.0303   time: 0.28s   best: 68.2287
2023-11-02 14:05:28,040:INFO:  Epoch 254/500:  train Loss: 70.0841   val Loss: 68.9333   time: 0.25s   best: 68.2287
2023-11-02 14:05:28,289:INFO:  Epoch 255/500:  train Loss: 70.0359   val Loss: 68.5170   time: 0.24s   best: 68.2287
2023-11-02 14:05:28,556:INFO:  Epoch 256/500:  train Loss: 70.0457   val Loss: 68.4745   time: 0.26s   best: 68.2287
2023-11-02 14:05:28,809:INFO:  Epoch 257/500:  train Loss: 70.2038   val Loss: 68.9242   time: 0.24s   best: 68.2287
2023-11-02 14:05:29,074:INFO:  Epoch 258/500:  train Loss: 70.0553   val Loss: 68.3614   time: 0.25s   best: 68.2287
2023-11-02 14:05:29,322:INFO:  Epoch 259/500:  train Loss: 74.0730   val Loss: 71.7302   time: 0.24s   best: 68.2287
2023-11-02 14:05:29,602:INFO:  Epoch 260/500:  train Loss: 75.6110   val Loss: 73.4526   time: 0.26s   best: 68.2287
2023-11-02 14:05:29,910:INFO:  Epoch 261/500:  train Loss: 71.7885   val Loss: 70.7498   time: 0.30s   best: 68.2287
2023-11-02 14:05:30,172:INFO:  Epoch 262/500:  train Loss: 70.4890   val Loss: 69.8383   time: 0.25s   best: 68.2287
2023-11-02 14:05:30,421:INFO:  Epoch 263/500:  train Loss: 70.6374   val Loss: 69.1836   time: 0.24s   best: 68.2287
2023-11-02 14:05:30,692:INFO:  Epoch 264/500:  train Loss: 70.0982   val Loss: 68.6615   time: 0.26s   best: 68.2287
2023-11-02 14:05:30,934:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:31,096:INFO:  Epoch 265/500:  train Loss: 71.4310   val Loss: 68.1860   time: 0.24s   best: 68.1860
2023-11-02 14:05:31,339:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:31,390:INFO:  Epoch 266/500:  train Loss: 70.3590   val Loss: 67.8194   time: 0.24s   best: 67.8194
2023-11-02 14:05:31,654:INFO:  Epoch 267/500:  train Loss: 70.7257   val Loss: 69.2610   time: 0.25s   best: 67.8194
2023-11-02 14:05:31,938:INFO:  Epoch 268/500:  train Loss: 69.8992   val Loss: 68.4132   time: 0.27s   best: 67.8194
2023-11-02 14:05:32,200:INFO:  Epoch 269/500:  train Loss: 69.9082   val Loss: 68.0690   time: 0.25s   best: 67.8194
2023-11-02 14:05:32,454:INFO:  Epoch 270/500:  train Loss: 70.3901   val Loss: 68.9857   time: 0.24s   best: 67.8194
2023-11-02 14:05:32,717:INFO:  Epoch 271/500:  train Loss: 69.0381   val Loss: 68.7627   time: 0.26s   best: 67.8194
2023-11-02 14:05:32,980:INFO:  Epoch 272/500:  train Loss: 70.6326   val Loss: 68.2441   time: 0.25s   best: 67.8194
2023-11-02 14:05:33,237:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:36,327:INFO:  Epoch 273/500:  train Loss: 69.4066   val Loss: 67.0552   time: 0.25s   best: 67.0552
2023-11-02 14:05:36,616:INFO:  Epoch 274/500:  train Loss: 68.6970   val Loss: 67.2706   time: 0.28s   best: 67.0552
2023-11-02 14:05:36,879:INFO:  Epoch 275/500:  train Loss: 68.2589   val Loss: 67.2098   time: 0.25s   best: 67.0552
2023-11-02 14:05:37,127:INFO:  Epoch 276/500:  train Loss: 70.0258   val Loss: 67.3026   time: 0.24s   best: 67.0552
2023-11-02 14:05:37,390:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:37,558:INFO:  Epoch 277/500:  train Loss: 69.0134   val Loss: 65.5526   time: 0.26s   best: 65.5526
2023-11-02 14:05:37,820:INFO:  Epoch 278/500:  train Loss: 69.6585   val Loss: 67.6929   time: 0.25s   best: 65.5526
2023-11-02 14:05:38,104:INFO:  Epoch 279/500:  train Loss: 71.9463   val Loss: 68.5798   time: 0.27s   best: 65.5526
2023-11-02 14:05:38,369:INFO:  Epoch 280/500:  train Loss: 74.1403   val Loss: 67.4472   time: 0.25s   best: 65.5526
2023-11-02 14:05:38,618:INFO:  Epoch 281/500:  train Loss: 76.1313   val Loss: 74.1030   time: 0.24s   best: 65.5526
2023-11-02 14:05:38,888:INFO:  Epoch 282/500:  train Loss: 72.7033   val Loss: 69.8568   time: 0.26s   best: 65.5526
2023-11-02 14:05:39,137:INFO:  Epoch 283/500:  train Loss: 68.0074   val Loss: 67.3859   time: 0.24s   best: 65.5526
2023-11-02 14:05:39,402:INFO:  Epoch 284/500:  train Loss: 67.8648   val Loss: 66.5440   time: 0.25s   best: 65.5526
2023-11-02 14:05:39,652:INFO:  Epoch 285/500:  train Loss: 68.8689   val Loss: 67.8415   time: 0.24s   best: 65.5526
2023-11-02 14:05:39,917:INFO:  Epoch 286/500:  train Loss: 68.3584   val Loss: 66.1177   time: 0.25s   best: 65.5526
2023-11-02 14:05:40,200:INFO:  Epoch 287/500:  train Loss: 68.1788   val Loss: 67.6990   time: 0.27s   best: 65.5526
2023-11-02 14:05:40,467:INFO:  Epoch 288/500:  train Loss: 68.4416   val Loss: 67.5907   time: 0.26s   best: 65.5526
2023-11-02 14:05:40,719:INFO:  Epoch 289/500:  train Loss: 67.6017   val Loss: 65.5991   time: 0.24s   best: 65.5526
2023-11-02 14:05:40,984:INFO:  Epoch 290/500:  train Loss: 67.2149   val Loss: 66.8095   time: 0.25s   best: 65.5526
2023-11-02 14:05:41,226:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:41,456:INFO:  Epoch 291/500:  train Loss: 68.1635   val Loss: 65.2508   time: 0.24s   best: 65.2508
2023-11-02 14:05:41,706:INFO:  Epoch 292/500:  train Loss: 67.6400   val Loss: 67.2967   time: 0.24s   best: 65.2508
2023-11-02 14:05:41,969:INFO:  Epoch 293/500:  train Loss: 68.0603   val Loss: 66.3100   time: 0.25s   best: 65.2508
2023-11-02 14:05:42,254:INFO:  Epoch 294/500:  train Loss: 67.1803   val Loss: 65.6746   time: 0.27s   best: 65.2508
2023-11-02 14:05:42,517:INFO:  Epoch 295/500:  train Loss: 66.7495   val Loss: 65.9547   time: 0.25s   best: 65.2508
2023-11-02 14:05:42,762:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:43,149:INFO:  Epoch 296/500:  train Loss: 68.6036   val Loss: 64.4317   time: 0.24s   best: 64.4317
2023-11-02 14:05:43,397:INFO:  Epoch 297/500:  train Loss: 69.0854   val Loss: 68.0001   time: 0.24s   best: 64.4317
2023-11-02 14:05:43,657:INFO:  Epoch 298/500:  train Loss: 68.9517   val Loss: 67.6508   time: 0.25s   best: 64.4317
2023-11-02 14:05:43,905:INFO:  Epoch 299/500:  train Loss: 68.7023   val Loss: 67.4141   time: 0.24s   best: 64.4317
2023-11-02 14:05:44,271:INFO:  Epoch 300/500:  train Loss: 67.9198   val Loss: 66.7412   time: 0.34s   best: 64.4317
2023-11-02 14:05:44,535:INFO:  Epoch 301/500:  train Loss: 68.8599   val Loss: 64.8579   time: 0.25s   best: 64.4317
2023-11-02 14:05:44,787:INFO:  Epoch 302/500:  train Loss: 69.2795   val Loss: 66.6904   time: 0.24s   best: 64.4317
2023-11-02 14:05:45,054:INFO:  Epoch 303/500:  train Loss: 73.4134   val Loss: 64.8931   time: 0.26s   best: 64.4317
2023-11-02 14:05:45,302:INFO:  Epoch 304/500:  train Loss: 71.1290   val Loss: 68.0407   time: 0.24s   best: 64.4317
2023-11-02 14:05:45,566:INFO:  Epoch 305/500:  train Loss: 69.1360   val Loss: 66.6492   time: 0.25s   best: 64.4317
2023-11-02 14:05:45,816:INFO:  Epoch 306/500:  train Loss: 67.6609   val Loss: 66.9401   time: 0.24s   best: 64.4317
2023-11-02 14:05:46,081:INFO:  Epoch 307/500:  train Loss: 67.1322   val Loss: 65.8736   time: 0.25s   best: 64.4317
2023-11-02 14:05:46,361:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:48,670:INFO:  Epoch 308/500:  train Loss: 67.0470   val Loss: 64.2822   time: 0.27s   best: 64.2822
2023-11-02 14:05:48,978:INFO:  Epoch 309/500:  train Loss: 67.0005   val Loss: 66.9399   time: 0.30s   best: 64.2822
2023-11-02 14:05:49,237:INFO:  Epoch 310/500:  train Loss: 66.8539   val Loss: 65.1600   time: 0.25s   best: 64.2822
2023-11-02 14:05:49,486:INFO:  Epoch 311/500:  train Loss: 67.4696   val Loss: 65.8319   time: 0.24s   best: 64.2822
2023-11-02 14:05:49,754:INFO:  Epoch 312/500:  train Loss: 67.2928   val Loss: 65.2143   time: 0.26s   best: 64.2822
2023-11-02 14:05:49,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:50,149:INFO:  Epoch 313/500:  train Loss: 66.8467   val Loss: 63.8229   time: 0.24s   best: 63.8229
2023-11-02 14:05:50,435:INFO:  Epoch 314/500:  train Loss: 66.7147   val Loss: 66.2719   time: 0.27s   best: 63.8229
2023-11-02 14:05:50,700:INFO:  Epoch 315/500:  train Loss: 67.3286   val Loss: 66.0362   time: 0.25s   best: 63.8229
2023-11-02 14:05:50,950:INFO:  Epoch 316/500:  train Loss: 73.1606   val Loss: 72.3047   time: 0.24s   best: 63.8229
2023-11-02 14:05:51,215:INFO:  Epoch 317/500:  train Loss: 76.9751   val Loss: 74.9485   time: 0.25s   best: 63.8229
2023-11-02 14:05:51,463:INFO:  Epoch 318/500:  train Loss: 73.5905   val Loss: 71.7002   time: 0.24s   best: 63.8229
2023-11-02 14:05:51,729:INFO:  Epoch 319/500:  train Loss: 70.9138   val Loss: 69.5393   time: 0.26s   best: 63.8229
2023-11-02 14:05:51,977:INFO:  Epoch 320/500:  train Loss: 68.4015   val Loss: 67.4556   time: 0.24s   best: 63.8229
2023-11-02 14:05:52,242:INFO:  Epoch 321/500:  train Loss: 68.1645   val Loss: 66.8204   time: 0.25s   best: 63.8229
2023-11-02 14:05:52,529:INFO:  Epoch 322/500:  train Loss: 67.9262   val Loss: 67.2035   time: 0.28s   best: 63.8229
2023-11-02 14:05:52,798:INFO:  Epoch 323/500:  train Loss: 67.7247   val Loss: 66.4195   time: 0.26s   best: 63.8229
2023-11-02 14:05:53,047:INFO:  Epoch 324/500:  train Loss: 67.7286   val Loss: 65.5802   time: 0.24s   best: 63.8229
2023-11-02 14:05:53,312:INFO:  Epoch 325/500:  train Loss: 68.9431   val Loss: 67.9708   time: 0.25s   best: 63.8229
2023-11-02 14:05:53,563:INFO:  Epoch 326/500:  train Loss: 69.5339   val Loss: 65.8177   time: 0.24s   best: 63.8229
2023-11-02 14:05:53,812:INFO:  Epoch 327/500:  train Loss: 68.4679   val Loss: 65.0297   time: 0.24s   best: 63.8229
2023-11-02 14:05:54,078:INFO:  Epoch 328/500:  train Loss: 68.0519   val Loss: 65.9087   time: 0.26s   best: 63.8229
2023-11-02 14:05:54,326:INFO:  Epoch 329/500:  train Loss: 67.9667   val Loss: 65.2382   time: 0.24s   best: 63.8229
2023-11-02 14:05:54,629:INFO:  Epoch 330/500:  train Loss: 68.2028   val Loss: 65.3958   time: 0.29s   best: 63.8229
2023-11-02 14:05:54,874:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:55,088:INFO:  Epoch 331/500:  train Loss: 67.5803   val Loss: 63.6294   time: 0.24s   best: 63.6294
2023-11-02 14:05:55,337:INFO:  Epoch 332/500:  train Loss: 67.9037   val Loss: 66.5535   time: 0.24s   best: 63.6294
2023-11-02 14:05:55,595:INFO:  Epoch 333/500:  train Loss: 70.5472   val Loss: 64.9983   time: 0.25s   best: 63.6294
2023-11-02 14:05:55,844:INFO:  Epoch 334/500:  train Loss: 68.3661   val Loss: 65.1807   time: 0.24s   best: 63.6294
2023-11-02 14:05:56,108:INFO:  Epoch 335/500:  train Loss: 68.5255   val Loss: 65.3710   time: 0.25s   best: 63.6294
2023-11-02 14:05:56,356:INFO:  Epoch 336/500:  train Loss: 67.3401   val Loss: 65.9744   time: 0.24s   best: 63.6294
2023-11-02 14:05:56,646:INFO:  Epoch 337/500:  train Loss: 66.4986   val Loss: 64.8331   time: 0.28s   best: 63.6294
2023-11-02 14:05:56,913:INFO:  Epoch 338/500:  train Loss: 66.5715   val Loss: 64.2758   time: 0.26s   best: 63.6294
2023-11-02 14:05:57,177:INFO:  Epoch 339/500:  train Loss: 67.0757   val Loss: 63.7165   time: 0.25s   best: 63.6294
2023-11-02 14:05:57,426:INFO:  Epoch 340/500:  train Loss: 67.6058   val Loss: 65.8542   time: 0.24s   best: 63.6294
2023-11-02 14:05:57,694:INFO:  Epoch 341/500:  train Loss: 71.3146   val Loss: 66.4601   time: 0.26s   best: 63.6294
2023-11-02 14:05:57,942:INFO:  Epoch 342/500:  train Loss: 73.8258   val Loss: 64.7589   time: 0.24s   best: 63.6294
2023-11-02 14:05:58,207:INFO:  Epoch 343/500:  train Loss: 70.7644   val Loss: 68.5331   time: 0.25s   best: 63.6294
2023-11-02 14:05:58,455:INFO:  Epoch 344/500:  train Loss: 67.6600   val Loss: 66.9399   time: 0.24s   best: 63.6294
2023-11-02 14:05:58,739:INFO:  Epoch 345/500:  train Loss: 66.2010   val Loss: 65.0119   time: 0.26s   best: 63.6294
2023-11-02 14:05:59,010:INFO:  Epoch 346/500:  train Loss: 65.3863   val Loss: 64.1048   time: 0.26s   best: 63.6294
2023-11-02 14:05:59,275:INFO:  Epoch 347/500:  train Loss: 65.0904   val Loss: 63.9279   time: 0.25s   best: 63.6294
2023-11-02 14:05:59,524:INFO:  Epoch 348/500:  train Loss: 65.2833   val Loss: 64.3540   time: 0.24s   best: 63.6294
2023-11-02 14:05:59,785:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:05:59,857:INFO:  Epoch 349/500:  train Loss: 64.8412   val Loss: 63.3904   time: 0.26s   best: 63.3904
2023-11-02 14:06:00,099:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:00,208:INFO:  Epoch 350/500:  train Loss: 65.2386   val Loss: 63.1970   time: 0.24s   best: 63.1970
2023-11-02 14:06:00,458:INFO:  Epoch 351/500:  train Loss: 65.1175   val Loss: 63.5649   time: 0.24s   best: 63.1970
2023-11-02 14:06:00,799:INFO:  Epoch 352/500:  train Loss: 65.1057   val Loss: 65.1847   time: 0.33s   best: 63.1970
2023-11-02 14:06:01,121:INFO:  Epoch 353/500:  train Loss: 66.2343   val Loss: 64.6889   time: 0.31s   best: 63.1970
2023-11-02 14:06:01,383:INFO:  Epoch 354/500:  train Loss: 66.8450   val Loss: 64.4649   time: 0.25s   best: 63.1970
2023-11-02 14:06:01,632:INFO:  Epoch 355/500:  train Loss: 67.5338   val Loss: 65.8023   time: 0.24s   best: 63.1970
2023-11-02 14:06:01,898:INFO:  Epoch 356/500:  train Loss: 67.7845   val Loss: 66.3444   time: 0.26s   best: 63.1970
2023-11-02 14:06:02,146:INFO:  Epoch 357/500:  train Loss: 67.4069   val Loss: 64.9731   time: 0.24s   best: 63.1970
2023-11-02 14:06:02,411:INFO:  Epoch 358/500:  train Loss: 66.1488   val Loss: 67.0925   time: 0.25s   best: 63.1970
2023-11-02 14:06:02,660:INFO:  Epoch 359/500:  train Loss: 68.4910   val Loss: 67.1417   time: 0.24s   best: 63.1970
2023-11-02 14:06:02,929:INFO:  Epoch 360/500:  train Loss: 67.1264   val Loss: 65.7925   time: 0.26s   best: 63.1970
2023-11-02 14:06:03,221:INFO:  Epoch 361/500:  train Loss: 67.7415   val Loss: 65.2083   time: 0.29s   best: 63.1970
2023-11-02 14:06:03,493:INFO:  Epoch 362/500:  train Loss: 68.8841   val Loss: 69.4239   time: 0.26s   best: 63.1970
2023-11-02 14:06:03,743:INFO:  Epoch 363/500:  train Loss: 76.4259   val Loss: 69.8969   time: 0.24s   best: 63.1970
2023-11-02 14:06:04,009:INFO:  Epoch 364/500:  train Loss: 74.8510   val Loss: 69.5087   time: 0.25s   best: 63.1970
2023-11-02 14:06:04,257:INFO:  Epoch 365/500:  train Loss: 70.2559   val Loss: 68.5131   time: 0.24s   best: 63.1970
2023-11-02 14:06:04,522:INFO:  Epoch 366/500:  train Loss: 68.4699   val Loss: 68.1017   time: 0.25s   best: 63.1970
2023-11-02 14:06:04,774:INFO:  Epoch 367/500:  train Loss: 71.7800   val Loss: 70.8011   time: 0.24s   best: 63.1970
2023-11-02 14:06:05,041:INFO:  Epoch 368/500:  train Loss: 71.2068   val Loss: 67.9723   time: 0.26s   best: 63.1970
2023-11-02 14:06:05,325:INFO:  Epoch 369/500:  train Loss: 67.0789   val Loss: 65.6024   time: 0.27s   best: 63.1970
2023-11-02 14:06:05,589:INFO:  Epoch 370/500:  train Loss: 67.9979   val Loss: 64.4122   time: 0.25s   best: 63.1970
2023-11-02 14:06:05,838:INFO:  Epoch 371/500:  train Loss: 66.6780   val Loss: 65.7058   time: 0.24s   best: 63.1970
2023-11-02 14:06:06,103:INFO:  Epoch 372/500:  train Loss: 66.1821   val Loss: 64.6048   time: 0.25s   best: 63.1970
2023-11-02 14:06:06,350:INFO:  Epoch 373/500:  train Loss: 67.1137   val Loss: 63.3422   time: 0.24s   best: 63.1970
2023-11-02 14:06:06,617:INFO:  Epoch 374/500:  train Loss: 67.1146   val Loss: 63.2459   time: 0.26s   best: 63.1970
2023-11-02 14:06:06,873:INFO:  Epoch 375/500:  train Loss: 68.5950   val Loss: 63.5096   time: 0.24s   best: 63.1970
2023-11-02 14:06:07,137:INFO:  Epoch 376/500:  train Loss: 66.9380   val Loss: 63.8337   time: 0.25s   best: 63.1970
2023-11-02 14:06:07,422:INFO:  Epoch 377/500:  train Loss: 67.0563   val Loss: 64.0455   time: 0.27s   best: 63.1970
2023-11-02 14:06:07,688:INFO:  Epoch 378/500:  train Loss: 65.8229   val Loss: 65.3048   time: 0.25s   best: 63.1970
2023-11-02 14:06:07,937:INFO:  Epoch 379/500:  train Loss: 65.4539   val Loss: 64.6156   time: 0.24s   best: 63.1970
2023-11-02 14:06:08,200:INFO:  Epoch 380/500:  train Loss: 65.6433   val Loss: 63.7215   time: 0.25s   best: 63.1970
2023-11-02 14:06:08,448:INFO:  Epoch 381/500:  train Loss: 65.8841   val Loss: 63.8235   time: 0.24s   best: 63.1970
2023-11-02 14:06:08,715:INFO:  Epoch 382/500:  train Loss: 65.6861   val Loss: 63.6294   time: 0.25s   best: 63.1970
2023-11-02 14:06:08,967:INFO:  Epoch 383/500:  train Loss: 65.4715   val Loss: 63.9176   time: 0.24s   best: 63.1970
2023-11-02 14:06:09,226:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:09,255:INFO:  Epoch 384/500:  train Loss: 65.3788   val Loss: 62.5108   time: 0.25s   best: 62.5108
2023-11-02 14:06:09,540:INFO:  Epoch 385/500:  train Loss: 65.6152   val Loss: 65.6851   time: 0.27s   best: 62.5108
2023-11-02 14:06:09,805:INFO:  Epoch 386/500:  train Loss: 67.6216   val Loss: 63.9750   time: 0.25s   best: 62.5108
2023-11-02 14:06:10,054:INFO:  Epoch 387/500:  train Loss: 68.0287   val Loss: 62.7008   time: 0.24s   best: 62.5108
2023-11-02 14:06:10,321:INFO:  Epoch 388/500:  train Loss: 66.7490   val Loss: 64.2653   time: 0.26s   best: 62.5108
2023-11-02 14:06:10,569:INFO:  Epoch 389/500:  train Loss: 68.0605   val Loss: 64.6929   time: 0.24s   best: 62.5108
2023-11-02 14:06:10,838:INFO:  Epoch 390/500:  train Loss: 67.8743   val Loss: 65.1467   time: 0.26s   best: 62.5108
2023-11-02 14:06:11,087:INFO:  Epoch 391/500:  train Loss: 69.2910   val Loss: 64.8919   time: 0.24s   best: 62.5108
2023-11-02 14:06:11,372:INFO:  Epoch 392/500:  train Loss: 67.8103   val Loss: 65.2144   time: 0.27s   best: 62.5108
2023-11-02 14:06:11,639:INFO:  Epoch 393/500:  train Loss: 70.5325   val Loss: 63.5431   time: 0.26s   best: 62.5108
2023-11-02 14:06:11,905:INFO:  Epoch 394/500:  train Loss: 72.5355   val Loss: 70.5887   time: 0.26s   best: 62.5108
2023-11-02 14:06:12,152:INFO:  Epoch 395/500:  train Loss: 72.4156   val Loss: 70.8062   time: 0.24s   best: 62.5108
2023-11-02 14:06:12,419:INFO:  Epoch 396/500:  train Loss: 70.5993   val Loss: 68.2138   time: 0.26s   best: 62.5108
2023-11-02 14:06:12,667:INFO:  Epoch 397/500:  train Loss: 69.9158   val Loss: 69.0030   time: 0.24s   best: 62.5108
2023-11-02 14:06:12,937:INFO:  Epoch 398/500:  train Loss: 69.7660   val Loss: 68.6931   time: 0.26s   best: 62.5108
2023-11-02 14:06:13,185:INFO:  Epoch 399/500:  train Loss: 68.4829   val Loss: 66.3566   time: 0.24s   best: 62.5108
2023-11-02 14:06:13,552:INFO:  Epoch 400/500:  train Loss: 66.8027   val Loss: 64.3118   time: 0.34s   best: 62.5108
2023-11-02 14:06:13,802:INFO:  Epoch 401/500:  train Loss: 65.4903   val Loss: 65.1202   time: 0.24s   best: 62.5108
2023-11-02 14:06:14,067:INFO:  Epoch 402/500:  train Loss: 65.4100   val Loss: 63.4361   time: 0.25s   best: 62.5108
2023-11-02 14:06:14,315:INFO:  Epoch 403/500:  train Loss: 64.9042   val Loss: 63.5761   time: 0.24s   best: 62.5108
2023-11-02 14:06:14,581:INFO:  Epoch 404/500:  train Loss: 65.5714   val Loss: 62.7703   time: 0.26s   best: 62.5108
2023-11-02 14:06:14,834:INFO:  Epoch 405/500:  train Loss: 66.9907   val Loss: 68.4671   time: 0.24s   best: 62.5108
2023-11-02 14:06:15,100:INFO:  Epoch 406/500:  train Loss: 67.8638   val Loss: 64.4877   time: 0.26s   best: 62.5108
2023-11-02 14:06:15,348:INFO:  Epoch 407/500:  train Loss: 66.7363   val Loss: 64.8284   time: 0.24s   best: 62.5108
2023-11-02 14:06:15,650:INFO:  Epoch 408/500:  train Loss: 68.4648   val Loss: 63.1716   time: 0.29s   best: 62.5108
2023-11-02 14:06:15,899:INFO:  Epoch 409/500:  train Loss: 66.7557   val Loss: 67.2048   time: 0.24s   best: 62.5108
2023-11-02 14:06:16,163:INFO:  Epoch 410/500:  train Loss: 78.9529   val Loss: 77.6418   time: 0.25s   best: 62.5108
2023-11-02 14:06:16,412:INFO:  Epoch 411/500:  train Loss: 80.3879   val Loss: 78.3748   time: 0.24s   best: 62.5108
2023-11-02 14:06:16,677:INFO:  Epoch 412/500:  train Loss: 79.5788   val Loss: 76.6426   time: 0.25s   best: 62.5108
2023-11-02 14:06:16,922:INFO:  Epoch 413/500:  train Loss: 75.1469   val Loss: 73.2427   time: 0.24s   best: 62.5108
2023-11-02 14:06:17,197:INFO:  Epoch 414/500:  train Loss: 74.5549   val Loss: 71.7265   time: 0.26s   best: 62.5108
2023-11-02 14:06:17,446:INFO:  Epoch 415/500:  train Loss: 76.5558   val Loss: 74.6617   time: 0.24s   best: 62.5108
2023-11-02 14:06:17,751:INFO:  Epoch 416/500:  train Loss: 76.1611   val Loss: 72.8921   time: 0.29s   best: 62.5108
2023-11-02 14:06:18,000:INFO:  Epoch 417/500:  train Loss: 69.5751   val Loss: 67.8433   time: 0.24s   best: 62.5108
2023-11-02 14:06:18,264:INFO:  Epoch 418/500:  train Loss: 70.1101   val Loss: 67.8349   time: 0.25s   best: 62.5108
2023-11-02 14:06:18,520:INFO:  Epoch 419/500:  train Loss: 67.8522   val Loss: 65.7668   time: 0.25s   best: 62.5108
2023-11-02 14:06:18,796:INFO:  Epoch 420/500:  train Loss: 67.2207   val Loss: 65.9048   time: 0.27s   best: 62.5108
2023-11-02 14:06:19,044:INFO:  Epoch 421/500:  train Loss: 66.8134   val Loss: 64.8978   time: 0.24s   best: 62.5108
2023-11-02 14:06:19,309:INFO:  Epoch 422/500:  train Loss: 66.3326   val Loss: 64.2962   time: 0.25s   best: 62.5108
2023-11-02 14:06:19,557:INFO:  Epoch 423/500:  train Loss: 64.9307   val Loss: 64.2656   time: 0.24s   best: 62.5108
2023-11-02 14:06:19,861:INFO:  Epoch 424/500:  train Loss: 66.4824   val Loss: 65.6163   time: 0.29s   best: 62.5108
2023-11-02 14:06:20,102:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:20,131:INFO:  Epoch 425/500:  train Loss: 66.1856   val Loss: 62.5003   time: 0.24s   best: 62.5003
2023-11-02 14:06:20,397:INFO:  Epoch 426/500:  train Loss: 67.1370   val Loss: 64.3669   time: 0.25s   best: 62.5003
2023-11-02 14:06:20,644:INFO:  Epoch 427/500:  train Loss: 70.5447   val Loss: 63.6839   time: 0.24s   best: 62.5003
2023-11-02 14:06:20,915:INFO:  Epoch 428/500:  train Loss: 69.0504   val Loss: 64.5642   time: 0.26s   best: 62.5003
2023-11-02 14:06:21,162:INFO:  Epoch 429/500:  train Loss: 68.0079   val Loss: 64.8225   time: 0.24s   best: 62.5003
2023-11-02 14:06:21,429:INFO:  Epoch 430/500:  train Loss: 66.4802   val Loss: 66.1845   time: 0.26s   best: 62.5003
2023-11-02 14:06:21,677:INFO:  Epoch 431/500:  train Loss: 68.2322   val Loss: 66.1142   time: 0.24s   best: 62.5003
2023-11-02 14:06:21,980:INFO:  Epoch 432/500:  train Loss: 67.7729   val Loss: 64.0167   time: 0.29s   best: 62.5003
2023-11-02 14:06:22,228:INFO:  Epoch 433/500:  train Loss: 66.1800   val Loss: 63.9719   time: 0.24s   best: 62.5003
2023-11-02 14:06:22,492:INFO:  Epoch 434/500:  train Loss: 65.5205   val Loss: 65.5882   time: 0.25s   best: 62.5003
2023-11-02 14:06:22,741:INFO:  Epoch 435/500:  train Loss: 67.2593   val Loss: 65.1644   time: 0.24s   best: 62.5003
2023-11-02 14:06:23,011:INFO:  Epoch 436/500:  train Loss: 68.3733   val Loss: 63.8591   time: 0.26s   best: 62.5003
2023-11-02 14:06:23,259:INFO:  Epoch 437/500:  train Loss: 66.9439   val Loss: 63.8484   time: 0.24s   best: 62.5003
2023-11-02 14:06:23,525:INFO:  Epoch 438/500:  train Loss: 65.7131   val Loss: 64.6163   time: 0.26s   best: 62.5003
2023-11-02 14:06:23,773:INFO:  Epoch 439/500:  train Loss: 65.6802   val Loss: 62.8846   time: 0.24s   best: 62.5003
2023-11-02 14:06:24,074:INFO:  Epoch 440/500:  train Loss: 65.2977   val Loss: 63.0111   time: 0.29s   best: 62.5003
2023-11-02 14:06:24,315:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:24,344:INFO:  Epoch 441/500:  train Loss: 64.3849   val Loss: 61.9833   time: 0.24s   best: 61.9833
2023-11-02 14:06:24,604:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:24,833:INFO:  Epoch 442/500:  train Loss: 63.4743   val Loss: 61.2168   time: 0.25s   best: 61.2168
2023-11-02 14:06:25,092:INFO:  Epoch 443/500:  train Loss: 62.7818   val Loss: 62.4255   time: 0.25s   best: 61.2168
2023-11-02 14:06:25,339:INFO:  Epoch 444/500:  train Loss: 64.2866   val Loss: 61.8047   time: 0.24s   best: 61.2168
2023-11-02 14:06:25,598:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:25,626:INFO:  Epoch 445/500:  train Loss: 63.5008   val Loss: 60.9247   time: 0.25s   best: 60.9247
2023-11-02 14:06:25,876:INFO:  Epoch 446/500:  train Loss: 63.2150   val Loss: 63.1248   time: 0.24s   best: 60.9247
2023-11-02 14:06:26,178:INFO:  Epoch 447/500:  train Loss: 64.4204   val Loss: 64.0376   time: 0.29s   best: 60.9247
2023-11-02 14:06:26,426:INFO:  Epoch 448/500:  train Loss: 64.0966   val Loss: 64.7563   time: 0.24s   best: 60.9247
2023-11-02 14:06:26,691:INFO:  Epoch 449/500:  train Loss: 66.2228   val Loss: 63.6522   time: 0.25s   best: 60.9247
2023-11-02 14:06:26,942:INFO:  Epoch 450/500:  train Loss: 65.8273   val Loss: 63.2507   time: 0.24s   best: 60.9247
2023-11-02 14:06:27,211:INFO:  Epoch 451/500:  train Loss: 65.1101   val Loss: 62.4763   time: 0.26s   best: 60.9247
2023-11-02 14:06:27,459:INFO:  Epoch 452/500:  train Loss: 65.1286   val Loss: 63.6021   time: 0.24s   best: 60.9247
2023-11-02 14:06:27,726:INFO:  Epoch 453/500:  train Loss: 65.3123   val Loss: 63.2525   time: 0.26s   best: 60.9247
2023-11-02 14:06:27,973:INFO:  Epoch 454/500:  train Loss: 70.4484   val Loss: 63.1417   time: 0.24s   best: 60.9247
2023-11-02 14:06:28,275:INFO:  Epoch 455/500:  train Loss: 70.8547   val Loss: 64.7133   time: 0.29s   best: 60.9247
2023-11-02 14:06:28,522:INFO:  Epoch 456/500:  train Loss: 70.6345   val Loss: 63.7963   time: 0.24s   best: 60.9247
2023-11-02 14:06:28,789:INFO:  Epoch 457/500:  train Loss: 68.6522   val Loss: 65.3841   time: 0.25s   best: 60.9247
2023-11-02 14:06:29,038:INFO:  Epoch 458/500:  train Loss: 69.1290   val Loss: 63.0410   time: 0.24s   best: 60.9247
2023-11-02 14:06:29,296:INFO:  Epoch 459/500:  train Loss: 66.2227   val Loss: 65.3562   time: 0.25s   best: 60.9247
2023-11-02 14:06:29,537:INFO:  Epoch 460/500:  train Loss: 66.3049   val Loss: 63.4207   time: 0.24s   best: 60.9247
2023-11-02 14:06:29,798:INFO:  Epoch 461/500:  train Loss: 64.6967   val Loss: 62.2648   time: 0.26s   best: 60.9247
2023-11-02 14:06:30,065:INFO:  Epoch 462/500:  train Loss: 64.9619   val Loss: 62.5534   time: 0.26s   best: 60.9247
2023-11-02 14:06:30,363:INFO:  Epoch 463/500:  train Loss: 67.7577   val Loss: 66.5772   time: 0.29s   best: 60.9247
2023-11-02 14:06:30,611:INFO:  Epoch 464/500:  train Loss: 68.1727   val Loss: 63.3714   time: 0.24s   best: 60.9247
2023-11-02 14:06:30,882:INFO:  Epoch 465/500:  train Loss: 64.6651   val Loss: 61.9453   time: 0.26s   best: 60.9247
2023-11-02 14:06:31,129:INFO:  Epoch 466/500:  train Loss: 64.1950   val Loss: 62.8617   time: 0.24s   best: 60.9247
2023-11-02 14:06:31,396:INFO:  Epoch 467/500:  train Loss: 64.5624   val Loss: 62.0419   time: 0.26s   best: 60.9247
2023-11-02 14:06:31,638:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:31,879:INFO:  Epoch 468/500:  train Loss: 63.6916   val Loss: 60.3659   time: 0.24s   best: 60.3659
2023-11-02 14:06:32,127:INFO:  Epoch 469/500:  train Loss: 62.7642   val Loss: 61.4214   time: 0.24s   best: 60.3659
2023-11-02 14:06:32,464:INFO:  Epoch 470/500:  train Loss: 65.3109   val Loss: 62.0473   time: 0.33s   best: 60.3659
2023-11-02 14:06:32,712:INFO:  Epoch 471/500:  train Loss: 67.8273   val Loss: 61.1446   time: 0.24s   best: 60.3659
2023-11-02 14:06:32,979:INFO:  Epoch 472/500:  train Loss: 67.2471   val Loss: 61.4484   time: 0.26s   best: 60.3659
2023-11-02 14:06:33,220:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:33,254:INFO:  Epoch 473/500:  train Loss: 64.5174   val Loss: 60.3422   time: 0.24s   best: 60.3422
2023-11-02 14:06:33,519:INFO:  Epoch 474/500:  train Loss: 63.9909   val Loss: 61.2185   time: 0.25s   best: 60.3422
2023-11-02 14:06:33,775:INFO:  Epoch 475/500:  train Loss: 63.6225   val Loss: 61.8939   time: 0.25s   best: 60.3422
2023-11-02 14:06:34,035:INFO:  Epoch 476/500:  train Loss: 62.8966   val Loss: 63.3359   time: 0.25s   best: 60.3422
2023-11-02 14:06:34,296:INFO:  Epoch 477/500:  train Loss: 65.4855   val Loss: 62.8562   time: 0.25s   best: 60.3422
2023-11-02 14:06:34,596:INFO:  Epoch 478/500:  train Loss: 66.5949   val Loss: 61.8322   time: 0.29s   best: 60.3422
2023-11-02 14:06:34,848:INFO:  Epoch 479/500:  train Loss: 63.1531   val Loss: 61.1155   time: 0.24s   best: 60.3422
2023-11-02 14:06:35,106:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:35,235:INFO:  Epoch 480/500:  train Loss: 64.7879   val Loss: 59.9799   time: 0.25s   best: 59.9799
2023-11-02 14:06:35,478:INFO:  Epoch 481/500:  train Loss: 69.1463   val Loss: 65.8351   time: 0.24s   best: 59.9799
2023-11-02 14:06:35,750:INFO:  Epoch 482/500:  train Loss: 71.2358   val Loss: 70.6978   time: 0.26s   best: 59.9799
2023-11-02 14:06:35,998:INFO:  Epoch 483/500:  train Loss: 70.5821   val Loss: 66.4646   time: 0.24s   best: 59.9799
2023-11-02 14:06:36,263:INFO:  Epoch 484/500:  train Loss: 65.3677   val Loss: 62.2301   time: 0.25s   best: 59.9799
2023-11-02 14:06:36,511:INFO:  Epoch 485/500:  train Loss: 64.5774   val Loss: 62.3151   time: 0.24s   best: 59.9799
2023-11-02 14:06:36,815:INFO:  Epoch 486/500:  train Loss: 62.6032   val Loss: 60.9729   time: 0.29s   best: 59.9799
2023-11-02 14:06:37,064:INFO:  Epoch 487/500:  train Loss: 61.8638   val Loss: 62.3541   time: 0.24s   best: 59.9799
2023-11-02 14:06:37,331:INFO:  Epoch 488/500:  train Loss: 61.9999   val Loss: 60.5948   time: 0.25s   best: 59.9799
2023-11-02 14:06:37,574:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:37,698:INFO:  Epoch 489/500:  train Loss: 61.3657   val Loss: 59.5815   time: 0.24s   best: 59.5815
2023-11-02 14:06:37,942:INFO:  Epoch 490/500:  train Loss: 64.8182   val Loss: 64.3806   time: 0.24s   best: 59.5815
2023-11-02 14:06:38,209:INFO:  Epoch 491/500:  train Loss: 63.9902   val Loss: 63.3045   time: 0.26s   best: 59.5815
2023-11-02 14:06:38,457:INFO:  Epoch 492/500:  train Loss: 64.1959   val Loss: 63.2456   time: 0.24s   best: 59.5815
2023-11-02 14:06:38,760:INFO:  Epoch 493/500:  train Loss: 63.3555   val Loss: 61.5783   time: 0.29s   best: 59.5815
2023-11-02 14:06:39,012:INFO:  Epoch 494/500:  train Loss: 61.7580   val Loss: 60.0367   time: 0.24s   best: 59.5815
2023-11-02 14:06:39,274:INFO:  Epoch 495/500:  train Loss: 61.4622   val Loss: 60.9301   time: 0.25s   best: 59.5815
2023-11-02 14:06:39,516:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:39,672:INFO:  Epoch 496/500:  train Loss: 61.5449   val Loss: 59.3198   time: 0.24s   best: 59.3198
2023-11-02 14:06:39,929:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:39,988:INFO:  Epoch 497/500:  train Loss: 60.1466   val Loss: 59.0330   time: 0.25s   best: 59.0330
2023-11-02 14:06:40,246:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + hidden)_44f5.pt
2023-11-02 14:06:40,281:INFO:  Epoch 498/500:  train Loss: 59.9630   val Loss: 58.7816   time: 0.25s   best: 58.7816
2023-11-02 14:06:40,530:INFO:  Epoch 499/500:  train Loss: 59.9623   val Loss: 59.9669   time: 0.24s   best: 58.7816
2023-11-02 14:06:40,893:INFO:  Epoch 500/500:  train Loss: 61.6554   val Loss: 60.2086   time: 0.34s   best: 58.7816
2023-11-02 14:06:40,894:INFO:  -----> Training complete in 2m 40s   best validation loss: 58.7816
 
2023-11-02 14:34:48,889:INFO:  Starting experiment lstm autoencoder debug (2 layer + 0.1 dropout)
2023-11-02 14:34:48,899:INFO:  Defining the model
2023-11-02 14:34:48,943:INFO:  Reading the dataset
2023-11-02 14:34:55,629:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:34:55,650:INFO:  Epoch 1/500:  train Loss: 100.0937   val Loss: 100.1778   time: 1.65s   best: 100.1778
2023-11-02 14:34:55,796:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:34:56,453:INFO:  Epoch 2/500:  train Loss: 100.1936   val Loss: 100.1616   time: 0.14s   best: 100.1616
2023-11-02 14:34:56,598:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:34:56,619:INFO:  Epoch 3/500:  train Loss: 99.8761   val Loss: 100.0889   time: 0.14s   best: 100.0889
2023-11-02 14:34:56,757:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:34:56,964:INFO:  Epoch 4/500:  train Loss: 99.0237   val Loss: 99.1397   time: 0.13s   best: 99.1397
2023-11-02 14:34:57,111:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:34:57,132:INFO:  Epoch 5/500:  train Loss: 98.9780   val Loss: 95.8726   time: 0.14s   best: 95.8726
2023-11-02 14:34:57,266:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:34:57,621:INFO:  Epoch 6/500:  train Loss: 99.1886   val Loss: 95.7580   time: 0.13s   best: 95.7580
2023-11-02 14:34:57,758:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:34:57,808:INFO:  Epoch 7/500:  train Loss: 97.7355   val Loss: 95.0336   time: 0.13s   best: 95.0336
2023-11-02 14:34:57,978:INFO:  Epoch 8/500:  train Loss: 98.0336   val Loss: 97.6157   time: 0.16s   best: 95.0336
2023-11-02 14:34:58,130:INFO:  Epoch 9/500:  train Loss: 98.1353   val Loss: 99.5641   time: 0.15s   best: 95.0336
2023-11-02 14:34:58,261:INFO:  Epoch 10/500:  train Loss: 98.3721   val Loss: 99.7123   time: 0.13s   best: 95.0336
2023-11-02 14:34:58,395:INFO:  Epoch 11/500:  train Loss: 98.9421   val Loss: 99.8083   time: 0.13s   best: 95.0336
2023-11-02 14:34:58,527:INFO:  Epoch 12/500:  train Loss: 98.9306   val Loss: 99.8187   time: 0.13s   best: 95.0336
2023-11-02 14:34:58,686:INFO:  Epoch 13/500:  train Loss: 99.0569   val Loss: 99.8254   time: 0.16s   best: 95.0336
2023-11-02 14:34:58,818:INFO:  Epoch 14/500:  train Loss: 99.0954   val Loss: 99.6755   time: 0.13s   best: 95.0336
2023-11-02 14:34:58,949:INFO:  Epoch 15/500:  train Loss: 98.7869   val Loss: 99.3656   time: 0.13s   best: 95.0336
2023-11-02 14:34:59,105:INFO:  Epoch 16/500:  train Loss: 98.3746   val Loss: 98.7547   time: 0.15s   best: 95.0336
2023-11-02 14:34:59,237:INFO:  Epoch 17/500:  train Loss: 97.6658   val Loss: 98.0523   time: 0.13s   best: 95.0336
2023-11-02 14:34:59,369:INFO:  Epoch 18/500:  train Loss: 96.4417   val Loss: 96.8725   time: 0.13s   best: 95.0336
2023-11-02 14:34:59,503:INFO:  Epoch 19/500:  train Loss: 95.9384   val Loss: 96.0531   time: 0.13s   best: 95.0336
2023-11-02 14:34:59,661:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:34:59,767:INFO:  Epoch 20/500:  train Loss: 93.3035   val Loss: 91.9784   time: 0.15s   best: 91.9784
2023-11-02 14:34:59,902:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:00,173:INFO:  Epoch 21/500:  train Loss: 94.1581   val Loss: 91.8980   time: 0.13s   best: 91.8980
2023-11-02 14:35:00,311:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:00,419:INFO:  Epoch 22/500:  train Loss: 91.0778   val Loss: 91.4342   time: 0.13s   best: 91.4342
2023-11-02 14:35:00,555:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:00,964:INFO:  Epoch 23/500:  train Loss: 91.3205   val Loss: 90.7299   time: 0.13s   best: 90.7299
2023-11-02 14:35:01,096:INFO:  Epoch 24/500:  train Loss: 91.3848   val Loss: 93.9055   time: 0.13s   best: 90.7299
2023-11-02 14:35:01,237:INFO:  Epoch 25/500:  train Loss: 90.2173   val Loss: 93.8262   time: 0.14s   best: 90.7299
2023-11-02 14:35:01,369:INFO:  Epoch 26/500:  train Loss: 89.9115   val Loss: 93.6729   time: 0.13s   best: 90.7299
2023-11-02 14:35:01,503:INFO:  Epoch 27/500:  train Loss: 89.3360   val Loss: 93.5338   time: 0.13s   best: 90.7299
2023-11-02 14:35:01,635:INFO:  Epoch 28/500:  train Loss: 89.2470   val Loss: 93.5766   time: 0.13s   best: 90.7299
2023-11-02 14:35:01,790:INFO:  Epoch 29/500:  train Loss: 88.3008   val Loss: 93.7483   time: 0.15s   best: 90.7299
2023-11-02 14:35:01,922:INFO:  Epoch 30/500:  train Loss: 87.7850   val Loss: 93.5670   time: 0.13s   best: 90.7299
2023-11-02 14:35:02,086:INFO:  Epoch 31/500:  train Loss: 89.1434   val Loss: 93.6960   time: 0.16s   best: 90.7299
2023-11-02 14:35:02,254:INFO:  Epoch 32/500:  train Loss: 89.0247   val Loss: 94.9572   time: 0.17s   best: 90.7299
2023-11-02 14:35:02,389:INFO:  Epoch 33/500:  train Loss: 87.8482   val Loss: 95.5731   time: 0.13s   best: 90.7299
2023-11-02 14:35:02,521:INFO:  Epoch 34/500:  train Loss: 88.6673   val Loss: 96.3284   time: 0.13s   best: 90.7299
2023-11-02 14:35:02,653:INFO:  Epoch 35/500:  train Loss: 89.3787   val Loss: 97.0195   time: 0.13s   best: 90.7299
2023-11-02 14:35:02,808:INFO:  Epoch 36/500:  train Loss: 89.3117   val Loss: 97.5041   time: 0.15s   best: 90.7299
2023-11-02 14:35:02,940:INFO:  Epoch 37/500:  train Loss: 89.6046   val Loss: 97.9304   time: 0.13s   best: 90.7299
2023-11-02 14:35:03,073:INFO:  Epoch 38/500:  train Loss: 90.7405   val Loss: 98.1667   time: 0.13s   best: 90.7299
2023-11-02 14:35:03,205:INFO:  Epoch 39/500:  train Loss: 90.5154   val Loss: 98.2449   time: 0.13s   best: 90.7299
2023-11-02 14:35:03,360:INFO:  Epoch 40/500:  train Loss: 90.6858   val Loss: 98.1125   time: 0.15s   best: 90.7299
2023-11-02 14:35:03,493:INFO:  Epoch 41/500:  train Loss: 90.2259   val Loss: 97.9050   time: 0.13s   best: 90.7299
2023-11-02 14:35:03,627:INFO:  Epoch 42/500:  train Loss: 90.8054   val Loss: 97.7112   time: 0.13s   best: 90.7299
2023-11-02 14:35:03,761:INFO:  Epoch 43/500:  train Loss: 90.4996   val Loss: 97.4903   time: 0.13s   best: 90.7299
2023-11-02 14:35:03,916:INFO:  Epoch 44/500:  train Loss: 90.4110   val Loss: 97.2606   time: 0.15s   best: 90.7299
2023-11-02 14:35:04,059:INFO:  Epoch 45/500:  train Loss: 90.3729   val Loss: 97.0671   time: 0.14s   best: 90.7299
2023-11-02 14:35:04,227:INFO:  Epoch 46/500:  train Loss: 90.5113   val Loss: 96.9204   time: 0.17s   best: 90.7299
2023-11-02 14:35:04,385:INFO:  Epoch 47/500:  train Loss: 89.5148   val Loss: 96.8458   time: 0.16s   best: 90.7299
2023-11-02 14:35:04,518:INFO:  Epoch 48/500:  train Loss: 89.8291   val Loss: 96.8457   time: 0.13s   best: 90.7299
2023-11-02 14:35:04,651:INFO:  Epoch 49/500:  train Loss: 90.0851   val Loss: 96.9037   time: 0.13s   best: 90.7299
2023-11-02 14:35:04,784:INFO:  Epoch 50/500:  train Loss: 89.8538   val Loss: 97.0262   time: 0.13s   best: 90.7299
2023-11-02 14:35:04,939:INFO:  Epoch 51/500:  train Loss: 89.1268   val Loss: 97.2180   time: 0.15s   best: 90.7299
2023-11-02 14:35:05,073:INFO:  Epoch 52/500:  train Loss: 88.5750   val Loss: 97.1997   time: 0.13s   best: 90.7299
2023-11-02 14:35:05,205:INFO:  Epoch 53/500:  train Loss: 88.9326   val Loss: 96.9684   time: 0.13s   best: 90.7299
2023-11-02 14:35:05,337:INFO:  Epoch 54/500:  train Loss: 88.3551   val Loss: 96.7702   time: 0.13s   best: 90.7299
2023-11-02 14:35:05,492:INFO:  Epoch 55/500:  train Loss: 88.7374   val Loss: 95.0776   time: 0.15s   best: 90.7299
2023-11-02 14:35:05,704:INFO:  Epoch 56/500:  train Loss: 88.4174   val Loss: 94.3905   time: 0.21s   best: 90.7299
2023-11-02 14:35:05,836:INFO:  Epoch 57/500:  train Loss: 87.6759   val Loss: 95.7380   time: 0.13s   best: 90.7299
2023-11-02 14:35:05,988:INFO:  Epoch 58/500:  train Loss: 87.6290   val Loss: 95.6271   time: 0.15s   best: 90.7299
2023-11-02 14:35:06,133:INFO:  Epoch 59/500:  train Loss: 88.4762   val Loss: 95.2455   time: 0.14s   best: 90.7299
2023-11-02 14:35:06,300:INFO:  Epoch 60/500:  train Loss: 87.3959   val Loss: 94.9674   time: 0.16s   best: 90.7299
2023-11-02 14:35:06,434:INFO:  Epoch 61/500:  train Loss: 87.9116   val Loss: 94.8404   time: 0.13s   best: 90.7299
2023-11-02 14:35:06,587:INFO:  Epoch 62/500:  train Loss: 87.6037   val Loss: 94.8463   time: 0.15s   best: 90.7299
2023-11-02 14:35:06,721:INFO:  Epoch 63/500:  train Loss: 87.6838   val Loss: 94.9924   time: 0.13s   best: 90.7299
2023-11-02 14:35:06,852:INFO:  Epoch 64/500:  train Loss: 87.6897   val Loss: 95.1875   time: 0.13s   best: 90.7299
2023-11-02 14:35:07,006:INFO:  Epoch 65/500:  train Loss: 87.8266   val Loss: 95.4343   time: 0.15s   best: 90.7299
2023-11-02 14:35:07,139:INFO:  Epoch 66/500:  train Loss: 87.6485   val Loss: 95.8419   time: 0.13s   best: 90.7299
2023-11-02 14:35:07,271:INFO:  Epoch 67/500:  train Loss: 87.2059   val Loss: 96.1114   time: 0.13s   best: 90.7299
2023-11-02 14:35:07,403:INFO:  Epoch 68/500:  train Loss: 87.4704   val Loss: 95.9965   time: 0.13s   best: 90.7299
2023-11-02 14:35:07,558:INFO:  Epoch 69/500:  train Loss: 86.8136   val Loss: 95.5606   time: 0.15s   best: 90.7299
2023-11-02 14:35:07,691:INFO:  Epoch 70/500:  train Loss: 86.0988   val Loss: 94.8215   time: 0.13s   best: 90.7299
2023-11-02 14:35:07,823:INFO:  Epoch 71/500:  train Loss: 85.9510   val Loss: 94.2345   time: 0.13s   best: 90.7299
2023-11-02 14:35:07,955:INFO:  Epoch 72/500:  train Loss: 85.8458   val Loss: 93.8266   time: 0.13s   best: 90.7299
2023-11-02 14:35:08,122:INFO:  Epoch 73/500:  train Loss: 85.5706   val Loss: 93.5382   time: 0.16s   best: 90.7299
2023-11-02 14:35:08,253:INFO:  Epoch 74/500:  train Loss: 85.0138   val Loss: 93.2303   time: 0.13s   best: 90.7299
2023-11-02 14:35:08,424:INFO:  Epoch 75/500:  train Loss: 85.2468   val Loss: 92.5975   time: 0.17s   best: 90.7299
2023-11-02 14:35:08,575:INFO:  Epoch 76/500:  train Loss: 85.0202   val Loss: 92.3281   time: 0.15s   best: 90.7299
2023-11-02 14:35:08,712:INFO:  Epoch 77/500:  train Loss: 83.9887   val Loss: 91.9867   time: 0.14s   best: 90.7299
2023-11-02 14:35:08,844:INFO:  Epoch 78/500:  train Loss: 84.6474   val Loss: 91.8264   time: 0.13s   best: 90.7299
2023-11-02 14:35:08,976:INFO:  Epoch 79/500:  train Loss: 83.9301   val Loss: 91.9440   time: 0.13s   best: 90.7299
2023-11-02 14:35:09,133:INFO:  Epoch 80/500:  train Loss: 83.6388   val Loss: 92.0887   time: 0.15s   best: 90.7299
2023-11-02 14:35:09,265:INFO:  Epoch 81/500:  train Loss: 83.4675   val Loss: 92.2182   time: 0.13s   best: 90.7299
2023-11-02 14:35:09,410:INFO:  Epoch 82/500:  train Loss: 84.1758   val Loss: 92.3504   time: 0.14s   best: 90.7299
2023-11-02 14:35:09,541:INFO:  Epoch 83/500:  train Loss: 84.0967   val Loss: 92.3812   time: 0.13s   best: 90.7299
2023-11-02 14:35:09,692:INFO:  Epoch 84/500:  train Loss: 84.4306   val Loss: 92.3019   time: 0.15s   best: 90.7299
2023-11-02 14:35:09,829:INFO:  Epoch 85/500:  train Loss: 84.0482   val Loss: 92.2232   time: 0.13s   best: 90.7299
2023-11-02 14:35:09,961:INFO:  Epoch 86/500:  train Loss: 84.1363   val Loss: 92.0652   time: 0.13s   best: 90.7299
2023-11-02 14:35:10,105:INFO:  Epoch 87/500:  train Loss: 84.1720   val Loss: 91.8948   time: 0.14s   best: 90.7299
2023-11-02 14:35:10,259:INFO:  Epoch 88/500:  train Loss: 84.2870   val Loss: 92.3834   time: 0.15s   best: 90.7299
2023-11-02 14:35:10,433:INFO:  Epoch 89/500:  train Loss: 83.5818   val Loss: 91.7924   time: 0.17s   best: 90.7299
2023-11-02 14:35:10,565:INFO:  Epoch 90/500:  train Loss: 84.6581   val Loss: 90.9443   time: 0.13s   best: 90.7299
2023-11-02 14:35:10,721:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:11,084:INFO:  Epoch 91/500:  train Loss: 83.1026   val Loss: 90.5451   time: 0.15s   best: 90.5451
2023-11-02 14:35:11,223:INFO:  Epoch 92/500:  train Loss: 84.0417   val Loss: 90.5849   time: 0.14s   best: 90.5451
2023-11-02 14:35:11,356:INFO:  Epoch 93/500:  train Loss: 83.9741   val Loss: 90.7472   time: 0.13s   best: 90.5451
2023-11-02 14:35:11,487:INFO:  Epoch 94/500:  train Loss: 84.8437   val Loss: 91.3339   time: 0.13s   best: 90.5451
2023-11-02 14:35:11,620:INFO:  Epoch 95/500:  train Loss: 84.7855   val Loss: 92.2418   time: 0.13s   best: 90.5451
2023-11-02 14:35:11,777:INFO:  Epoch 96/500:  train Loss: 85.3027   val Loss: 92.9827   time: 0.15s   best: 90.5451
2023-11-02 14:35:11,909:INFO:  Epoch 97/500:  train Loss: 84.9106   val Loss: 93.0770   time: 0.13s   best: 90.5451
2023-11-02 14:35:12,045:INFO:  Epoch 98/500:  train Loss: 85.0866   val Loss: 92.5888   time: 0.13s   best: 90.5451
2023-11-02 14:35:12,184:INFO:  Epoch 99/500:  train Loss: 85.0910   val Loss: 91.9422   time: 0.14s   best: 90.5451
2023-11-02 14:35:12,336:INFO:  Epoch 100/500:  train Loss: 84.7583   val Loss: 91.1636   time: 0.15s   best: 90.5451
2023-11-02 14:35:12,505:INFO:  Epoch 101/500:  train Loss: 83.9994   val Loss: 90.8135   time: 0.17s   best: 90.5451
2023-11-02 14:35:12,638:INFO:  Epoch 102/500:  train Loss: 83.5986   val Loss: 91.2749   time: 0.13s   best: 90.5451
2023-11-02 14:35:12,773:INFO:  Epoch 103/500:  train Loss: 84.2281   val Loss: 91.7652   time: 0.13s   best: 90.5451
2023-11-02 14:35:12,927:INFO:  Epoch 104/500:  train Loss: 85.2762   val Loss: 91.3523   time: 0.15s   best: 90.5451
2023-11-02 14:35:13,061:INFO:  Epoch 105/500:  train Loss: 83.7200   val Loss: 90.6174   time: 0.13s   best: 90.5451
2023-11-02 14:35:13,194:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:13,679:INFO:  Epoch 106/500:  train Loss: 84.0684   val Loss: 90.0133   time: 0.13s   best: 90.0133
2023-11-02 14:35:13,815:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:13,865:INFO:  Epoch 107/500:  train Loss: 83.0443   val Loss: 89.8759   time: 0.13s   best: 89.8759
2023-11-02 14:35:13,999:INFO:  Epoch 108/500:  train Loss: 83.4162   val Loss: 89.9766   time: 0.13s   best: 89.8759
2023-11-02 14:35:14,142:INFO:  Epoch 109/500:  train Loss: 83.7719   val Loss: 90.2232   time: 0.14s   best: 89.8759
2023-11-02 14:35:14,275:INFO:  Epoch 110/500:  train Loss: 83.6570   val Loss: 90.4206   time: 0.13s   best: 89.8759
2023-11-02 14:35:14,428:INFO:  Epoch 111/500:  train Loss: 83.5380   val Loss: 90.9222   time: 0.15s   best: 89.8759
2023-11-02 14:35:14,595:INFO:  Epoch 112/500:  train Loss: 84.5518   val Loss: 91.8409   time: 0.16s   best: 89.8759
2023-11-02 14:35:14,731:INFO:  Epoch 113/500:  train Loss: 84.7769   val Loss: 91.6318   time: 0.13s   best: 89.8759
2023-11-02 14:35:14,881:INFO:  Epoch 114/500:  train Loss: 84.2424   val Loss: 91.2537   time: 0.15s   best: 89.8759
2023-11-02 14:35:15,020:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:15,153:INFO:  Epoch 115/500:  train Loss: 83.8114   val Loss: 89.1898   time: 0.13s   best: 89.1898
2023-11-02 14:35:15,287:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:15,599:INFO:  Epoch 116/500:  train Loss: 81.5119   val Loss: 87.8250   time: 0.13s   best: 87.8250
2023-11-02 14:35:15,736:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:15,830:INFO:  Epoch 117/500:  train Loss: 82.0780   val Loss: 87.5367   time: 0.13s   best: 87.5367
2023-11-02 14:35:15,972:INFO:  Epoch 118/500:  train Loss: 81.7271   val Loss: 88.2845   time: 0.14s   best: 87.5367
2023-11-02 14:35:16,113:INFO:  Epoch 119/500:  train Loss: 81.0153   val Loss: 88.8028   time: 0.14s   best: 87.5367
2023-11-02 14:35:16,245:INFO:  Epoch 120/500:  train Loss: 81.7307   val Loss: 88.0230   time: 0.13s   best: 87.5367
2023-11-02 14:35:16,379:INFO:  Epoch 121/500:  train Loss: 82.4623   val Loss: 88.4631   time: 0.13s   best: 87.5367
2023-11-02 14:35:16,533:INFO:  Epoch 122/500:  train Loss: 82.2982   val Loss: 88.6371   time: 0.15s   best: 87.5367
2023-11-02 14:35:16,705:INFO:  Epoch 123/500:  train Loss: 81.6246   val Loss: 90.3321   time: 0.17s   best: 87.5367
2023-11-02 14:35:16,837:INFO:  Epoch 124/500:  train Loss: 83.6206   val Loss: 90.1198   time: 0.13s   best: 87.5367
2023-11-02 14:35:16,991:INFO:  Epoch 125/500:  train Loss: 81.5203   val Loss: 87.8718   time: 0.15s   best: 87.5367
2023-11-02 14:35:17,127:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:17,435:INFO:  Epoch 126/500:  train Loss: 80.5736   val Loss: 87.1116   time: 0.13s   best: 87.1116
2023-11-02 14:35:17,576:INFO:  Epoch 127/500:  train Loss: 80.5877   val Loss: 88.5221   time: 0.14s   best: 87.1116
2023-11-02 14:35:17,708:INFO:  Epoch 128/500:  train Loss: 81.5294   val Loss: 88.1112   time: 0.13s   best: 87.1116
2023-11-02 14:35:17,839:INFO:  Epoch 129/500:  train Loss: 80.0623   val Loss: 87.5583   time: 0.13s   best: 87.1116
2023-11-02 14:35:17,972:INFO:  Epoch 130/500:  train Loss: 80.4437   val Loss: 88.0860   time: 0.13s   best: 87.1116
2023-11-02 14:35:18,139:INFO:  Epoch 131/500:  train Loss: 80.1366   val Loss: 88.2727   time: 0.16s   best: 87.1116
2023-11-02 14:35:18,270:INFO:  Epoch 132/500:  train Loss: 80.1623   val Loss: 88.7326   time: 0.13s   best: 87.1116
2023-11-02 14:35:18,402:INFO:  Epoch 133/500:  train Loss: 79.8577   val Loss: 96.9265   time: 0.13s   best: 87.1116
2023-11-02 14:35:18,558:INFO:  Epoch 134/500:  train Loss: 80.7191   val Loss: 92.8549   time: 0.15s   best: 87.1116
2023-11-02 14:35:18,728:INFO:  Epoch 135/500:  train Loss: 81.1254   val Loss: 88.4556   time: 0.17s   best: 87.1116
2023-11-02 14:35:18,860:INFO:  Epoch 136/500:  train Loss: 85.3444   val Loss: 87.7594   time: 0.13s   best: 87.1116
2023-11-02 14:35:18,994:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:19,082:INFO:  Epoch 137/500:  train Loss: 84.2150   val Loss: 86.6899   time: 0.13s   best: 86.6899
2023-11-02 14:35:19,215:INFO:  Epoch 138/500:  train Loss: 83.2105   val Loss: 90.7775   time: 0.13s   best: 86.6899
2023-11-02 14:35:19,346:INFO:  Epoch 139/500:  train Loss: 86.6612   val Loss: 91.0259   time: 0.13s   best: 86.6899
2023-11-02 14:35:19,481:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:19,550:INFO:  Epoch 140/500:  train Loss: 84.0412   val Loss: 86.3361   time: 0.13s   best: 86.3361
2023-11-02 14:35:19,705:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_95b3.pt
2023-11-02 14:35:20,002:INFO:  Epoch 141/500:  train Loss: 81.1095   val Loss: 84.3860   time: 0.15s   best: 84.3860
2023-11-02 14:35:20,153:INFO:  Epoch 142/500:  train Loss: 80.7178   val Loss: 84.6519   time: 0.15s   best: 84.3860
2023-11-02 14:35:20,284:INFO:  Epoch 143/500:  train Loss: 79.8795   val Loss: 85.1435   time: 0.13s   best: 84.3860
2023-11-02 14:35:20,419:INFO:  Epoch 144/500:  train Loss: 79.3642   val Loss: 93.0028   time: 0.13s   best: 84.3860
2023-11-02 14:35:20,550:INFO:  Epoch 145/500:  train Loss: 82.2307   val Loss: 84.6483   time: 0.13s   best: 84.3860
2023-11-02 14:35:20,726:INFO:  Epoch 146/500:  train Loss: 78.8085   val Loss: 84.6701   time: 0.17s   best: 84.3860
2023-11-02 14:35:20,912:INFO:  Epoch 147/500:  train Loss: 80.5053   val Loss: 86.7743   time: 0.18s   best: 84.3860
2023-11-02 14:35:21,044:INFO:  Epoch 148/500:  train Loss: 81.7419   val Loss: 103.8421   time: 0.13s   best: 84.3860
2023-11-02 14:35:21,198:INFO:  Epoch 149/500:  train Loss: 221.0106   val Loss: 87.5913   time: 0.15s   best: 84.3860
2023-11-02 14:35:21,336:INFO:  Epoch 150/500:  train Loss: 86.6279   val Loss: 100.2086   time: 0.13s   best: 84.3860
2023-11-02 14:35:21,476:INFO:  Epoch 151/500:  train Loss: 100.4008   val Loss: 100.6963   time: 0.14s   best: 84.3860
2023-11-02 14:35:21,607:INFO:  Epoch 152/500:  train Loss: 100.7010   val Loss: 100.7708   time: 0.13s   best: 84.3860
2023-11-02 14:35:21,774:INFO:  Epoch 153/500:  train Loss: 100.7585   val Loss: 100.8077   time: 0.16s   best: 84.3860
2023-11-02 14:35:21,906:INFO:  Epoch 154/500:  train Loss: 100.7605   val Loss: 100.8198   time: 0.13s   best: 84.3860
2023-11-02 14:35:22,043:INFO:  Epoch 155/500:  train Loss: 100.7633   val Loss: 100.8149   time: 0.13s   best: 84.3860
2023-11-02 14:35:22,178:INFO:  Epoch 156/500:  train Loss: 100.7683   val Loss: 100.7977   time: 0.13s   best: 84.3860
2023-11-02 14:35:22,332:INFO:  Epoch 157/500:  train Loss: 100.7420   val Loss: 100.7714   time: 0.15s   best: 84.3860
2023-11-02 14:35:22,467:INFO:  Epoch 158/500:  train Loss: 100.7394   val Loss: 100.7381   time: 0.13s   best: 84.3860
2023-11-02 14:35:22,600:INFO:  Epoch 159/500:  train Loss: 100.6957   val Loss: 100.6992   time: 0.13s   best: 84.3860
2023-11-02 14:35:22,757:INFO:  Epoch 160/500:  train Loss: 100.6379   val Loss: 100.6559   time: 0.13s   best: 84.3860
2023-11-02 14:35:22,919:INFO:  Epoch 161/500:  train Loss: 100.6070   val Loss: 100.6089   time: 0.16s   best: 84.3860
2023-11-02 14:35:23,056:INFO:  Epoch 162/500:  train Loss: 100.5734   val Loss: 100.5589   time: 0.13s   best: 84.3860
2023-11-02 14:35:23,188:INFO:  Epoch 163/500:  train Loss: 100.5168   val Loss: 100.5062   time: 0.13s   best: 84.3860
2023-11-02 14:35:23,342:INFO:  Epoch 164/500:  train Loss: 100.4561   val Loss: 100.4512   time: 0.15s   best: 84.3860
2023-11-02 14:35:23,474:INFO:  Epoch 165/500:  train Loss: 100.4295   val Loss: 100.3939   time: 0.13s   best: 84.3860
2023-11-02 14:35:23,607:INFO:  Epoch 166/500:  train Loss: 100.3626   val Loss: 100.3348   time: 0.13s   best: 84.3860
2023-11-02 14:35:23,740:INFO:  Epoch 167/500:  train Loss: 100.2207   val Loss: 100.2689   time: 0.13s   best: 84.3860
2023-11-02 14:35:23,895:INFO:  Epoch 168/500:  train Loss: 100.2319   val Loss: 100.1984   time: 0.15s   best: 84.3860
2023-11-02 14:35:24,027:INFO:  Epoch 169/500:  train Loss: 99.9671   val Loss: 100.1074   time: 0.13s   best: 84.3860
2023-11-02 14:35:24,169:INFO:  Epoch 170/500:  train Loss: 99.1197   val Loss: 95.7399   time: 0.14s   best: 84.3860
2023-11-02 14:35:24,301:INFO:  Epoch 171/500:  train Loss: 98.6965   val Loss: 94.6569   time: 0.13s   best: 84.3860
2023-11-02 14:35:24,455:INFO:  Epoch 172/500:  train Loss: 97.7650   val Loss: 98.5706   time: 0.15s   best: 84.3860
2023-11-02 14:35:24,588:INFO:  Epoch 173/500:  train Loss: 97.7568   val Loss: 96.8753   time: 0.13s   best: 84.3860
2023-11-02 14:35:24,721:INFO:  Epoch 174/500:  train Loss: 94.6610   val Loss: 94.3872   time: 0.13s   best: 84.3860
2023-11-02 14:35:24,887:INFO:  Epoch 175/500:  train Loss: 90.8943   val Loss: 94.6935   time: 0.16s   best: 84.3860
2023-11-02 14:35:25,059:INFO:  Epoch 176/500:  train Loss: 89.5344   val Loss: 99.8443   time: 0.17s   best: 84.3860
2023-11-02 14:35:25,192:INFO:  Epoch 177/500:  train Loss: 91.8389   val Loss: 103.6150   time: 0.13s   best: 84.3860
2023-11-02 14:35:25,324:INFO:  Epoch 178/500:  train Loss: 93.8167   val Loss: 103.1406   time: 0.13s   best: 84.3860
2023-11-02 14:35:25,475:INFO:  Epoch 179/500:  train Loss: 92.4333   val Loss: 100.5762   time: 0.15s   best: 84.3860
2023-11-02 14:35:25,606:INFO:  Epoch 180/500:  train Loss: 89.8656   val Loss: 96.4553   time: 0.13s   best: 84.3860
2023-11-02 14:35:25,742:INFO:  Epoch 181/500:  train Loss: 87.7647   val Loss: 95.8788   time: 0.13s   best: 84.3860
2023-11-02 14:35:25,874:INFO:  Epoch 182/500:  train Loss: 87.9937   val Loss: 94.7393   time: 0.13s   best: 84.3860
2023-11-02 14:35:26,030:INFO:  Epoch 183/500:  train Loss: 87.8661   val Loss: 94.5703   time: 0.15s   best: 84.3860
2023-11-02 14:35:26,169:INFO:  Epoch 184/500:  train Loss: 88.2393   val Loss: 95.0364   time: 0.14s   best: 84.3860
2023-11-02 14:35:26,303:INFO:  Epoch 185/500:  train Loss: 87.9471   val Loss: 96.0632   time: 0.13s   best: 84.3860
2023-11-02 14:35:26,433:INFO:  Epoch 186/500:  train Loss: 88.5525   val Loss: 96.8130   time: 0.13s   best: 84.3860
2023-11-02 14:35:26,586:INFO:  Epoch 187/500:  train Loss: 88.9752   val Loss: 98.0207   time: 0.15s   best: 84.3860
2023-11-02 14:35:26,724:INFO:  Epoch 188/500:  train Loss: 89.3495   val Loss: 98.9148   time: 0.14s   best: 84.3860
2023-11-02 14:35:26,856:INFO:  Epoch 189/500:  train Loss: 89.8929   val Loss: 98.8472   time: 0.13s   best: 84.3860
2023-11-02 14:35:27,020:INFO:  Epoch 190/500:  train Loss: 89.2835   val Loss: 98.0493   time: 0.13s   best: 84.3860
2023-11-02 14:35:27,181:INFO:  Epoch 191/500:  train Loss: 88.3781   val Loss: 96.4449   time: 0.16s   best: 84.3860
2023-11-02 14:35:27,313:INFO:  Epoch 192/500:  train Loss: 87.8119   val Loss: 95.9175   time: 0.13s   best: 84.3860
2023-11-02 14:35:27,445:INFO:  Epoch 193/500:  train Loss: 87.3987   val Loss: 95.2485   time: 0.13s   best: 84.3860
2023-11-02 14:35:27,597:INFO:  Epoch 194/500:  train Loss: 88.1462   val Loss: 95.4358   time: 0.15s   best: 84.3860
2023-11-02 14:35:27,731:INFO:  Epoch 195/500:  train Loss: 87.2423   val Loss: 96.3215   time: 0.13s   best: 84.3860
2023-11-02 14:35:27,861:INFO:  Epoch 196/500:  train Loss: 87.1882   val Loss: 97.1066   time: 0.13s   best: 84.3860
2023-11-02 14:35:27,993:INFO:  Epoch 197/500:  train Loss: 87.3501   val Loss: 96.7978   time: 0.13s   best: 84.3860
2023-11-02 14:35:28,159:INFO:  Epoch 198/500:  train Loss: 87.0015   val Loss: 95.8756   time: 0.16s   best: 84.3860
2023-11-02 14:35:28,294:INFO:  Epoch 199/500:  train Loss: 86.2929   val Loss: 95.3307   time: 0.13s   best: 84.3860
2023-11-02 14:35:28,425:INFO:  Epoch 200/500:  train Loss: 86.1150   val Loss: 94.6984   time: 0.13s   best: 84.3860
2023-11-02 14:35:28,557:INFO:  Epoch 201/500:  train Loss: 86.2278   val Loss: 94.0680   time: 0.13s   best: 84.3860
2023-11-02 14:35:28,715:INFO:  Epoch 202/500:  train Loss: 86.2090   val Loss: 94.3903   time: 0.16s   best: 84.3860
2023-11-02 14:35:28,848:INFO:  Epoch 203/500:  train Loss: 86.1881   val Loss: 95.1492   time: 0.13s   best: 84.3860
2023-11-02 14:35:28,980:INFO:  Epoch 204/500:  train Loss: 86.4763   val Loss: 95.7454   time: 0.13s   best: 84.3860
2023-11-02 14:35:29,148:INFO:  Epoch 205/500:  train Loss: 86.1750   val Loss: 95.5098   time: 0.17s   best: 84.3860
2023-11-02 14:35:29,301:INFO:  Epoch 206/500:  train Loss: 85.6906   val Loss: 94.7577   time: 0.15s   best: 84.3860
2023-11-02 14:35:29,433:INFO:  Epoch 207/500:  train Loss: 85.2531   val Loss: 93.9008   time: 0.13s   best: 84.3860
2023-11-02 14:35:29,565:INFO:  Epoch 208/500:  train Loss: 84.1060   val Loss: 92.9391   time: 0.13s   best: 84.3860
2023-11-02 14:35:29,716:INFO:  Epoch 209/500:  train Loss: 83.9556   val Loss: 92.3923   time: 0.15s   best: 84.3860
2023-11-02 14:35:29,853:INFO:  Epoch 210/500:  train Loss: 83.4356   val Loss: 92.9231   time: 0.13s   best: 84.3860
2023-11-02 14:35:29,987:INFO:  Epoch 211/500:  train Loss: 83.0408   val Loss: 93.5770   time: 0.13s   best: 84.3860
2023-11-02 14:35:30,125:INFO:  Epoch 212/500:  train Loss: 83.3934   val Loss: 92.6895   time: 0.14s   best: 84.3860
2023-11-02 14:35:30,279:INFO:  Epoch 213/500:  train Loss: 84.8532   val Loss: 91.2860   time: 0.15s   best: 84.3860
2023-11-02 14:35:30,415:INFO:  Epoch 214/500:  train Loss: 82.5256   val Loss: 89.6403   time: 0.13s   best: 84.3860
2023-11-02 14:35:30,547:INFO:  Epoch 215/500:  train Loss: 83.4388   val Loss: 90.2210   time: 0.13s   best: 84.3860
2023-11-02 14:35:30,681:INFO:  Epoch 216/500:  train Loss: 82.9877   val Loss: 94.5787   time: 0.13s   best: 84.3860
2023-11-02 14:35:30,837:INFO:  Epoch 217/500:  train Loss: 84.6926   val Loss: 91.2465   time: 0.15s   best: 84.3860
2023-11-02 14:35:30,970:INFO:  Epoch 218/500:  train Loss: 83.0085   val Loss: 89.0403   time: 0.13s   best: 84.3860
2023-11-02 14:35:31,102:INFO:  Epoch 219/500:  train Loss: 82.2531   val Loss: 89.9995   time: 0.13s   best: 84.3860
2023-11-02 14:35:31,272:INFO:  Epoch 220/500:  train Loss: 81.6939   val Loss: 89.0412   time: 0.17s   best: 84.3860
2023-11-02 14:35:31,424:INFO:  Epoch 221/500:  train Loss: 81.6147   val Loss: 91.5237   time: 0.15s   best: 84.3860
2023-11-02 14:35:31,556:INFO:  Epoch 222/500:  train Loss: 81.9783   val Loss: 91.8016   time: 0.13s   best: 84.3860
2023-11-02 14:35:31,688:INFO:  Epoch 223/500:  train Loss: 80.2169   val Loss: 90.0986   time: 0.13s   best: 84.3860
2023-11-02 14:35:31,842:INFO:  Epoch 224/500:  train Loss: 79.6486   val Loss: 88.8298   time: 0.15s   best: 84.3860
2023-11-02 14:35:31,974:INFO:  Epoch 225/500:  train Loss: 79.6134   val Loss: 89.1612   time: 0.13s   best: 84.3860
2023-11-02 14:35:32,113:INFO:  Epoch 226/500:  train Loss: 78.4952   val Loss: 89.2164   time: 0.14s   best: 84.3860
2023-11-02 14:35:32,246:INFO:  Epoch 227/500:  train Loss: 77.0915   val Loss: 89.2957   time: 0.13s   best: 84.3860
2023-11-02 14:35:32,424:INFO:  Epoch 228/500:  train Loss: 75.9116   val Loss: 107.2592   time: 0.17s   best: 84.3860
2023-11-02 14:35:32,578:INFO:  Epoch 229/500:  train Loss: 88.8131   val Loss: 99.5585   time: 0.15s   best: 84.3860
2023-11-02 14:35:32,712:INFO:  Epoch 230/500:  train Loss: 97.4722   val Loss: 101.2209   time: 0.13s   best: 84.3860
2023-11-02 14:35:32,846:INFO:  Epoch 231/500:  train Loss: 101.2978   val Loss: 102.0121   time: 0.13s   best: 84.3860
2023-11-02 14:35:32,997:INFO:  Epoch 232/500:  train Loss: 102.0532   val Loss: 102.3529   time: 0.15s   best: 84.3860
2023-11-02 14:35:33,131:INFO:  Epoch 233/500:  train Loss: 102.3136   val Loss: 102.5479   time: 0.13s   best: 84.3860
2023-11-02 14:35:33,297:INFO:  Epoch 234/500:  train Loss: 102.4591   val Loss: 102.6476   time: 0.16s   best: 84.3860
2023-11-02 14:35:33,449:INFO:  Epoch 235/500:  train Loss: 102.5404   val Loss: 102.6829   time: 0.15s   best: 84.3860
2023-11-02 14:35:33,581:INFO:  Epoch 236/500:  train Loss: 102.5582   val Loss: 102.6730   time: 0.13s   best: 84.3860
2023-11-02 14:35:33,713:INFO:  Epoch 237/500:  train Loss: 102.5276   val Loss: 102.6321   time: 0.13s   best: 84.3860
2023-11-02 14:35:33,846:INFO:  Epoch 238/500:  train Loss: 102.4916   val Loss: 102.5695   time: 0.13s   best: 84.3860
2023-11-02 14:35:33,999:INFO:  Epoch 239/500:  train Loss: 102.4303   val Loss: 102.4921   time: 0.15s   best: 84.3860
2023-11-02 14:35:34,140:INFO:  Epoch 240/500:  train Loss: 102.3600   val Loss: 102.4045   time: 0.14s   best: 84.3860
2023-11-02 14:35:34,271:INFO:  Epoch 241/500:  train Loss: 102.2506   val Loss: 102.3105   time: 0.13s   best: 84.3860
2023-11-02 14:35:34,402:INFO:  Epoch 242/500:  train Loss: 102.1645   val Loss: 102.2123   time: 0.13s   best: 84.3860
2023-11-02 14:35:34,557:INFO:  Epoch 243/500:  train Loss: 102.0621   val Loss: 102.1115   time: 0.15s   best: 84.3860
2023-11-02 14:35:34,690:INFO:  Epoch 244/500:  train Loss: 101.9642   val Loss: 102.0088   time: 0.13s   best: 84.3860
2023-11-02 14:35:34,823:INFO:  Epoch 245/500:  train Loss: 101.8653   val Loss: 101.9048   time: 0.13s   best: 84.3860
2023-11-02 14:35:34,957:INFO:  Epoch 246/500:  train Loss: 101.7647   val Loss: 101.7996   time: 0.13s   best: 84.3860
2023-11-02 14:35:35,113:INFO:  Epoch 247/500:  train Loss: 101.6843   val Loss: 101.6933   time: 0.15s   best: 84.3860
2023-11-02 14:35:35,248:INFO:  Epoch 248/500:  train Loss: 101.5687   val Loss: 101.5855   time: 0.13s   best: 84.3860
2023-11-02 14:35:35,415:INFO:  Epoch 249/500:  train Loss: 101.4696   val Loss: 101.4754   time: 0.17s   best: 84.3860
2023-11-02 14:35:35,564:INFO:  Epoch 250/500:  train Loss: 101.3627   val Loss: 101.3627   time: 0.15s   best: 84.3860
2023-11-02 14:35:35,702:INFO:  Epoch 251/500:  train Loss: 101.2542   val Loss: 101.2468   time: 0.14s   best: 84.3860
2023-11-02 14:35:35,835:INFO:  Epoch 252/500:  train Loss: 101.1325   val Loss: 101.1264   time: 0.13s   best: 84.3860
2023-11-02 14:35:35,967:INFO:  Epoch 253/500:  train Loss: 101.0251   val Loss: 101.0005   time: 0.13s   best: 84.3860
2023-11-02 14:35:36,129:INFO:  Epoch 254/500:  train Loss: 100.8917   val Loss: 100.8673   time: 0.16s   best: 84.3860
2023-11-02 14:35:36,262:INFO:  Epoch 255/500:  train Loss: 100.7649   val Loss: 100.7252   time: 0.13s   best: 84.3860
2023-11-02 14:35:36,394:INFO:  Epoch 256/500:  train Loss: 100.6341   val Loss: 100.5717   time: 0.13s   best: 84.3860
2023-11-02 14:35:36,525:INFO:  Epoch 257/500:  train Loss: 100.4609   val Loss: 100.4025   time: 0.13s   best: 84.3860
2023-11-02 14:35:36,683:INFO:  Epoch 258/500:  train Loss: 100.2244   val Loss: 100.4535   time: 0.15s   best: 84.3860
2023-11-02 14:35:36,816:INFO:  Epoch 259/500:  train Loss: 100.6100   val Loss: 100.8645   time: 0.13s   best: 84.3860
2023-11-02 14:35:36,949:INFO:  Epoch 260/500:  train Loss: 100.9157   val Loss: 101.0714   time: 0.13s   best: 84.3860
2023-11-02 14:35:37,081:INFO:  Epoch 261/500:  train Loss: 101.0685   val Loss: 101.1689   time: 0.13s   best: 84.3860
2023-11-02 14:35:37,236:INFO:  Epoch 262/500:  train Loss: 101.1180   val Loss: 101.2021   time: 0.15s   best: 84.3860
2023-11-02 14:35:37,386:INFO:  Epoch 263/500:  train Loss: 101.1256   val Loss: 101.1949   time: 0.15s   best: 84.3860
2023-11-02 14:35:37,537:INFO:  Epoch 264/500:  train Loss: 101.1105   val Loss: 101.1610   time: 0.15s   best: 84.3860
2023-11-02 14:35:37,669:INFO:  Epoch 265/500:  train Loss: 101.0778   val Loss: 101.1087   time: 0.13s   best: 84.3860
2023-11-02 14:35:37,825:INFO:  Epoch 266/500:  train Loss: 101.0248   val Loss: 101.0435   time: 0.15s   best: 84.3860
2023-11-02 14:35:37,957:INFO:  Epoch 267/500:  train Loss: 100.9656   val Loss: 100.9686   time: 0.13s   best: 84.3860
2023-11-02 14:35:38,093:INFO:  Epoch 268/500:  train Loss: 100.8963   val Loss: 100.8865   time: 0.13s   best: 84.3860
2023-11-02 14:35:38,251:INFO:  Epoch 269/500:  train Loss: 100.8160   val Loss: 100.7984   time: 0.16s   best: 84.3860
2023-11-02 14:35:38,382:INFO:  Epoch 270/500:  train Loss: 100.7243   val Loss: 100.7051   time: 0.13s   best: 84.3860
2023-11-02 14:35:38,515:INFO:  Epoch 271/500:  train Loss: 100.6167   val Loss: 100.6071   time: 0.13s   best: 84.3860
2023-11-02 14:35:38,647:INFO:  Epoch 272/500:  train Loss: 100.5367   val Loss: 100.5044   time: 0.13s   best: 84.3860
2023-11-02 14:35:38,805:INFO:  Epoch 273/500:  train Loss: 100.4390   val Loss: 100.3965   time: 0.15s   best: 84.3860
2023-11-02 14:35:38,937:INFO:  Epoch 274/500:  train Loss: 100.3327   val Loss: 100.2823   time: 0.13s   best: 84.3860
2023-11-02 14:35:39,069:INFO:  Epoch 275/500:  train Loss: 100.1524   val Loss: 99.9557   time: 0.13s   best: 84.3860
2023-11-02 14:35:39,203:INFO:  Epoch 276/500:  train Loss: 103.3302   val Loss: 100.3499   time: 0.13s   best: 84.3860
2023-11-02 14:35:39,359:INFO:  Epoch 277/500:  train Loss: 100.7342   val Loss: 101.2682   time: 0.15s   best: 84.3860
2023-11-02 14:35:39,527:INFO:  Epoch 278/500:  train Loss: 101.3772   val Loss: 101.6681   time: 0.16s   best: 84.3860
2023-11-02 14:35:39,660:INFO:  Epoch 279/500:  train Loss: 101.6713   val Loss: 101.8547   time: 0.13s   best: 84.3860
2023-11-02 14:35:39,811:INFO:  Epoch 280/500:  train Loss: 101.8010   val Loss: 101.9339   time: 0.15s   best: 84.3860
2023-11-02 14:35:39,946:INFO:  Epoch 281/500:  train Loss: 101.8477   val Loss: 101.9512   time: 0.13s   best: 84.3860
2023-11-02 14:35:40,094:INFO:  Epoch 282/500:  train Loss: 101.8383   val Loss: 101.9296   time: 0.15s   best: 84.3860
2023-11-02 14:35:40,230:INFO:  Epoch 283/500:  train Loss: 101.8252   val Loss: 101.8825   time: 0.13s   best: 84.3860
2023-11-02 14:35:40,386:INFO:  Epoch 284/500:  train Loss: 101.7741   val Loss: 101.8181   time: 0.15s   best: 84.3860
2023-11-02 14:35:40,517:INFO:  Epoch 285/500:  train Loss: 101.7027   val Loss: 101.7419   time: 0.13s   best: 84.3860
2023-11-02 14:35:40,653:INFO:  Epoch 286/500:  train Loss: 101.6328   val Loss: 101.6572   time: 0.13s   best: 84.3860
2023-11-02 14:35:40,787:INFO:  Epoch 287/500:  train Loss: 101.5414   val Loss: 101.5666   time: 0.13s   best: 84.3860
2023-11-02 14:35:40,942:INFO:  Epoch 288/500:  train Loss: 101.4642   val Loss: 101.4715   time: 0.15s   best: 84.3860
2023-11-02 14:35:41,073:INFO:  Epoch 289/500:  train Loss: 101.3411   val Loss: 101.3727   time: 0.13s   best: 84.3860
2023-11-02 14:35:41,206:INFO:  Epoch 290/500:  train Loss: 101.2645   val Loss: 101.2709   time: 0.13s   best: 84.3860
2023-11-02 14:35:41,338:INFO:  Epoch 291/500:  train Loss: 101.1680   val Loss: 101.1661   time: 0.13s   best: 84.3860
2023-11-02 14:35:41,491:INFO:  Epoch 292/500:  train Loss: 101.0648   val Loss: 101.0580   time: 0.15s   best: 84.3860
2023-11-02 14:35:41,659:INFO:  Epoch 293/500:  train Loss: 100.9671   val Loss: 100.9462   time: 0.17s   best: 84.3860
2023-11-02 14:35:41,793:INFO:  Epoch 294/500:  train Loss: 100.8569   val Loss: 100.8305   time: 0.13s   best: 84.3860
2023-11-02 14:35:41,944:INFO:  Epoch 295/500:  train Loss: 100.7487   val Loss: 100.7107   time: 0.15s   best: 84.3860
2023-11-02 14:35:42,084:INFO:  Epoch 296/500:  train Loss: 100.6265   val Loss: 100.5858   time: 0.14s   best: 84.3860
2023-11-02 14:35:42,221:INFO:  Epoch 297/500:  train Loss: 100.4925   val Loss: 100.4546   time: 0.13s   best: 84.3860
2023-11-02 14:35:42,353:INFO:  Epoch 298/500:  train Loss: 100.3662   val Loss: 100.3162   time: 0.13s   best: 84.3860
2023-11-02 14:35:42,507:INFO:  Epoch 299/500:  train Loss: 100.1752   val Loss: 100.1799   time: 0.15s   best: 84.3860
2023-11-02 14:35:42,639:INFO:  Epoch 300/500:  train Loss: 99.9309   val Loss: 96.9836   time: 0.13s   best: 84.3860
2023-11-02 14:35:42,773:INFO:  Epoch 301/500:  train Loss: 98.6727   val Loss: 98.2112   time: 0.13s   best: 84.3860
2023-11-02 14:35:42,905:INFO:  Epoch 302/500:  train Loss: 96.3244   val Loss: 97.7986   time: 0.13s   best: 84.3860
2023-11-02 14:35:43,062:INFO:  Epoch 303/500:  train Loss: 98.8993   val Loss: 100.4668   time: 0.15s   best: 84.3860
2023-11-02 14:35:43,195:INFO:  Epoch 304/500:  train Loss: 100.1047   val Loss: 98.9085   time: 0.13s   best: 84.3860
2023-11-02 14:35:43,326:INFO:  Epoch 305/500:  train Loss: 97.1067   val Loss: 95.6500   time: 0.13s   best: 84.3860
2023-11-02 14:35:43,459:INFO:  Epoch 306/500:  train Loss: 93.8678   val Loss: 107.5211   time: 0.13s   best: 84.3860
2023-11-02 14:35:43,635:INFO:  Epoch 307/500:  train Loss: 101.6458   val Loss: 103.0939   time: 0.17s   best: 84.3860
2023-11-02 14:35:43,785:INFO:  Epoch 308/500:  train Loss: 92.6970   val Loss: 94.8691   time: 0.15s   best: 84.3860
2023-11-02 14:35:43,917:INFO:  Epoch 309/500:  train Loss: 90.2624   val Loss: 94.5592   time: 0.13s   best: 84.3860
2023-11-02 14:35:44,068:INFO:  Epoch 310/500:  train Loss: 89.3590   val Loss: 97.0549   time: 0.13s   best: 84.3860
2023-11-02 14:35:44,212:INFO:  Epoch 311/500:  train Loss: 90.0355   val Loss: 102.2380   time: 0.14s   best: 84.3860
2023-11-02 14:35:44,344:INFO:  Epoch 312/500:  train Loss: 90.6952   val Loss: 99.6029   time: 0.13s   best: 84.3860
2023-11-02 14:35:44,477:INFO:  Epoch 313/500:  train Loss: 88.5968   val Loss: 95.7418   time: 0.13s   best: 84.3860
2023-11-02 14:35:44,632:INFO:  Epoch 314/500:  train Loss: 87.8445   val Loss: 94.6446   time: 0.15s   best: 84.3860
2023-11-02 14:35:44,766:INFO:  Epoch 315/500:  train Loss: 87.6736   val Loss: 95.7261   time: 0.13s   best: 84.3860
2023-11-02 14:35:44,898:INFO:  Epoch 316/500:  train Loss: 88.3032   val Loss: 98.1622   time: 0.13s   best: 84.3860
2023-11-02 14:35:45,030:INFO:  Epoch 317/500:  train Loss: 89.0922   val Loss: 99.4134   time: 0.13s   best: 84.3860
2023-11-02 14:35:45,187:INFO:  Epoch 318/500:  train Loss: 89.3144   val Loss: 98.6355   time: 0.15s   best: 84.3860
2023-11-02 14:35:45,319:INFO:  Epoch 319/500:  train Loss: 88.6884   val Loss: 97.9500   time: 0.13s   best: 84.3860
2023-11-02 14:35:45,450:INFO:  Epoch 320/500:  train Loss: 88.1761   val Loss: 97.9374   time: 0.13s   best: 84.3860
2023-11-02 14:35:45,584:INFO:  Epoch 321/500:  train Loss: 87.9191   val Loss: 100.2286   time: 0.13s   best: 84.3860
2023-11-02 14:35:45,777:INFO:  Epoch 322/500:  train Loss: 97.0564   val Loss: 94.1364   time: 0.19s   best: 84.3860
2023-11-02 14:35:45,909:INFO:  Epoch 323/500:  train Loss: 91.1506   val Loss: 99.1731   time: 0.13s   best: 84.3860
2023-11-02 14:35:46,043:INFO:  Epoch 324/500:  train Loss: 99.7129   val Loss: 100.2103   time: 0.13s   best: 84.3860
2023-11-02 14:35:46,183:INFO:  Epoch 325/500:  train Loss: 98.8715   val Loss: 97.4247   time: 0.14s   best: 84.3860
2023-11-02 14:35:46,338:INFO:  Epoch 326/500:  train Loss: 95.9688   val Loss: 98.0729   time: 0.15s   best: 84.3860
2023-11-02 14:35:46,471:INFO:  Epoch 327/500:  train Loss: 97.5956   val Loss: 114.9429   time: 0.13s   best: 84.3860
2023-11-02 14:35:46,602:INFO:  Epoch 328/500:  train Loss: 102.4970   val Loss: 100.2975   time: 0.13s   best: 84.3860
2023-11-02 14:35:46,738:INFO:  Epoch 329/500:  train Loss: 90.6739   val Loss: 95.3088   time: 0.13s   best: 84.3860
2023-11-02 14:35:46,977:INFO:  Epoch 330/500:  train Loss: 90.7314   val Loss: 96.1444   time: 0.24s   best: 84.3860
2023-11-02 14:35:47,109:INFO:  Epoch 331/500:  train Loss: 89.6584   val Loss: 99.9560   time: 0.13s   best: 84.3860
2023-11-02 14:35:47,244:INFO:  Epoch 332/500:  train Loss: 90.8092   val Loss: 102.0283   time: 0.13s   best: 84.3860
2023-11-02 14:35:47,377:INFO:  Epoch 333/500:  train Loss: 90.9840   val Loss: 99.2856   time: 0.13s   best: 84.3860
2023-11-02 14:35:47,602:INFO:  Epoch 334/500:  train Loss: 89.3701   val Loss: 97.1282   time: 0.20s   best: 84.3860
2023-11-02 14:35:47,759:INFO:  Epoch 335/500:  train Loss: 88.4956   val Loss: 97.4452   time: 0.13s   best: 84.3860
2023-11-02 14:35:47,923:INFO:  Epoch 336/500:  train Loss: 88.8635   val Loss: 98.2667   time: 0.15s   best: 84.3860
2023-11-02 14:35:48,083:INFO:  Epoch 337/500:  train Loss: 88.9994   val Loss: 98.9830   time: 0.15s   best: 84.3860
2023-11-02 14:35:48,234:INFO:  Epoch 338/500:  train Loss: 88.9581   val Loss: 98.7622   time: 0.14s   best: 84.3860
2023-11-02 14:35:48,375:INFO:  Epoch 339/500:  train Loss: 89.0195   val Loss: 97.9636   time: 0.13s   best: 84.3860
2023-11-02 14:35:48,538:INFO:  Epoch 340/500:  train Loss: 88.7194   val Loss: 97.5931   time: 0.15s   best: 84.3860
2023-11-02 14:35:48,679:INFO:  Epoch 341/500:  train Loss: 88.9943   val Loss: 97.8001   time: 0.13s   best: 84.3860
2023-11-02 14:35:48,822:INFO:  Epoch 342/500:  train Loss: 88.9419   val Loss: 98.4477   time: 0.13s   best: 84.3860
2023-11-02 14:35:48,964:INFO:  Epoch 343/500:  train Loss: 89.3426   val Loss: 97.9195   time: 0.13s   best: 84.3860
2023-11-02 14:35:49,124:INFO:  Epoch 344/500:  train Loss: 88.6782   val Loss: 97.7080   time: 0.15s   best: 84.3860
2023-11-02 14:35:49,269:INFO:  Epoch 345/500:  train Loss: 88.7590   val Loss: 97.7433   time: 0.13s   best: 84.3860
2023-11-02 14:35:49,410:INFO:  Epoch 346/500:  train Loss: 88.9712   val Loss: 97.3048   time: 0.13s   best: 84.3860
2023-11-02 14:35:49,573:INFO:  Epoch 347/500:  train Loss: 89.0501   val Loss: 96.7413   time: 0.15s   best: 84.3860
2023-11-02 14:35:49,714:INFO:  Epoch 348/500:  train Loss: 88.5592   val Loss: 96.9057   time: 0.13s   best: 84.3860
2023-11-02 14:35:49,892:INFO:  Epoch 349/500:  train Loss: 88.5527   val Loss: 98.0810   time: 0.17s   best: 84.3860
2023-11-02 14:35:50,034:INFO:  Epoch 350/500:  train Loss: 89.6436   val Loss: 98.5840   time: 0.13s   best: 84.3860
2023-11-02 14:35:50,204:INFO:  Epoch 351/500:  train Loss: 88.8071   val Loss: 97.3034   time: 0.16s   best: 84.3860
2023-11-02 14:35:50,346:INFO:  Epoch 352/500:  train Loss: 88.3089   val Loss: 96.4432   time: 0.13s   best: 84.3860
2023-11-02 14:35:50,488:INFO:  Epoch 353/500:  train Loss: 87.0301   val Loss: 96.2635   time: 0.13s   best: 84.3860
2023-11-02 14:35:50,651:INFO:  Epoch 354/500:  train Loss: 85.8648   val Loss: 95.5328   time: 0.15s   best: 84.3860
2023-11-02 14:35:50,796:INFO:  Epoch 355/500:  train Loss: 85.5711   val Loss: 94.6857   time: 0.13s   best: 84.3860
2023-11-02 14:35:50,937:INFO:  Epoch 356/500:  train Loss: 86.7864   val Loss: 94.1511   time: 0.13s   best: 84.3860
2023-11-02 14:35:51,078:INFO:  Epoch 357/500:  train Loss: 87.6237   val Loss: 95.3059   time: 0.13s   best: 84.3860
2023-11-02 14:35:51,241:INFO:  Epoch 358/500:  train Loss: 87.9413   val Loss: 97.4894   time: 0.15s   best: 84.3860
2023-11-02 14:35:51,382:INFO:  Epoch 359/500:  train Loss: 87.9609   val Loss: 97.3373   time: 0.13s   best: 84.3860
2023-11-02 14:35:51,523:INFO:  Epoch 360/500:  train Loss: 87.0167   val Loss: 94.5285   time: 0.13s   best: 84.3860
2023-11-02 14:35:51,685:INFO:  Epoch 361/500:  train Loss: 85.2260   val Loss: 93.3887   time: 0.13s   best: 84.3860
2023-11-02 14:35:51,828:INFO:  Epoch 362/500:  train Loss: 84.3991   val Loss: 93.3970   time: 0.13s   best: 84.3860
2023-11-02 14:35:52,040:INFO:  Epoch 363/500:  train Loss: 84.4229   val Loss: 93.4620   time: 0.20s   best: 84.3860
2023-11-02 14:35:52,207:INFO:  Epoch 364/500:  train Loss: 83.5886   val Loss: 89.7142   time: 0.14s   best: 84.3860
2023-11-02 14:35:52,340:INFO:  Epoch 365/500:  train Loss: 84.2484   val Loss: 101.5577   time: 0.13s   best: 84.3860
2023-11-02 14:35:52,493:INFO:  Epoch 366/500:  train Loss: 102.3708   val Loss: 92.8384   time: 0.14s   best: 84.3860
2023-11-02 14:35:52,632:INFO:  Epoch 367/500:  train Loss: 90.9792   val Loss: 95.8523   time: 0.13s   best: 84.3860
2023-11-02 14:35:52,796:INFO:  Epoch 368/500:  train Loss: 93.7579   val Loss: 96.0615   time: 0.15s   best: 84.3860
2023-11-02 14:35:52,937:INFO:  Epoch 369/500:  train Loss: 93.5208   val Loss: 108.7625   time: 0.13s   best: 84.3860
2023-11-02 14:35:53,078:INFO:  Epoch 370/500:  train Loss: 94.9856   val Loss: 95.5913   time: 0.13s   best: 84.3860
2023-11-02 14:35:53,219:INFO:  Epoch 371/500:  train Loss: 88.7103   val Loss: 95.1267   time: 0.13s   best: 84.3860
2023-11-02 14:35:53,383:INFO:  Epoch 372/500:  train Loss: 89.6504   val Loss: 100.4428   time: 0.15s   best: 84.3860
2023-11-02 14:35:53,517:INFO:  Epoch 373/500:  train Loss: 91.4682   val Loss: 99.2330   time: 0.13s   best: 84.3860
2023-11-02 14:35:53,665:INFO:  Epoch 374/500:  train Loss: 89.1969   val Loss: 95.6680   time: 0.14s   best: 84.3860
2023-11-02 14:35:53,830:INFO:  Epoch 375/500:  train Loss: 88.9074   val Loss: 96.7210   time: 0.15s   best: 84.3860
2023-11-02 14:35:53,970:INFO:  Epoch 376/500:  train Loss: 89.5771   val Loss: 99.5101   time: 0.13s   best: 84.3860
2023-11-02 14:35:54,148:INFO:  Epoch 377/500:  train Loss: 89.9092   val Loss: 97.6322   time: 0.17s   best: 84.3860
2023-11-02 14:35:54,295:INFO:  Epoch 378/500:  train Loss: 89.5483   val Loss: 96.1276   time: 0.14s   best: 84.3860
2023-11-02 14:35:54,456:INFO:  Epoch 379/500:  train Loss: 88.9043   val Loss: 97.0135   time: 0.15s   best: 84.3860
2023-11-02 14:35:54,597:INFO:  Epoch 380/500:  train Loss: 89.8139   val Loss: 98.6149   time: 0.13s   best: 84.3860
2023-11-02 14:35:54,740:INFO:  Epoch 381/500:  train Loss: 89.2877   val Loss: 97.0539   time: 0.13s   best: 84.3860
2023-11-02 14:35:54,904:INFO:  Epoch 382/500:  train Loss: 88.7987   val Loss: 96.3205   time: 0.15s   best: 84.3860
2023-11-02 14:35:55,046:INFO:  Epoch 383/500:  train Loss: 88.2561   val Loss: 97.0208   time: 0.13s   best: 84.3860
2023-11-02 14:35:55,187:INFO:  Epoch 384/500:  train Loss: 87.7189   val Loss: 96.8448   time: 0.13s   best: 84.3860
2023-11-02 14:35:55,330:INFO:  Epoch 385/500:  train Loss: 86.8389   val Loss: 94.3475   time: 0.13s   best: 84.3860
2023-11-02 14:35:55,498:INFO:  Epoch 386/500:  train Loss: 85.3716   val Loss: 92.9851   time: 0.16s   best: 84.3860
2023-11-02 14:35:55,633:INFO:  Epoch 387/500:  train Loss: 84.5979   val Loss: 92.7999   time: 0.13s   best: 84.3860
2023-11-02 14:35:55,790:INFO:  Epoch 388/500:  train Loss: 84.0416   val Loss: 91.8267   time: 0.15s   best: 84.3860
2023-11-02 14:35:55,953:INFO:  Epoch 389/500:  train Loss: 84.5250   val Loss: 91.6223   time: 0.15s   best: 84.3860
2023-11-02 14:35:56,095:INFO:  Epoch 390/500:  train Loss: 88.1624   val Loss: 90.1693   time: 0.13s   best: 84.3860
2023-11-02 14:35:56,280:INFO:  Epoch 391/500:  train Loss: 87.9208   val Loss: 93.0436   time: 0.17s   best: 84.3860
2023-11-02 14:35:56,421:INFO:  Epoch 392/500:  train Loss: 89.8479   val Loss: 99.2492   time: 0.13s   best: 84.3860
2023-11-02 14:35:56,581:INFO:  Epoch 393/500:  train Loss: 92.4151   val Loss: 100.6411   time: 0.15s   best: 84.3860
2023-11-02 14:35:56,722:INFO:  Epoch 394/500:  train Loss: 90.1051   val Loss: 94.4998   time: 0.13s   best: 84.3860
2023-11-02 14:35:56,865:INFO:  Epoch 395/500:  train Loss: 88.3915   val Loss: 94.0122   time: 0.13s   best: 84.3860
2023-11-02 14:35:57,026:INFO:  Epoch 396/500:  train Loss: 87.4364   val Loss: 95.9911   time: 0.15s   best: 84.3860
2023-11-02 14:35:57,168:INFO:  Epoch 397/500:  train Loss: 86.7819   val Loss: 96.0779   time: 0.13s   best: 84.3860
2023-11-02 14:35:57,310:INFO:  Epoch 398/500:  train Loss: 86.1075   val Loss: 93.9600   time: 0.13s   best: 84.3860
2023-11-02 14:35:57,451:INFO:  Epoch 399/500:  train Loss: 85.3816   val Loss: 92.6685   time: 0.13s   best: 84.3860
2023-11-02 14:35:57,613:INFO:  Epoch 400/500:  train Loss: 85.4894   val Loss: 92.1587   time: 0.15s   best: 84.3860
2023-11-02 14:35:57,754:INFO:  Epoch 401/500:  train Loss: 84.7161   val Loss: 93.2147   time: 0.13s   best: 84.3860
2023-11-02 14:35:57,897:INFO:  Epoch 402/500:  train Loss: 84.4551   val Loss: 92.7483   time: 0.13s   best: 84.3860
2023-11-02 14:35:58,059:INFO:  Epoch 403/500:  train Loss: 84.5961   val Loss: 91.8215   time: 0.13s   best: 84.3860
2023-11-02 14:35:58,224:INFO:  Epoch 404/500:  train Loss: 83.5343   val Loss: 91.3032   time: 0.14s   best: 84.3860
2023-11-02 14:35:58,391:INFO:  Epoch 405/500:  train Loss: 83.8154   val Loss: 91.4226   time: 0.16s   best: 84.3860
2023-11-02 14:35:58,532:INFO:  Epoch 406/500:  train Loss: 83.8626   val Loss: 92.0548   time: 0.13s   best: 84.3860
2023-11-02 14:35:58,692:INFO:  Epoch 407/500:  train Loss: 84.1568   val Loss: 91.8978   time: 0.15s   best: 84.3860
2023-11-02 14:35:58,834:INFO:  Epoch 408/500:  train Loss: 83.2080   val Loss: 90.1150   time: 0.13s   best: 84.3860
2023-11-02 14:35:58,975:INFO:  Epoch 409/500:  train Loss: 87.0061   val Loss: 89.6032   time: 0.13s   best: 84.3860
2023-11-02 14:35:59,136:INFO:  Epoch 410/500:  train Loss: 87.9241   val Loss: 93.0596   time: 0.13s   best: 84.3860
2023-11-02 14:35:59,283:INFO:  Epoch 411/500:  train Loss: 90.1639   val Loss: 94.7656   time: 0.13s   best: 84.3860
2023-11-02 14:35:59,424:INFO:  Epoch 412/500:  train Loss: 91.5117   val Loss: 102.4953   time: 0.13s   best: 84.3860
2023-11-02 14:35:59,565:INFO:  Epoch 413/500:  train Loss: 91.5054   val Loss: 93.5975   time: 0.13s   best: 84.3860
2023-11-02 14:35:59,728:INFO:  Epoch 414/500:  train Loss: 88.4110   val Loss: 92.2153   time: 0.15s   best: 84.3860
2023-11-02 14:35:59,871:INFO:  Epoch 415/500:  train Loss: 87.2661   val Loss: 94.3275   time: 0.13s   best: 84.3860
2023-11-02 14:36:00,012:INFO:  Epoch 416/500:  train Loss: 87.0216   val Loss: 94.4196   time: 0.13s   best: 84.3860
2023-11-02 14:36:00,156:INFO:  Epoch 417/500:  train Loss: 86.0777   val Loss: 91.4306   time: 0.13s   best: 84.3860
2023-11-02 14:36:00,338:INFO:  Epoch 418/500:  train Loss: 84.7971   val Loss: 91.3824   time: 0.16s   best: 84.3860
2023-11-02 14:36:00,501:INFO:  Epoch 419/500:  train Loss: 84.8237   val Loss: 91.6873   time: 0.15s   best: 84.3860
2023-11-02 14:36:00,641:INFO:  Epoch 420/500:  train Loss: 84.2972   val Loss: 91.8960   time: 0.13s   best: 84.3860
2023-11-02 14:36:00,806:INFO:  Epoch 421/500:  train Loss: 84.4299   val Loss: 90.9502   time: 0.15s   best: 84.3860
2023-11-02 14:36:00,946:INFO:  Epoch 422/500:  train Loss: 83.8959   val Loss: 90.6426   time: 0.13s   best: 84.3860
2023-11-02 14:36:01,087:INFO:  Epoch 423/500:  train Loss: 84.2717   val Loss: 91.1694   time: 0.13s   best: 84.3860
2023-11-02 14:36:01,228:INFO:  Epoch 424/500:  train Loss: 83.9696   val Loss: 92.3708   time: 0.13s   best: 84.3860
2023-11-02 14:36:01,392:INFO:  Epoch 425/500:  train Loss: 84.6907   val Loss: 91.5508   time: 0.15s   best: 84.3860
2023-11-02 14:36:01,533:INFO:  Epoch 426/500:  train Loss: 83.7985   val Loss: 90.4742   time: 0.13s   best: 84.3860
2023-11-02 14:36:01,674:INFO:  Epoch 427/500:  train Loss: 84.0501   val Loss: 90.8731   time: 0.13s   best: 84.3860
2023-11-02 14:36:01,839:INFO:  Epoch 428/500:  train Loss: 84.3001   val Loss: 91.0431   time: 0.15s   best: 84.3860
2023-11-02 14:36:01,980:INFO:  Epoch 429/500:  train Loss: 83.7960   val Loss: 90.4430   time: 0.13s   best: 84.3860
2023-11-02 14:36:02,121:INFO:  Epoch 430/500:  train Loss: 83.7807   val Loss: 90.7079   time: 0.13s   best: 84.3860
2023-11-02 14:36:02,269:INFO:  Epoch 431/500:  train Loss: 83.6385   val Loss: 91.3173   time: 0.14s   best: 84.3860
2023-11-02 14:36:02,460:INFO:  Epoch 432/500:  train Loss: 83.4747   val Loss: 90.9893   time: 0.18s   best: 84.3860
2023-11-02 14:36:02,606:INFO:  Epoch 433/500:  train Loss: 83.6865   val Loss: 89.2133   time: 0.14s   best: 84.3860
2023-11-02 14:36:02,748:INFO:  Epoch 434/500:  train Loss: 82.5409   val Loss: 89.3735   time: 0.13s   best: 84.3860
2023-11-02 14:36:02,912:INFO:  Epoch 435/500:  train Loss: 82.4137   val Loss: 94.5474   time: 0.15s   best: 84.3860
2023-11-02 14:36:03,053:INFO:  Epoch 436/500:  train Loss: 85.0459   val Loss: 88.3935   time: 0.13s   best: 84.3860
2023-11-02 14:36:03,195:INFO:  Epoch 437/500:  train Loss: 81.9354   val Loss: 88.6457   time: 0.13s   best: 84.3860
2023-11-02 14:36:03,337:INFO:  Epoch 438/500:  train Loss: 82.6028   val Loss: 94.0703   time: 0.13s   best: 84.3860
2023-11-02 14:36:03,499:INFO:  Epoch 439/500:  train Loss: 92.6681   val Loss: 89.9387   time: 0.15s   best: 84.3860
2023-11-02 14:36:03,641:INFO:  Epoch 440/500:  train Loss: 90.4155   val Loss: 98.0066   time: 0.13s   best: 84.3860
2023-11-02 14:36:03,784:INFO:  Epoch 441/500:  train Loss: 97.0806   val Loss: 96.1759   time: 0.13s   best: 84.3860
2023-11-02 14:36:03,947:INFO:  Epoch 442/500:  train Loss: 93.8568   val Loss: 95.3762   time: 0.15s   best: 84.3860
2023-11-02 14:36:04,090:INFO:  Epoch 443/500:  train Loss: 94.4998   val Loss: 104.3612   time: 0.13s   best: 84.3860
2023-11-02 14:36:04,238:INFO:  Epoch 444/500:  train Loss: 93.3995   val Loss: 93.0981   time: 0.14s   best: 84.3860
2023-11-02 14:36:04,380:INFO:  Epoch 445/500:  train Loss: 87.3812   val Loss: 91.7454   time: 0.13s   best: 84.3860
2023-11-02 14:36:04,578:INFO:  Epoch 446/500:  train Loss: 86.7913   val Loss: 92.8689   time: 0.19s   best: 84.3860
2023-11-02 14:36:04,719:INFO:  Epoch 447/500:  train Loss: 86.7795   val Loss: 95.7373   time: 0.13s   best: 84.3860
2023-11-02 14:36:04,861:INFO:  Epoch 448/500:  train Loss: 86.4284   val Loss: 93.2460   time: 0.13s   best: 84.3860
2023-11-02 14:36:05,022:INFO:  Epoch 449/500:  train Loss: 85.3056   val Loss: 92.3186   time: 0.15s   best: 84.3860
2023-11-02 14:36:05,164:INFO:  Epoch 450/500:  train Loss: 86.0726   val Loss: 91.9974   time: 0.13s   best: 84.3860
2023-11-02 14:36:05,305:INFO:  Epoch 451/500:  train Loss: 85.1932   val Loss: 92.7279   time: 0.13s   best: 84.3860
2023-11-02 14:36:05,447:INFO:  Epoch 452/500:  train Loss: 85.3300   val Loss: 94.3848   time: 0.13s   best: 84.3860
2023-11-02 14:36:05,608:INFO:  Epoch 453/500:  train Loss: 85.1107   val Loss: 91.7574   time: 0.15s   best: 84.3860
2023-11-02 14:36:05,749:INFO:  Epoch 454/500:  train Loss: 83.5399   val Loss: 89.9758   time: 0.13s   best: 84.3860
2023-11-02 14:36:05,893:INFO:  Epoch 455/500:  train Loss: 82.9637   val Loss: 89.3880   time: 0.13s   best: 84.3860
2023-11-02 14:36:06,056:INFO:  Epoch 456/500:  train Loss: 82.2202   val Loss: 90.7274   time: 0.15s   best: 84.3860
2023-11-02 14:36:06,201:INFO:  Epoch 457/500:  train Loss: 82.0653   val Loss: 89.8927   time: 0.13s   best: 84.3860
2023-11-02 14:36:06,347:INFO:  Epoch 458/500:  train Loss: 82.1317   val Loss: 87.5898   time: 0.13s   best: 84.3860
2023-11-02 14:36:06,488:INFO:  Epoch 459/500:  train Loss: 81.5267   val Loss: 88.7319   time: 0.13s   best: 84.3860
2023-11-02 14:36:06,687:INFO:  Epoch 460/500:  train Loss: 80.6335   val Loss: 89.5416   time: 0.19s   best: 84.3860
2023-11-02 14:36:06,830:INFO:  Epoch 461/500:  train Loss: 80.8895   val Loss: 89.3602   time: 0.13s   best: 84.3860
2023-11-02 14:36:06,972:INFO:  Epoch 462/500:  train Loss: 81.1096   val Loss: 90.4823   time: 0.13s   best: 84.3860
2023-11-02 14:36:07,134:INFO:  Epoch 463/500:  train Loss: 80.1924   val Loss: 88.5645   time: 0.15s   best: 84.3860
2023-11-02 14:36:07,275:INFO:  Epoch 464/500:  train Loss: 78.6001   val Loss: 94.0631   time: 0.13s   best: 84.3860
2023-11-02 14:36:07,417:INFO:  Epoch 465/500:  train Loss: 85.0155   val Loss: 91.5872   time: 0.13s   best: 84.3860
2023-11-02 14:36:07,557:INFO:  Epoch 466/500:  train Loss: 81.2263   val Loss: 90.3314   time: 0.13s   best: 84.3860
2023-11-02 14:36:07,719:INFO:  Epoch 467/500:  train Loss: 80.5730   val Loss: 90.5063   time: 0.15s   best: 84.3860
2023-11-02 14:36:07,863:INFO:  Epoch 468/500:  train Loss: 80.2680   val Loss: 91.0309   time: 0.13s   best: 84.3860
2023-11-02 14:36:08,004:INFO:  Epoch 469/500:  train Loss: 80.0002   val Loss: 89.4896   time: 0.13s   best: 84.3860
2023-11-02 14:36:08,168:INFO:  Epoch 470/500:  train Loss: 80.2497   val Loss: 90.8325   time: 0.13s   best: 84.3860
2023-11-02 14:36:08,316:INFO:  Epoch 471/500:  train Loss: 79.8114   val Loss: 90.2200   time: 0.14s   best: 84.3860
2023-11-02 14:36:08,457:INFO:  Epoch 472/500:  train Loss: 80.0511   val Loss: 89.6119   time: 0.13s   best: 84.3860
2023-11-02 14:36:08,598:INFO:  Epoch 473/500:  train Loss: 78.1484   val Loss: 87.1367   time: 0.13s   best: 84.3860
2023-11-02 14:36:08,797:INFO:  Epoch 474/500:  train Loss: 83.5308   val Loss: 88.8776   time: 0.19s   best: 84.3860
2023-11-02 14:36:08,939:INFO:  Epoch 475/500:  train Loss: 79.9358   val Loss: 91.2237   time: 0.13s   best: 84.3860
2023-11-02 14:36:09,081:INFO:  Epoch 476/500:  train Loss: 81.2842   val Loss: 91.7238   time: 0.13s   best: 84.3860
2023-11-02 14:36:09,243:INFO:  Epoch 477/500:  train Loss: 80.0524   val Loss: 93.6833   time: 0.13s   best: 84.3860
2023-11-02 14:36:09,389:INFO:  Epoch 478/500:  train Loss: 81.1257   val Loss: 91.2434   time: 0.13s   best: 84.3860
2023-11-02 14:36:09,530:INFO:  Epoch 479/500:  train Loss: 81.8161   val Loss: 90.9108   time: 0.13s   best: 84.3860
2023-11-02 14:36:09,671:INFO:  Epoch 480/500:  train Loss: 81.5583   val Loss: 90.5621   time: 0.13s   best: 84.3860
2023-11-02 14:36:09,836:INFO:  Epoch 481/500:  train Loss: 80.4527   val Loss: 89.2218   time: 0.15s   best: 84.3860
2023-11-02 14:36:09,977:INFO:  Epoch 482/500:  train Loss: 80.6526   val Loss: 90.3547   time: 0.13s   best: 84.3860
2023-11-02 14:36:10,118:INFO:  Epoch 483/500:  train Loss: 79.8229   val Loss: 89.4667   time: 0.13s   best: 84.3860
2023-11-02 14:36:10,265:INFO:  Epoch 484/500:  train Loss: 78.8976   val Loss: 88.7500   time: 0.14s   best: 84.3860
2023-11-02 14:36:10,427:INFO:  Epoch 485/500:  train Loss: 78.4814   val Loss: 89.3068   time: 0.15s   best: 84.3860
2023-11-02 14:36:10,568:INFO:  Epoch 486/500:  train Loss: 79.3655   val Loss: 89.5898   time: 0.13s   best: 84.3860
2023-11-02 14:36:10,708:INFO:  Epoch 487/500:  train Loss: 79.2314   val Loss: 90.5078   time: 0.13s   best: 84.3860
2023-11-02 14:36:10,909:INFO:  Epoch 488/500:  train Loss: 78.9544   val Loss: 89.4708   time: 0.20s   best: 84.3860
2023-11-02 14:36:11,044:INFO:  Epoch 489/500:  train Loss: 78.0996   val Loss: 89.2035   time: 0.13s   best: 84.3860
2023-11-02 14:36:11,208:INFO:  Epoch 490/500:  train Loss: 77.3723   val Loss: 89.1030   time: 0.15s   best: 84.3860
2023-11-02 14:36:11,368:INFO:  Epoch 491/500:  train Loss: 79.0958   val Loss: 88.2533   time: 0.15s   best: 84.3860
2023-11-02 14:36:11,512:INFO:  Epoch 492/500:  train Loss: 79.1824   val Loss: 89.4608   time: 0.13s   best: 84.3860
2023-11-02 14:36:11,653:INFO:  Epoch 493/500:  train Loss: 79.5644   val Loss: 88.1097   time: 0.13s   best: 84.3860
2023-11-02 14:36:11,795:INFO:  Epoch 494/500:  train Loss: 77.5046   val Loss: 88.5072   time: 0.13s   best: 84.3860
2023-11-02 14:36:11,958:INFO:  Epoch 495/500:  train Loss: 77.5722   val Loss: 88.5960   time: 0.15s   best: 84.3860
2023-11-02 14:36:12,099:INFO:  Epoch 496/500:  train Loss: 77.8032   val Loss: 88.3793   time: 0.13s   best: 84.3860
2023-11-02 14:36:12,246:INFO:  Epoch 497/500:  train Loss: 76.4414   val Loss: 89.5209   time: 0.14s   best: 84.3860
2023-11-02 14:36:12,389:INFO:  Epoch 498/500:  train Loss: 77.2463   val Loss: 89.3646   time: 0.13s   best: 84.3860
2023-11-02 14:36:12,551:INFO:  Epoch 499/500:  train Loss: 78.5604   val Loss: 90.9720   time: 0.15s   best: 84.3860
2023-11-02 14:36:12,692:INFO:  Epoch 500/500:  train Loss: 77.0020   val Loss: 87.2418   time: 0.13s   best: 84.3860
2023-11-02 14:36:12,692:INFO:  -----> Training complete in 1m 19s   best validation loss: 84.3860
 
2023-11-02 14:41:02,574:INFO:  Starting experiment lstm autoencoder debug (2 layer + 0.1 dropout)
2023-11-02 14:41:02,575:INFO:  Defining the model
2023-11-02 14:41:02,619:INFO:  Reading the dataset
2023-11-02 14:41:22,977:INFO:  Starting experiment lstm autoencoder debug (2 layer + 0.1 dropout)
2023-11-02 14:41:22,978:INFO:  Defining the model
2023-11-02 14:41:23,021:INFO:  Reading the dataset
2023-11-02 14:41:29,384:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:41:29,405:INFO:  Epoch 1/500:  train Loss: 102.0824   val Loss: 98.0386   time: 1.45s   best: 98.0386
2023-11-02 14:41:29,570:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:41:29,753:INFO:  Epoch 2/500:  train Loss: 100.5019   val Loss: 95.4421   time: 0.16s   best: 95.4421
2023-11-02 14:41:29,893:INFO:  Epoch 3/500:  train Loss: 98.4894   val Loss: 100.0159   time: 0.14s   best: 95.4421
2023-11-02 14:41:30,046:INFO:  Epoch 4/500:  train Loss: 98.7264   val Loss: 100.0652   time: 0.15s   best: 95.4421
2023-11-02 14:41:30,181:INFO:  Epoch 5/500:  train Loss: 99.1917   val Loss: 100.0964   time: 0.13s   best: 95.4421
2023-11-02 14:41:30,353:INFO:  Epoch 6/500:  train Loss: 99.7311   val Loss: 100.1146   time: 0.17s   best: 95.4421
2023-11-02 14:41:30,509:INFO:  Epoch 7/500:  train Loss: 99.7185   val Loss: 100.1236   time: 0.15s   best: 95.4421
2023-11-02 14:41:30,643:INFO:  Epoch 8/500:  train Loss: 99.7586   val Loss: 100.1295   time: 0.13s   best: 95.4421
2023-11-02 14:41:30,777:INFO:  Epoch 9/500:  train Loss: 99.8280   val Loss: 100.1312   time: 0.13s   best: 95.4421
2023-11-02 14:41:30,909:INFO:  Epoch 10/500:  train Loss: 99.8759   val Loss: 100.1296   time: 0.13s   best: 95.4421
2023-11-02 14:41:31,069:INFO:  Epoch 11/500:  train Loss: 99.7624   val Loss: 100.1230   time: 0.16s   best: 95.4421
2023-11-02 14:41:31,202:INFO:  Epoch 12/500:  train Loss: 99.7095   val Loss: 100.1163   time: 0.13s   best: 95.4421
2023-11-02 14:41:31,336:INFO:  Epoch 13/500:  train Loss: 99.6833   val Loss: 100.1045   time: 0.13s   best: 95.4421
2023-11-02 14:41:31,470:INFO:  Epoch 14/500:  train Loss: 99.6501   val Loss: 100.0450   time: 0.13s   best: 95.4421
2023-11-02 14:41:31,628:INFO:  Epoch 15/500:  train Loss: 99.2248   val Loss: 99.8663   time: 0.16s   best: 95.4421
2023-11-02 14:41:31,762:INFO:  Epoch 16/500:  train Loss: 99.1774   val Loss: 99.5564   time: 0.13s   best: 95.4421
2023-11-02 14:41:31,895:INFO:  Epoch 17/500:  train Loss: 98.5040   val Loss: 99.0089   time: 0.13s   best: 95.4421
2023-11-02 14:41:32,040:INFO:  Epoch 18/500:  train Loss: 98.1997   val Loss: 98.1233   time: 0.14s   best: 95.4421
2023-11-02 14:41:32,198:INFO:  Epoch 19/500:  train Loss: 96.6907   val Loss: 95.8744   time: 0.15s   best: 95.4421
2023-11-02 14:41:32,370:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:41:32,426:INFO:  Epoch 20/500:  train Loss: 97.3903   val Loss: 93.3707   time: 0.17s   best: 93.3707
2023-11-02 14:41:32,588:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:41:33,007:INFO:  Epoch 21/500:  train Loss: 97.2891   val Loss: 93.1437   time: 0.16s   best: 93.1437
2023-11-02 14:41:33,149:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:41:33,192:INFO:  Epoch 22/500:  train Loss: 96.1400   val Loss: 92.8437   time: 0.14s   best: 92.8437
2023-11-02 14:41:33,329:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:41:33,853:INFO:  Epoch 23/500:  train Loss: 96.9025   val Loss: 92.7219   time: 0.13s   best: 92.7219
2023-11-02 14:41:33,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:41:34,154:INFO:  Epoch 24/500:  train Loss: 96.7570   val Loss: 92.2442   time: 0.14s   best: 92.2442
2023-11-02 14:41:34,305:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:41:34,369:INFO:  Epoch 25/500:  train Loss: 94.4318   val Loss: 91.6054   time: 0.15s   best: 91.6054
2023-11-02 14:41:34,527:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:41:35,160:INFO:  Epoch 26/500:  train Loss: 96.1464   val Loss: 90.9147   time: 0.15s   best: 90.9147
2023-11-02 14:41:35,301:INFO:  Epoch 27/500:  train Loss: 95.5637   val Loss: 91.2130   time: 0.14s   best: 90.9147
2023-11-02 14:41:35,435:INFO:  Epoch 28/500:  train Loss: 94.0827   val Loss: 92.8398   time: 0.13s   best: 90.9147
2023-11-02 14:41:35,570:INFO:  Epoch 29/500:  train Loss: 93.8008   val Loss: 93.7932   time: 0.13s   best: 90.9147
2023-11-02 14:41:35,726:INFO:  Epoch 30/500:  train Loss: 93.0091   val Loss: 93.6553   time: 0.15s   best: 90.9147
2023-11-02 14:41:35,859:INFO:  Epoch 31/500:  train Loss: 92.5709   val Loss: 93.5280   time: 0.13s   best: 90.9147
2023-11-02 14:41:35,999:INFO:  Epoch 32/500:  train Loss: 92.0980   val Loss: 93.5876   time: 0.14s   best: 90.9147
2023-11-02 14:41:36,134:INFO:  Epoch 33/500:  train Loss: 90.8727   val Loss: 93.7761   time: 0.13s   best: 90.9147
2023-11-02 14:41:36,291:INFO:  Epoch 34/500:  train Loss: 90.2130   val Loss: 94.1566   time: 0.15s   best: 90.9147
2023-11-02 14:41:36,450:INFO:  Epoch 35/500:  train Loss: 89.8787   val Loss: 94.7587   time: 0.15s   best: 90.9147
2023-11-02 14:41:36,597:INFO:  Epoch 36/500:  train Loss: 89.5102   val Loss: 95.4183   time: 0.14s   best: 90.9147
2023-11-02 14:41:36,752:INFO:  Epoch 37/500:  train Loss: 88.5675   val Loss: 94.1812   time: 0.15s   best: 90.9147
2023-11-02 14:41:36,887:INFO:  Epoch 38/500:  train Loss: 88.2399   val Loss: 95.3419   time: 0.13s   best: 90.9147
2023-11-02 14:41:37,022:INFO:  Epoch 39/500:  train Loss: 86.9666   val Loss: 98.2483   time: 0.13s   best: 90.9147
2023-11-02 14:41:37,157:INFO:  Epoch 40/500:  train Loss: 86.2880   val Loss: 99.5960   time: 0.13s   best: 90.9147
2023-11-02 14:41:37,313:INFO:  Epoch 41/500:  train Loss: 86.9789   val Loss: 100.4676   time: 0.15s   best: 90.9147
2023-11-02 14:41:37,448:INFO:  Epoch 42/500:  train Loss: 87.2542   val Loss: 101.0009   time: 0.13s   best: 90.9147
2023-11-02 14:41:37,583:INFO:  Epoch 43/500:  train Loss: 86.7578   val Loss: 101.1226   time: 0.13s   best: 90.9147
2023-11-02 14:41:37,718:INFO:  Epoch 44/500:  train Loss: 86.3038   val Loss: 99.0432   time: 0.13s   best: 90.9147
2023-11-02 14:41:37,873:INFO:  Epoch 45/500:  train Loss: 85.7129   val Loss: 97.6636   time: 0.15s   best: 90.9147
2023-11-02 14:41:38,018:INFO:  Epoch 46/500:  train Loss: 86.5901   val Loss: 97.5660   time: 0.14s   best: 90.9147
2023-11-02 14:41:38,153:INFO:  Epoch 47/500:  train Loss: 86.3672   val Loss: 97.1145   time: 0.13s   best: 90.9147
2023-11-02 14:41:38,287:INFO:  Epoch 48/500:  train Loss: 85.3075   val Loss: 98.9999   time: 0.13s   best: 90.9147
2023-11-02 14:41:38,444:INFO:  Epoch 49/500:  train Loss: 85.7374   val Loss: 98.8480   time: 0.15s   best: 90.9147
2023-11-02 14:41:38,618:INFO:  Epoch 50/500:  train Loss: 86.4068   val Loss: 98.3709   time: 0.17s   best: 90.9147
2023-11-02 14:41:38,751:INFO:  Epoch 51/500:  train Loss: 85.6039   val Loss: 97.8917   time: 0.13s   best: 90.9147
2023-11-02 14:41:38,906:INFO:  Epoch 52/500:  train Loss: 85.8301   val Loss: 96.7539   time: 0.15s   best: 90.9147
2023-11-02 14:41:39,042:INFO:  Epoch 53/500:  train Loss: 85.6193   val Loss: 94.5414   time: 0.13s   best: 90.9147
2023-11-02 14:41:39,175:INFO:  Epoch 54/500:  train Loss: 85.3805   val Loss: 93.9718   time: 0.13s   best: 90.9147
2023-11-02 14:41:39,309:INFO:  Epoch 55/500:  train Loss: 86.0124   val Loss: 94.0135   time: 0.13s   best: 90.9147
2023-11-02 14:41:39,468:INFO:  Epoch 56/500:  train Loss: 85.3324   val Loss: 94.0415   time: 0.16s   best: 90.9147
2023-11-02 14:41:39,601:INFO:  Epoch 57/500:  train Loss: 84.9006   val Loss: 95.3411   time: 0.13s   best: 90.9147
2023-11-02 14:41:39,736:INFO:  Epoch 58/500:  train Loss: 84.5020   val Loss: 95.8212   time: 0.13s   best: 90.9147
2023-11-02 14:41:39,979:INFO:  Epoch 59/500:  train Loss: 84.6609   val Loss: 94.9945   time: 0.24s   best: 90.9147
2023-11-02 14:41:40,114:INFO:  Epoch 60/500:  train Loss: 84.3523   val Loss: 95.6077   time: 0.13s   best: 90.9147
2023-11-02 14:41:40,247:INFO:  Epoch 61/500:  train Loss: 83.2281   val Loss: 96.1629   time: 0.13s   best: 90.9147
2023-11-02 14:41:40,382:INFO:  Epoch 62/500:  train Loss: 83.0440   val Loss: 96.7599   time: 0.13s   best: 90.9147
2023-11-02 14:41:40,538:INFO:  Epoch 63/500:  train Loss: 82.7284   val Loss: 96.9456   time: 0.15s   best: 90.9147
2023-11-02 14:41:40,709:INFO:  Epoch 64/500:  train Loss: 82.1918   val Loss: 97.0687   time: 0.17s   best: 90.9147
2023-11-02 14:41:40,844:INFO:  Epoch 65/500:  train Loss: 81.9258   val Loss: 96.7727   time: 0.13s   best: 90.9147
2023-11-02 14:41:40,978:INFO:  Epoch 66/500:  train Loss: 81.0489   val Loss: 96.7047   time: 0.13s   best: 90.9147
2023-11-02 14:41:41,135:INFO:  Epoch 67/500:  train Loss: 82.2558   val Loss: 98.5738   time: 0.15s   best: 90.9147
2023-11-02 14:41:41,270:INFO:  Epoch 68/500:  train Loss: 83.7352   val Loss: 95.9887   time: 0.13s   best: 90.9147
2023-11-02 14:41:41,404:INFO:  Epoch 69/500:  train Loss: 81.3202   val Loss: 96.2479   time: 0.13s   best: 90.9147
2023-11-02 14:41:41,562:INFO:  Epoch 70/500:  train Loss: 81.8011   val Loss: 95.6877   time: 0.16s   best: 90.9147
2023-11-02 14:41:41,696:INFO:  Epoch 71/500:  train Loss: 81.6202   val Loss: 95.3307   time: 0.13s   best: 90.9147
2023-11-02 14:41:41,829:INFO:  Epoch 72/500:  train Loss: 82.6576   val Loss: 93.8872   time: 0.13s   best: 90.9147
2023-11-02 14:41:41,966:INFO:  Epoch 73/500:  train Loss: 81.8049   val Loss: 93.2817   time: 0.13s   best: 90.9147
2023-11-02 14:41:42,131:INFO:  Epoch 74/500:  train Loss: 82.2018   val Loss: 94.5551   time: 0.16s   best: 90.9147
2023-11-02 14:41:42,264:INFO:  Epoch 75/500:  train Loss: 82.8737   val Loss: 94.7616   time: 0.13s   best: 90.9147
2023-11-02 14:41:42,399:INFO:  Epoch 76/500:  train Loss: 83.2492   val Loss: 94.9449   time: 0.13s   best: 90.9147
2023-11-02 14:41:42,532:INFO:  Epoch 77/500:  train Loss: 83.4762   val Loss: 95.2935   time: 0.13s   best: 90.9147
2023-11-02 14:41:42,727:INFO:  Epoch 78/500:  train Loss: 83.0708   val Loss: 95.8009   time: 0.19s   best: 90.9147
2023-11-02 14:41:42,861:INFO:  Epoch 79/500:  train Loss: 82.4788   val Loss: 96.4739   time: 0.13s   best: 90.9147
2023-11-02 14:41:42,996:INFO:  Epoch 80/500:  train Loss: 81.9135   val Loss: 97.0753   time: 0.13s   best: 90.9147
2023-11-02 14:41:43,131:INFO:  Epoch 81/500:  train Loss: 81.2953   val Loss: 96.8958   time: 0.13s   best: 90.9147
2023-11-02 14:41:43,299:INFO:  Epoch 82/500:  train Loss: 81.6989   val Loss: 101.5564   time: 0.17s   best: 90.9147
2023-11-02 14:41:43,433:INFO:  Epoch 83/500:  train Loss: 83.2868   val Loss: 96.7502   time: 0.13s   best: 90.9147
2023-11-02 14:41:43,567:INFO:  Epoch 84/500:  train Loss: 81.2307   val Loss: 96.9678   time: 0.13s   best: 90.9147
2023-11-02 14:41:43,728:INFO:  Epoch 85/500:  train Loss: 81.3908   val Loss: 96.7003   time: 0.16s   best: 90.9147
2023-11-02 14:41:43,861:INFO:  Epoch 86/500:  train Loss: 82.5329   val Loss: 96.5649   time: 0.13s   best: 90.9147
2023-11-02 14:41:44,006:INFO:  Epoch 87/500:  train Loss: 82.8316   val Loss: 96.6741   time: 0.14s   best: 90.9147
2023-11-02 14:41:44,141:INFO:  Epoch 88/500:  train Loss: 82.7677   val Loss: 96.9643   time: 0.13s   best: 90.9147
2023-11-02 14:41:44,297:INFO:  Epoch 89/500:  train Loss: 82.4109   val Loss: 97.4209   time: 0.15s   best: 90.9147
2023-11-02 14:41:44,432:INFO:  Epoch 90/500:  train Loss: 82.3263   val Loss: 98.0413   time: 0.13s   best: 90.9147
2023-11-02 14:41:44,568:INFO:  Epoch 91/500:  train Loss: 81.6903   val Loss: 98.7578   time: 0.13s   best: 90.9147
2023-11-02 14:41:44,719:INFO:  Epoch 92/500:  train Loss: 81.1999   val Loss: 99.4786   time: 0.15s   best: 90.9147
2023-11-02 14:41:44,892:INFO:  Epoch 93/500:  train Loss: 81.1016   val Loss: 100.2570   time: 0.17s   best: 90.9147
2023-11-02 14:41:45,026:INFO:  Epoch 94/500:  train Loss: 81.1690   val Loss: 100.8851   time: 0.13s   best: 90.9147
2023-11-02 14:41:45,160:INFO:  Epoch 95/500:  train Loss: 80.8456   val Loss: 100.9684   time: 0.13s   best: 90.9147
2023-11-02 14:41:45,319:INFO:  Epoch 96/500:  train Loss: 81.1647   val Loss: 100.5672   time: 0.16s   best: 90.9147
2023-11-02 14:41:45,453:INFO:  Epoch 97/500:  train Loss: 80.7477   val Loss: 99.1063   time: 0.13s   best: 90.9147
2023-11-02 14:41:45,588:INFO:  Epoch 98/500:  train Loss: 80.9711   val Loss: 98.4570   time: 0.13s   best: 90.9147
2023-11-02 14:41:45,722:INFO:  Epoch 99/500:  train Loss: 80.5599   val Loss: 97.6841   time: 0.13s   best: 90.9147
2023-11-02 14:41:45,877:INFO:  Epoch 100/500:  train Loss: 81.3961   val Loss: 97.4036   time: 0.15s   best: 90.9147
2023-11-02 14:41:46,022:INFO:  Epoch 101/500:  train Loss: 81.1573   val Loss: 97.6331   time: 0.14s   best: 90.9147
2023-11-02 14:41:46,157:INFO:  Epoch 102/500:  train Loss: 81.2151   val Loss: 98.0454   time: 0.13s   best: 90.9147
2023-11-02 14:41:46,290:INFO:  Epoch 103/500:  train Loss: 79.9432   val Loss: 97.4906   time: 0.13s   best: 90.9147
2023-11-02 14:41:46,445:INFO:  Epoch 104/500:  train Loss: 81.2156   val Loss: 103.5189   time: 0.15s   best: 90.9147
2023-11-02 14:41:46,584:INFO:  Epoch 105/500:  train Loss: 84.2122   val Loss: 98.6882   time: 0.14s   best: 90.9147
2023-11-02 14:41:46,718:INFO:  Epoch 106/500:  train Loss: 80.3447   val Loss: 98.8758   time: 0.13s   best: 90.9147
2023-11-02 14:41:46,911:INFO:  Epoch 107/500:  train Loss: 81.6881   val Loss: 98.8610   time: 0.19s   best: 90.9147
2023-11-02 14:41:47,046:INFO:  Epoch 108/500:  train Loss: 82.0918   val Loss: 98.9195   time: 0.13s   best: 90.9147
2023-11-02 14:41:47,182:INFO:  Epoch 109/500:  train Loss: 82.0008   val Loss: 99.2900   time: 0.13s   best: 90.9147
2023-11-02 14:41:47,316:INFO:  Epoch 110/500:  train Loss: 81.8220   val Loss: 99.7810   time: 0.13s   best: 90.9147
2023-11-02 14:41:47,472:INFO:  Epoch 111/500:  train Loss: 82.1052   val Loss: 100.3449   time: 0.15s   best: 90.9147
2023-11-02 14:41:47,607:INFO:  Epoch 112/500:  train Loss: 81.8788   val Loss: 100.8098   time: 0.13s   best: 90.9147
2023-11-02 14:41:47,741:INFO:  Epoch 113/500:  train Loss: 81.7773   val Loss: 101.1624   time: 0.13s   best: 90.9147
2023-11-02 14:41:47,875:INFO:  Epoch 114/500:  train Loss: 81.7573   val Loss: 101.2713   time: 0.13s   best: 90.9147
2023-11-02 14:41:48,042:INFO:  Epoch 115/500:  train Loss: 81.6873   val Loss: 101.0995   time: 0.16s   best: 90.9147
2023-11-02 14:41:48,179:INFO:  Epoch 116/500:  train Loss: 81.5770   val Loss: 100.7798   time: 0.13s   best: 90.9147
2023-11-02 14:41:48,312:INFO:  Epoch 117/500:  train Loss: 81.4856   val Loss: 100.3790   time: 0.13s   best: 90.9147
2023-11-02 14:41:48,446:INFO:  Epoch 118/500:  train Loss: 81.2288   val Loss: 100.0913   time: 0.13s   best: 90.9147
2023-11-02 14:41:48,604:INFO:  Epoch 119/500:  train Loss: 82.0122   val Loss: 100.0637   time: 0.16s   best: 90.9147
2023-11-02 14:41:48,739:INFO:  Epoch 120/500:  train Loss: 82.0757   val Loss: 100.1020   time: 0.13s   best: 90.9147
2023-11-02 14:41:48,904:INFO:  Epoch 121/500:  train Loss: 81.4244   val Loss: 100.2230   time: 0.16s   best: 90.9147
2023-11-02 14:41:49,066:INFO:  Epoch 122/500:  train Loss: 81.4779   val Loss: 100.5930   time: 0.16s   best: 90.9147
2023-11-02 14:41:49,200:INFO:  Epoch 123/500:  train Loss: 81.1380   val Loss: 100.9303   time: 0.13s   best: 90.9147
2023-11-02 14:41:49,333:INFO:  Epoch 124/500:  train Loss: 81.1754   val Loss: 101.2599   time: 0.13s   best: 90.9147
2023-11-02 14:41:49,468:INFO:  Epoch 125/500:  train Loss: 80.7664   val Loss: 101.1791   time: 0.13s   best: 90.9147
2023-11-02 14:41:49,626:INFO:  Epoch 126/500:  train Loss: 80.0753   val Loss: 100.6120   time: 0.16s   best: 90.9147
2023-11-02 14:41:49,760:INFO:  Epoch 127/500:  train Loss: 80.7770   val Loss: 99.3613   time: 0.13s   best: 90.9147
2023-11-02 14:41:49,893:INFO:  Epoch 128/500:  train Loss: 79.9709   val Loss: 97.7829   time: 0.13s   best: 90.9147
2023-11-02 14:41:50,036:INFO:  Epoch 129/500:  train Loss: 80.8192   val Loss: 96.7056   time: 0.14s   best: 90.9147
2023-11-02 14:41:50,194:INFO:  Epoch 130/500:  train Loss: 80.4609   val Loss: 96.6776   time: 0.15s   best: 90.9147
2023-11-02 14:41:50,327:INFO:  Epoch 131/500:  train Loss: 80.6399   val Loss: 97.9108   time: 0.13s   best: 90.9147
2023-11-02 14:41:50,461:INFO:  Epoch 132/500:  train Loss: 82.0453   val Loss: 98.3866   time: 0.13s   best: 90.9147
2023-11-02 14:41:50,596:INFO:  Epoch 133/500:  train Loss: 81.9558   val Loss: 99.0095   time: 0.13s   best: 90.9147
2023-11-02 14:41:50,753:INFO:  Epoch 134/500:  train Loss: 82.0115   val Loss: 99.6325   time: 0.15s   best: 90.9147
2023-11-02 14:41:50,888:INFO:  Epoch 135/500:  train Loss: 81.5425   val Loss: 100.1390   time: 0.13s   best: 90.9147
2023-11-02 14:41:51,057:INFO:  Epoch 136/500:  train Loss: 82.2403   val Loss: 100.4763   time: 0.17s   best: 90.9147
2023-11-02 14:41:51,215:INFO:  Epoch 137/500:  train Loss: 82.1420   val Loss: 100.4863   time: 0.15s   best: 90.9147
2023-11-02 14:41:51,349:INFO:  Epoch 138/500:  train Loss: 81.8565   val Loss: 100.1535   time: 0.13s   best: 90.9147
2023-11-02 14:41:51,482:INFO:  Epoch 139/500:  train Loss: 81.8229   val Loss: 99.7237   time: 0.13s   best: 90.9147
2023-11-02 14:41:51,618:INFO:  Epoch 140/500:  train Loss: 82.1310   val Loss: 99.5061   time: 0.13s   best: 90.9147
2023-11-02 14:41:51,771:INFO:  Epoch 141/500:  train Loss: 81.8190   val Loss: 99.5392   time: 0.15s   best: 90.9147
2023-11-02 14:41:51,905:INFO:  Epoch 142/500:  train Loss: 81.9911   val Loss: 99.7655   time: 0.13s   best: 90.9147
2023-11-02 14:41:52,051:INFO:  Epoch 143/500:  train Loss: 82.0143   val Loss: 100.1150   time: 0.14s   best: 90.9147
2023-11-02 14:41:52,186:INFO:  Epoch 144/500:  train Loss: 81.6156   val Loss: 100.4616   time: 0.13s   best: 90.9147
2023-11-02 14:41:52,341:INFO:  Epoch 145/500:  train Loss: 81.5451   val Loss: 100.7941   time: 0.15s   best: 90.9147
2023-11-02 14:41:52,475:INFO:  Epoch 146/500:  train Loss: 81.3626   val Loss: 100.9650   time: 0.13s   best: 90.9147
2023-11-02 14:41:52,610:INFO:  Epoch 147/500:  train Loss: 81.8332   val Loss: 100.7033   time: 0.13s   best: 90.9147
2023-11-02 14:41:52,745:INFO:  Epoch 148/500:  train Loss: 81.3138   val Loss: 100.5333   time: 0.13s   best: 90.9147
2023-11-02 14:41:52,901:INFO:  Epoch 149/500:  train Loss: 81.4644   val Loss: 100.2277   time: 0.15s   best: 90.9147
2023-11-02 14:41:53,071:INFO:  Epoch 150/500:  train Loss: 81.4197   val Loss: 99.7920   time: 0.17s   best: 90.9147
2023-11-02 14:41:53,207:INFO:  Epoch 151/500:  train Loss: 81.0733   val Loss: 99.6875   time: 0.13s   best: 90.9147
2023-11-02 14:41:53,363:INFO:  Epoch 152/500:  train Loss: 81.4107   val Loss: 99.7297   time: 0.15s   best: 90.9147
2023-11-02 14:41:53,497:INFO:  Epoch 153/500:  train Loss: 80.8939   val Loss: 99.6764   time: 0.13s   best: 90.9147
2023-11-02 14:41:53,631:INFO:  Epoch 154/500:  train Loss: 80.5283   val Loss: 99.4128   time: 0.13s   best: 90.9147
2023-11-02 14:41:53,769:INFO:  Epoch 155/500:  train Loss: 78.6032   val Loss: 99.3856   time: 0.13s   best: 90.9147
2023-11-02 14:41:53,923:INFO:  Epoch 156/500:  train Loss: 81.2991   val Loss: 99.8245   time: 0.15s   best: 90.9147
2023-11-02 14:41:54,067:INFO:  Epoch 157/500:  train Loss: 78.9202   val Loss: 97.2210   time: 0.14s   best: 90.9147
2023-11-02 14:41:54,202:INFO:  Epoch 158/500:  train Loss: 80.3547   val Loss: 99.6851   time: 0.13s   best: 90.9147
2023-11-02 14:41:54,356:INFO:  Epoch 159/500:  train Loss: 81.7624   val Loss: 99.6757   time: 0.15s   best: 90.9147
2023-11-02 14:41:54,496:INFO:  Epoch 160/500:  train Loss: 81.6103   val Loss: 99.7432   time: 0.13s   best: 90.9147
2023-11-02 14:41:54,632:INFO:  Epoch 161/500:  train Loss: 81.6275   val Loss: 99.8522   time: 0.13s   best: 90.9147
2023-11-02 14:41:54,765:INFO:  Epoch 162/500:  train Loss: 81.7036   val Loss: 100.2324   time: 0.13s   best: 90.9147
2023-11-02 14:41:54,922:INFO:  Epoch 163/500:  train Loss: 81.8890   val Loss: 100.3288   time: 0.15s   best: 90.9147
2023-11-02 14:41:55,071:INFO:  Epoch 164/500:  train Loss: 81.6971   val Loss: 100.1140   time: 0.13s   best: 90.9147
2023-11-02 14:41:55,262:INFO:  Epoch 165/500:  train Loss: 81.7609   val Loss: 99.7728   time: 0.19s   best: 90.9147
2023-11-02 14:41:55,422:INFO:  Epoch 166/500:  train Loss: 81.7453   val Loss: 99.4242   time: 0.15s   best: 90.9147
2023-11-02 14:41:55,565:INFO:  Epoch 167/500:  train Loss: 81.8240   val Loss: 99.1525   time: 0.14s   best: 90.9147
2023-11-02 14:41:55,699:INFO:  Epoch 168/500:  train Loss: 81.7520   val Loss: 99.3848   time: 0.13s   best: 90.9147
2023-11-02 14:41:55,832:INFO:  Epoch 169/500:  train Loss: 81.9111   val Loss: 99.9895   time: 0.13s   best: 90.9147
2023-11-02 14:41:56,001:INFO:  Epoch 170/500:  train Loss: 81.5227   val Loss: 100.2374   time: 0.17s   best: 90.9147
2023-11-02 14:41:56,143:INFO:  Epoch 171/500:  train Loss: 81.5935   val Loss: 100.2833   time: 0.14s   best: 90.9147
2023-11-02 14:41:56,276:INFO:  Epoch 172/500:  train Loss: 81.1051   val Loss: 100.0115   time: 0.13s   best: 90.9147
2023-11-02 14:41:56,409:INFO:  Epoch 173/500:  train Loss: 81.1745   val Loss: 99.7451   time: 0.13s   best: 90.9147
2023-11-02 14:41:56,565:INFO:  Epoch 174/500:  train Loss: 80.5538   val Loss: 99.8962   time: 0.15s   best: 90.9147
2023-11-02 14:41:56,699:INFO:  Epoch 175/500:  train Loss: 80.0887   val Loss: 99.3571   time: 0.13s   best: 90.9147
2023-11-02 14:41:56,832:INFO:  Epoch 176/500:  train Loss: 81.0601   val Loss: 97.9271   time: 0.13s   best: 90.9147
2023-11-02 14:41:56,967:INFO:  Epoch 177/500:  train Loss: 79.7183   val Loss: 98.2770   time: 0.13s   best: 90.9147
2023-11-02 14:41:57,127:INFO:  Epoch 178/500:  train Loss: 80.9435   val Loss: 98.0631   time: 0.16s   best: 90.9147
2023-11-02 14:41:57,296:INFO:  Epoch 179/500:  train Loss: 80.7967   val Loss: 98.2394   time: 0.17s   best: 90.9147
2023-11-02 14:41:57,433:INFO:  Epoch 180/500:  train Loss: 81.2586   val Loss: 98.4119   time: 0.13s   best: 90.9147
2023-11-02 14:41:57,587:INFO:  Epoch 181/500:  train Loss: 79.9754   val Loss: 97.4484   time: 0.15s   best: 90.9147
2023-11-02 14:41:57,723:INFO:  Epoch 182/500:  train Loss: 79.4039   val Loss: 96.8815   time: 0.13s   best: 90.9147
2023-11-02 14:41:57,856:INFO:  Epoch 183/500:  train Loss: 80.1620   val Loss: 99.8981   time: 0.13s   best: 90.9147
2023-11-02 14:41:57,992:INFO:  Epoch 184/500:  train Loss: 81.6523   val Loss: 100.1884   time: 0.13s   best: 90.9147
2023-11-02 14:41:58,155:INFO:  Epoch 185/500:  train Loss: 81.5205   val Loss: 100.0868   time: 0.16s   best: 90.9147
2023-11-02 14:41:58,288:INFO:  Epoch 186/500:  train Loss: 81.9800   val Loss: 99.8469   time: 0.13s   best: 90.9147
2023-11-02 14:41:58,422:INFO:  Epoch 187/500:  train Loss: 82.0142   val Loss: 99.8071   time: 0.13s   best: 90.9147
2023-11-02 14:41:58,556:INFO:  Epoch 188/500:  train Loss: 82.1946   val Loss: 99.6288   time: 0.13s   best: 90.9147
2023-11-02 14:41:58,724:INFO:  Epoch 189/500:  train Loss: 82.2246   val Loss: 99.9808   time: 0.17s   best: 90.9147
2023-11-02 14:41:58,857:INFO:  Epoch 190/500:  train Loss: 82.0521   val Loss: 100.5034   time: 0.13s   best: 90.9147
2023-11-02 14:41:58,991:INFO:  Epoch 191/500:  train Loss: 81.9896   val Loss: 100.6752   time: 0.13s   best: 90.9147
2023-11-02 14:41:59,148:INFO:  Epoch 192/500:  train Loss: 82.1476   val Loss: 100.4359   time: 0.15s   best: 90.9147
2023-11-02 14:41:59,281:INFO:  Epoch 193/500:  train Loss: 82.1523   val Loss: 99.7997   time: 0.13s   best: 90.9147
2023-11-02 14:41:59,455:INFO:  Epoch 194/500:  train Loss: 82.0405   val Loss: 99.4085   time: 0.17s   best: 90.9147
2023-11-02 14:41:59,592:INFO:  Epoch 195/500:  train Loss: 82.0067   val Loss: 99.3584   time: 0.14s   best: 90.9147
2023-11-02 14:41:59,749:INFO:  Epoch 196/500:  train Loss: 82.1692   val Loss: 99.4104   time: 0.15s   best: 90.9147
2023-11-02 14:41:59,883:INFO:  Epoch 197/500:  train Loss: 81.8079   val Loss: 99.7631   time: 0.13s   best: 90.9147
2023-11-02 14:42:00,020:INFO:  Epoch 198/500:  train Loss: 81.6763   val Loss: 100.2401   time: 0.13s   best: 90.9147
2023-11-02 14:42:00,159:INFO:  Epoch 199/500:  train Loss: 81.4081   val Loss: 100.5893   time: 0.14s   best: 90.9147
2023-11-02 14:42:00,316:INFO:  Epoch 200/500:  train Loss: 80.9803   val Loss: 100.0618   time: 0.15s   best: 90.9147
2023-11-02 14:42:00,450:INFO:  Epoch 201/500:  train Loss: 81.0880   val Loss: 98.8823   time: 0.13s   best: 90.9147
2023-11-02 14:42:00,585:INFO:  Epoch 202/500:  train Loss: 81.3728   val Loss: 98.2082   time: 0.13s   best: 90.9147
2023-11-02 14:42:00,742:INFO:  Epoch 203/500:  train Loss: 81.2492   val Loss: 98.2022   time: 0.15s   best: 90.9147
2023-11-02 14:42:00,875:INFO:  Epoch 204/500:  train Loss: 79.6001   val Loss: 99.0868   time: 0.13s   best: 90.9147
2023-11-02 14:42:01,010:INFO:  Epoch 205/500:  train Loss: 81.6465   val Loss: 97.6820   time: 0.13s   best: 90.9147
2023-11-02 14:42:01,144:INFO:  Epoch 206/500:  train Loss: 79.4499   val Loss: 98.4890   time: 0.13s   best: 90.9147
2023-11-02 14:42:01,301:INFO:  Epoch 207/500:  train Loss: 79.6616   val Loss: 98.7002   time: 0.15s   best: 90.9147
2023-11-02 14:42:01,474:INFO:  Epoch 208/500:  train Loss: 78.8078   val Loss: 97.6838   time: 0.17s   best: 90.9147
2023-11-02 14:42:01,610:INFO:  Epoch 209/500:  train Loss: 79.2638   val Loss: 99.3924   time: 0.13s   best: 90.9147
2023-11-02 14:42:01,745:INFO:  Epoch 210/500:  train Loss: 81.3113   val Loss: 99.7604   time: 0.13s   best: 90.9147
2023-11-02 14:42:01,900:INFO:  Epoch 211/500:  train Loss: 81.1094   val Loss: 100.0222   time: 0.15s   best: 90.9147
2023-11-02 14:42:02,038:INFO:  Epoch 212/500:  train Loss: 81.2581   val Loss: 99.8321   time: 0.14s   best: 90.9147
2023-11-02 14:42:02,178:INFO:  Epoch 213/500:  train Loss: 81.5394   val Loss: 99.7583   time: 0.14s   best: 90.9147
2023-11-02 14:42:02,334:INFO:  Epoch 214/500:  train Loss: 81.3560   val Loss: 99.9069   time: 0.15s   best: 90.9147
2023-11-02 14:42:02,467:INFO:  Epoch 215/500:  train Loss: 81.3240   val Loss: 100.2923   time: 0.13s   best: 90.9147
2023-11-02 14:42:02,601:INFO:  Epoch 216/500:  train Loss: 81.0091   val Loss: 100.5708   time: 0.13s   best: 90.9147
2023-11-02 14:42:02,735:INFO:  Epoch 217/500:  train Loss: 80.8718   val Loss: 100.1072   time: 0.13s   best: 90.9147
2023-11-02 14:42:02,890:INFO:  Epoch 218/500:  train Loss: 79.9513   val Loss: 100.3296   time: 0.15s   best: 90.9147
2023-11-02 14:42:03,024:INFO:  Epoch 219/500:  train Loss: 82.0023   val Loss: 95.9052   time: 0.13s   best: 90.9147
2023-11-02 14:42:03,159:INFO:  Epoch 220/500:  train Loss: 82.9730   val Loss: 93.7996   time: 0.13s   best: 90.9147
2023-11-02 14:42:03,293:INFO:  Epoch 221/500:  train Loss: 84.1822   val Loss: 94.2593   time: 0.13s   best: 90.9147
2023-11-02 14:42:03,451:INFO:  Epoch 222/500:  train Loss: 83.7853   val Loss: 96.6310   time: 0.16s   best: 90.9147
2023-11-02 14:42:03,624:INFO:  Epoch 223/500:  train Loss: 82.7129   val Loss: 100.3271   time: 0.17s   best: 90.9147
2023-11-02 14:42:03,759:INFO:  Epoch 224/500:  train Loss: 81.0577   val Loss: 102.9740   time: 0.13s   best: 90.9147
2023-11-02 14:42:03,893:INFO:  Epoch 225/500:  train Loss: 77.9251   val Loss: 106.0185   time: 0.13s   best: 90.9147
2023-11-02 14:42:04,052:INFO:  Epoch 226/500:  train Loss: 82.7872   val Loss: 99.0393   time: 0.16s   best: 90.9147
2023-11-02 14:42:04,191:INFO:  Epoch 227/500:  train Loss: 81.9792   val Loss: 97.2991   time: 0.14s   best: 90.9147
2023-11-02 14:42:04,327:INFO:  Epoch 228/500:  train Loss: 83.5423   val Loss: 98.9182   time: 0.13s   best: 90.9147
2023-11-02 14:42:04,482:INFO:  Epoch 229/500:  train Loss: 83.1058   val Loss: 102.3749   time: 0.15s   best: 90.9147
2023-11-02 14:42:04,617:INFO:  Epoch 230/500:  train Loss: 82.3382   val Loss: 103.4520   time: 0.13s   best: 90.9147
2023-11-02 14:42:04,750:INFO:  Epoch 231/500:  train Loss: 82.1953   val Loss: 100.9649   time: 0.13s   best: 90.9147
2023-11-02 14:42:04,882:INFO:  Epoch 232/500:  train Loss: 82.5900   val Loss: 98.3192   time: 0.13s   best: 90.9147
2023-11-02 14:42:05,041:INFO:  Epoch 233/500:  train Loss: 83.2074   val Loss: 98.6301   time: 0.16s   best: 90.9147
2023-11-02 14:42:05,176:INFO:  Epoch 234/500:  train Loss: 82.7254   val Loss: 100.9489   time: 0.13s   best: 90.9147
2023-11-02 14:42:05,309:INFO:  Epoch 235/500:  train Loss: 82.0661   val Loss: 101.7010   time: 0.13s   best: 90.9147
2023-11-02 14:42:05,444:INFO:  Epoch 236/500:  train Loss: 81.6067   val Loss: 100.4940   time: 0.13s   best: 90.9147
2023-11-02 14:42:05,640:INFO:  Epoch 237/500:  train Loss: 81.7760   val Loss: 100.0353   time: 0.19s   best: 90.9147
2023-11-02 14:42:05,773:INFO:  Epoch 238/500:  train Loss: 81.3196   val Loss: 100.2030   time: 0.13s   best: 90.9147
2023-11-02 14:42:05,907:INFO:  Epoch 239/500:  train Loss: 81.6662   val Loss: 101.2284   time: 0.13s   best: 90.9147
2023-11-02 14:42:06,045:INFO:  Epoch 240/500:  train Loss: 81.4013   val Loss: 101.5073   time: 0.14s   best: 90.9147
2023-11-02 14:42:06,205:INFO:  Epoch 241/500:  train Loss: 81.3240   val Loss: 100.5568   time: 0.16s   best: 90.9147
2023-11-02 14:42:06,341:INFO:  Epoch 242/500:  train Loss: 81.1614   val Loss: 100.5369   time: 0.13s   best: 90.9147
2023-11-02 14:42:06,474:INFO:  Epoch 243/500:  train Loss: 80.8593   val Loss: 98.1682   time: 0.13s   best: 90.9147
2023-11-02 14:42:06,632:INFO:  Epoch 244/500:  train Loss: 82.7968   val Loss: 96.9653   time: 0.16s   best: 90.9147
2023-11-02 14:42:06,767:INFO:  Epoch 245/500:  train Loss: 82.6967   val Loss: 99.0158   time: 0.13s   best: 90.9147
2023-11-02 14:42:06,900:INFO:  Epoch 246/500:  train Loss: 82.1007   val Loss: 100.6557   time: 0.13s   best: 90.9147
2023-11-02 14:42:07,034:INFO:  Epoch 247/500:  train Loss: 81.0825   val Loss: 100.5051   time: 0.13s   best: 90.9147
2023-11-02 14:42:07,190:INFO:  Epoch 248/500:  train Loss: 81.4867   val Loss: 99.9896   time: 0.15s   best: 90.9147
2023-11-02 14:42:07,323:INFO:  Epoch 249/500:  train Loss: 81.3414   val Loss: 100.0205   time: 0.13s   best: 90.9147
2023-11-02 14:42:07,458:INFO:  Epoch 250/500:  train Loss: 81.0092   val Loss: 100.9812   time: 0.13s   best: 90.9147
2023-11-02 14:42:07,592:INFO:  Epoch 251/500:  train Loss: 81.0704   val Loss: 102.3233   time: 0.13s   best: 90.9147
2023-11-02 14:42:07,786:INFO:  Epoch 252/500:  train Loss: 80.9166   val Loss: 101.9172   time: 0.19s   best: 90.9147
2023-11-02 14:42:07,919:INFO:  Epoch 253/500:  train Loss: 80.6674   val Loss: 100.8450   time: 0.13s   best: 90.9147
2023-11-02 14:42:08,056:INFO:  Epoch 254/500:  train Loss: 80.7593   val Loss: 100.7453   time: 0.13s   best: 90.9147
2023-11-02 14:42:08,196:INFO:  Epoch 255/500:  train Loss: 80.3147   val Loss: 100.7667   time: 0.14s   best: 90.9147
2023-11-02 14:42:08,355:INFO:  Epoch 256/500:  train Loss: 80.1100   val Loss: 101.2788   time: 0.16s   best: 90.9147
2023-11-02 14:42:08,489:INFO:  Epoch 257/500:  train Loss: 79.9707   val Loss: 101.1930   time: 0.13s   best: 90.9147
2023-11-02 14:42:08,624:INFO:  Epoch 258/500:  train Loss: 79.7749   val Loss: 100.1915   time: 0.13s   best: 90.9147
2023-11-02 14:42:08,780:INFO:  Epoch 259/500:  train Loss: 78.6446   val Loss: 99.6075   time: 0.15s   best: 90.9147
2023-11-02 14:42:08,913:INFO:  Epoch 260/500:  train Loss: 78.2913   val Loss: 100.2980   time: 0.13s   best: 90.9147
2023-11-02 14:42:09,049:INFO:  Epoch 261/500:  train Loss: 78.0543   val Loss: 102.2154   time: 0.13s   best: 90.9147
2023-11-02 14:42:09,186:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:42:09,208:INFO:  Epoch 262/500:  train Loss: 89.7651   val Loss: 90.6576   time: 0.13s   best: 90.6576
2023-11-02 14:42:09,361:INFO:  Epoch 263/500:  train Loss: 85.5480   val Loss: 94.2895   time: 0.15s   best: 90.6576
2023-11-02 14:42:09,496:INFO:  Epoch 264/500:  train Loss: 85.7785   val Loss: 99.1398   time: 0.13s   best: 90.6576
2023-11-02 14:42:09,629:INFO:  Epoch 265/500:  train Loss: 84.1431   val Loss: 105.7525   time: 0.13s   best: 90.6576
2023-11-02 14:42:09,822:INFO:  Epoch 266/500:  train Loss: 82.3916   val Loss: 105.7886   time: 0.17s   best: 90.6576
2023-11-02 14:42:09,955:INFO:  Epoch 267/500:  train Loss: 82.3004   val Loss: 104.7036   time: 0.13s   best: 90.6576
2023-11-02 14:42:10,095:INFO:  Epoch 268/500:  train Loss: 82.2489   val Loss: 104.6679   time: 0.14s   best: 90.6576
2023-11-02 14:42:10,229:INFO:  Epoch 269/500:  train Loss: 81.7814   val Loss: 102.8747   time: 0.13s   best: 90.6576
2023-11-02 14:42:10,386:INFO:  Epoch 270/500:  train Loss: 81.1110   val Loss: 117.6906   time: 0.15s   best: 90.6576
2023-11-02 14:42:10,519:INFO:  Epoch 271/500:  train Loss: 159.5783   val Loss: 101.7523   time: 0.13s   best: 90.6576
2023-11-02 14:42:10,655:INFO:  Epoch 272/500:  train Loss: 101.7669   val Loss: 101.8369   time: 0.13s   best: 90.6576
2023-11-02 14:42:10,788:INFO:  Epoch 273/500:  train Loss: 99.1353   val Loss: 110.2668   time: 0.13s   best: 90.6576
2023-11-02 14:42:10,944:INFO:  Epoch 274/500:  train Loss: 109.8721   val Loss: 99.3145   time: 0.15s   best: 90.6576
2023-11-02 14:42:11,077:INFO:  Epoch 275/500:  train Loss: 104.2861   val Loss: 102.9000   time: 0.13s   best: 90.6576
2023-11-02 14:42:11,212:INFO:  Epoch 276/500:  train Loss: 103.6727   val Loss: 102.2928   time: 0.13s   best: 90.6576
2023-11-02 14:42:11,347:INFO:  Epoch 277/500:  train Loss: 102.0409   val Loss: 103.2608   time: 0.13s   best: 90.6576
2023-11-02 14:42:11,504:INFO:  Epoch 278/500:  train Loss: 103.0666   val Loss: 102.8887   time: 0.15s   best: 90.6576
2023-11-02 14:42:11,638:INFO:  Epoch 279/500:  train Loss: 102.2763   val Loss: 101.6823   time: 0.13s   best: 90.6576
2023-11-02 14:42:11,808:INFO:  Epoch 280/500:  train Loss: 103.3356   val Loss: 102.6214   time: 0.16s   best: 90.6576
2023-11-02 14:42:11,964:INFO:  Epoch 281/500:  train Loss: 103.0268   val Loss: 103.5605   time: 0.15s   best: 90.6576
2023-11-02 14:42:12,105:INFO:  Epoch 282/500:  train Loss: 103.9113   val Loss: 103.3617   time: 0.14s   best: 90.6576
2023-11-02 14:42:12,238:INFO:  Epoch 283/500:  train Loss: 103.5239   val Loss: 102.5508   time: 0.13s   best: 90.6576
2023-11-02 14:42:12,373:INFO:  Epoch 284/500:  train Loss: 102.4388   val Loss: 101.7136   time: 0.13s   best: 90.6576
2023-11-02 14:42:12,529:INFO:  Epoch 285/500:  train Loss: 101.5464   val Loss: 101.3142   time: 0.15s   best: 90.6576
2023-11-02 14:42:12,662:INFO:  Epoch 286/500:  train Loss: 101.2007   val Loss: 101.1395   time: 0.13s   best: 90.6576
2023-11-02 14:42:12,796:INFO:  Epoch 287/500:  train Loss: 100.6682   val Loss: 96.9563   time: 0.13s   best: 90.6576
2023-11-02 14:42:12,930:INFO:  Epoch 288/500:  train Loss: 115.4790   val Loss: 95.5099   time: 0.13s   best: 90.6576
2023-11-02 14:42:13,084:INFO:  Epoch 289/500:  train Loss: 100.6535   val Loss: 101.4127   time: 0.15s   best: 90.6576
2023-11-02 14:42:13,219:INFO:  Epoch 290/500:  train Loss: 101.7122   val Loss: 101.8392   time: 0.13s   best: 90.6576
2023-11-02 14:42:13,353:INFO:  Epoch 291/500:  train Loss: 102.0284   val Loss: 101.8615   time: 0.13s   best: 90.6576
2023-11-02 14:42:13,487:INFO:  Epoch 292/500:  train Loss: 101.9863   val Loss: 101.7005   time: 0.13s   best: 90.6576
2023-11-02 14:42:13,643:INFO:  Epoch 293/500:  train Loss: 101.7889   val Loss: 101.4858   time: 0.15s   best: 90.6576
2023-11-02 14:42:13,777:INFO:  Epoch 294/500:  train Loss: 101.5645   val Loss: 101.2714   time: 0.13s   best: 90.6576
2023-11-02 14:42:13,958:INFO:  Epoch 295/500:  train Loss: 101.3407   val Loss: 101.0467   time: 0.18s   best: 90.6576
2023-11-02 14:42:14,117:INFO:  Epoch 296/500:  train Loss: 101.0946   val Loss: 100.7762   time: 0.16s   best: 90.6576
2023-11-02 14:42:14,257:INFO:  Epoch 297/500:  train Loss: 100.5372   val Loss: 97.8218   time: 0.14s   best: 90.6576
2023-11-02 14:42:14,392:INFO:  Epoch 298/500:  train Loss: 95.6060   val Loss: 104.0372   time: 0.13s   best: 90.6576
2023-11-02 14:42:14,526:INFO:  Epoch 299/500:  train Loss: 107.5001   val Loss: 102.6101   time: 0.13s   best: 90.6576
2023-11-02 14:42:14,682:INFO:  Epoch 300/500:  train Loss: 102.3988   val Loss: 97.3918   time: 0.15s   best: 90.6576
2023-11-02 14:42:14,816:INFO:  Epoch 301/500:  train Loss: 96.1047   val Loss: 95.0333   time: 0.13s   best: 90.6576
2023-11-02 14:42:14,949:INFO:  Epoch 302/500:  train Loss: 93.4311   val Loss: 104.5449   time: 0.13s   best: 90.6576
2023-11-02 14:42:15,083:INFO:  Epoch 303/500:  train Loss: 95.7370   val Loss: 105.4063   time: 0.13s   best: 90.6576
2023-11-02 14:42:15,241:INFO:  Epoch 304/500:  train Loss: 92.6494   val Loss: 100.3774   time: 0.16s   best: 90.6576
2023-11-02 14:42:15,375:INFO:  Epoch 305/500:  train Loss: 90.3541   val Loss: 98.2298   time: 0.13s   best: 90.6576
2023-11-02 14:42:15,509:INFO:  Epoch 306/500:  train Loss: 88.3920   val Loss: 98.8655   time: 0.13s   best: 90.6576
2023-11-02 14:42:15,644:INFO:  Epoch 307/500:  train Loss: 90.6568   val Loss: 106.3382   time: 0.13s   best: 90.6576
2023-11-02 14:42:15,800:INFO:  Epoch 308/500:  train Loss: 90.0044   val Loss: 110.9724   time: 0.15s   best: 90.6576
2023-11-02 14:42:15,970:INFO:  Epoch 309/500:  train Loss: 88.6814   val Loss: 104.4308   time: 0.17s   best: 90.6576
2023-11-02 14:42:16,108:INFO:  Epoch 310/500:  train Loss: 88.4304   val Loss: 100.7596   time: 0.14s   best: 90.6576
2023-11-02 14:42:16,266:INFO:  Epoch 311/500:  train Loss: 88.0976   val Loss: 100.6608   time: 0.15s   best: 90.6576
2023-11-02 14:42:16,400:INFO:  Epoch 312/500:  train Loss: 86.3962   val Loss: 99.1074   time: 0.13s   best: 90.6576
2023-11-02 14:42:16,534:INFO:  Epoch 313/500:  train Loss: 85.9831   val Loss: 98.6846   time: 0.13s   best: 90.6576
2023-11-02 14:42:16,670:INFO:  Epoch 314/500:  train Loss: 110.8627   val Loss: 94.1023   time: 0.13s   best: 90.6576
2023-11-02 14:42:16,825:INFO:  Epoch 315/500:  train Loss: 96.9963   val Loss: 93.7553   time: 0.15s   best: 90.6576
2023-11-02 14:42:16,958:INFO:  Epoch 316/500:  train Loss: 90.2078   val Loss: 113.8900   time: 0.13s   best: 90.6576
2023-11-02 14:42:17,092:INFO:  Epoch 317/500:  train Loss: 87.4771   val Loss: 102.1548   time: 0.13s   best: 90.6576
2023-11-02 14:42:17,227:INFO:  Epoch 318/500:  train Loss: 89.2851   val Loss: 95.2322   time: 0.13s   best: 90.6576
2023-11-02 14:42:17,385:INFO:  Epoch 319/500:  train Loss: 88.2558   val Loss: 102.3479   time: 0.15s   best: 90.6576
2023-11-02 14:42:17,518:INFO:  Epoch 320/500:  train Loss: 87.3192   val Loss: 108.2643   time: 0.13s   best: 90.6576
2023-11-02 14:42:17,652:INFO:  Epoch 321/500:  train Loss: 85.8382   val Loss: 100.0035   time: 0.13s   best: 90.6576
2023-11-02 14:42:17,809:INFO:  Epoch 322/500:  train Loss: 86.5981   val Loss: 98.5327   time: 0.15s   best: 90.6576
2023-11-02 14:42:17,942:INFO:  Epoch 323/500:  train Loss: 83.8555   val Loss: 107.9217   time: 0.13s   best: 90.6576
2023-11-02 14:42:18,116:INFO:  Epoch 324/500:  train Loss: 81.3931   val Loss: 104.4620   time: 0.17s   best: 90.6576
2023-11-02 14:42:18,251:INFO:  Epoch 325/500:  train Loss: 80.7951   val Loss: 98.4665   time: 0.13s   best: 90.6576
2023-11-02 14:42:18,410:INFO:  Epoch 326/500:  train Loss: 84.5230   val Loss: 100.9913   time: 0.15s   best: 90.6576
2023-11-02 14:42:18,542:INFO:  Epoch 327/500:  train Loss: 121.8574   val Loss: 102.9574   time: 0.13s   best: 90.6576
2023-11-02 14:42:18,677:INFO:  Epoch 328/500:  train Loss: 103.2220   val Loss: 103.0489   time: 0.13s   best: 90.6576
2023-11-02 14:42:18,812:INFO:  Epoch 329/500:  train Loss: 103.3291   val Loss: 102.9989   time: 0.13s   best: 90.6576
2023-11-02 14:42:18,968:INFO:  Epoch 330/500:  train Loss: 103.3851   val Loss: 102.9110   time: 0.15s   best: 90.6576
2023-11-02 14:42:19,103:INFO:  Epoch 331/500:  train Loss: 103.3665   val Loss: 102.7759   time: 0.13s   best: 90.6576
2023-11-02 14:42:19,323:INFO:  Epoch 332/500:  train Loss: 103.2761   val Loss: 102.5986   time: 0.22s   best: 90.6576
2023-11-02 14:42:19,477:INFO:  Epoch 333/500:  train Loss: 103.0480   val Loss: 102.3793   time: 0.15s   best: 90.6576
2023-11-02 14:42:19,674:INFO:  Epoch 334/500:  train Loss: 102.8038   val Loss: 102.0700   time: 0.18s   best: 90.6576
2023-11-02 14:42:19,825:INFO:  Epoch 335/500:  train Loss: 102.0154   val Loss: 98.1624   time: 0.14s   best: 90.6576
2023-11-02 14:42:19,987:INFO:  Epoch 336/500:  train Loss: 96.6537   val Loss: 106.9994   time: 0.15s   best: 90.6576
2023-11-02 14:42:20,170:INFO:  Epoch 337/500:  train Loss: 93.7641   val Loss: 102.2439   time: 0.17s   best: 90.6576
2023-11-02 14:42:20,313:INFO:  Epoch 338/500:  train Loss: 93.4877   val Loss: 97.9898   time: 0.13s   best: 90.6576
2023-11-02 14:42:20,481:INFO:  Epoch 339/500:  train Loss: 91.9878   val Loss: 105.7802   time: 0.15s   best: 90.6576
2023-11-02 14:42:20,625:INFO:  Epoch 340/500:  train Loss: 87.9022   val Loss: 96.6214   time: 0.13s   best: 90.6576
2023-11-02 14:42:20,769:INFO:  Epoch 341/500:  train Loss: 88.4328   val Loss: 100.5306   time: 0.13s   best: 90.6576
2023-11-02 14:42:20,912:INFO:  Epoch 342/500:  train Loss: 97.5397   val Loss: 112.2886   time: 0.13s   best: 90.6576
2023-11-02 14:42:21,074:INFO:  Epoch 343/500:  train Loss: 87.6631   val Loss: 103.4830   time: 0.15s   best: 90.6576
2023-11-02 14:42:21,218:INFO:  Epoch 344/500:  train Loss: 90.5220   val Loss: 98.9247   time: 0.13s   best: 90.6576
2023-11-02 14:42:21,364:INFO:  Epoch 345/500:  train Loss: 91.2215   val Loss: 103.4965   time: 0.13s   best: 90.6576
2023-11-02 14:42:21,527:INFO:  Epoch 346/500:  train Loss: 89.9197   val Loss: 106.5230   time: 0.15s   best: 90.6576
2023-11-02 14:42:21,670:INFO:  Epoch 347/500:  train Loss: 89.4342   val Loss: 104.1816   time: 0.13s   best: 90.6576
2023-11-02 14:42:21,814:INFO:  Epoch 348/500:  train Loss: 89.4548   val Loss: 103.7939   time: 0.13s   best: 90.6576
2023-11-02 14:42:21,957:INFO:  Epoch 349/500:  train Loss: 89.4603   val Loss: 104.9771   time: 0.13s   best: 90.6576
2023-11-02 14:42:22,124:INFO:  Epoch 350/500:  train Loss: 88.1315   val Loss: 103.5741   time: 0.16s   best: 90.6576
2023-11-02 14:42:22,303:INFO:  Epoch 351/500:  train Loss: 87.1516   val Loss: 102.4571   time: 0.17s   best: 90.6576
2023-11-02 14:42:22,449:INFO:  Epoch 352/500:  train Loss: 85.7253   val Loss: 100.0288   time: 0.13s   best: 90.6576
2023-11-02 14:42:22,614:INFO:  Epoch 353/500:  train Loss: 84.6184   val Loss: 102.4820   time: 0.15s   best: 90.6576
2023-11-02 14:42:22,758:INFO:  Epoch 354/500:  train Loss: 83.6059   val Loss: 113.6660   time: 0.13s   best: 90.6576
2023-11-02 14:42:22,902:INFO:  Epoch 355/500:  train Loss: 81.0150   val Loss: 97.6540   time: 0.13s   best: 90.6576
2023-11-02 14:42:23,064:INFO:  Epoch 356/500:  train Loss: 85.3649   val Loss: 97.8006   time: 0.13s   best: 90.6576
2023-11-02 14:42:23,207:INFO:  Epoch 357/500:  train Loss: 83.9576   val Loss: 107.4488   time: 0.13s   best: 90.6576
2023-11-02 14:42:23,354:INFO:  Epoch 358/500:  train Loss: 82.5753   val Loss: 113.3376   time: 0.14s   best: 90.6576
2023-11-02 14:42:23,497:INFO:  Epoch 359/500:  train Loss: 81.8538   val Loss: 98.1631   time: 0.13s   best: 90.6576
2023-11-02 14:42:23,660:INFO:  Epoch 360/500:  train Loss: 83.5561   val Loss: 101.3074   time: 0.15s   best: 90.6576
2023-11-02 14:42:23,805:INFO:  Epoch 361/500:  train Loss: 82.2928   val Loss: 107.6322   time: 0.13s   best: 90.6576
2023-11-02 14:42:23,947:INFO:  Epoch 362/500:  train Loss: 81.3607   val Loss: 118.2096   time: 0.13s   best: 90.6576
2023-11-02 14:42:24,092:INFO:  Epoch 363/500:  train Loss: 98.6892   val Loss: 101.8102   time: 0.13s   best: 90.6576
2023-11-02 14:42:24,291:INFO:  Epoch 364/500:  train Loss: 102.3724   val Loss: 102.3049   time: 0.19s   best: 90.6576
2023-11-02 14:42:24,441:INFO:  Epoch 365/500:  train Loss: 102.8369   val Loss: 102.5535   time: 0.14s   best: 90.6576
2023-11-02 14:42:24,584:INFO:  Epoch 366/500:  train Loss: 103.0381   val Loss: 102.6453   time: 0.13s   best: 90.6576
2023-11-02 14:42:24,747:INFO:  Epoch 367/500:  train Loss: 103.1027   val Loss: 102.6513   time: 0.15s   best: 90.6576
2023-11-02 14:42:24,892:INFO:  Epoch 368/500:  train Loss: 103.0676   val Loss: 102.6107   time: 0.13s   best: 90.6576
2023-11-02 14:42:25,035:INFO:  Epoch 369/500:  train Loss: 102.9940   val Loss: 102.5449   time: 0.13s   best: 90.6576
2023-11-02 14:42:25,196:INFO:  Epoch 370/500:  train Loss: 102.8635   val Loss: 102.4644   time: 0.15s   best: 90.6576
2023-11-02 14:42:25,344:INFO:  Epoch 371/500:  train Loss: 102.7331   val Loss: 102.3760   time: 0.13s   best: 90.6576
2023-11-02 14:42:25,488:INFO:  Epoch 372/500:  train Loss: 102.6223   val Loss: 102.2833   time: 0.13s   best: 90.6576
2023-11-02 14:42:25,631:INFO:  Epoch 373/500:  train Loss: 102.4752   val Loss: 102.1878   time: 0.13s   best: 90.6576
2023-11-02 14:42:25,797:INFO:  Epoch 374/500:  train Loss: 102.3497   val Loss: 102.0899   time: 0.15s   best: 90.6576
2023-11-02 14:42:25,940:INFO:  Epoch 375/500:  train Loss: 102.2046   val Loss: 101.9897   time: 0.13s   best: 90.6576
2023-11-02 14:42:26,087:INFO:  Epoch 376/500:  train Loss: 102.0786   val Loss: 101.8869   time: 0.14s   best: 90.6576
2023-11-02 14:42:26,253:INFO:  Epoch 377/500:  train Loss: 101.9235   val Loss: 101.7809   time: 0.13s   best: 90.6576
2023-11-02 14:42:26,472:INFO:  Epoch 378/500:  train Loss: 101.7766   val Loss: 101.6715   time: 0.21s   best: 90.6576
2023-11-02 14:42:26,614:INFO:  Epoch 379/500:  train Loss: 101.6208   val Loss: 101.5576   time: 0.13s   best: 90.6576
2023-11-02 14:42:26,759:INFO:  Epoch 380/500:  train Loss: 101.4567   val Loss: 101.4379   time: 0.13s   best: 90.6576
2023-11-02 14:42:26,922:INFO:  Epoch 381/500:  train Loss: 101.3110   val Loss: 101.3108   time: 0.15s   best: 90.6576
2023-11-02 14:42:27,065:INFO:  Epoch 382/500:  train Loss: 101.1201   val Loss: 101.1749   time: 0.13s   best: 90.6576
2023-11-02 14:42:27,209:INFO:  Epoch 383/500:  train Loss: 100.9422   val Loss: 101.0276   time: 0.13s   best: 90.6576
2023-11-02 14:42:27,372:INFO:  Epoch 384/500:  train Loss: 100.6965   val Loss: 100.8617   time: 0.15s   best: 90.6576
2023-11-02 14:42:27,516:INFO:  Epoch 385/500:  train Loss: 100.1116   val Loss: 98.3092   time: 0.13s   best: 90.6576
2023-11-02 14:42:27,658:INFO:  Epoch 386/500:  train Loss: 100.1364   val Loss: 101.3362   time: 0.13s   best: 90.6576
2023-11-02 14:42:27,802:INFO:  Epoch 387/500:  train Loss: 101.4729   val Loss: 101.5999   time: 0.13s   best: 90.6576
2023-11-02 14:42:27,965:INFO:  Epoch 388/500:  train Loss: 101.8395   val Loss: 101.6965   time: 0.15s   best: 90.6576
2023-11-02 14:42:28,110:INFO:  Epoch 389/500:  train Loss: 101.9630   val Loss: 101.7151   time: 0.13s   best: 90.6576
2023-11-02 14:42:28,255:INFO:  Epoch 390/500:  train Loss: 102.0085   val Loss: 101.6904   time: 0.13s   best: 90.6576
2023-11-02 14:42:28,421:INFO:  Epoch 391/500:  train Loss: 101.9669   val Loss: 101.6396   time: 0.15s   best: 90.6576
2023-11-02 14:42:28,600:INFO:  Epoch 392/500:  train Loss: 101.9164   val Loss: 101.5650   time: 0.17s   best: 90.6576
2023-11-02 14:42:28,744:INFO:  Epoch 393/500:  train Loss: 101.5392   val Loss: 99.5570   time: 0.13s   best: 90.6576
2023-11-02 14:42:28,908:INFO:  Epoch 394/500:  train Loss: 98.5507   val Loss: 95.1553   time: 0.15s   best: 90.6576
2023-11-02 14:42:29,051:INFO:  Epoch 395/500:  train Loss: 91.8896   val Loss: 109.9651   time: 0.13s   best: 90.6576
2023-11-02 14:42:29,194:INFO:  Epoch 396/500:  train Loss: 90.8935   val Loss: 112.2066   time: 0.13s   best: 90.6576
2023-11-02 14:42:29,346:INFO:  Epoch 397/500:  train Loss: 87.9495   val Loss: 100.6526   time: 0.15s   best: 90.6576
2023-11-02 14:42:29,504:INFO:  Epoch 398/500:  train Loss: 88.5546   val Loss: 95.9704   time: 0.15s   best: 90.6576
2023-11-02 14:42:29,662:INFO:  Epoch 399/500:  train Loss: 88.7063   val Loss: 94.8536   time: 0.15s   best: 90.6576
2023-11-02 14:42:29,806:INFO:  Epoch 400/500:  train Loss: 85.5448   val Loss: 96.1937   time: 0.13s   best: 90.6576
2023-11-02 14:42:29,969:INFO:  Epoch 401/500:  train Loss: 82.9012   val Loss: 96.7925   time: 0.15s   best: 90.6576
2023-11-02 14:42:30,115:INFO:  Epoch 402/500:  train Loss: 80.7695   val Loss: 94.6239   time: 0.13s   best: 90.6576
2023-11-02 14:42:30,261:INFO:  Epoch 403/500:  train Loss: 80.4838   val Loss: 92.5460   time: 0.13s   best: 90.6576
2023-11-02 14:42:30,407:INFO:  Epoch 404/500:  train Loss: 81.2093   val Loss: 92.1658   time: 0.13s   best: 90.6576
2023-11-02 14:42:30,604:INFO:  Epoch 405/500:  train Loss: 80.3273   val Loss: 94.0402   time: 0.19s   best: 90.6576
2023-11-02 14:42:30,749:INFO:  Epoch 406/500:  train Loss: 83.1587   val Loss: 95.4096   time: 0.13s   best: 90.6576
2023-11-02 14:42:30,893:INFO:  Epoch 407/500:  train Loss: 86.6968   val Loss: 99.7761   time: 0.13s   best: 90.6576
2023-11-02 14:42:31,057:INFO:  Epoch 408/500:  train Loss: 87.2029   val Loss: 101.6562   time: 0.15s   best: 90.6576
2023-11-02 14:42:31,200:INFO:  Epoch 409/500:  train Loss: 85.8569   val Loss: 101.9319   time: 0.13s   best: 90.6576
2023-11-02 14:42:31,345:INFO:  Epoch 410/500:  train Loss: 84.0421   val Loss: 97.8360   time: 0.13s   best: 90.6576
2023-11-02 14:42:31,489:INFO:  Epoch 411/500:  train Loss: 83.0654   val Loss: 93.8118   time: 0.13s   best: 90.6576
2023-11-02 14:42:31,652:INFO:  Epoch 412/500:  train Loss: 83.7084   val Loss: 94.8044   time: 0.15s   best: 90.6576
2023-11-02 14:42:31,796:INFO:  Epoch 413/500:  train Loss: 82.1308   val Loss: 96.3150   time: 0.13s   best: 90.6576
2023-11-02 14:42:31,939:INFO:  Epoch 414/500:  train Loss: 82.0957   val Loss: 93.3254   time: 0.13s   best: 90.6576
2023-11-02 14:42:32,106:INFO:  Epoch 415/500:  train Loss: 82.4555   val Loss: 91.7354   time: 0.16s   best: 90.6576
2023-11-02 14:42:32,250:INFO:  Epoch 416/500:  train Loss: 82.3093   val Loss: 92.9524   time: 0.13s   best: 90.6576
2023-11-02 14:42:32,395:INFO:  Epoch 417/500:  train Loss: 81.7746   val Loss: 94.6814   time: 0.13s   best: 90.6576
2023-11-02 14:42:32,538:INFO:  Epoch 418/500:  train Loss: 81.7219   val Loss: 92.8101   time: 0.13s   best: 90.6576
2023-11-02 14:42:32,742:INFO:  Epoch 419/500:  train Loss: 81.7885   val Loss: 91.8092   time: 0.19s   best: 90.6576
2023-11-02 14:42:32,885:INFO:  Epoch 420/500:  train Loss: 82.1233   val Loss: 94.3487   time: 0.13s   best: 90.6576
2023-11-02 14:42:33,028:INFO:  Epoch 421/500:  train Loss: 81.2118   val Loss: 95.3925   time: 0.13s   best: 90.6576
2023-11-02 14:42:33,188:INFO:  Epoch 422/500:  train Loss: 80.7550   val Loss: 92.0514   time: 0.15s   best: 90.6576
2023-11-02 14:42:33,332:INFO:  Epoch 423/500:  train Loss: 81.7411   val Loss: 91.3224   time: 0.13s   best: 90.6576
2023-11-02 14:42:33,477:INFO:  Epoch 424/500:  train Loss: 81.3642   val Loss: 93.6382   time: 0.13s   best: 90.6576
2023-11-02 14:42:33,621:INFO:  Epoch 425/500:  train Loss: 80.2495   val Loss: 94.0557   time: 0.13s   best: 90.6576
2023-11-02 14:42:33,784:INFO:  Epoch 426/500:  train Loss: 80.0905   val Loss: 92.7108   time: 0.15s   best: 90.6576
2023-11-02 14:42:33,928:INFO:  Epoch 427/500:  train Loss: 79.9799   val Loss: 91.6329   time: 0.13s   best: 90.6576
2023-11-02 14:42:34,075:INFO:  Epoch 428/500:  train Loss: 79.9648   val Loss: 91.5330   time: 0.13s   best: 90.6576
2023-11-02 14:42:34,240:INFO:  Epoch 429/500:  train Loss: 82.2374   val Loss: 91.3709   time: 0.15s   best: 90.6576
2023-11-02 14:42:34,384:INFO:  Epoch 430/500:  train Loss: 79.5630   val Loss: 92.6730   time: 0.13s   best: 90.6576
2023-11-02 14:42:34,529:INFO:  Epoch 431/500:  train Loss: 79.8113   val Loss: 91.5183   time: 0.13s   best: 90.6576
2023-11-02 14:42:34,672:INFO:  Epoch 432/500:  train Loss: 80.3519   val Loss: 91.6904   time: 0.13s   best: 90.6576
2023-11-02 14:42:34,874:INFO:  Epoch 433/500:  train Loss: 80.4147   val Loss: 92.8478   time: 0.19s   best: 90.6576
2023-11-02 14:42:35,018:INFO:  Epoch 434/500:  train Loss: 79.9190   val Loss: 91.9587   time: 0.13s   best: 90.6576
2023-11-02 14:42:35,162:INFO:  Epoch 435/500:  train Loss: 80.0357   val Loss: 91.3112   time: 0.13s   best: 90.6576
2023-11-02 14:42:35,327:INFO:  Epoch 436/500:  train Loss: 80.4112   val Loss: 91.5362   time: 0.15s   best: 90.6576
2023-11-02 14:42:35,471:INFO:  Epoch 437/500:  train Loss: 80.1120   val Loss: 91.7425   time: 0.13s   best: 90.6576
2023-11-02 14:42:35,615:INFO:  Epoch 438/500:  train Loss: 79.5293   val Loss: 92.0673   time: 0.13s   best: 90.6576
2023-11-02 14:42:35,758:INFO:  Epoch 439/500:  train Loss: 79.2894   val Loss: 91.8734   time: 0.13s   best: 90.6576
2023-11-02 14:42:35,916:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder debug (2 layer + 0.1 dropout)_e4d7.pt
2023-11-02 14:42:35,946:INFO:  Epoch 440/500:  train Loss: 79.4789   val Loss: 88.1390   time: 0.15s   best: 88.1390
2023-11-02 14:42:36,093:INFO:  Epoch 441/500:  train Loss: 89.9088   val Loss: 92.0961   time: 0.14s   best: 88.1390
2023-11-02 14:42:36,238:INFO:  Epoch 442/500:  train Loss: 79.5412   val Loss: 94.4914   time: 0.13s   best: 88.1390
2023-11-02 14:42:36,404:INFO:  Epoch 443/500:  train Loss: 79.1306   val Loss: 93.5033   time: 0.15s   best: 88.1390
2023-11-02 14:42:36,548:INFO:  Epoch 444/500:  train Loss: 80.1209   val Loss: 92.2078   time: 0.13s   best: 88.1390
2023-11-02 14:42:36,691:INFO:  Epoch 445/500:  train Loss: 80.2548   val Loss: 91.1396   time: 0.13s   best: 88.1390
2023-11-02 14:42:36,890:INFO:  Epoch 446/500:  train Loss: 80.5684   val Loss: 91.7973   time: 0.19s   best: 88.1390
2023-11-02 14:42:37,033:INFO:  Epoch 447/500:  train Loss: 79.9561   val Loss: 93.2162   time: 0.13s   best: 88.1390
2023-11-02 14:42:37,177:INFO:  Epoch 448/500:  train Loss: 79.2085   val Loss: 92.9536   time: 0.13s   best: 88.1390
2023-11-02 14:42:37,320:INFO:  Epoch 449/500:  train Loss: 79.3596   val Loss: 92.3617   time: 0.13s   best: 88.1390
2023-11-02 14:42:37,485:INFO:  Epoch 450/500:  train Loss: 79.5334   val Loss: 91.6044   time: 0.15s   best: 88.1390
2023-11-02 14:42:37,629:INFO:  Epoch 451/500:  train Loss: 79.7578   val Loss: 91.1859   time: 0.13s   best: 88.1390
2023-11-02 14:42:37,773:INFO:  Epoch 452/500:  train Loss: 79.5470   val Loss: 91.5320   time: 0.13s   best: 88.1390
2023-11-02 14:42:37,940:INFO:  Epoch 453/500:  train Loss: 79.2313   val Loss: 92.1039   time: 0.13s   best: 88.1390
2023-11-02 14:42:38,085:INFO:  Epoch 454/500:  train Loss: 79.1467   val Loss: 92.3990   time: 0.13s   best: 88.1390
2023-11-02 14:42:38,230:INFO:  Epoch 455/500:  train Loss: 79.2831   val Loss: 92.2401   time: 0.13s   best: 88.1390
2023-11-02 14:42:38,373:INFO:  Epoch 456/500:  train Loss: 78.9625   val Loss: 91.7124   time: 0.13s   best: 88.1390
2023-11-02 14:42:38,540:INFO:  Epoch 457/500:  train Loss: 79.1881   val Loss: 91.5466   time: 0.15s   best: 88.1390
2023-11-02 14:42:38,682:INFO:  Epoch 458/500:  train Loss: 78.7087   val Loss: 92.0546   time: 0.13s   best: 88.1390
2023-11-02 14:42:38,826:INFO:  Epoch 459/500:  train Loss: 79.0152   val Loss: 92.2704   time: 0.13s   best: 88.1390
2023-11-02 14:42:39,026:INFO:  Epoch 460/500:  train Loss: 77.2730   val Loss: 92.0297   time: 0.19s   best: 88.1390
2023-11-02 14:42:39,169:INFO:  Epoch 461/500:  train Loss: 80.2966   val Loss: 89.3728   time: 0.13s   best: 88.1390
2023-11-02 14:42:39,312:INFO:  Epoch 462/500:  train Loss: 82.7912   val Loss: 90.9096   time: 0.13s   best: 88.1390
2023-11-02 14:42:39,458:INFO:  Epoch 463/500:  train Loss: 81.7740   val Loss: 95.6358   time: 0.13s   best: 88.1390
2023-11-02 14:42:39,621:INFO:  Epoch 464/500:  train Loss: 80.7392   val Loss: 98.8721   time: 0.15s   best: 88.1390
2023-11-02 14:42:39,764:INFO:  Epoch 465/500:  train Loss: 79.8583   val Loss: 95.2091   time: 0.13s   best: 88.1390
2023-11-02 14:42:39,907:INFO:  Epoch 466/500:  train Loss: 80.2969   val Loss: 91.9945   time: 0.13s   best: 88.1390
2023-11-02 14:42:40,073:INFO:  Epoch 467/500:  train Loss: 81.0271   val Loss: 91.6131   time: 0.15s   best: 88.1390
2023-11-02 14:42:40,219:INFO:  Epoch 468/500:  train Loss: 80.9983   val Loss: 92.7258   time: 0.13s   best: 88.1390
2023-11-02 14:42:40,364:INFO:  Epoch 469/500:  train Loss: 80.5523   val Loss: 92.5987   time: 0.13s   best: 88.1390
2023-11-02 14:42:40,509:INFO:  Epoch 470/500:  train Loss: 80.4278   val Loss: 92.3837   time: 0.13s   best: 88.1390
2023-11-02 14:42:40,673:INFO:  Epoch 471/500:  train Loss: 80.5843   val Loss: 93.4358   time: 0.15s   best: 88.1390
2023-11-02 14:42:40,815:INFO:  Epoch 472/500:  train Loss: 80.2065   val Loss: 93.3040   time: 0.13s   best: 88.1390
2023-11-02 14:42:40,996:INFO:  Epoch 473/500:  train Loss: 80.2070   val Loss: 92.4705   time: 0.17s   best: 88.1390
2023-11-02 14:42:41,159:INFO:  Epoch 474/500:  train Loss: 80.3149   val Loss: 92.7951   time: 0.15s   best: 88.1390
2023-11-02 14:42:41,302:INFO:  Epoch 475/500:  train Loss: 79.9780   val Loss: 93.0508   time: 0.13s   best: 88.1390
2023-11-02 14:42:41,448:INFO:  Epoch 476/500:  train Loss: 79.9845   val Loss: 92.8152   time: 0.13s   best: 88.1390
2023-11-02 14:42:41,592:INFO:  Epoch 477/500:  train Loss: 80.3601   val Loss: 92.2879   time: 0.13s   best: 88.1390
2023-11-02 14:42:41,755:INFO:  Epoch 478/500:  train Loss: 79.8788   val Loss: 92.0421   time: 0.15s   best: 88.1390
2023-11-02 14:42:41,901:INFO:  Epoch 479/500:  train Loss: 79.9906   val Loss: 92.4296   time: 0.13s   best: 88.1390
2023-11-02 14:42:42,044:INFO:  Epoch 480/500:  train Loss: 79.8372   val Loss: 93.4181   time: 0.13s   best: 88.1390
2023-11-02 14:42:42,214:INFO:  Epoch 481/500:  train Loss: 79.5478   val Loss: 92.4799   time: 0.16s   best: 88.1390
2023-11-02 14:42:42,358:INFO:  Epoch 482/500:  train Loss: 79.5718   val Loss: 91.8243   time: 0.13s   best: 88.1390
2023-11-02 14:42:42,502:INFO:  Epoch 483/500:  train Loss: 79.6734   val Loss: 92.1522   time: 0.13s   best: 88.1390
2023-11-02 14:42:42,645:INFO:  Epoch 484/500:  train Loss: 79.4519   val Loss: 92.7499   time: 0.13s   best: 88.1390
2023-11-02 14:42:42,813:INFO:  Epoch 485/500:  train Loss: 79.3328   val Loss: 92.4696   time: 0.15s   best: 88.1390
2023-11-02 14:42:42,957:INFO:  Epoch 486/500:  train Loss: 79.0567   val Loss: 91.9924   time: 0.13s   best: 88.1390
2023-11-02 14:42:43,135:INFO:  Epoch 487/500:  train Loss: 79.5052   val Loss: 92.4907   time: 0.17s   best: 88.1390
2023-11-02 14:42:43,296:INFO:  Epoch 488/500:  train Loss: 79.0421   val Loss: 92.6043   time: 0.15s   best: 88.1390
2023-11-02 14:42:43,442:INFO:  Epoch 489/500:  train Loss: 79.2579   val Loss: 91.9898   time: 0.13s   best: 88.1390
2023-11-02 14:42:43,586:INFO:  Epoch 490/500:  train Loss: 78.8675   val Loss: 90.9406   time: 0.13s   best: 88.1390
2023-11-02 14:42:43,729:INFO:  Epoch 491/500:  train Loss: 78.8094   val Loss: 91.3819   time: 0.13s   best: 88.1390
2023-11-02 14:42:43,894:INFO:  Epoch 492/500:  train Loss: 78.6660   val Loss: 92.7638   time: 0.15s   best: 88.1390
2023-11-02 14:42:44,037:INFO:  Epoch 493/500:  train Loss: 78.3734   val Loss: 92.6036   time: 0.13s   best: 88.1390
2023-11-02 14:42:44,185:INFO:  Epoch 494/500:  train Loss: 78.5422   val Loss: 92.5507   time: 0.14s   best: 88.1390
2023-11-02 14:42:44,350:INFO:  Epoch 495/500:  train Loss: 78.4580   val Loss: 92.5336   time: 0.15s   best: 88.1390
2023-11-02 14:42:44,495:INFO:  Epoch 496/500:  train Loss: 78.6286   val Loss: 91.6267   time: 0.13s   best: 88.1390
2023-11-02 14:42:44,638:INFO:  Epoch 497/500:  train Loss: 78.4925   val Loss: 90.6795   time: 0.13s   best: 88.1390
2023-11-02 14:42:44,789:INFO:  Epoch 498/500:  train Loss: 77.6253   val Loss: 95.7447   time: 0.15s   best: 88.1390
2023-11-02 14:42:44,948:INFO:  Epoch 499/500:  train Loss: 81.8379   val Loss: 92.2015   time: 0.15s   best: 88.1390
2023-11-02 14:42:45,119:INFO:  Epoch 500/500:  train Loss: 81.0266   val Loss: 92.8352   time: 0.17s   best: 88.1390
2023-11-02 14:42:45,130:INFO:  -----> Training complete in 1m 17s   best validation loss: 88.1390
 
2023-11-02 14:47:42,697:INFO:  Starting experiment lstm autoencoder with 1/3 dataset (0.1 dropout)
2023-11-02 14:47:42,705:INFO:  Defining the model
2023-11-02 14:47:42,759:INFO:  Reading the dataset
2023-11-02 15:22:00,682:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 1/3 dataset (0.1 dropout)_e9e3.pt
2023-11-02 15:27:39,492:INFO:  Starting experiment lstm autoencoder with 1/3 dataset (0.1 dropout)
2023-11-02 15:27:39,503:INFO:  Defining the model
2023-11-02 15:27:39,556:INFO:  Reading the dataset
2023-11-02 15:59:27,858:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 1/3 dataset (0.1 dropout)_e2dc.pt
2023-11-02 16:10:08,423:INFO:  Starting experiment lstm autoencoder with 0.33 dataset (0.1 dropout)
2023-11-02 16:10:08,423:INFO:  Defining the model
2023-11-02 16:10:08,485:INFO:  Reading the dataset
2023-11-02 16:39:48,765:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.33 dataset (0.1 dropout)_8f99.pt
2023-11-02 16:39:48,794:INFO:  Epoch 1/500:  train Loss: 76.9602   val Loss: 108.6206   time: 145.85s   best: 108.6206
2023-11-02 16:42:13,119:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.33 dataset (0.1 dropout)_8f99.pt
2023-11-02 16:42:13,143:INFO:  Epoch 2/500:  train Loss: 64.4578   val Loss: 106.8208   time: 144.32s   best: 106.8208
2023-11-02 16:44:37,549:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.33 dataset (0.1 dropout)_8f99.pt
2023-11-02 16:44:37,577:INFO:  Epoch 3/500:  train Loss: 61.2925   val Loss: 103.7063   time: 144.40s   best: 103.7063
2023-11-02 16:47:02,488:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.33 dataset (0.1 dropout)_8f99.pt
2023-11-02 16:47:02,512:INFO:  Epoch 4/500:  train Loss: 56.7423   val Loss: 98.6878   time: 144.91s   best: 98.6878
2023-11-02 16:49:27,375:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.33 dataset (0.1 dropout)_8f99.pt
2023-11-02 16:49:27,404:INFO:  Epoch 5/500:  train Loss: 52.0492   val Loss: 98.4130   time: 144.85s   best: 98.4130
2023-11-02 16:51:52,905:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.33 dataset (0.1 dropout)_8f99.pt
2023-11-02 16:51:52,929:INFO:  Epoch 6/500:  train Loss: 48.0630   val Loss: 97.6719   time: 145.50s   best: 97.6719
2023-11-02 16:54:15,679:INFO:  Epoch 7/500:  train Loss: 44.7483   val Loss: 101.4301   time: 142.75s   best: 97.6719
2023-11-02 16:56:41,149:INFO:  Epoch 8/500:  train Loss: 41.8545   val Loss: 99.5161   time: 145.46s   best: 97.6719
2023-11-02 16:59:04,208:INFO:  Epoch 9/500:  train Loss: 39.6085   val Loss: 98.8418   time: 143.05s   best: 97.6719
2023-11-02 17:01:28,733:INFO:  Epoch 10/500:  train Loss: 37.7015   val Loss: 99.5307   time: 144.52s   best: 97.6719
2023-11-02 17:03:51,552:INFO:  Epoch 11/500:  train Loss: 36.1797   val Loss: 100.1701   time: 142.81s   best: 97.6719
2023-11-02 17:06:16,349:INFO:  Epoch 12/500:  train Loss: 34.8948   val Loss: 103.6137   time: 144.79s   best: 97.6719
2023-11-02 17:08:41,661:INFO:  Epoch 13/500:  train Loss: 33.6925   val Loss: 106.4161   time: 145.30s   best: 97.6719
2023-11-02 17:11:05,656:INFO:  Epoch 14/500:  train Loss: 32.8131   val Loss: 109.4929   time: 143.98s   best: 97.6719
2023-11-02 17:13:29,732:INFO:  Epoch 15/500:  train Loss: 31.8638   val Loss: 112.0118   time: 144.05s   best: 97.6719
2023-11-02 17:15:51,878:INFO:  Epoch 16/500:  train Loss: 31.0698   val Loss: 124.0943   time: 142.13s   best: 97.6719
2023-11-02 17:18:14,284:INFO:  Epoch 17/500:  train Loss: 30.4115   val Loss: 124.2281   time: 142.40s   best: 97.6719
2023-11-02 17:20:37,292:INFO:  Epoch 18/500:  train Loss: 29.7174   val Loss: 128.2995   time: 142.99s   best: 97.6719
2023-11-02 17:22:59,961:INFO:  Epoch 19/500:  train Loss: 29.3025   val Loss: 124.1634   time: 142.66s   best: 97.6719
2023-11-02 17:25:23,805:INFO:  Epoch 20/500:  train Loss: 28.6595   val Loss: 118.4530   time: 143.82s   best: 97.6719
2023-11-02 17:27:46,631:INFO:  Epoch 21/500:  train Loss: 28.2517   val Loss: 123.0918   time: 142.80s   best: 97.6719
2023-11-02 17:30:11,028:INFO:  Epoch 22/500:  train Loss: 27.7909   val Loss: 121.6333   time: 144.37s   best: 97.6719
2023-11-02 17:32:35,114:INFO:  Epoch 23/500:  train Loss: 27.4105   val Loss: 121.9636   time: 144.06s   best: 97.6719
2023-11-02 17:34:57,724:INFO:  Epoch 24/500:  train Loss: 27.0492   val Loss: 117.4993   time: 142.59s   best: 97.6719
2023-11-02 17:37:20,084:INFO:  Epoch 25/500:  train Loss: 26.7142   val Loss: 119.8671   time: 142.34s   best: 97.6719
2023-11-02 17:39:44,196:INFO:  Epoch 26/500:  train Loss: 26.4719   val Loss: 115.2245   time: 144.10s   best: 97.6719
2023-11-02 17:42:09,040:INFO:  Epoch 27/500:  train Loss: 26.0955   val Loss: 116.8839   time: 144.83s   best: 97.6719
2023-11-02 17:44:32,563:INFO:  Epoch 28/500:  train Loss: 25.8694   val Loss: 108.7237   time: 143.50s   best: 97.6719
2023-11-02 17:46:57,916:INFO:  Epoch 29/500:  train Loss: 25.5927   val Loss: 112.5594   time: 145.33s   best: 97.6719
2023-11-02 17:49:23,343:INFO:  Epoch 30/500:  train Loss: 25.3538   val Loss: 106.9427   time: 145.41s   best: 97.6719
2023-11-02 17:51:41,679:INFO:  Starting experiment lstm autoencoder with 10 mils dataset (0.1 dropout)
2023-11-02 17:51:41,693:INFO:  Defining the model
2023-11-02 17:51:41,762:INFO:  Reading the dataset
2023-11-02 18:21:31,805:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:21:31,837:INFO:  Epoch 1/500:  train Loss: 73.6896   val Loss: 75.9669   time: 217.72s   best: 75.9669
2023-11-02 18:25:05,765:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:25:05,790:INFO:  Epoch 2/500:  train Loss: 62.4880   val Loss: 66.5735   time: 213.92s   best: 66.5735
2023-11-02 18:28:40,347:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:28:40,655:INFO:  Epoch 3/500:  train Loss: 58.0850   val Loss: 62.8912   time: 214.54s   best: 62.8912
2023-11-02 18:32:16,244:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:32:16,610:INFO:  Epoch 4/500:  train Loss: 53.1762   val Loss: 58.0659   time: 215.58s   best: 58.0659
2023-11-02 18:35:50,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:35:50,474:INFO:  Epoch 5/500:  train Loss: 47.6962   val Loss: 53.6318   time: 213.83s   best: 53.6318
2023-11-02 18:39:27,749:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:39:27,779:INFO:  Epoch 6/500:  train Loss: 43.3992   val Loss: 50.9526   time: 217.27s   best: 50.9526
2023-11-02 18:43:03,251:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:43:03,293:INFO:  Epoch 7/500:  train Loss: 40.4666   val Loss: 48.8816   time: 215.46s   best: 48.8816
2023-11-02 18:46:37,315:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:46:37,975:INFO:  Epoch 8/500:  train Loss: 38.3487   val Loss: 47.4488   time: 214.02s   best: 47.4488
2023-11-02 18:50:12,057:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:50:12,097:INFO:  Epoch 9/500:  train Loss: 36.5201   val Loss: 45.1753   time: 214.08s   best: 45.1753
2023-11-02 18:53:45,798:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:53:45,835:INFO:  Epoch 10/500:  train Loss: 35.1402   val Loss: 43.5388   time: 213.70s   best: 43.5388
2023-11-02 18:57:19,226:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 18:57:19,258:INFO:  Epoch 11/500:  train Loss: 33.8836   val Loss: 43.0701   time: 213.39s   best: 43.0701
2023-11-02 19:00:53,133:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:00:53,292:INFO:  Epoch 12/500:  train Loss: 33.0256   val Loss: 42.3787   time: 213.86s   best: 42.3787
2023-11-02 19:04:26,782:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:04:26,810:INFO:  Epoch 13/500:  train Loss: 31.9495   val Loss: 41.7876   time: 213.46s   best: 41.7876
2023-11-02 19:08:04,874:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:08:04,907:INFO:  Epoch 14/500:  train Loss: 31.2274   val Loss: 41.4245   time: 218.05s   best: 41.4245
2023-11-02 19:11:39,305:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:11:39,330:INFO:  Epoch 15/500:  train Loss: 30.5152   val Loss: 39.8660   time: 214.38s   best: 39.8660
2023-11-02 19:15:13,943:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:15:13,969:INFO:  Epoch 16/500:  train Loss: 29.8713   val Loss: 39.5820   time: 214.59s   best: 39.5820
2023-11-02 19:18:51,722:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:18:51,749:INFO:  Epoch 17/500:  train Loss: 29.2623   val Loss: 38.8808   time: 217.74s   best: 38.8808
2023-11-02 19:22:26,025:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:22:26,054:INFO:  Epoch 18/500:  train Loss: 29.1293   val Loss: 38.0273   time: 214.26s   best: 38.0273
2023-11-02 19:26:00,631:INFO:  Epoch 19/500:  train Loss: 28.4920   val Loss: 38.1011   time: 214.58s   best: 38.0273
2023-11-02 19:29:36,562:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:29:36,591:INFO:  Epoch 20/500:  train Loss: 28.0184   val Loss: 37.9083   time: 215.91s   best: 37.9083
2023-11-02 19:33:11,679:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:33:12,021:INFO:  Epoch 21/500:  train Loss: 27.6145   val Loss: 36.5604   time: 215.08s   best: 36.5604
2023-11-02 19:36:45,178:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:36:45,227:INFO:  Epoch 22/500:  train Loss: 27.2241   val Loss: 35.9710   time: 213.15s   best: 35.9710
2023-11-02 19:40:20,649:INFO:  Epoch 23/500:  train Loss: 26.9771   val Loss: 38.4223   time: 215.41s   best: 35.9710
2023-11-02 19:43:54,051:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:43:54,084:INFO:  Epoch 24/500:  train Loss: 26.7946   val Loss: 35.7757   time: 213.38s   best: 35.7757
2023-11-02 19:47:29,754:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:47:29,797:INFO:  Epoch 25/500:  train Loss: 26.3027   val Loss: 35.6991   time: 215.65s   best: 35.6991
2023-11-02 19:51:06,483:INFO:  Epoch 26/500:  train Loss: 26.0171   val Loss: 36.9274   time: 216.68s   best: 35.6991
2023-11-02 19:54:40,192:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 19:54:40,231:INFO:  Epoch 27/500:  train Loss: 25.7659   val Loss: 33.9395   time: 213.50s   best: 33.9395
2023-11-02 19:58:15,389:INFO:  Epoch 28/500:  train Loss: 25.4376   val Loss: 34.6053   time: 215.15s   best: 33.9395
2023-11-02 20:01:48,638:INFO:  Epoch 29/500:  train Loss: 25.2195   val Loss: 35.1750   time: 213.21s   best: 33.9395
2023-11-02 20:05:21,536:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 20:05:21,561:INFO:  Epoch 30/500:  train Loss: 24.9393   val Loss: 33.4154   time: 212.87s   best: 33.4154
2023-11-02 20:08:54,977:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 20:08:55,124:INFO:  Epoch 31/500:  train Loss: 24.7278   val Loss: 33.2897   time: 213.40s   best: 33.2897
2023-11-02 20:12:31,550:INFO:  Epoch 32/500:  train Loss: 24.5785   val Loss: 33.4537   time: 216.41s   best: 33.2897
2023-11-02 20:16:05,093:INFO:  Epoch 33/500:  train Loss: 24.4199   val Loss: 33.5305   time: 213.53s   best: 33.2897
2023-11-02 20:19:38,624:INFO:  Epoch 34/500:  train Loss: 24.1902   val Loss: 33.3878   time: 213.50s   best: 33.2897
2023-11-02 20:23:16,241:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 20:23:16,274:INFO:  Epoch 35/500:  train Loss: 24.0494   val Loss: 33.1368   time: 217.60s   best: 33.1368
2023-11-02 20:26:51,141:INFO:  Epoch 36/500:  train Loss: 23.8510   val Loss: 35.8742   time: 214.87s   best: 33.1368
2023-11-02 20:30:28,168:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 20:30:28,200:INFO:  Epoch 37/500:  train Loss: 23.8500   val Loss: 31.8456   time: 217.00s   best: 31.8456
2023-11-02 20:34:05,358:INFO:  Epoch 38/500:  train Loss: 23.6032   val Loss: 33.5549   time: 217.15s   best: 31.8456
2023-11-02 20:37:38,337:INFO:  Epoch 39/500:  train Loss: 23.4056   val Loss: 32.8806   time: 212.96s   best: 31.8456
2023-11-02 20:41:11,822:INFO:  Epoch 40/500:  train Loss: 23.2254   val Loss: 32.7447   time: 213.47s   best: 31.8456
2023-11-02 20:44:45,588:INFO:  Epoch 41/500:  train Loss: 23.1412   val Loss: 35.9523   time: 213.71s   best: 31.8456
2023-11-02 20:48:22,692:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 20:48:22,722:INFO:  Epoch 42/500:  train Loss: 23.0291   val Loss: 31.2719   time: 217.08s   best: 31.2719
2023-11-02 20:51:58,173:INFO:  Epoch 43/500:  train Loss: 22.9219   val Loss: 32.3606   time: 215.43s   best: 31.2719
2023-11-02 20:55:31,919:INFO:  Epoch 44/500:  train Loss: 22.7640   val Loss: 33.9084   time: 213.71s   best: 31.2719
2023-11-02 20:59:05,503:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 20:59:05,544:INFO:  Epoch 45/500:  train Loss: 22.6126   val Loss: 31.2022   time: 213.56s   best: 31.2022
2023-11-02 21:02:42,449:INFO:  Epoch 46/500:  train Loss: 22.5080   val Loss: 33.7146   time: 216.89s   best: 31.2022
2023-11-02 21:06:19,265:INFO:  Epoch 47/500:  train Loss: 22.8337   val Loss: 31.5046   time: 216.80s   best: 31.2022
2023-11-02 21:09:55,088:INFO:  Epoch 48/500:  train Loss: 22.3551   val Loss: 31.4875   time: 215.79s   best: 31.2022
2023-11-02 21:13:31,096:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 21:13:31,143:INFO:  Epoch 49/500:  train Loss: 22.1927   val Loss: 30.5851   time: 215.99s   best: 30.5851
2023-11-02 21:17:08,012:INFO:  Epoch 50/500:  train Loss: 22.1182   val Loss: 31.3930   time: 216.87s   best: 30.5851
2023-11-02 21:20:42,430:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 21:20:42,460:INFO:  Epoch 51/500:  train Loss: 22.5492   val Loss: 30.4808   time: 214.40s   best: 30.4808
2023-11-02 21:24:16,622:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 21:24:16,756:INFO:  Epoch 52/500:  train Loss: 22.0086   val Loss: 30.3511   time: 214.15s   best: 30.3511
2023-11-02 21:27:53,421:INFO:  Epoch 53/500:  train Loss: 21.9889   val Loss: 31.6788   time: 216.64s   best: 30.3511
2023-11-02 21:31:27,572:INFO:  Epoch 54/500:  train Loss: 21.8043   val Loss: 30.7117   time: 214.13s   best: 30.3511
2023-11-02 21:35:01,616:INFO:  Epoch 55/500:  train Loss: 21.8341   val Loss: 31.5452   time: 214.03s   best: 30.3511
2023-11-02 21:38:37,490:INFO:  Epoch 56/500:  train Loss: 21.7432   val Loss: 31.8035   time: 215.82s   best: 30.3511
2023-11-02 21:42:13,435:INFO:  Epoch 57/500:  train Loss: 21.5983   val Loss: 30.4775   time: 215.92s   best: 30.3511
2023-11-02 21:45:47,408:INFO:  Epoch 58/500:  train Loss: 21.5267   val Loss: 32.1900   time: 213.95s   best: 30.3511
2023-11-02 21:49:25,202:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 21:49:25,361:INFO:  Epoch 59/500:  train Loss: 21.4349   val Loss: 30.1596   time: 217.74s   best: 30.1596
2023-11-02 21:53:03,107:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 21:53:03,269:INFO:  Epoch 60/500:  train Loss: 21.3904   val Loss: 30.0423   time: 217.72s   best: 30.0423
2023-11-02 21:56:40,808:INFO:  Epoch 61/500:  train Loss: 21.3041   val Loss: 31.0994   time: 217.52s   best: 30.0423
2023-11-02 22:00:16,269:INFO:  Epoch 62/500:  train Loss: 21.2056   val Loss: 31.9807   time: 215.43s   best: 30.0423
2023-11-02 22:03:51,830:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 22:03:51,853:INFO:  Epoch 63/500:  train Loss: 21.2405   val Loss: 30.0243   time: 215.53s   best: 30.0243
2023-11-02 22:07:27,746:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 22:07:27,773:INFO:  Epoch 64/500:  train Loss: 21.0405   val Loss: 29.8136   time: 215.88s   best: 29.8136
2023-11-02 22:11:04,092:INFO:  Epoch 65/500:  train Loss: 21.2070   val Loss: 31.6735   time: 216.31s   best: 29.8136
2023-11-02 22:14:37,506:INFO:  Epoch 66/500:  train Loss: 20.9308   val Loss: 30.8813   time: 213.40s   best: 29.8136
2023-11-02 22:18:10,931:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 22:18:10,979:INFO:  Epoch 67/500:  train Loss: 20.8518   val Loss: 29.4803   time: 213.39s   best: 29.4803
2023-11-02 22:21:45,496:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 22:21:45,533:INFO:  Epoch 68/500:  train Loss: 20.9666   val Loss: 29.4405   time: 214.50s   best: 29.4405
2023-11-02 22:25:22,063:INFO:  Epoch 69/500:  train Loss: 20.7868   val Loss: 30.0239   time: 216.53s   best: 29.4405
2023-11-02 22:28:57,591:INFO:  Epoch 70/500:  train Loss: 20.7532   val Loss: 30.0746   time: 215.50s   best: 29.4405
2023-11-02 22:32:31,806:INFO:  Epoch 71/500:  train Loss: 20.7385   val Loss: 29.9996   time: 214.19s   best: 29.4405
2023-11-02 22:36:10,510:INFO:  Epoch 72/500:  train Loss: 20.6044   val Loss: 30.2581   time: 218.66s   best: 29.4405
2023-11-02 22:39:44,327:INFO:  Epoch 73/500:  train Loss: 20.5046   val Loss: 31.2217   time: 213.72s   best: 29.4405
2023-11-02 22:43:19,857:INFO:  Epoch 74/500:  train Loss: 20.5467   val Loss: 36.6746   time: 215.51s   best: 29.4405
2023-11-02 22:46:56,441:INFO:  Epoch 75/500:  train Loss: 20.5837   val Loss: 29.7157   time: 216.55s   best: 29.4405
2023-11-02 22:50:33,669:INFO:  Epoch 76/500:  train Loss: 20.4280   val Loss: 31.1494   time: 217.22s   best: 29.4405
2023-11-02 22:54:06,784:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 22:54:06,813:INFO:  Epoch 77/500:  train Loss: 20.3945   val Loss: 29.3848   time: 213.08s   best: 29.3848
2023-11-02 22:57:44,812:INFO:  Epoch 78/500:  train Loss: 20.2732   val Loss: 29.8997   time: 218.00s   best: 29.3848
2023-11-02 23:01:22,427:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 23:01:22,462:INFO:  Epoch 79/500:  train Loss: 20.3141   val Loss: 28.8228   time: 217.57s   best: 28.8228
2023-11-02 23:04:59,411:INFO:  Epoch 80/500:  train Loss: 20.3462   val Loss: 33.8160   time: 216.92s   best: 28.8228
2023-11-02 23:08:32,861:INFO:  Epoch 81/500:  train Loss: 20.2736   val Loss: 29.4579   time: 213.43s   best: 28.8228
2023-11-02 23:12:06,415:INFO:  Epoch 82/500:  train Loss: 20.2338   val Loss: 30.2121   time: 213.54s   best: 28.8228
2023-11-02 23:15:41,419:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 23:15:41,455:INFO:  Epoch 83/500:  train Loss: 20.0648   val Loss: 28.5459   time: 214.98s   best: 28.5459
2023-11-02 23:19:14,440:INFO:  Epoch 84/500:  train Loss: 20.0127   val Loss: 29.9902   time: 212.97s   best: 28.5459
2023-11-02 23:22:51,258:INFO:  Epoch 85/500:  train Loss: 20.1132   val Loss: 29.3319   time: 216.79s   best: 28.5459
2023-11-02 23:26:28,437:INFO:  Epoch 86/500:  train Loss: 20.3100   val Loss: 29.6419   time: 217.15s   best: 28.5459
2023-11-02 23:30:01,804:INFO:  Epoch 87/500:  train Loss: 19.9244   val Loss: 28.9141   time: 213.34s   best: 28.5459
2023-11-02 23:33:34,443:INFO:  Epoch 88/500:  train Loss: 19.8722   val Loss: 36.1740   time: 212.61s   best: 28.5459
2023-11-02 23:37:07,770:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-02 23:37:07,940:INFO:  Epoch 89/500:  train Loss: 20.1718   val Loss: 28.5031   time: 213.30s   best: 28.5031
2023-11-02 23:40:41,524:INFO:  Epoch 90/500:  train Loss: 19.9073   val Loss: 28.7109   time: 213.57s   best: 28.5031
2023-11-02 23:44:18,379:INFO:  Epoch 91/500:  train Loss: 19.7016   val Loss: 29.0422   time: 216.83s   best: 28.5031
2023-11-02 23:47:55,742:INFO:  Epoch 92/500:  train Loss: 19.7195   val Loss: 28.6650   time: 217.35s   best: 28.5031
2023-11-02 23:51:30,366:INFO:  Epoch 93/500:  train Loss: 19.6678   val Loss: 29.2280   time: 214.61s   best: 28.5031
2023-11-02 23:55:05,950:INFO:  Epoch 94/500:  train Loss: 19.6875   val Loss: 28.6076   time: 215.56s   best: 28.5031
2023-11-02 23:58:39,439:INFO:  Epoch 95/500:  train Loss: 19.9907   val Loss: 30.4069   time: 213.48s   best: 28.5031
2023-11-03 00:02:13,053:INFO:  Epoch 96/500:  train Loss: 19.7727   val Loss: 28.7782   time: 213.59s   best: 28.5031
2023-11-03 00:05:46,866:INFO:  Epoch 97/500:  train Loss: 19.5458   val Loss: 32.3883   time: 213.79s   best: 28.5031
2023-11-03 00:09:20,953:INFO:  Epoch 98/500:  train Loss: 19.5364   val Loss: 29.1444   time: 214.07s   best: 28.5031
2023-11-03 00:12:58,620:INFO:  Epoch 99/500:  train Loss: 19.4860   val Loss: 29.0675   time: 217.64s   best: 28.5031
2023-11-03 00:16:32,461:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 00:16:32,489:INFO:  Epoch 100/500:  train Loss: 19.4204   val Loss: 28.3518   time: 213.83s   best: 28.3518
2023-11-03 00:20:08,425:INFO:  Epoch 101/500:  train Loss: 19.5960   val Loss: 29.0533   time: 215.92s   best: 28.3518
2023-11-03 00:23:46,496:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 00:23:46,523:INFO:  Epoch 102/500:  train Loss: 19.3403   val Loss: 28.2358   time: 218.04s   best: 28.2358
2023-11-03 00:27:22,683:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 00:27:22,709:INFO:  Epoch 103/500:  train Loss: 20.2396   val Loss: 27.8762   time: 216.11s   best: 27.8762
2023-11-03 00:31:00,344:INFO:  Epoch 104/500:  train Loss: 19.3650   val Loss: 28.0563   time: 217.63s   best: 27.8762
2023-11-03 00:34:37,682:INFO:  Epoch 105/500:  train Loss: 19.2398   val Loss: 28.4504   time: 217.31s   best: 27.8762
2023-11-03 00:38:13,892:INFO:  Epoch 106/500:  train Loss: 19.3993   val Loss: 30.5083   time: 216.15s   best: 27.8762
2023-11-03 00:41:48,035:INFO:  Epoch 107/500:  train Loss: 19.4429   val Loss: 29.8880   time: 214.14s   best: 27.8762
2023-11-03 00:45:20,968:INFO:  Epoch 108/500:  train Loss: 19.2964   val Loss: 39.5725   time: 212.91s   best: 27.8762
2023-11-03 00:48:54,793:INFO:  Epoch 109/500:  train Loss: 19.5392   val Loss: 35.6975   time: 213.81s   best: 27.8762
2023-11-03 00:52:28,981:INFO:  Epoch 110/500:  train Loss: 19.2409   val Loss: 28.3045   time: 214.18s   best: 27.8762
2023-11-03 00:56:06,196:INFO:  Epoch 111/500:  train Loss: 19.1628   val Loss: 28.2305   time: 217.20s   best: 27.8762
2023-11-03 00:59:40,065:INFO:  Epoch 112/500:  train Loss: 19.3791   val Loss: 29.7326   time: 213.77s   best: 27.8762
2023-11-03 01:03:14,034:INFO:  Epoch 113/500:  train Loss: 19.0753   val Loss: 28.7126   time: 213.95s   best: 27.8762
2023-11-03 01:06:51,150:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 01:06:52,268:INFO:  Epoch 114/500:  train Loss: 19.1343   val Loss: 27.8122   time: 217.10s   best: 27.8122
2023-11-03 01:10:26,430:INFO:  Epoch 115/500:  train Loss: 19.0143   val Loss: 28.2452   time: 214.16s   best: 27.8122
2023-11-03 01:14:02,075:INFO:  Epoch 116/500:  train Loss: 18.9792   val Loss: 30.9855   time: 215.57s   best: 27.8122
2023-11-03 01:17:36,795:INFO:  Epoch 117/500:  train Loss: 19.0476   val Loss: 28.8531   time: 214.71s   best: 27.8122
2023-11-03 01:21:10,488:INFO:  Epoch 118/500:  train Loss: 18.9088   val Loss: 29.9523   time: 213.68s   best: 27.8122
2023-11-03 01:24:45,331:INFO:  Epoch 119/500:  train Loss: 19.0197   val Loss: 28.0980   time: 214.82s   best: 27.8122
2023-11-03 01:28:19,886:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 01:28:19,917:INFO:  Epoch 120/500:  train Loss: 18.8789   val Loss: 27.7943   time: 214.54s   best: 27.7943
2023-11-03 01:31:57,429:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 01:31:57,470:INFO:  Epoch 121/500:  train Loss: 18.8768   val Loss: 27.4191   time: 217.51s   best: 27.4191
2023-11-03 01:35:33,305:INFO:  Epoch 122/500:  train Loss: 18.8038   val Loss: 27.7035   time: 215.82s   best: 27.4191
2023-11-03 01:39:06,963:INFO:  Epoch 123/500:  train Loss: 18.8523   val Loss: 27.5331   time: 213.62s   best: 27.4191
2023-11-03 01:42:43,798:INFO:  Epoch 124/500:  train Loss: 18.8245   val Loss: 27.7226   time: 216.69s   best: 27.4191
2023-11-03 01:46:17,554:INFO:  Epoch 125/500:  train Loss: 18.9357   val Loss: 27.9337   time: 213.73s   best: 27.4191
2023-11-03 01:49:54,840:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 01:49:54,875:INFO:  Epoch 126/500:  train Loss: 18.7617   val Loss: 27.2173   time: 217.26s   best: 27.2173
2023-11-03 01:53:30,626:INFO:  Epoch 127/500:  train Loss: 18.7602   val Loss: 27.2697   time: 215.74s   best: 27.2173
2023-11-03 01:57:04,166:INFO:  Epoch 128/500:  train Loss: 18.7445   val Loss: 27.2271   time: 213.53s   best: 27.2173
2023-11-03 02:00:37,656:INFO:  Epoch 129/500:  train Loss: 18.6322   val Loss: 27.5724   time: 213.46s   best: 27.2173
2023-11-03 02:04:11,543:INFO:  Epoch 130/500:  train Loss: 18.7245   val Loss: 28.1338   time: 213.88s   best: 27.2173
2023-11-03 02:07:46,639:INFO:  Epoch 131/500:  train Loss: 18.7506   val Loss: 28.3991   time: 215.07s   best: 27.2173
2023-11-03 02:11:20,565:INFO:  Epoch 132/500:  train Loss: 18.6267   val Loss: 27.5712   time: 213.90s   best: 27.2173
2023-11-03 02:14:54,552:INFO:  Epoch 133/500:  train Loss: 18.6956   val Loss: 28.9248   time: 213.96s   best: 27.2173
2023-11-03 02:18:28,204:INFO:  Epoch 134/500:  train Loss: 18.6951   val Loss: 27.6190   time: 213.64s   best: 27.2173
2023-11-03 02:22:03,079:INFO:  Epoch 135/500:  train Loss: 18.4788   val Loss: 27.7078   time: 214.85s   best: 27.2173
2023-11-03 02:25:37,230:INFO:  Epoch 136/500:  train Loss: 18.5393   val Loss: 31.3401   time: 214.13s   best: 27.2173
2023-11-03 02:29:15,093:INFO:  Epoch 137/500:  train Loss: 18.5709   val Loss: 27.4800   time: 217.86s   best: 27.2173
2023-11-03 02:32:49,870:INFO:  Epoch 138/500:  train Loss: 18.4326   val Loss: 27.4374   time: 214.77s   best: 27.2173
2023-11-03 02:36:23,909:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 02:36:24,042:INFO:  Epoch 139/500:  train Loss: 18.3925   val Loss: 26.9148   time: 214.03s   best: 26.9148
2023-11-03 02:40:00,573:INFO:  Epoch 140/500:  train Loss: 18.5461   val Loss: 27.5097   time: 216.51s   best: 26.9148
2023-11-03 02:43:38,607:INFO:  Epoch 141/500:  train Loss: 18.4578   val Loss: 27.5599   time: 218.00s   best: 26.9148
2023-11-03 02:47:12,596:INFO:  Epoch 142/500:  train Loss: 18.4174   val Loss: 27.0829   time: 213.97s   best: 26.9148
2023-11-03 02:50:50,519:INFO:  Epoch 143/500:  train Loss: 18.4484   val Loss: 27.1595   time: 217.91s   best: 26.9148
2023-11-03 02:54:28,283:INFO:  Epoch 144/500:  train Loss: 18.3465   val Loss: 28.2117   time: 217.75s   best: 26.9148
2023-11-03 02:58:06,260:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 02:58:06,291:INFO:  Epoch 145/500:  train Loss: 18.3352   val Loss: 26.7248   time: 217.95s   best: 26.7248
2023-11-03 03:01:42,403:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 03:01:42,429:INFO:  Epoch 146/500:  train Loss: 18.7880   val Loss: 26.5402   time: 216.10s   best: 26.5402
2023-11-03 03:05:16,676:INFO:  Epoch 147/500:  train Loss: 18.3760   val Loss: 27.6475   time: 214.25s   best: 26.5402
2023-11-03 03:08:50,487:INFO:  Epoch 148/500:  train Loss: 18.2344   val Loss: 27.0230   time: 213.80s   best: 26.5402
2023-11-03 03:12:28,413:INFO:  Epoch 149/500:  train Loss: 18.3363   val Loss: 27.2355   time: 217.90s   best: 26.5402
2023-11-03 03:16:05,110:INFO:  Epoch 150/500:  train Loss: 18.2687   val Loss: 30.0639   time: 216.67s   best: 26.5402
2023-11-03 03:19:39,828:INFO:  Epoch 151/500:  train Loss: 18.1970   val Loss: 26.7321   time: 214.47s   best: 26.5402
2023-11-03 03:23:16,442:INFO:  Epoch 152/500:  train Loss: 18.2574   val Loss: 26.7041   time: 216.59s   best: 26.5402
2023-11-03 03:26:51,228:INFO:  Epoch 153/500:  train Loss: 18.3510   val Loss: 26.8492   time: 214.77s   best: 26.5402
2023-11-03 03:30:26,601:INFO:  Epoch 154/500:  train Loss: 18.2176   val Loss: 27.2245   time: 215.33s   best: 26.5402
2023-11-03 03:34:04,659:INFO:  Epoch 155/500:  train Loss: 18.1621   val Loss: 27.1076   time: 218.02s   best: 26.5402
2023-11-03 03:37:42,843:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 03:37:42,871:INFO:  Epoch 156/500:  train Loss: 18.1859   val Loss: 26.4149   time: 218.16s   best: 26.4149
2023-11-03 03:41:20,765:INFO:  Epoch 157/500:  train Loss: 18.1886   val Loss: 27.0718   time: 217.89s   best: 26.4149
2023-11-03 03:44:56,969:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 03:44:56,995:INFO:  Epoch 158/500:  train Loss: 18.0952   val Loss: 26.2716   time: 216.18s   best: 26.2716
2023-11-03 03:48:31,275:INFO:  Epoch 159/500:  train Loss: 18.0311   val Loss: 26.7058   time: 214.27s   best: 26.2716
2023-11-03 03:52:05,462:INFO:  Epoch 160/500:  train Loss: 18.2074   val Loss: 28.2756   time: 214.17s   best: 26.2716
2023-11-03 03:55:43,723:INFO:  Epoch 161/500:  train Loss: 17.9991   val Loss: 26.6790   time: 218.24s   best: 26.2716
2023-11-03 03:59:17,908:INFO:  Epoch 162/500:  train Loss: 18.1332   val Loss: 28.9150   time: 214.16s   best: 26.2716
2023-11-03 04:02:51,955:INFO:  Epoch 163/500:  train Loss: 18.2398   val Loss: 27.8335   time: 214.02s   best: 26.2716
2023-11-03 04:06:29,677:INFO:  Epoch 164/500:  train Loss: 17.9790   val Loss: 26.7314   time: 217.70s   best: 26.2716
2023-11-03 04:10:07,207:INFO:  Epoch 165/500:  train Loss: 17.9826   val Loss: 26.3462   time: 217.50s   best: 26.2716
2023-11-03 04:13:44,371:INFO:  Epoch 166/500:  train Loss: 18.1720   val Loss: 26.4518   time: 217.13s   best: 26.2716
2023-11-03 04:17:18,769:INFO:  Epoch 167/500:  train Loss: 17.9638   val Loss: 28.3198   time: 214.38s   best: 26.2716
2023-11-03 04:20:57,092:INFO:  Epoch 168/500:  train Loss: 18.0173   val Loss: 26.3530   time: 218.31s   best: 26.2716
2023-11-03 04:24:36,017:INFO:  Epoch 169/500:  train Loss: 17.9373   val Loss: 30.4122   time: 218.91s   best: 26.2716
2023-11-03 04:28:10,238:INFO:  Epoch 170/500:  train Loss: 18.0207   val Loss: 26.4253   time: 214.20s   best: 26.2716
2023-11-03 04:31:44,181:INFO:  Epoch 171/500:  train Loss: 17.8376   val Loss: 27.6639   time: 213.90s   best: 26.2716
2023-11-03 04:35:18,355:INFO:  Epoch 172/500:  train Loss: 18.0121   val Loss: 27.5042   time: 214.14s   best: 26.2716
2023-11-03 04:38:56,911:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 04:38:56,942:INFO:  Epoch 173/500:  train Loss: 17.8774   val Loss: 25.9794   time: 218.53s   best: 25.9794
2023-11-03 04:42:30,802:INFO:  Epoch 174/500:  train Loss: 17.8344   val Loss: 27.1345   time: 213.85s   best: 25.9794
2023-11-03 04:46:05,591:INFO:  Epoch 175/500:  train Loss: 17.7729   val Loss: 26.4762   time: 214.76s   best: 25.9794
2023-11-03 04:49:43,534:INFO:  Epoch 176/500:  train Loss: 17.9876   val Loss: 27.2934   time: 217.90s   best: 25.9794
2023-11-03 04:53:18,953:INFO:  Epoch 177/500:  train Loss: 18.0294   val Loss: 28.4187   time: 215.40s   best: 25.9794
2023-11-03 04:56:53,343:INFO:  Epoch 178/500:  train Loss: 17.9053   val Loss: 26.3891   time: 214.37s   best: 25.9794
2023-11-03 05:00:28,772:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 05:00:28,810:INFO:  Epoch 179/500:  train Loss: 17.7455   val Loss: 25.8490   time: 215.41s   best: 25.8490
2023-11-03 05:04:05,669:INFO:  Epoch 180/500:  train Loss: 17.7524   val Loss: 30.1898   time: 216.86s   best: 25.8490
2023-11-03 05:07:42,096:INFO:  Epoch 181/500:  train Loss: 17.9227   val Loss: 26.3323   time: 216.41s   best: 25.8490
2023-11-03 05:11:16,106:INFO:  Epoch 182/500:  train Loss: 17.7204   val Loss: 28.0737   time: 213.98s   best: 25.8490
2023-11-03 05:14:50,445:INFO:  Epoch 183/500:  train Loss: 17.7210   val Loss: 27.3875   time: 214.09s   best: 25.8490
2023-11-03 05:18:27,988:INFO:  Epoch 184/500:  train Loss: 17.7894   val Loss: 27.1149   time: 217.54s   best: 25.8490
2023-11-03 05:22:02,028:INFO:  Epoch 185/500:  train Loss: 17.6708   val Loss: 26.3871   time: 214.03s   best: 25.8490
2023-11-03 05:25:36,103:INFO:  Epoch 186/500:  train Loss: 17.6899   val Loss: 25.9458   time: 214.06s   best: 25.8490
2023-11-03 05:29:12,538:INFO:  Epoch 187/500:  train Loss: 17.6788   val Loss: 27.0819   time: 216.42s   best: 25.8490
2023-11-03 05:32:46,674:INFO:  Epoch 188/500:  train Loss: 17.6778   val Loss: 26.5201   time: 213.99s   best: 25.8490
2023-11-03 05:36:24,551:INFO:  Epoch 189/500:  train Loss: 17.6573   val Loss: 26.4105   time: 217.87s   best: 25.8490
2023-11-03 05:40:02,209:INFO:  Epoch 190/500:  train Loss: 17.6535   val Loss: 26.1378   time: 217.63s   best: 25.8490
2023-11-03 05:43:39,094:INFO:  Epoch 191/500:  train Loss: 17.6000   val Loss: 25.8778   time: 216.86s   best: 25.8490
2023-11-03 05:47:14,087:INFO:  Epoch 192/500:  train Loss: 17.6270   val Loss: 26.2624   time: 214.97s   best: 25.8490
2023-11-03 05:50:47,494:INFO:  Epoch 193/500:  train Loss: 17.5860   val Loss: 28.4887   time: 213.40s   best: 25.8490
2023-11-03 05:54:24,453:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 05:54:24,498:INFO:  Epoch 194/500:  train Loss: 17.5481   val Loss: 25.6656   time: 216.94s   best: 25.6656
2023-11-03 05:58:02,180:INFO:  Epoch 195/500:  train Loss: 17.6380   val Loss: 26.3188   time: 217.68s   best: 25.6656
2023-11-03 06:01:36,128:INFO:  Epoch 196/500:  train Loss: 17.6018   val Loss: 26.5477   time: 213.92s   best: 25.6656
2023-11-03 06:05:13,856:INFO:  Epoch 197/500:  train Loss: 17.8636   val Loss: 26.8462   time: 217.70s   best: 25.6656
2023-11-03 06:08:50,790:INFO:  Epoch 198/500:  train Loss: 17.7345   val Loss: 25.9805   time: 216.87s   best: 25.6656
2023-11-03 06:12:27,823:INFO:  Epoch 199/500:  train Loss: 17.4236   val Loss: 26.0974   time: 217.02s   best: 25.6656
2023-11-03 06:16:04,757:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 06:16:04,804:INFO:  Epoch 200/500:  train Loss: 17.5722   val Loss: 25.4854   time: 216.92s   best: 25.4854
2023-11-03 06:19:38,693:INFO:  Epoch 201/500:  train Loss: 17.4972   val Loss: 27.2784   time: 213.89s   best: 25.4854
2023-11-03 06:23:12,948:INFO:  Epoch 202/500:  train Loss: 17.8428   val Loss: 27.6517   time: 214.23s   best: 25.4854
2023-11-03 06:26:47,122:INFO:  Epoch 203/500:  train Loss: 17.4592   val Loss: 27.5774   time: 214.14s   best: 25.4854
2023-11-03 06:30:22,548:INFO:  Epoch 204/500:  train Loss: 17.4963   val Loss: 26.2214   time: 215.40s   best: 25.4854
2023-11-03 06:33:58,575:INFO:  Epoch 205/500:  train Loss: 17.4483   val Loss: 29.4360   time: 216.00s   best: 25.4854
2023-11-03 06:37:36,358:INFO:  Epoch 206/500:  train Loss: 17.5391   val Loss: 26.0949   time: 217.77s   best: 25.4854
2023-11-03 06:41:14,031:INFO:  Epoch 207/500:  train Loss: 17.3652   val Loss: 25.5559   time: 217.64s   best: 25.4854
2023-11-03 06:44:50,323:INFO:  Epoch 208/500:  train Loss: 17.4278   val Loss: 26.6440   time: 216.26s   best: 25.4854
2023-11-03 06:48:26,717:INFO:  Epoch 209/500:  train Loss: 17.4541   val Loss: 26.7727   time: 216.37s   best: 25.4854
2023-11-03 06:52:04,746:INFO:  Epoch 210/500:  train Loss: 17.4247   val Loss: 26.5133   time: 218.02s   best: 25.4854
2023-11-03 06:55:39,801:INFO:  Epoch 211/500:  train Loss: 17.8455   val Loss: 25.8782   time: 215.03s   best: 25.4854
2023-11-03 06:59:16,569:INFO:  Epoch 212/500:  train Loss: 17.3155   val Loss: 25.5224   time: 216.70s   best: 25.4854
2023-11-03 07:02:53,469:INFO:  Epoch 213/500:  train Loss: 17.3517   val Loss: 27.1612   time: 216.88s   best: 25.4854
2023-11-03 07:06:27,177:INFO:  Epoch 214/500:  train Loss: 17.2529   val Loss: 25.7869   time: 213.69s   best: 25.4854
2023-11-03 07:10:01,118:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 07:10:01,145:INFO:  Epoch 215/500:  train Loss: 17.6178   val Loss: 25.0156   time: 213.91s   best: 25.0156
2023-11-03 07:13:35,335:INFO:  Epoch 216/500:  train Loss: 17.3412   val Loss: 26.1027   time: 214.18s   best: 25.0156
2023-11-03 07:17:13,689:INFO:  Epoch 217/500:  train Loss: 17.4714   val Loss: 26.4293   time: 218.33s   best: 25.0156
2023-11-03 07:20:52,040:INFO:  Epoch 218/500:  train Loss: 17.3713   val Loss: 26.2792   time: 218.33s   best: 25.0156
2023-11-03 07:24:26,572:INFO:  Epoch 219/500:  train Loss: 17.2401   val Loss: 25.6271   time: 214.49s   best: 25.0156
2023-11-03 07:28:00,734:INFO:  Epoch 220/500:  train Loss: 17.2977   val Loss: 26.7519   time: 214.14s   best: 25.0156
2023-11-03 07:31:35,582:INFO:  Epoch 221/500:  train Loss: 17.3060   val Loss: 25.5218   time: 214.82s   best: 25.0156
2023-11-03 07:35:12,476:INFO:  Epoch 222/500:  train Loss: 17.2772   val Loss: 27.5552   time: 216.87s   best: 25.0156
2023-11-03 07:38:50,092:INFO:  Epoch 223/500:  train Loss: 17.5798   val Loss: 26.0101   time: 217.59s   best: 25.0156
2023-11-03 07:42:26,504:INFO:  Epoch 224/500:  train Loss: 17.1711   val Loss: 25.2521   time: 216.39s   best: 25.0156
2023-11-03 07:46:01,805:INFO:  Epoch 225/500:  train Loss: 17.3535   val Loss: 26.9647   time: 215.28s   best: 25.0156
2023-11-03 07:49:37,985:INFO:  Epoch 226/500:  train Loss: 17.3701   val Loss: 29.5666   time: 216.15s   best: 25.0156
2023-11-03 07:53:11,687:INFO:  Epoch 227/500:  train Loss: 17.2011   val Loss: 25.7098   time: 213.68s   best: 25.0156
2023-11-03 07:56:49,925:INFO:  Epoch 228/500:  train Loss: 17.1987   val Loss: 27.7208   time: 218.21s   best: 25.0156
2023-11-03 08:00:25,949:INFO:  Epoch 229/500:  train Loss: 17.2346   val Loss: 25.8876   time: 216.01s   best: 25.0156
2023-11-03 08:04:02,319:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 08:04:02,340:INFO:  Epoch 230/500:  train Loss: 17.1796   val Loss: 24.9432   time: 216.35s   best: 24.9432
2023-11-03 08:07:36,818:INFO:  Epoch 231/500:  train Loss: 17.1265   val Loss: 26.0664   time: 214.47s   best: 24.9432
2023-11-03 08:11:13,340:INFO:  Epoch 232/500:  train Loss: 17.0825   val Loss: 25.7727   time: 216.51s   best: 24.9432
2023-11-03 08:14:49,958:INFO:  Epoch 233/500:  train Loss: 17.1337   val Loss: 25.2812   time: 216.58s   best: 24.9432
2023-11-03 08:18:23,597:INFO:  Epoch 234/500:  train Loss: 17.1428   val Loss: 25.6021   time: 213.63s   best: 24.9432
2023-11-03 08:22:01,002:INFO:  Epoch 235/500:  train Loss: 17.0338   val Loss: 25.3134   time: 217.38s   best: 24.9432
2023-11-03 08:25:38,195:INFO:  Epoch 236/500:  train Loss: 17.2994   val Loss: 26.7285   time: 217.18s   best: 24.9432
2023-11-03 08:29:15,167:INFO:  Epoch 237/500:  train Loss: 17.1099   val Loss: 25.4570   time: 216.95s   best: 24.9432
2023-11-03 08:32:48,189:INFO:  Epoch 238/500:  train Loss: 17.0822   val Loss: 26.1840   time: 213.00s   best: 24.9432
2023-11-03 08:36:26,327:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 08:36:26,409:INFO:  Epoch 239/500:  train Loss: 17.0496   val Loss: 24.9382   time: 218.12s   best: 24.9382
2023-11-03 08:40:01,711:INFO:  Epoch 240/500:  train Loss: 17.2086   val Loss: 24.9460   time: 215.29s   best: 24.9382
2023-11-03 08:43:39,293:INFO:  Epoch 241/500:  train Loss: 17.0068   val Loss: 25.4633   time: 217.55s   best: 24.9382
2023-11-03 08:47:16,673:INFO:  Epoch 242/500:  train Loss: 17.0190   val Loss: 27.0751   time: 217.35s   best: 24.9382
2023-11-03 08:50:53,449:INFO:  Epoch 243/500:  train Loss: 17.0494   val Loss: 25.7733   time: 216.75s   best: 24.9382
2023-11-03 08:54:30,758:INFO:  Epoch 244/500:  train Loss: 17.0352   val Loss: 25.0041   time: 217.29s   best: 24.9382
2023-11-03 08:58:08,916:INFO:  Epoch 245/500:  train Loss: 16.9947   val Loss: 26.2372   time: 218.14s   best: 24.9382
2023-11-03 09:01:45,955:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 09:01:46,022:INFO:  Epoch 246/500:  train Loss: 17.0339   val Loss: 24.7452   time: 217.02s   best: 24.7452
2023-11-03 09:05:21,223:INFO:  Epoch 247/500:  train Loss: 16.9852   val Loss: 26.3061   time: 215.18s   best: 24.7452
2023-11-03 09:08:54,643:INFO:  Epoch 248/500:  train Loss: 17.0707   val Loss: 25.9876   time: 213.40s   best: 24.7452
2023-11-03 09:12:28,201:INFO:  Epoch 249/500:  train Loss: 17.0337   val Loss: 25.7931   time: 213.54s   best: 24.7452
2023-11-03 09:16:05,176:INFO:  Epoch 250/500:  train Loss: 17.0647   val Loss: 31.1317   time: 216.94s   best: 24.7452
2023-11-03 09:19:38,736:INFO:  Epoch 251/500:  train Loss: 17.1408   val Loss: 25.2973   time: 213.55s   best: 24.7452
2023-11-03 09:23:15,306:INFO:  Epoch 252/500:  train Loss: 16.8849   val Loss: 26.3749   time: 216.55s   best: 24.7452
2023-11-03 09:26:52,581:INFO:  Epoch 253/500:  train Loss: 16.9792   val Loss: 26.7106   time: 217.27s   best: 24.7452
2023-11-03 09:30:28,165:INFO:  Epoch 254/500:  train Loss: 16.9105   val Loss: 25.9627   time: 215.56s   best: 24.7452
2023-11-03 09:34:04,707:INFO:  Epoch 255/500:  train Loss: 16.9681   val Loss: 25.5043   time: 216.53s   best: 24.7452
2023-11-03 09:37:39,342:INFO:  Epoch 256/500:  train Loss: 16.9236   val Loss: 25.9290   time: 214.62s   best: 24.7452
2023-11-03 09:41:13,405:INFO:  Epoch 257/500:  train Loss: 17.2173   val Loss: 25.5001   time: 214.06s   best: 24.7452
2023-11-03 09:44:48,729:INFO:  Epoch 258/500:  train Loss: 16.9943   val Loss: 25.0388   time: 215.30s   best: 24.7452
2023-11-03 09:48:23,430:INFO:  Epoch 259/500:  train Loss: 16.9925   val Loss: 26.4669   time: 214.69s   best: 24.7452
2023-11-03 09:52:00,687:INFO:  Epoch 260/500:  train Loss: 16.9758   val Loss: 25.8414   time: 217.24s   best: 24.7452
2023-11-03 09:55:34,562:INFO:  Epoch 261/500:  train Loss: 16.8531   val Loss: 25.5499   time: 213.85s   best: 24.7452
2023-11-03 09:59:09,298:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 09:59:09,325:INFO:  Epoch 262/500:  train Loss: 16.8687   val Loss: 24.7220   time: 214.72s   best: 24.7220
2023-11-03 10:02:44,090:INFO:  Epoch 263/500:  train Loss: 17.0031   val Loss: 25.8537   time: 214.76s   best: 24.7220
2023-11-03 10:06:20,306:INFO:  Epoch 264/500:  train Loss: 16.9207   val Loss: 25.5532   time: 216.21s   best: 24.7220
2023-11-03 10:09:54,962:INFO:  Epoch 265/500:  train Loss: 17.0117   val Loss: 29.0421   time: 214.63s   best: 24.7220
2023-11-03 10:13:29,093:INFO:  Epoch 266/500:  train Loss: 17.3341   val Loss: 26.2766   time: 214.11s   best: 24.7220
2023-11-03 10:17:06,989:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 10:17:07,013:INFO:  Epoch 267/500:  train Loss: 16.8867   val Loss: 24.6706   time: 217.86s   best: 24.6706
2023-11-03 10:20:41,528:INFO:  Epoch 268/500:  train Loss: 16.9127   val Loss: 25.3541   time: 214.51s   best: 24.6706
2023-11-03 10:24:17,961:INFO:  Epoch 269/500:  train Loss: 16.8087   val Loss: 25.1261   time: 216.42s   best: 24.6706
2023-11-03 10:27:57,084:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 10:27:57,108:INFO:  Epoch 270/500:  train Loss: 16.7738   val Loss: 24.6675   time: 219.09s   best: 24.6675
2023-11-03 10:31:33,270:INFO:  Epoch 271/500:  train Loss: 16.8552   val Loss: 25.2069   time: 216.15s   best: 24.6675
2023-11-03 10:35:08,400:INFO:  Epoch 272/500:  train Loss: 16.9623   val Loss: 25.3076   time: 215.10s   best: 24.6675
2023-11-03 10:38:42,320:INFO:  Epoch 273/500:  train Loss: 16.8807   val Loss: 26.4921   time: 213.90s   best: 24.6675
2023-11-03 10:42:17,114:INFO:  Epoch 274/500:  train Loss: 16.8732   val Loss: 24.7119   time: 214.77s   best: 24.6675
2023-11-03 10:45:53,769:INFO:  Epoch 275/500:  train Loss: 16.8602   val Loss: 25.4481   time: 216.63s   best: 24.6675
2023-11-03 10:49:27,493:INFO:  Epoch 276/500:  train Loss: 16.8118   val Loss: 24.7453   time: 213.70s   best: 24.6675
2023-11-03 10:53:05,155:INFO:  Epoch 277/500:  train Loss: 16.7285   val Loss: 30.0993   time: 217.61s   best: 24.6675
2023-11-03 10:56:42,541:INFO:  Epoch 278/500:  train Loss: 16.7917   val Loss: 24.7300   time: 217.37s   best: 24.6675
2023-11-03 11:00:21,986:INFO:  Epoch 279/500:  train Loss: 16.7772   val Loss: 27.5609   time: 219.44s   best: 24.6675
2023-11-03 11:03:56,555:INFO:  Epoch 280/500:  train Loss: 16.9035   val Loss: 25.7779   time: 214.56s   best: 24.6675
2023-11-03 11:07:35,269:INFO:  Epoch 281/500:  train Loss: 16.7386   val Loss: 25.5408   time: 218.68s   best: 24.6675
2023-11-03 11:11:12,842:INFO:  Epoch 282/500:  train Loss: 17.0135   val Loss: 25.2256   time: 217.55s   best: 24.6675
2023-11-03 11:14:50,379:INFO:  Epoch 283/500:  train Loss: 16.8630   val Loss: 25.5471   time: 217.51s   best: 24.6675
2023-11-03 11:18:26,618:INFO:  Epoch 284/500:  train Loss: 16.7837   val Loss: 25.4767   time: 216.22s   best: 24.6675
2023-11-03 11:22:00,771:INFO:  Epoch 285/500:  train Loss: 16.6698   val Loss: 25.0329   time: 214.13s   best: 24.6675
2023-11-03 11:25:34,820:INFO:  Epoch 286/500:  train Loss: 16.6504   val Loss: 24.7074   time: 213.98s   best: 24.6675
2023-11-03 11:29:10,241:INFO:  Epoch 287/500:  train Loss: 16.7846   val Loss: 26.7757   time: 215.38s   best: 24.6675
2023-11-03 11:32:44,511:INFO:  Epoch 288/500:  train Loss: 16.7652   val Loss: 26.7678   time: 214.25s   best: 24.6675
2023-11-03 11:36:21,996:INFO:  Epoch 289/500:  train Loss: 16.8213   val Loss: 24.8894   time: 217.47s   best: 24.6675
2023-11-03 11:39:57,961:INFO:  Epoch 290/500:  train Loss: 16.8367   val Loss: 24.9932   time: 215.95s   best: 24.6675
2023-11-03 11:43:35,848:INFO:  Epoch 291/500:  train Loss: 16.7348   val Loss: 25.7071   time: 217.88s   best: 24.6675
2023-11-03 11:47:09,807:INFO:  Epoch 292/500:  train Loss: 16.7278   val Loss: 25.7610   time: 213.95s   best: 24.6675
2023-11-03 11:50:43,614:INFO:  Epoch 293/500:  train Loss: 16.7406   val Loss: 24.9437   time: 213.80s   best: 24.6675
2023-11-03 11:54:20,093:INFO:  Epoch 294/500:  train Loss: 16.9718   val Loss: 29.1436   time: 216.46s   best: 24.6675
2023-11-03 11:57:53,823:INFO:  Epoch 295/500:  train Loss: 16.8901   val Loss: 24.8092   time: 213.71s   best: 24.6675
2023-11-03 12:01:30,916:INFO:  Epoch 296/500:  train Loss: 16.6413   val Loss: 24.9287   time: 217.09s   best: 24.6675
2023-11-03 12:05:05,326:INFO:  Epoch 297/500:  train Loss: 16.7610   val Loss: 24.9001   time: 214.38s   best: 24.6675
2023-11-03 12:08:43,018:INFO:  Epoch 298/500:  train Loss: 16.6282   val Loss: 26.0293   time: 217.62s   best: 24.6675
2023-11-03 12:12:17,207:INFO:  Epoch 299/500:  train Loss: 16.7230   val Loss: 24.8294   time: 214.16s   best: 24.6675
2023-11-03 12:15:51,019:INFO:  Epoch 300/500:  train Loss: 17.0052   val Loss: 25.9000   time: 213.80s   best: 24.6675
2023-11-03 12:19:25,309:INFO:  Epoch 301/500:  train Loss: 16.6488   val Loss: 25.0637   time: 214.27s   best: 24.6675
2023-11-03 12:22:59,681:INFO:  Epoch 302/500:  train Loss: 16.6059   val Loss: 24.7173   time: 214.35s   best: 24.6675
2023-11-03 12:26:34,047:INFO:  Epoch 303/500:  train Loss: 16.5672   val Loss: 25.5242   time: 214.36s   best: 24.6675
2023-11-03 12:30:07,774:INFO:  Epoch 304/500:  train Loss: 16.6307   val Loss: 26.3634   time: 213.72s   best: 24.6675
2023-11-03 12:33:41,823:INFO:  Epoch 305/500:  train Loss: 16.6330   val Loss: 24.9100   time: 214.04s   best: 24.6675
2023-11-03 12:37:20,506:INFO:  Epoch 306/500:  train Loss: 16.6111   val Loss: 25.1620   time: 218.52s   best: 24.6675
2023-11-03 12:40:58,409:INFO:  Epoch 307/500:  train Loss: 16.7578   val Loss: 26.0484   time: 217.88s   best: 24.6675
2023-11-03 12:44:32,432:INFO:  Epoch 308/500:  train Loss: 16.9748   val Loss: 25.5596   time: 214.01s   best: 24.6675
2023-11-03 12:48:06,217:INFO:  Epoch 309/500:  train Loss: 16.6498   val Loss: 25.0028   time: 213.78s   best: 24.6675
2023-11-03 12:51:40,147:INFO:  Epoch 310/500:  train Loss: 16.5264   val Loss: 25.1412   time: 213.77s   best: 24.6675
2023-11-03 12:55:14,177:INFO:  Epoch 311/500:  train Loss: 16.5735   val Loss: 25.2765   time: 214.02s   best: 24.6675
2023-11-03 12:58:51,199:INFO:  Epoch 312/500:  train Loss: 16.4976   val Loss: 24.8183   time: 217.01s   best: 24.6675
2023-11-03 13:02:25,724:INFO:  Epoch 313/500:  train Loss: 16.4992   val Loss: 25.2574   time: 214.52s   best: 24.6675
2023-11-03 13:05:59,596:INFO:  Epoch 314/500:  train Loss: 16.6332   val Loss: 24.9104   time: 213.85s   best: 24.6675
2023-11-03 13:09:33,815:INFO:  Epoch 315/500:  train Loss: 16.5154   val Loss: 25.6793   time: 214.20s   best: 24.6675
2023-11-03 13:13:08,061:INFO:  Epoch 316/500:  train Loss: 16.5908   val Loss: 26.9469   time: 214.22s   best: 24.6675
2023-11-03 13:16:42,188:INFO:  Epoch 317/500:  train Loss: 17.8969   val Loss: 24.7503   time: 214.11s   best: 24.6675
2023-11-03 13:20:18,453:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 13:20:18,482:INFO:  Epoch 318/500:  train Loss: 16.5705   val Loss: 24.3600   time: 216.24s   best: 24.3600
2023-11-03 13:23:53,423:INFO:  Epoch 319/500:  train Loss: 16.5146   val Loss: 28.7921   time: 214.93s   best: 24.3600
2023-11-03 13:27:27,924:INFO:  Epoch 320/500:  train Loss: 16.5811   val Loss: 25.3153   time: 214.47s   best: 24.3600
2023-11-03 13:31:04,412:INFO:  Epoch 321/500:  train Loss: 16.5614   val Loss: 24.7381   time: 216.46s   best: 24.3600
2023-11-03 13:34:42,240:INFO:  Epoch 322/500:  train Loss: 16.6315   val Loss: 25.0344   time: 217.81s   best: 24.3600
2023-11-03 13:38:18,071:INFO:  Epoch 323/500:  train Loss: 16.4276   val Loss: 25.5980   time: 215.82s   best: 24.3600
2023-11-03 13:41:54,164:INFO:  Epoch 324/500:  train Loss: 16.4891   val Loss: 25.2150   time: 216.07s   best: 24.3600
2023-11-03 13:45:28,131:INFO:  Epoch 325/500:  train Loss: 16.4556   val Loss: 25.8560   time: 213.96s   best: 24.3600
2023-11-03 13:49:02,842:INFO:  Epoch 326/500:  train Loss: 16.4386   val Loss: 25.1343   time: 214.69s   best: 24.3600
2023-11-03 13:52:38,597:INFO:  Epoch 327/500:  train Loss: 16.3720   val Loss: 26.2407   time: 215.75s   best: 24.3600
2023-11-03 13:56:12,919:INFO:  Epoch 328/500:  train Loss: 16.7591   val Loss: 25.5463   time: 214.32s   best: 24.3600
2023-11-03 13:59:48,263:INFO:  Epoch 329/500:  train Loss: 16.4748   val Loss: 24.7013   time: 215.31s   best: 24.3600
2023-11-03 14:03:24,655:INFO:  Epoch 330/500:  train Loss: 16.4435   val Loss: 25.0037   time: 216.37s   best: 24.3600
2023-11-03 14:06:59,741:INFO:  Epoch 331/500:  train Loss: 16.4437   val Loss: 25.5416   time: 215.07s   best: 24.3600
2023-11-03 14:10:34,255:INFO:  Epoch 332/500:  train Loss: 16.5121   val Loss: 28.0606   time: 214.48s   best: 24.3600
2023-11-03 14:14:11,905:INFO:  Epoch 333/500:  train Loss: 16.4440   val Loss: 24.8498   time: 217.63s   best: 24.3600
2023-11-03 14:17:49,248:INFO:  Epoch 334/500:  train Loss: 16.3991   val Loss: 24.4958   time: 217.34s   best: 24.3600
2023-11-03 14:21:25,364:INFO:  Epoch 335/500:  train Loss: 16.4394   val Loss: 25.4178   time: 216.11s   best: 24.3600
2023-11-03 14:25:03,548:INFO:  Epoch 336/500:  train Loss: 16.5389   val Loss: 25.3520   time: 218.15s   best: 24.3600
2023-11-03 14:28:37,863:INFO:  Epoch 337/500:  train Loss: 16.9182   val Loss: 25.1281   time: 214.29s   best: 24.3600
2023-11-03 14:32:15,046:INFO:  Epoch 338/500:  train Loss: 16.4095   val Loss: 24.7545   time: 217.16s   best: 24.3600
2023-11-03 14:35:50,975:INFO:  Epoch 339/500:  train Loss: 16.4145   val Loss: 24.9520   time: 215.92s   best: 24.3600
2023-11-03 14:39:25,409:INFO:  Epoch 340/500:  train Loss: 16.5728   val Loss: 25.5479   time: 214.42s   best: 24.3600
2023-11-03 14:43:03,930:INFO:  Epoch 341/500:  train Loss: 16.4229   val Loss: 24.8848   time: 218.50s   best: 24.3600
2023-11-03 14:46:42,308:INFO:  Epoch 342/500:  train Loss: 16.4890   val Loss: 26.0540   time: 218.36s   best: 24.3600
2023-11-03 14:50:16,755:INFO:  Epoch 343/500:  train Loss: 16.4286   val Loss: 25.1042   time: 214.42s   best: 24.3600
2023-11-03 14:53:53,249:INFO:  Epoch 344/500:  train Loss: 16.5019   val Loss: 24.5849   time: 216.48s   best: 24.3600
2023-11-03 14:57:27,087:INFO:  Epoch 345/500:  train Loss: 16.4058   val Loss: 25.2500   time: 213.81s   best: 24.3600
2023-11-03 15:01:01,536:INFO:  Epoch 346/500:  train Loss: 16.5175   val Loss: 25.4299   time: 214.42s   best: 24.3600
2023-11-03 15:04:37,613:INFO:  Epoch 347/500:  train Loss: 16.4554   val Loss: 24.9531   time: 216.05s   best: 24.3600
2023-11-03 15:08:13,222:INFO:  Epoch 348/500:  train Loss: 16.3689   val Loss: 24.5669   time: 215.59s   best: 24.3600
2023-11-03 15:11:47,486:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 15:11:47,515:INFO:  Epoch 349/500:  train Loss: 16.3628   val Loss: 24.1370   time: 214.24s   best: 24.1370
2023-11-03 15:15:21,499:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 15:15:21,526:INFO:  Epoch 350/500:  train Loss: 16.3458   val Loss: 24.0805   time: 213.97s   best: 24.0805
2023-11-03 15:18:59,182:INFO:  Epoch 351/500:  train Loss: 16.3486   val Loss: 25.0539   time: 217.66s   best: 24.0805
2023-11-03 15:22:33,728:INFO:  Epoch 352/500:  train Loss: 16.5186   val Loss: 25.6561   time: 214.54s   best: 24.0805
2023-11-03 15:26:07,600:INFO:  Epoch 353/500:  train Loss: 16.3170   val Loss: 25.2794   time: 213.85s   best: 24.0805
2023-11-03 15:29:42,621:INFO:  Epoch 354/500:  train Loss: 16.2576   val Loss: 24.6944   time: 214.99s   best: 24.0805
2023-11-03 15:33:17,730:INFO:  Epoch 355/500:  train Loss: 16.3766   val Loss: 24.6886   time: 215.09s   best: 24.0805
2023-11-03 15:36:54,076:INFO:  Epoch 356/500:  train Loss: 16.5212   val Loss: 24.2983   time: 216.32s   best: 24.0805
2023-11-03 15:40:31,102:INFO:  Epoch 357/500:  train Loss: 16.2787   val Loss: 24.6005   time: 217.01s   best: 24.0805
2023-11-03 15:44:08,847:INFO:  Epoch 358/500:  train Loss: 16.3899   val Loss: 24.5534   time: 217.72s   best: 24.0805
2023-11-03 15:47:43,259:INFO:  Epoch 359/500:  train Loss: 16.3781   val Loss: 25.2381   time: 214.39s   best: 24.0805
2023-11-03 15:51:17,480:INFO:  Epoch 360/500:  train Loss: 16.2342   val Loss: 25.1023   time: 214.19s   best: 24.0805
2023-11-03 15:54:52,647:INFO:  Epoch 361/500:  train Loss: 16.3290   val Loss: 24.6181   time: 215.13s   best: 24.0805
2023-11-03 15:58:30,493:INFO:  Epoch 362/500:  train Loss: 16.3878   val Loss: 24.5863   time: 217.82s   best: 24.0805
2023-11-03 16:02:05,227:INFO:  Epoch 363/500:  train Loss: 16.3867   val Loss: 24.3494   time: 214.71s   best: 24.0805
2023-11-03 16:05:39,789:INFO:  Epoch 364/500:  train Loss: 16.2062   val Loss: 28.3630   time: 214.53s   best: 24.0805
2023-11-03 16:09:17,409:INFO:  Epoch 365/500:  train Loss: 16.4833   val Loss: 24.3625   time: 217.58s   best: 24.0805
2023-11-03 16:12:54,699:INFO:  Epoch 366/500:  train Loss: 16.2722   val Loss: 42.0251   time: 217.27s   best: 24.0805
2023-11-03 16:16:31,345:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 16:16:31,374:INFO:  Epoch 367/500:  train Loss: 16.7903   val Loss: 24.0505   time: 216.62s   best: 24.0505
2023-11-03 16:20:07,150:INFO:  Epoch 368/500:  train Loss: 16.3387   val Loss: 28.4181   time: 215.76s   best: 24.0505
2023-11-03 16:23:42,165:INFO:  Epoch 369/500:  train Loss: 16.3264   val Loss: 25.1443   time: 214.98s   best: 24.0505
2023-11-03 16:27:16,066:INFO:  Epoch 370/500:  train Loss: 16.2694   val Loss: 24.5304   time: 213.89s   best: 24.0505
2023-11-03 16:30:52,727:INFO:  Epoch 371/500:  train Loss: 16.2176   val Loss: 24.3351   time: 216.65s   best: 24.0505
2023-11-03 16:34:28,349:INFO:  Epoch 372/500:  train Loss: 16.2910   val Loss: 26.0065   time: 215.60s   best: 24.0505
2023-11-03 16:38:06,729:INFO:  Epoch 373/500:  train Loss: 16.2696   val Loss: 25.6964   time: 218.36s   best: 24.0505
2023-11-03 16:41:40,672:INFO:  Epoch 374/500:  train Loss: 16.1843   val Loss: 24.0812   time: 213.92s   best: 24.0505
2023-11-03 16:45:17,721:INFO:  Epoch 375/500:  train Loss: 16.2417   val Loss: 25.0954   time: 217.03s   best: 24.0505
2023-11-03 16:48:51,937:INFO:  Epoch 376/500:  train Loss: 16.2140   val Loss: 24.1488   time: 214.19s   best: 24.0505
2023-11-03 16:52:26,601:INFO:  Epoch 377/500:  train Loss: 16.1907   val Loss: 24.2543   time: 214.65s   best: 24.0505
2023-11-03 16:56:01,225:INFO:  Epoch 378/500:  train Loss: 16.3163   val Loss: 24.2963   time: 214.62s   best: 24.0505
2023-11-03 16:59:35,840:INFO:  Epoch 379/500:  train Loss: 16.3831   val Loss: 25.4732   time: 214.59s   best: 24.0505
2023-11-03 17:03:10,137:INFO:  Epoch 380/500:  train Loss: 16.1784   val Loss: 25.2438   time: 214.28s   best: 24.0505
2023-11-03 17:06:44,359:INFO:  Epoch 381/500:  train Loss: 16.2498   val Loss: 25.1940   time: 214.20s   best: 24.0505
2023-11-03 17:10:21,319:INFO:  Epoch 382/500:  train Loss: 16.1949   val Loss: 24.4817   time: 216.95s   best: 24.0505
2023-11-03 17:13:55,306:INFO:  Epoch 383/500:  train Loss: 16.1716   val Loss: 24.9481   time: 213.98s   best: 24.0505
2023-11-03 17:17:31,659:INFO:  Epoch 384/500:  train Loss: 16.2022   val Loss: 24.7374   time: 216.24s   best: 24.0505
2023-11-03 17:21:07,986:INFO:  Epoch 385/500:  train Loss: 16.3386   val Loss: 24.5062   time: 216.30s   best: 24.0505
2023-11-03 17:24:42,567:INFO:  Epoch 386/500:  train Loss: 16.1954   val Loss: 24.3154   time: 214.56s   best: 24.0505
2023-11-03 17:28:18,965:INFO:  Epoch 387/500:  train Loss: 16.0847   val Loss: 27.4494   time: 216.37s   best: 24.0505
2023-11-03 17:31:53,107:INFO:  Epoch 388/500:  train Loss: 16.1175   val Loss: 24.8143   time: 214.12s   best: 24.0505
2023-11-03 17:35:27,054:INFO:  Epoch 389/500:  train Loss: 16.1565   val Loss: 28.3163   time: 213.94s   best: 24.0505
2023-11-03 17:39:05,234:INFO:  Epoch 390/500:  train Loss: 16.2726   val Loss: 24.1082   time: 218.17s   best: 24.0505
2023-11-03 17:42:43,114:INFO:  Epoch 391/500:  train Loss: 16.1818   val Loss: 25.7288   time: 217.86s   best: 24.0505
2023-11-03 17:46:21,013:INFO:  Epoch 392/500:  train Loss: 16.1211   val Loss: 26.8487   time: 217.87s   best: 24.0505
2023-11-03 17:49:59,664:INFO:  Epoch 393/500:  train Loss: 16.1856   val Loss: 25.5266   time: 218.64s   best: 24.0505
2023-11-03 17:53:37,769:INFO:  Epoch 394/500:  train Loss: 16.1328   val Loss: 24.4980   time: 218.09s   best: 24.0505
2023-11-03 17:57:15,648:INFO:  Epoch 395/500:  train Loss: 16.0654   val Loss: 24.7086   time: 217.86s   best: 24.0505
2023-11-03 18:00:47,647:INFO:  Epoch 396/500:  train Loss: 16.1649   val Loss: 24.1158   time: 211.98s   best: 24.0505
2023-11-03 18:04:22,518:INFO:  Epoch 397/500:  train Loss: 16.1522   val Loss: 24.7414   time: 214.86s   best: 24.0505
2023-11-03 18:07:54,016:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 10 mils dataset (0.1 dropout)_ff03.pt
2023-11-03 18:07:54,040:INFO:  Epoch 398/500:  train Loss: 16.1586   val Loss: 23.7835   time: 211.47s   best: 23.7835
2023-11-03 18:11:25,568:INFO:  Epoch 399/500:  train Loss: 16.1593   val Loss: 25.3311   time: 211.53s   best: 23.7835
2023-11-03 18:15:02,388:INFO:  Epoch 400/500:  train Loss: 16.1523   val Loss: 24.4518   time: 216.79s   best: 23.7835
2023-11-03 18:18:39,352:INFO:  Epoch 401/500:  train Loss: 16.2696   val Loss: 25.5623   time: 216.94s   best: 23.7835
2023-11-03 18:22:10,924:INFO:  Epoch 402/500:  train Loss: 16.1108   val Loss: 26.1521   time: 211.56s   best: 23.7835
2023-11-03 18:25:42,455:INFO:  Epoch 403/500:  train Loss: 16.1993   val Loss: 24.1183   time: 211.51s   best: 23.7835
2023-11-03 18:29:14,125:INFO:  Epoch 404/500:  train Loss: 16.0623   val Loss: 24.6582   time: 211.64s   best: 23.7835
2023-11-03 18:32:45,643:INFO:  Epoch 405/500:  train Loss: 16.0872   val Loss: 25.3076   time: 211.50s   best: 23.7835
2023-11-03 18:36:17,963:INFO:  Epoch 406/500:  train Loss: 16.5638   val Loss: 24.6878   time: 212.30s   best: 23.7835
2023-11-03 18:39:51,278:INFO:  Epoch 407/500:  train Loss: 16.0890   val Loss: 25.2103   time: 213.29s   best: 23.7835
2023-11-03 18:43:24,269:INFO:  Epoch 408/500:  train Loss: 16.0765   val Loss: 24.6687   time: 212.96s   best: 23.7835
2023-11-03 18:46:58,913:INFO:  Epoch 409/500:  train Loss: 16.0827   val Loss: 24.5130   time: 214.62s   best: 23.7835
2023-11-03 18:50:35,802:INFO:  Epoch 410/500:  train Loss: 16.0089   val Loss: 24.8841   time: 216.88s   best: 23.7835
2023-11-03 18:54:09,134:INFO:  Epoch 411/500:  train Loss: 16.5038   val Loss: 24.1837   time: 213.32s   best: 23.7835
2023-11-03 18:57:42,728:INFO:  Epoch 412/500:  train Loss: 16.0741   val Loss: 25.9266   time: 213.53s   best: 23.7835
2023-11-03 19:01:15,757:INFO:  Epoch 413/500:  train Loss: 16.3407   val Loss: 24.5978   time: 213.00s   best: 23.7835
2023-11-03 19:04:49,132:INFO:  Epoch 414/500:  train Loss: 16.2938   val Loss: 25.4453   time: 213.36s   best: 23.7835
2023-11-03 19:08:22,473:INFO:  Epoch 415/500:  train Loss: 16.0867   val Loss: 24.5922   time: 212.97s   best: 23.7835
2023-11-03 19:11:55,996:INFO:  Epoch 416/500:  train Loss: 16.1091   val Loss: 25.1804   time: 213.52s   best: 23.7835
2023-11-03 19:15:29,037:INFO:  Epoch 417/500:  train Loss: 16.0835   val Loss: 24.0527   time: 213.03s   best: 23.7835
2023-11-03 19:19:05,799:INFO:  Epoch 418/500:  train Loss: 16.1177   val Loss: 25.9082   time: 216.74s   best: 23.7835
2023-11-03 19:22:43,980:INFO:  Epoch 419/500:  train Loss: 16.0587   val Loss: 26.2170   time: 218.16s   best: 23.7835
2023-11-03 19:26:16,686:INFO:  Epoch 420/500:  train Loss: 16.1179   val Loss: 24.8859   time: 212.68s   best: 23.7835
2023-11-03 19:29:49,871:INFO:  Epoch 421/500:  train Loss: 16.2990   val Loss: 25.2994   time: 213.17s   best: 23.7835
2023-11-03 19:33:26,581:INFO:  Epoch 422/500:  train Loss: 16.3251   val Loss: 24.9635   time: 216.66s   best: 23.7835
2023-11-03 19:36:59,965:INFO:  Epoch 423/500:  train Loss: 16.0361   val Loss: 24.1422   time: 213.36s   best: 23.7835
2023-11-03 19:40:33,423:INFO:  Epoch 424/500:  train Loss: 16.0717   val Loss: 25.6515   time: 213.43s   best: 23.7835
2023-11-03 19:44:06,628:INFO:  Epoch 425/500:  train Loss: 16.0531   val Loss: 24.8807   time: 213.18s   best: 23.7835
2023-11-03 19:47:43,319:INFO:  Epoch 426/500:  train Loss: 16.0597   val Loss: 24.8802   time: 216.68s   best: 23.7835
2023-11-03 19:51:15,018:INFO:  Epoch 427/500:  train Loss: 16.2538   val Loss: 28.5329   time: 211.65s   best: 23.7835
2023-11-03 19:54:51,081:INFO:  Epoch 428/500:  train Loss: 16.2018   val Loss: 24.9811   time: 216.03s   best: 23.7835
2023-11-03 19:58:28,027:INFO:  Epoch 429/500:  train Loss: 16.0095   val Loss: 24.2332   time: 216.92s   best: 23.7835
2023-11-03 20:01:59,852:INFO:  Epoch 430/500:  train Loss: 16.0084   val Loss: 24.6741   time: 211.76s   best: 23.7835
2023-11-03 20:05:37,034:INFO:  Epoch 431/500:  train Loss: 16.1733   val Loss: 25.3955   time: 217.16s   best: 23.7835
2023-11-03 20:09:09,901:INFO:  Epoch 432/500:  train Loss: 16.4333   val Loss: 24.6112   time: 212.85s   best: 23.7835
2023-11-03 20:12:42,875:INFO:  Epoch 433/500:  train Loss: 16.1470   val Loss: 24.5232   time: 212.96s   best: 23.7835
2023-11-03 20:16:16,687:INFO:  Epoch 434/500:  train Loss: 16.5189   val Loss: 26.5626   time: 213.79s   best: 23.7835
2023-11-03 20:19:53,495:INFO:  Epoch 435/500:  train Loss: 16.0929   val Loss: 24.7270   time: 216.80s   best: 23.7835
2023-11-03 20:23:25,187:INFO:  Epoch 436/500:  train Loss: 16.0042   val Loss: 24.1751   time: 211.68s   best: 23.7835
2023-11-03 20:26:59,971:INFO:  Epoch 437/500:  train Loss: 16.1754   val Loss: 24.4930   time: 214.76s   best: 23.7835
2023-11-03 20:30:33,586:INFO:  Epoch 438/500:  train Loss: 15.9756   val Loss: 25.0007   time: 213.58s   best: 23.7835
2023-11-03 20:34:06,352:INFO:  Epoch 439/500:  train Loss: 16.0508   val Loss: 24.6269   time: 212.75s   best: 23.7835
2023-11-03 20:37:42,433:INFO:  Epoch 440/500:  train Loss: 15.9952   val Loss: 25.0069   time: 216.05s   best: 23.7835
2023-11-03 20:41:15,192:INFO:  Epoch 441/500:  train Loss: 15.9907   val Loss: 24.5067   time: 212.73s   best: 23.7835
2023-11-03 20:44:48,201:INFO:  Epoch 442/500:  train Loss: 16.1297   val Loss: 24.2665   time: 212.80s   best: 23.7835
2023-11-03 20:48:22,092:INFO:  Epoch 443/500:  train Loss: 15.9875   val Loss: 24.5008   time: 213.87s   best: 23.7835
2023-11-03 20:51:55,273:INFO:  Epoch 444/500:  train Loss: 16.2606   val Loss: 25.0076   time: 213.15s   best: 23.7835
2023-11-03 20:55:31,405:INFO:  Epoch 445/500:  train Loss: 16.0220   val Loss: 26.6243   time: 216.10s   best: 23.7835
2023-11-03 20:59:04,748:INFO:  Epoch 446/500:  train Loss: 15.9602   val Loss: 24.3553   time: 213.32s   best: 23.7835
2023-11-03 21:02:37,489:INFO:  Epoch 447/500:  train Loss: 15.9391   val Loss: 25.3108   time: 212.70s   best: 23.7835
2023-11-03 21:06:13,864:INFO:  Epoch 448/500:  train Loss: 15.9381   val Loss: 26.1919   time: 216.34s   best: 23.7835
2023-11-03 21:09:47,678:INFO:  Epoch 449/500:  train Loss: 16.3619   val Loss: 27.3905   time: 213.81s   best: 23.7835
2023-11-03 21:13:24,430:INFO:  Epoch 450/500:  train Loss: 15.9944   val Loss: 26.0894   time: 216.49s   best: 23.7835
2023-11-03 21:17:00,575:INFO:  Epoch 451/500:  train Loss: 15.9304   val Loss: 24.6593   time: 216.12s   best: 23.7835
2023-11-03 21:20:33,719:INFO:  Epoch 452/500:  train Loss: 16.0122   val Loss: 25.3314   time: 213.12s   best: 23.7835
2023-11-03 21:24:11,267:INFO:  Epoch 453/500:  train Loss: 15.9993   val Loss: 24.8767   time: 217.52s   best: 23.7835
2023-11-03 21:27:46,240:INFO:  Epoch 454/500:  train Loss: 15.9125   val Loss: 25.4892   time: 214.96s   best: 23.7835
2023-11-03 21:31:19,466:INFO:  Epoch 455/500:  train Loss: 15.9974   val Loss: 25.4964   time: 213.21s   best: 23.7835
2023-11-03 21:34:53,821:INFO:  Epoch 456/500:  train Loss: 15.9043   val Loss: 24.5880   time: 214.33s   best: 23.7835
2023-11-03 21:38:26,150:INFO:  Epoch 457/500:  train Loss: 15.8985   val Loss: 25.3635   time: 212.30s   best: 23.7835
2023-11-03 21:41:57,935:INFO:  Epoch 458/500:  train Loss: 15.9285   val Loss: 24.9845   time: 211.76s   best: 23.7835
2023-11-03 21:45:33,972:INFO:  Epoch 459/500:  train Loss: 16.1135   val Loss: 24.5635   time: 216.01s   best: 23.7835
2023-11-03 21:49:05,775:INFO:  Epoch 460/500:  train Loss: 15.9848   val Loss: 25.5246   time: 211.79s   best: 23.7835
2023-11-03 21:52:38,974:INFO:  Epoch 461/500:  train Loss: 15.9366   val Loss: 26.4396   time: 213.17s   best: 23.7835
2023-11-03 21:56:13,024:INFO:  Epoch 462/500:  train Loss: 15.9991   val Loss: 27.5075   time: 214.03s   best: 23.7835
2023-11-03 21:59:50,140:INFO:  Epoch 463/500:  train Loss: 16.0970   val Loss: 25.3571   time: 217.09s   best: 23.7835
2023-11-03 22:03:23,818:INFO:  Epoch 464/500:  train Loss: 15.9076   val Loss: 24.1296   time: 213.65s   best: 23.7835
2023-11-03 22:07:00,469:INFO:  Epoch 465/500:  train Loss: 16.0333   val Loss: 26.4245   time: 216.63s   best: 23.7835
2023-11-03 22:10:36,926:INFO:  Epoch 466/500:  train Loss: 16.0474   val Loss: 24.9730   time: 216.43s   best: 23.7835
2023-11-03 22:14:11,379:INFO:  Epoch 467/500:  train Loss: 16.0377   val Loss: 24.9219   time: 214.45s   best: 23.7835
2023-11-03 22:17:47,038:INFO:  Epoch 468/500:  train Loss: 15.9845   val Loss: 25.5193   time: 215.64s   best: 23.7835
2023-11-03 22:21:19,887:INFO:  Epoch 469/500:  train Loss: 15.9655   val Loss: 25.3169   time: 212.81s   best: 23.7835
2023-11-03 22:24:52,150:INFO:  Epoch 470/500:  train Loss: 16.2232   val Loss: 24.7234   time: 212.22s   best: 23.7835
2023-11-03 22:28:28,511:INFO:  Epoch 471/500:  train Loss: 15.9960   val Loss: 25.2126   time: 216.34s   best: 23.7835
2023-11-03 22:32:00,072:INFO:  Epoch 472/500:  train Loss: 15.9178   val Loss: 25.0716   time: 211.53s   best: 23.7835
2023-11-03 22:35:34,627:INFO:  Epoch 473/500:  train Loss: 15.9054   val Loss: 24.1642   time: 214.54s   best: 23.7835
2023-11-03 22:39:09,480:INFO:  Epoch 474/500:  train Loss: 15.9990   val Loss: 25.1187   time: 214.82s   best: 23.7835
2023-11-03 22:42:46,860:INFO:  Epoch 475/500:  train Loss: 15.9177   val Loss: 25.5861   time: 217.38s   best: 23.7835
2023-11-03 22:46:20,680:INFO:  Epoch 476/500:  train Loss: 15.8581   val Loss: 25.8891   time: 213.81s   best: 23.7835
2023-11-03 22:49:56,846:INFO:  Epoch 477/500:  train Loss: 15.8508   val Loss: 25.2284   time: 216.13s   best: 23.7835
2023-11-03 22:53:33,159:INFO:  Epoch 478/500:  train Loss: 15.9991   val Loss: 25.8525   time: 216.28s   best: 23.7835
2023-11-03 22:57:06,217:INFO:  Epoch 479/500:  train Loss: 16.0847   val Loss: 24.0913   time: 213.00s   best: 23.7835
2023-11-03 23:00:38,218:INFO:  Epoch 480/500:  train Loss: 15.8920   val Loss: 24.0315   time: 211.88s   best: 23.7835
2023-11-03 23:04:12,244:INFO:  Epoch 481/500:  train Loss: 15.9086   val Loss: 24.6378   time: 214.02s   best: 23.7835
2023-11-03 23:07:45,837:INFO:  Epoch 482/500:  train Loss: 15.8625   val Loss: 24.5115   time: 213.57s   best: 23.7835
2023-11-03 23:11:19,349:INFO:  Epoch 483/500:  train Loss: 15.8118   val Loss: 24.5755   time: 213.50s   best: 23.7835
2023-11-03 23:14:53,125:INFO:  Epoch 484/500:  train Loss: 15.9242   val Loss: 27.0501   time: 213.77s   best: 23.7835
2023-11-03 23:18:28,893:INFO:  Epoch 485/500:  train Loss: 15.8225   val Loss: 24.4606   time: 215.75s   best: 23.7835
2023-11-03 23:22:03,988:INFO:  Epoch 486/500:  train Loss: 15.9382   val Loss: 26.8088   time: 215.05s   best: 23.7835
2023-11-03 23:25:39,144:INFO:  Epoch 487/500:  train Loss: 15.8702   val Loss: 25.2215   time: 215.11s   best: 23.7835
2023-11-03 23:29:14,037:INFO:  Epoch 488/500:  train Loss: 15.9911   val Loss: 25.4655   time: 214.89s   best: 23.7835
2023-11-03 23:32:46,471:INFO:  Epoch 489/500:  train Loss: 15.8683   val Loss: 24.1884   time: 212.42s   best: 23.7835
2023-11-03 23:36:20,603:INFO:  Epoch 490/500:  train Loss: 15.7781   val Loss: 24.4079   time: 214.10s   best: 23.7835
2023-11-03 23:39:54,283:INFO:  Epoch 491/500:  train Loss: 15.8909   val Loss: 26.4120   time: 213.64s   best: 23.7835
2023-11-03 23:43:27,559:INFO:  Epoch 492/500:  train Loss: 15.8405   val Loss: 28.0742   time: 213.25s   best: 23.7835
2023-11-03 23:47:01,633:INFO:  Epoch 493/500:  train Loss: 15.8407   val Loss: 25.2712   time: 214.06s   best: 23.7835
2023-11-03 23:50:37,904:INFO:  Epoch 494/500:  train Loss: 15.8588   val Loss: 24.1889   time: 216.24s   best: 23.7835
2023-11-03 23:54:11,392:INFO:  Epoch 495/500:  train Loss: 15.7388   val Loss: 25.3709   time: 213.46s   best: 23.7835
2023-11-03 23:57:46,843:INFO:  Epoch 496/500:  train Loss: 15.8229   val Loss: 24.0606   time: 215.44s   best: 23.7835
2023-11-04 00:01:20,387:INFO:  Epoch 497/500:  train Loss: 15.9341   val Loss: 25.2501   time: 213.53s   best: 23.7835
2023-11-04 00:04:57,277:INFO:  Epoch 498/500:  train Loss: 15.9760   val Loss: 24.5274   time: 216.87s   best: 23.7835
2023-11-04 00:08:33,547:INFO:  Epoch 499/500:  train Loss: 16.4474   val Loss: 24.7589   time: 216.25s   best: 23.7835
2023-11-04 00:12:08,008:INFO:  Epoch 500/500:  train Loss: 15.8729   val Loss: 24.8479   time: 214.45s   best: 23.7835
2023-11-04 00:12:08,083:INFO:  -----> Training complete in 1794m 14s   best validation loss: 23.7835
 
2023-11-04 00:12:43,480:INFO:  Starting experiment lstm autoencoder with 0.5 dataset (0.1 dropout)
2023-11-04 00:12:43,481:INFO:  Defining the model
2023-11-04 00:12:43,565:INFO:  Reading the dataset
2023-11-04 00:44:20,240:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 00:44:20,269:INFO:  Epoch 1/500:  train Loss: 80.8110   val Loss: 73.2820   time: 249.44s   best: 73.2820
2023-11-04 00:48:36,402:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 00:48:36,439:INFO:  Epoch 2/500:  train Loss: 70.0390   val Loss: 68.6778   time: 256.12s   best: 68.6778
2023-11-04 00:52:49,908:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 00:52:49,955:INFO:  Epoch 3/500:  train Loss: 65.8447   val Loss: 63.8664   time: 253.45s   best: 63.8664
2023-11-04 00:57:11,715:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 00:57:11,748:INFO:  Epoch 4/500:  train Loss: 62.5178   val Loss: 60.6979   time: 261.76s   best: 60.6979
2023-11-04 01:01:27,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:01:27,115:INFO:  Epoch 5/500:  train Loss: 60.0951   val Loss: 58.3922   time: 255.34s   best: 58.3922
2023-11-04 01:05:57,805:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:05:57,836:INFO:  Epoch 6/500:  train Loss: 57.7236   val Loss: 56.4220   time: 270.69s   best: 56.4220
2023-11-04 01:10:08,722:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:10:08,753:INFO:  Epoch 7/500:  train Loss: 54.8705   val Loss: 52.9429   time: 250.87s   best: 52.9429
2023-11-04 01:14:31,225:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:14:31,253:INFO:  Epoch 8/500:  train Loss: 52.1100   val Loss: 50.6228   time: 262.47s   best: 50.6228
2023-11-04 01:18:38,363:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:18:38,401:INFO:  Epoch 9/500:  train Loss: 49.6796   val Loss: 50.0143   time: 247.09s   best: 50.0143
2023-11-04 01:22:47,852:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:22:47,884:INFO:  Epoch 10/500:  train Loss: 47.6896   val Loss: 46.9703   time: 249.45s   best: 46.9703
2023-11-04 01:27:25,990:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:27:26,094:INFO:  Epoch 11/500:  train Loss: 46.0410   val Loss: 46.7688   time: 278.10s   best: 46.7688
2023-11-04 01:31:38,223:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:31:38,358:INFO:  Epoch 12/500:  train Loss: 44.6795   val Loss: 45.3860   time: 252.11s   best: 45.3860
2023-11-04 01:36:06,269:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:36:06,370:INFO:  Epoch 13/500:  train Loss: 43.2768   val Loss: 42.9433   time: 267.89s   best: 42.9433
2023-11-04 01:40:26,298:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:40:26,440:INFO:  Epoch 14/500:  train Loss: 41.8187   val Loss: 42.0587   time: 259.90s   best: 42.0587
2023-11-04 01:44:51,325:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:44:51,364:INFO:  Epoch 15/500:  train Loss: 40.6260   val Loss: 40.9733   time: 264.85s   best: 40.9733
2023-11-04 01:49:09,274:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:49:09,301:INFO:  Epoch 16/500:  train Loss: 39.5425   val Loss: 39.7691   time: 257.90s   best: 39.7691
2023-11-04 01:53:29,534:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:53:30,233:INFO:  Epoch 17/500:  train Loss: 38.5015   val Loss: 38.6447   time: 260.23s   best: 38.6447
2023-11-04 01:58:19,500:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 01:58:19,540:INFO:  Epoch 18/500:  train Loss: 37.5073   val Loss: 37.9200   time: 289.26s   best: 37.9200
2023-11-04 02:04:05,357:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 02:04:05,401:INFO:  Epoch 19/500:  train Loss: 37.0799   val Loss: 37.8818   time: 345.81s   best: 37.8818
2023-11-04 02:09:51,862:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 02:09:51,903:INFO:  Epoch 20/500:  train Loss: 35.8925   val Loss: 36.3970   time: 346.45s   best: 36.3970
2023-11-04 02:15:39,105:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 02:15:39,147:INFO:  Epoch 21/500:  train Loss: 35.1731   val Loss: 35.5256   time: 347.19s   best: 35.5256
2023-11-04 02:20:17,605:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 02:20:17,626:INFO:  Epoch 22/500:  train Loss: 34.4216   val Loss: 34.8488   time: 278.45s   best: 34.8488
2023-11-04 02:24:42,685:INFO:  Epoch 23/500:  train Loss: 34.0172   val Loss: 34.8670   time: 265.06s   best: 34.8488
2023-11-04 02:29:16,395:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 02:29:16,416:INFO:  Epoch 24/500:  train Loss: 33.6184   val Loss: 33.7166   time: 273.70s   best: 33.7166
2023-11-04 02:33:26,394:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 02:33:26,415:INFO:  Epoch 25/500:  train Loss: 32.9143   val Loss: 33.2190   time: 249.97s   best: 33.2190
2023-11-04 02:37:35,435:INFO:  Epoch 26/500:  train Loss: 32.5454   val Loss: 33.5334   time: 249.02s   best: 33.2190
2023-11-04 02:41:44,399:INFO:  Epoch 27/500:  train Loss: 32.0267   val Loss: 33.3192   time: 248.96s   best: 33.2190
2023-11-04 02:45:56,039:INFO:  Epoch 28/500:  train Loss: 31.7483   val Loss: 34.4683   time: 251.64s   best: 33.2190
2023-11-04 02:50:00,781:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 02:50:00,803:INFO:  Epoch 29/500:  train Loss: 31.5002   val Loss: 32.0967   time: 244.74s   best: 32.0967
2023-11-04 02:54:13,066:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 02:54:13,089:INFO:  Epoch 30/500:  train Loss: 31.0995   val Loss: 31.9913   time: 252.26s   best: 31.9913
2023-11-04 02:58:21,568:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 02:58:21,588:INFO:  Epoch 31/500:  train Loss: 30.8324   val Loss: 31.9195   time: 248.47s   best: 31.9195
2023-11-04 03:02:37,704:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 03:02:37,724:INFO:  Epoch 32/500:  train Loss: 30.4026   val Loss: 31.7513   time: 256.11s   best: 31.7513
2023-11-04 03:06:47,066:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 03:06:47,087:INFO:  Epoch 33/500:  train Loss: 30.1494   val Loss: 31.1880   time: 249.34s   best: 31.1880
2023-11-04 03:11:52,995:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 03:11:53,035:INFO:  Epoch 34/500:  train Loss: 29.8645   val Loss: 30.6931   time: 305.90s   best: 30.6931
2023-11-04 03:17:02,703:INFO:  Epoch 35/500:  train Loss: 29.6485   val Loss: 31.1995   time: 309.67s   best: 30.6931
2023-11-04 03:22:46,337:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 03:22:46,376:INFO:  Epoch 36/500:  train Loss: 29.4216   val Loss: 30.2559   time: 343.61s   best: 30.2559
2023-11-04 03:28:33,667:INFO:  Epoch 37/500:  train Loss: 29.6801   val Loss: 30.3951   time: 347.29s   best: 30.2559
2023-11-04 03:34:20,479:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 03:34:20,518:INFO:  Epoch 38/500:  train Loss: 28.9471   val Loss: 30.2013   time: 346.80s   best: 30.2013
2023-11-04 03:39:45,417:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 03:39:45,455:INFO:  Epoch 39/500:  train Loss: 28.7420   val Loss: 29.9547   time: 324.89s   best: 29.9547
2023-11-04 03:45:32,600:INFO:  Epoch 40/500:  train Loss: 28.7023   val Loss: 29.9908   time: 347.14s   best: 29.9547
2023-11-04 03:50:57,274:INFO:  Epoch 41/500:  train Loss: 28.4084   val Loss: 30.6489   time: 324.67s   best: 29.9547
2023-11-04 03:55:04,677:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 03:55:04,702:INFO:  Epoch 42/500:  train Loss: 28.1711   val Loss: 29.7696   time: 247.39s   best: 29.7696
2023-11-04 03:59:19,792:INFO:  Epoch 43/500:  train Loss: 28.1930   val Loss: 35.0038   time: 255.09s   best: 29.7696
2023-11-04 04:03:30,817:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 04:03:30,837:INFO:  Epoch 44/500:  train Loss: 28.0609   val Loss: 29.0120   time: 251.02s   best: 29.0120
2023-11-04 04:08:59,460:INFO:  Epoch 45/500:  train Loss: 27.8947   val Loss: 29.1861   time: 328.62s   best: 29.0120
2023-11-04 04:14:46,222:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 04:14:46,262:INFO:  Epoch 46/500:  train Loss: 27.4500   val Loss: 28.8327   time: 346.74s   best: 28.8327
2023-11-04 04:19:36,942:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 04:19:37,601:INFO:  Epoch 47/500:  train Loss: 27.3750   val Loss: 28.6573   time: 290.67s   best: 28.6573
2023-11-04 04:23:50,930:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 04:23:50,950:INFO:  Epoch 48/500:  train Loss: 27.2694   val Loss: 28.3431   time: 253.32s   best: 28.3431
2023-11-04 04:28:18,637:INFO:  Epoch 49/500:  train Loss: 27.0551   val Loss: 28.7729   time: 267.69s   best: 28.3431
2023-11-04 04:32:25,533:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 04:32:25,553:INFO:  Epoch 50/500:  train Loss: 26.8214   val Loss: 28.1085   time: 246.89s   best: 28.1085
2023-11-04 04:36:40,959:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 04:36:40,979:INFO:  Epoch 51/500:  train Loss: 26.8451   val Loss: 27.6572   time: 255.40s   best: 27.6572
2023-11-04 04:40:44,893:INFO:  Epoch 52/500:  train Loss: 26.7417   val Loss: 28.1031   time: 243.91s   best: 27.6572
2023-11-04 04:46:03,034:INFO:  Epoch 53/500:  train Loss: 26.4314   val Loss: 28.3284   time: 318.14s   best: 27.6572
2023-11-04 04:51:46,687:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 04:51:46,726:INFO:  Epoch 54/500:  train Loss: 26.3530   val Loss: 27.6121   time: 343.64s   best: 27.6121
2023-11-04 04:57:33,260:INFO:  Epoch 55/500:  train Loss: 26.3404   val Loss: 27.7949   time: 346.53s   best: 27.6121
2023-11-04 05:03:17,354:INFO:  Epoch 56/500:  train Loss: 26.0864   val Loss: 27.7883   time: 344.09s   best: 27.6121
2023-11-04 05:08:43,564:INFO:  Epoch 57/500:  train Loss: 25.9234   val Loss: 27.8189   time: 326.21s   best: 27.6121
2023-11-04 05:14:31,573:INFO:  Epoch 58/500:  train Loss: 26.1158   val Loss: 28.8690   time: 348.01s   best: 27.6121
2023-11-04 05:18:40,347:INFO:  Epoch 59/500:  train Loss: 25.7783   val Loss: 29.3610   time: 248.77s   best: 27.6121
2023-11-04 05:22:47,392:INFO:  Epoch 60/500:  train Loss: 25.9984   val Loss: 27.6664   time: 247.04s   best: 27.6121
2023-11-04 05:27:07,264:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 05:27:07,283:INFO:  Epoch 61/500:  train Loss: 25.5849   val Loss: 27.1647   time: 259.86s   best: 27.1647
2023-11-04 05:31:13,574:INFO:  Epoch 62/500:  train Loss: 25.5707   val Loss: 31.7705   time: 246.29s   best: 27.1647
2023-11-04 05:36:57,899:INFO:  Epoch 63/500:  train Loss: 25.3121   val Loss: 27.2368   time: 344.32s   best: 27.1647
2023-11-04 05:42:41,821:INFO:  Epoch 64/500:  train Loss: 25.7463   val Loss: 27.5156   time: 343.92s   best: 27.1647
2023-11-04 05:47:59,268:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 05:47:59,289:INFO:  Epoch 65/500:  train Loss: 25.2432   val Loss: 26.6808   time: 317.44s   best: 26.6808
2023-11-04 05:52:21,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 05:52:22,018:INFO:  Epoch 66/500:  train Loss: 25.1087   val Loss: 26.6307   time: 262.69s   best: 26.6307
2023-11-04 05:56:31,316:INFO:  Epoch 67/500:  train Loss: 24.8580   val Loss: 27.3171   time: 249.30s   best: 26.6307
2023-11-04 06:00:38,731:INFO:  Epoch 68/500:  train Loss: 24.8672   val Loss: 26.8790   time: 247.41s   best: 26.6307
2023-11-04 06:05:45,487:INFO:  Epoch 69/500:  train Loss: 24.7968   val Loss: 27.2252   time: 306.75s   best: 26.6307
2023-11-04 06:09:56,888:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 06:09:56,909:INFO:  Epoch 70/500:  train Loss: 24.8116   val Loss: 26.6003   time: 251.39s   best: 26.6003
2023-11-04 06:14:06,574:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 06:14:06,594:INFO:  Epoch 71/500:  train Loss: 24.5814   val Loss: 26.2280   time: 249.64s   best: 26.2280
2023-11-04 06:18:13,076:INFO:  Epoch 72/500:  train Loss: 24.5158   val Loss: 27.4057   time: 246.48s   best: 26.2280
2023-11-04 06:22:36,783:INFO:  Epoch 73/500:  train Loss: 24.3983   val Loss: 26.5995   time: 263.71s   best: 26.2280
2023-11-04 06:26:46,853:INFO:  Epoch 74/500:  train Loss: 24.3344   val Loss: 26.4197   time: 250.07s   best: 26.2280
2023-11-04 06:30:55,793:INFO:  Epoch 75/500:  train Loss: 24.2973   val Loss: 27.1031   time: 248.93s   best: 26.2280
2023-11-04 06:35:01,570:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 06:35:01,589:INFO:  Epoch 76/500:  train Loss: 24.4129   val Loss: 26.0454   time: 245.77s   best: 26.0454
2023-11-04 06:40:40,669:INFO:  Epoch 77/500:  train Loss: 24.4313   val Loss: 28.2373   time: 339.08s   best: 26.0454
2023-11-04 06:45:26,730:INFO:  Epoch 78/500:  train Loss: 24.1109   val Loss: 26.4075   time: 286.06s   best: 26.0454
2023-11-04 06:49:33,038:INFO:  Epoch 79/500:  train Loss: 23.9444   val Loss: 26.0466   time: 246.31s   best: 26.0454
2023-11-04 06:53:50,972:INFO:  Epoch 80/500:  train Loss: 23.9019   val Loss: 26.2202   time: 257.93s   best: 26.0454
2023-11-04 06:58:09,798:INFO:  Epoch 81/500:  train Loss: 23.8885   val Loss: 29.6434   time: 258.82s   best: 26.0454
2023-11-04 07:02:18,602:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 07:02:18,628:INFO:  Epoch 82/500:  train Loss: 23.9501   val Loss: 25.9129   time: 248.80s   best: 25.9129
2023-11-04 07:06:25,284:INFO:  Epoch 83/500:  train Loss: 23.7441   val Loss: 26.4943   time: 246.66s   best: 25.9129
2023-11-04 07:10:32,529:INFO:  Epoch 84/500:  train Loss: 23.7106   val Loss: 26.1342   time: 247.24s   best: 25.9129
2023-11-04 07:14:48,371:INFO:  Epoch 85/500:  train Loss: 23.5900   val Loss: 26.0312   time: 255.84s   best: 25.9129
2023-11-04 07:18:57,811:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 07:18:57,832:INFO:  Epoch 86/500:  train Loss: 23.7125   val Loss: 25.7660   time: 249.44s   best: 25.7660
2023-11-04 07:23:04,744:INFO:  Epoch 87/500:  train Loss: 23.4631   val Loss: 27.2251   time: 246.91s   best: 25.7660
2023-11-04 07:27:13,540:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 07:27:13,561:INFO:  Epoch 88/500:  train Loss: 23.4630   val Loss: 25.3320   time: 248.79s   best: 25.3320
2023-11-04 07:31:30,121:INFO:  Epoch 89/500:  train Loss: 23.3187   val Loss: 25.7481   time: 256.56s   best: 25.3320
2023-11-04 07:35:34,749:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 07:35:34,768:INFO:  Epoch 90/500:  train Loss: 23.3756   val Loss: 25.1938   time: 244.62s   best: 25.1938
2023-11-04 07:39:43,944:INFO:  Epoch 91/500:  train Loss: 23.2692   val Loss: 26.7474   time: 249.18s   best: 25.1938
2023-11-04 07:43:53,037:INFO:  Epoch 92/500:  train Loss: 23.3465   val Loss: 25.8192   time: 249.09s   best: 25.1938
2023-11-04 07:48:05,582:INFO:  Epoch 93/500:  train Loss: 23.1162   val Loss: 25.6115   time: 252.54s   best: 25.1938
2023-11-04 07:52:23,092:INFO:  Epoch 94/500:  train Loss: 23.1187   val Loss: 25.7587   time: 257.51s   best: 25.1938
2023-11-04 07:56:31,811:INFO:  Epoch 95/500:  train Loss: 22.9967   val Loss: 25.9276   time: 248.72s   best: 25.1938
2023-11-04 08:00:56,663:INFO:  Epoch 96/500:  train Loss: 22.9717   val Loss: 25.3913   time: 264.83s   best: 25.1938
2023-11-04 08:06:09,299:INFO:  Epoch 97/500:  train Loss: 22.9445   val Loss: 26.2749   time: 312.63s   best: 25.1938
2023-11-04 08:10:43,745:INFO:  Epoch 98/500:  train Loss: 22.8745   val Loss: 26.2309   time: 274.45s   best: 25.1938
2023-11-04 08:14:53,228:INFO:  Epoch 99/500:  train Loss: 22.7971   val Loss: 25.5957   time: 249.48s   best: 25.1938
2023-11-04 08:18:56,925:INFO:  Epoch 100/500:  train Loss: 22.7984   val Loss: 26.4729   time: 243.70s   best: 25.1938
2023-11-04 08:23:19,994:INFO:  Epoch 101/500:  train Loss: 22.7403   val Loss: 25.5868   time: 263.07s   best: 25.1938
2023-11-04 08:28:43,199:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 08:28:43,233:INFO:  Epoch 102/500:  train Loss: 22.7659   val Loss: 25.0696   time: 323.20s   best: 25.0696
2023-11-04 08:34:11,493:INFO:  Epoch 103/500:  train Loss: 22.6286   val Loss: 25.2606   time: 328.26s   best: 25.0696
2023-11-04 08:39:03,908:INFO:  Epoch 104/500:  train Loss: 22.5802   val Loss: 25.4828   time: 292.41s   best: 25.0696
2023-11-04 08:43:18,746:INFO:  Epoch 105/500:  train Loss: 22.7979   val Loss: 25.7129   time: 254.82s   best: 25.0696
2023-11-04 08:47:44,647:INFO:  Epoch 106/500:  train Loss: 22.5103   val Loss: 25.7596   time: 265.90s   best: 25.0696
2023-11-04 08:52:42,581:INFO:  Epoch 107/500:  train Loss: 22.4905   val Loss: 25.3558   time: 297.93s   best: 25.0696
2023-11-04 08:56:50,384:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 08:56:50,403:INFO:  Epoch 108/500:  train Loss: 22.4005   val Loss: 25.0418   time: 247.80s   best: 25.0418
2023-11-04 09:00:58,611:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 09:00:58,632:INFO:  Epoch 109/500:  train Loss: 22.4785   val Loss: 24.9579   time: 248.20s   best: 24.9579
2023-11-04 09:05:19,836:INFO:  Epoch 110/500:  train Loss: 22.5111   val Loss: 26.2332   time: 261.20s   best: 24.9579
2023-11-04 09:09:29,542:INFO:  Epoch 111/500:  train Loss: 22.8895   val Loss: 25.5389   time: 249.70s   best: 24.9579
2023-11-04 09:14:12,305:INFO:  Epoch 112/500:  train Loss: 22.4197   val Loss: 25.4268   time: 282.76s   best: 24.9579
2023-11-04 09:19:01,719:INFO:  Epoch 113/500:  train Loss: 22.2439   val Loss: 25.4319   time: 289.40s   best: 24.9579
2023-11-04 09:23:11,441:INFO:  Epoch 114/500:  train Loss: 22.3137   val Loss: 25.2528   time: 249.72s   best: 24.9579
2023-11-04 09:27:16,938:INFO:  Epoch 115/500:  train Loss: 22.1206   val Loss: 25.2498   time: 245.50s   best: 24.9579
2023-11-04 09:31:22,335:INFO:  Epoch 116/500:  train Loss: 22.1511   val Loss: 25.7373   time: 245.40s   best: 24.9579
2023-11-04 09:36:08,829:INFO:  Epoch 117/500:  train Loss: 22.0459   val Loss: 25.0202   time: 286.49s   best: 24.9579
2023-11-04 09:40:16,953:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 09:40:16,973:INFO:  Epoch 118/500:  train Loss: 22.0053   val Loss: 24.9186   time: 248.11s   best: 24.9186
2023-11-04 09:44:51,213:INFO:  Epoch 119/500:  train Loss: 22.0224   val Loss: 25.3492   time: 274.24s   best: 24.9186
2023-11-04 09:49:17,067:INFO:  Epoch 120/500:  train Loss: 22.0708   val Loss: 25.0203   time: 265.85s   best: 24.9186
2023-11-04 09:53:40,293:INFO:  Epoch 121/500:  train Loss: 21.9404   val Loss: 25.9767   time: 263.23s   best: 24.9186
2023-11-04 09:57:43,851:INFO:  Epoch 122/500:  train Loss: 21.8733   val Loss: 25.3520   time: 243.56s   best: 24.9186
2023-11-04 10:01:53,261:INFO:  Epoch 123/500:  train Loss: 22.3292   val Loss: 24.9747   time: 249.41s   best: 24.9186
2023-11-04 10:06:01,093:INFO:  Epoch 124/500:  train Loss: 22.0741   val Loss: 25.3187   time: 247.83s   best: 24.9186
2023-11-04 10:10:21,641:INFO:  Epoch 125/500:  train Loss: 21.7948   val Loss: 25.1478   time: 260.55s   best: 24.9186
2023-11-04 10:14:27,974:INFO:  Epoch 126/500:  train Loss: 22.0730   val Loss: 25.2810   time: 246.33s   best: 24.9186
2023-11-04 10:19:01,510:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 10:19:01,535:INFO:  Epoch 127/500:  train Loss: 22.1564   val Loss: 24.6302   time: 273.53s   best: 24.6302
2023-11-04 10:23:18,617:INFO:  Epoch 128/500:  train Loss: 21.8130   val Loss: 25.0002   time: 257.08s   best: 24.6302
2023-11-04 10:27:21,900:INFO:  Epoch 129/500:  train Loss: 21.7488   val Loss: 29.4890   time: 243.28s   best: 24.6302
2023-11-04 10:31:39,465:INFO:  Epoch 130/500:  train Loss: 21.6565   val Loss: 26.2071   time: 257.56s   best: 24.6302
2023-11-04 10:35:47,623:INFO:  Epoch 131/500:  train Loss: 21.6285   val Loss: 25.2003   time: 248.16s   best: 24.6302
2023-11-04 10:40:58,206:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 10:40:58,455:INFO:  Epoch 132/500:  train Loss: 21.7067   val Loss: 24.3738   time: 310.57s   best: 24.3738
2023-11-04 10:46:24,051:INFO:  Epoch 133/500:  train Loss: 21.6929   val Loss: 26.6219   time: 325.60s   best: 24.3738
2023-11-04 10:52:10,178:INFO:  Epoch 134/500:  train Loss: 21.6889   val Loss: 24.6496   time: 346.12s   best: 24.3738
2023-11-04 10:57:54,791:INFO:  Epoch 135/500:  train Loss: 21.6895   val Loss: 29.2593   time: 344.61s   best: 24.3738
2023-11-04 11:02:26,440:INFO:  Epoch 136/500:  train Loss: 21.5091   val Loss: 24.6562   time: 271.65s   best: 24.3738
2023-11-04 11:07:20,055:INFO:  Epoch 137/500:  train Loss: 21.4753   val Loss: 24.6440   time: 293.61s   best: 24.3738
2023-11-04 11:11:29,833:INFO:  Epoch 138/500:  train Loss: 21.5108   val Loss: 25.3361   time: 249.78s   best: 24.3738
2023-11-04 11:15:37,147:INFO:  Epoch 139/500:  train Loss: 21.5364   val Loss: 24.9849   time: 247.31s   best: 24.3738
2023-11-04 11:19:43,107:INFO:  Epoch 140/500:  train Loss: 21.4792   val Loss: 24.8385   time: 245.96s   best: 24.3738
2023-11-04 11:23:45,986:INFO:  Epoch 141/500:  train Loss: 21.3382   val Loss: 24.3898   time: 242.88s   best: 24.3738
2023-11-04 11:27:50,742:INFO:  Epoch 142/500:  train Loss: 21.2832   val Loss: 24.7646   time: 244.75s   best: 24.3738
2023-11-04 11:31:54,512:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 11:31:54,532:INFO:  Epoch 143/500:  train Loss: 21.3035   val Loss: 24.2603   time: 243.77s   best: 24.2603
2023-11-04 11:36:02,866:INFO:  Epoch 144/500:  train Loss: 21.6432   val Loss: 24.6592   time: 248.33s   best: 24.2603
2023-11-04 11:40:14,463:INFO:  Epoch 145/500:  train Loss: 21.2611   val Loss: 24.2812   time: 251.60s   best: 24.2603
2023-11-04 11:44:17,235:INFO:  Epoch 146/500:  train Loss: 21.1881   val Loss: 24.5689   time: 242.77s   best: 24.2603
2023-11-04 11:48:25,461:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 11:48:25,481:INFO:  Epoch 147/500:  train Loss: 21.1631   val Loss: 24.2035   time: 248.22s   best: 24.2035
2023-11-04 11:52:33,200:INFO:  Epoch 148/500:  train Loss: 21.2966   val Loss: 25.0782   time: 247.72s   best: 24.2035
2023-11-04 11:56:40,556:INFO:  Epoch 149/500:  train Loss: 21.4220   val Loss: 24.6838   time: 247.35s   best: 24.2035
2023-11-04 12:00:55,753:INFO:  Epoch 150/500:  train Loss: 21.2580   val Loss: 24.3357   time: 255.20s   best: 24.2035
2023-11-04 12:05:02,356:INFO:  Epoch 151/500:  train Loss: 21.1309   val Loss: 24.4278   time: 246.60s   best: 24.2035
2023-11-04 12:09:12,748:INFO:  Epoch 152/500:  train Loss: 21.1505   val Loss: 25.0767   time: 250.39s   best: 24.2035
2023-11-04 12:13:26,230:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 12:13:26,267:INFO:  Epoch 153/500:  train Loss: 21.0836   val Loss: 24.0767   time: 253.48s   best: 24.0767
2023-11-04 12:17:47,201:INFO:  Epoch 154/500:  train Loss: 21.2495   val Loss: 24.5995   time: 260.93s   best: 24.0767
2023-11-04 12:22:21,006:INFO:  Epoch 155/500:  train Loss: 21.0471   val Loss: 24.6170   time: 273.80s   best: 24.0767
2023-11-04 12:26:28,613:INFO:  Epoch 156/500:  train Loss: 20.9275   val Loss: 24.6282   time: 247.61s   best: 24.0767
2023-11-04 12:31:07,315:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 12:31:07,336:INFO:  Epoch 157/500:  train Loss: 21.0868   val Loss: 24.0665   time: 278.70s   best: 24.0665
2023-11-04 12:36:19,691:INFO:  Epoch 158/500:  train Loss: 20.9279   val Loss: 24.8749   time: 312.35s   best: 24.0665
2023-11-04 12:40:28,228:INFO:  Epoch 159/500:  train Loss: 21.0587   val Loss: 24.3523   time: 248.54s   best: 24.0665
2023-11-04 12:44:57,127:INFO:  Epoch 160/500:  train Loss: 21.1112   val Loss: 24.4324   time: 268.90s   best: 24.0665
2023-11-04 12:49:04,486:INFO:  Epoch 161/500:  train Loss: 20.8344   val Loss: 24.2400   time: 247.36s   best: 24.0665
2023-11-04 12:53:09,594:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 12:53:09,616:INFO:  Epoch 162/500:  train Loss: 20.8290   val Loss: 23.7653   time: 245.10s   best: 23.7653
2023-11-04 12:58:17,297:INFO:  Epoch 163/500:  train Loss: 21.3106   val Loss: 24.6008   time: 307.68s   best: 23.7653
2023-11-04 13:04:01,252:INFO:  Epoch 164/500:  train Loss: 21.1461   val Loss: 24.5441   time: 343.95s   best: 23.7653
2023-11-04 13:09:38,555:INFO:  Epoch 165/500:  train Loss: 20.8541   val Loss: 24.7146   time: 337.30s   best: 23.7653
2023-11-04 13:13:49,815:INFO:  Epoch 166/500:  train Loss: 20.9234   val Loss: 24.5084   time: 251.26s   best: 23.7653
2023-11-04 13:17:56,371:INFO:  Epoch 167/500:  train Loss: 21.4317   val Loss: 24.2662   time: 246.55s   best: 23.7653
2023-11-04 13:21:59,785:INFO:  Epoch 168/500:  train Loss: 20.6962   val Loss: 24.4545   time: 243.41s   best: 23.7653
2023-11-04 13:26:06,020:INFO:  Epoch 169/500:  train Loss: 20.7935   val Loss: 26.2103   time: 246.23s   best: 23.7653
2023-11-04 13:30:16,022:INFO:  Epoch 170/500:  train Loss: 20.7329   val Loss: 24.2106   time: 250.00s   best: 23.7653
2023-11-04 13:34:27,378:INFO:  Epoch 171/500:  train Loss: 20.6999   val Loss: 24.9992   time: 251.36s   best: 23.7653
2023-11-04 13:38:33,347:INFO:  Epoch 172/500:  train Loss: 20.6248   val Loss: 24.5591   time: 245.97s   best: 23.7653
2023-11-04 13:42:40,067:INFO:  Epoch 173/500:  train Loss: 20.8086   val Loss: 24.4269   time: 246.72s   best: 23.7653
2023-11-04 13:46:43,585:INFO:  Epoch 174/500:  train Loss: 20.8607   val Loss: 24.0616   time: 243.52s   best: 23.7653
2023-11-04 13:50:49,888:INFO:  Epoch 175/500:  train Loss: 20.6060   val Loss: 24.5168   time: 246.30s   best: 23.7653
2023-11-04 13:55:05,853:INFO:  Epoch 176/500:  train Loss: 20.6536   val Loss: 24.9150   time: 255.96s   best: 23.7653
2023-11-04 13:59:25,240:INFO:  Epoch 177/500:  train Loss: 20.5203   val Loss: 23.8536   time: 259.39s   best: 23.7653
2023-11-04 14:03:28,588:INFO:  Epoch 178/500:  train Loss: 20.5061   val Loss: 24.5818   time: 243.35s   best: 23.7653
2023-11-04 14:07:34,657:INFO:  Epoch 179/500:  train Loss: 20.5411   val Loss: 24.4016   time: 246.05s   best: 23.7653
2023-11-04 14:11:44,576:INFO:  Epoch 180/500:  train Loss: 20.4567   val Loss: 23.8972   time: 249.92s   best: 23.7653
2023-11-04 14:15:50,811:INFO:  Epoch 181/500:  train Loss: 20.8772   val Loss: 24.3384   time: 246.23s   best: 23.7653
2023-11-04 14:20:09,270:INFO:  Epoch 182/500:  train Loss: 20.5762   val Loss: 23.9346   time: 258.46s   best: 23.7653
2023-11-04 14:24:15,729:INFO:  Epoch 183/500:  train Loss: 20.4329   val Loss: 23.8757   time: 246.46s   best: 23.7653
2023-11-04 14:28:16,489:INFO:  Epoch 184/500:  train Loss: 20.8106   val Loss: 25.5640   time: 240.76s   best: 23.7653
2023-11-04 14:32:18,413:INFO:  Epoch 185/500:  train Loss: 20.3694   val Loss: 23.8166   time: 241.91s   best: 23.7653
2023-11-04 14:36:29,536:INFO:  Epoch 186/500:  train Loss: 20.3290   val Loss: 24.3920   time: 251.12s   best: 23.7653
2023-11-04 14:40:35,701:INFO:  Epoch 187/500:  train Loss: 20.3387   val Loss: 24.0797   time: 246.16s   best: 23.7653
2023-11-04 14:44:42,022:INFO:  Epoch 188/500:  train Loss: 20.4339   val Loss: 24.2946   time: 246.30s   best: 23.7653
2023-11-04 14:48:47,445:INFO:  Epoch 189/500:  train Loss: 20.2539   val Loss: 24.0996   time: 245.42s   best: 23.7653
2023-11-04 14:52:53,369:INFO:  Epoch 190/500:  train Loss: 20.4003   val Loss: 24.0433   time: 245.90s   best: 23.7653
2023-11-04 14:56:57,432:INFO:  Epoch 191/500:  train Loss: 20.6620   val Loss: 23.9218   time: 244.06s   best: 23.7653
2023-11-04 15:01:02,739:INFO:  Epoch 192/500:  train Loss: 20.2066   val Loss: 23.9914   time: 245.31s   best: 23.7653
2023-11-04 15:06:23,091:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 15:06:23,121:INFO:  Epoch 193/500:  train Loss: 20.8489   val Loss: 23.5707   time: 320.34s   best: 23.5707
2023-11-04 15:12:07,119:INFO:  Epoch 194/500:  train Loss: 20.2984   val Loss: 25.2558   time: 344.00s   best: 23.5707
2023-11-04 15:17:42,900:INFO:  Epoch 195/500:  train Loss: 20.2962   val Loss: 23.8534   time: 335.78s   best: 23.5707
2023-11-04 15:22:08,712:INFO:  Epoch 196/500:  train Loss: 20.2962   val Loss: 23.5797   time: 265.81s   best: 23.5707
2023-11-04 15:26:12,442:INFO:  Epoch 197/500:  train Loss: 20.1495   val Loss: 23.7193   time: 243.73s   best: 23.5707
2023-11-04 15:30:18,849:INFO:  Epoch 198/500:  train Loss: 20.2714   val Loss: 23.8226   time: 246.41s   best: 23.5707
2023-11-04 15:34:25,407:INFO:  Epoch 199/500:  train Loss: 20.1255   val Loss: 23.9789   time: 246.56s   best: 23.5707
2023-11-04 15:38:30,945:INFO:  Epoch 200/500:  train Loss: 20.1553   val Loss: 24.1578   time: 245.54s   best: 23.5707
2023-11-04 15:42:36,925:INFO:  Epoch 201/500:  train Loss: 20.1143   val Loss: 23.9358   time: 245.98s   best: 23.5707
2023-11-04 15:46:40,386:INFO:  Epoch 202/500:  train Loss: 20.2276   val Loss: 23.8036   time: 243.46s   best: 23.5707
2023-11-04 15:50:46,946:INFO:  Epoch 203/500:  train Loss: 20.0630   val Loss: 23.8711   time: 246.56s   best: 23.5707
2023-11-04 15:54:53,122:INFO:  Epoch 204/500:  train Loss: 20.0831   val Loss: 23.7808   time: 246.18s   best: 23.5707
2023-11-04 15:59:31,536:INFO:  Epoch 205/500:  train Loss: 20.1852   val Loss: 26.9909   time: 278.41s   best: 23.5707
2023-11-04 16:03:50,932:INFO:  Epoch 206/500:  train Loss: 20.3378   val Loss: 25.0442   time: 259.39s   best: 23.5707
2023-11-04 16:08:20,428:INFO:  Epoch 207/500:  train Loss: 20.0603   val Loss: 24.7434   time: 269.50s   best: 23.5707
2023-11-04 16:12:27,567:INFO:  Epoch 208/500:  train Loss: 20.1123   val Loss: 27.5011   time: 247.14s   best: 23.5707
2023-11-04 16:16:33,294:INFO:  Epoch 209/500:  train Loss: 20.0570   val Loss: 24.0216   time: 245.73s   best: 23.5707
2023-11-04 16:20:37,467:INFO:  Epoch 210/500:  train Loss: 20.0966   val Loss: 24.1853   time: 244.17s   best: 23.5707
2023-11-04 16:24:41,369:INFO:  Epoch 211/500:  train Loss: 19.9464   val Loss: 23.9426   time: 243.90s   best: 23.5707
2023-11-04 16:28:47,304:INFO:  Epoch 212/500:  train Loss: 19.9631   val Loss: 23.8857   time: 245.93s   best: 23.5707
2023-11-04 16:32:51,170:INFO:  Epoch 213/500:  train Loss: 19.9813   val Loss: 29.3616   time: 243.87s   best: 23.5707
2023-11-04 16:36:57,355:INFO:  Epoch 214/500:  train Loss: 19.9645   val Loss: 23.7901   time: 246.18s   best: 23.5707
2023-11-04 16:42:06,471:INFO:  Epoch 215/500:  train Loss: 20.2568   val Loss: 23.6430   time: 309.12s   best: 23.5707
2023-11-04 16:47:46,652:INFO:  Epoch 216/500:  train Loss: 19.8632   val Loss: 23.8922   time: 340.18s   best: 23.5707
2023-11-04 16:52:16,057:INFO:  Epoch 217/500:  train Loss: 19.8172   val Loss: 23.7501   time: 269.40s   best: 23.5707
2023-11-04 16:56:19,590:INFO:  Epoch 218/500:  train Loss: 20.3785   val Loss: 23.8921   time: 243.53s   best: 23.5707
2023-11-04 17:00:22,851:INFO:  Epoch 219/500:  train Loss: 19.9950   val Loss: 24.3987   time: 243.26s   best: 23.5707
2023-11-04 17:04:43,313:INFO:  Epoch 220/500:  train Loss: 19.8087   val Loss: 24.0614   time: 260.46s   best: 23.5707
2023-11-04 17:08:44,395:INFO:  Epoch 221/500:  train Loss: 19.8060   val Loss: 23.9498   time: 241.08s   best: 23.5707
2023-11-04 17:12:51,179:INFO:  Epoch 222/500:  train Loss: 19.7804   val Loss: 23.7297   time: 246.78s   best: 23.5707
2023-11-04 17:16:54,279:INFO:  Epoch 223/500:  train Loss: 19.8183   val Loss: 24.0920   time: 243.10s   best: 23.5707
2023-11-04 17:21:00,106:INFO:  Epoch 224/500:  train Loss: 19.9533   val Loss: 23.7835   time: 245.83s   best: 23.5707
2023-11-04 17:25:10,949:INFO:  Epoch 225/500:  train Loss: 19.8787   val Loss: 24.0728   time: 250.84s   best: 23.5707
2023-11-04 17:29:17,234:INFO:  Epoch 226/500:  train Loss: 19.7891   val Loss: 24.7601   time: 246.28s   best: 23.5707
2023-11-04 17:33:25,723:INFO:  Epoch 227/500:  train Loss: 20.0700   val Loss: 24.1418   time: 248.49s   best: 23.5707
2023-11-04 17:37:31,570:INFO:  Epoch 228/500:  train Loss: 19.8492   val Loss: 23.7377   time: 245.85s   best: 23.5707
2023-11-04 17:42:31,601:INFO:  Epoch 229/500:  train Loss: 19.7370   val Loss: 24.2576   time: 300.03s   best: 23.5707
2023-11-04 17:48:13,673:INFO:  Epoch 230/500:  train Loss: 19.7375   val Loss: 24.1794   time: 342.07s   best: 23.5707
2023-11-04 17:53:55,606:INFO:  Epoch 231/500:  train Loss: 19.9459   val Loss: 24.0035   time: 341.93s   best: 23.5707
2023-11-04 17:59:36,496:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 17:59:36,530:INFO:  Epoch 232/500:  train Loss: 19.6348   val Loss: 23.5158   time: 340.88s   best: 23.5158
2023-11-04 18:04:59,262:INFO:  Epoch 233/500:  train Loss: 19.6266   val Loss: 24.1529   time: 322.73s   best: 23.5158
2023-11-04 18:10:24,593:INFO:  Epoch 234/500:  train Loss: 19.7269   val Loss: 23.8280   time: 325.32s   best: 23.5158
2023-11-04 18:16:06,979:INFO:  Epoch 235/500:  train Loss: 19.7354   val Loss: 24.4067   time: 342.38s   best: 23.5158
2023-11-04 18:17:58,821:INFO:  Starting experiment lstm autoencoder with 0.3 dataset (0.1 dropout)
2023-11-04 18:17:58,834:INFO:  Defining the model
2023-11-04 18:17:58,893:INFO:  Reading the dataset
2023-11-04 18:20:10,705:INFO:  Epoch 236/500:  train Loss: 19.6966   val Loss: 23.7221   time: 243.71s   best: 23.5158
2023-11-04 18:24:14,781:INFO:  Epoch 237/500:  train Loss: 19.5388   val Loss: 23.8801   time: 244.07s   best: 23.5158
2023-11-04 18:28:20,198:INFO:  Epoch 238/500:  train Loss: 19.5842   val Loss: 24.4238   time: 245.42s   best: 23.5158
2023-11-04 18:32:32,030:INFO:  Epoch 239/500:  train Loss: 19.7414   val Loss: 23.8855   time: 251.83s   best: 23.5158
2023-11-04 18:36:35,114:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 18:36:35,347:INFO:  Epoch 240/500:  train Loss: 19.4942   val Loss: 23.3682   time: 243.08s   best: 23.3682
2023-11-04 18:40:41,432:INFO:  Epoch 241/500:  train Loss: 19.6720   val Loss: 23.7753   time: 246.08s   best: 23.3682
2023-11-04 18:45:58,088:INFO:  Epoch 242/500:  train Loss: 19.9112   val Loss: 24.6873   time: 316.66s   best: 23.3682
2023-11-04 18:50:13,310:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 18:50:13,346:INFO:  Epoch 1/500:  train Loss: 86.3971   val Loss: 82.8394   time: 170.13s   best: 82.8394
2023-11-04 18:51:39,840:INFO:  Epoch 243/500:  train Loss: 19.7466   val Loss: 24.0228   time: 341.75s   best: 23.3682
2023-11-04 18:53:02,237:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 18:53:02,447:INFO:  Epoch 2/500:  train Loss: 76.0220   val Loss: 74.9112   time: 168.89s   best: 74.9112
2023-11-04 18:55:51,987:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 18:55:52,007:INFO:  Epoch 3/500:  train Loss: 70.1949   val Loss: 68.9249   time: 169.54s   best: 68.9249
2023-11-04 18:56:06,550:INFO:  Epoch 244/500:  train Loss: 19.5805   val Loss: 23.6633   time: 266.70s   best: 23.3682
2023-11-04 18:58:40,089:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 18:58:40,114:INFO:  Epoch 4/500:  train Loss: 67.5527   val Loss: 66.1663   time: 168.07s   best: 66.1663
2023-11-04 19:00:09,763:INFO:  Epoch 245/500:  train Loss: 19.4620   val Loss: 23.9394   time: 243.21s   best: 23.3682
2023-11-04 19:01:28,450:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:01:28,485:INFO:  Epoch 5/500:  train Loss: 65.5313   val Loss: 64.1132   time: 168.33s   best: 64.1132
2023-11-04 19:04:17,630:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:04:17,655:INFO:  Epoch 6/500:  train Loss: 63.6894   val Loss: 63.0933   time: 169.13s   best: 63.0933
2023-11-04 19:05:32,467:INFO:  Epoch 246/500:  train Loss: 19.6183   val Loss: 23.3870   time: 322.70s   best: 23.3682
2023-11-04 19:07:05,792:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:07:05,818:INFO:  Epoch 7/500:  train Loss: 62.0344   val Loss: 61.7515   time: 168.12s   best: 61.7515
2023-11-04 19:09:37,357:INFO:  Epoch 247/500:  train Loss: 19.5108   val Loss: 24.0776   time: 244.88s   best: 23.3682
2023-11-04 19:09:53,946:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:09:54,373:INFO:  Epoch 8/500:  train Loss: 60.6554   val Loss: 59.9632   time: 168.11s   best: 59.9632
2023-11-04 19:12:44,044:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:12:44,064:INFO:  Epoch 9/500:  train Loss: 59.4199   val Loss: 58.6023   time: 169.67s   best: 58.6023
2023-11-04 19:14:55,940:INFO:  Epoch 248/500:  train Loss: 19.7015   val Loss: 23.5167   time: 318.58s   best: 23.3682
2023-11-04 19:15:32,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:15:32,472:INFO:  Epoch 10/500:  train Loss: 57.6269   val Loss: 57.0089   time: 168.38s   best: 57.0089
2023-11-04 19:18:21,462:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:18:21,483:INFO:  Epoch 11/500:  train Loss: 55.6498   val Loss: 55.0404   time: 168.97s   best: 55.0404
2023-11-04 19:20:40,915:INFO:  Epoch 249/500:  train Loss: 19.4717   val Loss: 25.6470   time: 344.95s   best: 23.3682
2023-11-04 19:21:10,835:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:21:10,880:INFO:  Epoch 12/500:  train Loss: 53.7527   val Loss: 52.5911   time: 169.34s   best: 52.5911
2023-11-04 19:24:00,506:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:24:00,526:INFO:  Epoch 13/500:  train Loss: 52.3005   val Loss: 51.5113   time: 169.61s   best: 51.5113
2023-11-04 19:26:25,073:INFO:  Epoch 250/500:  train Loss: 19.4264   val Loss: 23.5391   time: 344.14s   best: 23.3682
2023-11-04 19:26:49,504:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:26:49,655:INFO:  Epoch 14/500:  train Loss: 51.0505   val Loss: 49.9641   time: 168.96s   best: 49.9641
2023-11-04 19:29:39,267:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:29:39,287:INFO:  Epoch 15/500:  train Loss: 49.5151   val Loss: 49.1546   time: 169.58s   best: 49.1546
2023-11-04 19:31:11,007:INFO:  Epoch 251/500:  train Loss: 19.4492   val Loss: 23.8824   time: 285.92s   best: 23.3682
2023-11-04 19:32:28,781:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:32:28,812:INFO:  Epoch 16/500:  train Loss: 48.3477   val Loss: 47.8662   time: 169.48s   best: 47.8662
2023-11-04 19:35:17,375:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:35:17,395:INFO:  Epoch 17/500:  train Loss: 47.1294   val Loss: 46.4100   time: 168.55s   best: 46.4100
2023-11-04 19:35:29,104:INFO:  Epoch 252/500:  train Loss: 19.8953   val Loss: 23.4724   time: 258.09s   best: 23.3682
2023-11-04 19:38:05,449:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:38:05,479:INFO:  Epoch 18/500:  train Loss: 45.8375   val Loss: 45.6671   time: 168.04s   best: 45.6671
2023-11-04 19:40:45,749:INFO:  Epoch 253/500:  train Loss: 19.4596   val Loss: 23.9832   time: 316.64s   best: 23.3682
2023-11-04 19:40:54,626:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:40:54,650:INFO:  Epoch 19/500:  train Loss: 44.8835   val Loss: 45.2325   time: 169.12s   best: 45.2325
2023-11-04 19:43:44,029:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:43:44,049:INFO:  Epoch 20/500:  train Loss: 43.7179   val Loss: 44.0008   time: 169.36s   best: 44.0008
2023-11-04 19:46:29,925:INFO:  Epoch 254/500:  train Loss: 19.3444   val Loss: 23.7833   time: 343.89s   best: 23.3682
2023-11-04 19:46:32,679:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:46:32,700:INFO:  Epoch 21/500:  train Loss: 42.9594   val Loss: 42.9409   time: 168.62s   best: 42.9409
2023-11-04 19:49:22,203:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:49:22,223:INFO:  Epoch 22/500:  train Loss: 42.0601   val Loss: 42.7428   time: 169.49s   best: 42.7428
2023-11-04 19:50:34,941:INFO:  Epoch 255/500:  train Loss: 19.4307   val Loss: 24.2599   time: 244.85s   best: 23.3682
2023-11-04 19:52:11,101:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:52:11,141:INFO:  Epoch 23/500:  train Loss: 41.4135   val Loss: 42.1773   time: 168.86s   best: 42.1773
2023-11-04 19:54:50,386:INFO:  Epoch 256/500:  train Loss: 19.4413   val Loss: 24.3127   time: 255.42s   best: 23.3682
2023-11-04 19:54:59,908:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:54:59,929:INFO:  Epoch 24/500:  train Loss: 40.6756   val Loss: 40.8586   time: 168.74s   best: 40.8586
2023-11-04 19:57:48,887:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 19:57:48,908:INFO:  Epoch 25/500:  train Loss: 39.8935   val Loss: 40.5132   time: 168.94s   best: 40.5132
2023-11-04 19:58:54,453:INFO:  Epoch 257/500:  train Loss: 19.4751   val Loss: 23.7208   time: 244.04s   best: 23.3682
2023-11-04 20:00:37,716:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 20:00:37,745:INFO:  Epoch 26/500:  train Loss: 39.3072   val Loss: 39.9684   time: 168.80s   best: 39.9684
2023-11-04 20:02:58,773:INFO:  Epoch 258/500:  train Loss: 19.3109   val Loss: 24.1375   time: 244.31s   best: 23.3682
2023-11-04 20:03:26,873:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 20:03:26,899:INFO:  Epoch 27/500:  train Loss: 38.8944   val Loss: 39.7693   time: 169.12s   best: 39.7693
2023-11-04 20:06:15,077:INFO:  Epoch 28/500:  train Loss: 38.1437   val Loss: 39.7752   time: 168.17s   best: 39.7693
2023-11-04 20:07:02,203:INFO:  Epoch 259/500:  train Loss: 19.4017   val Loss: 24.1329   time: 243.42s   best: 23.3682
2023-11-04 20:09:03,348:INFO:  Epoch 29/500:  train Loss: 37.5576   val Loss: 39.9248   time: 168.27s   best: 39.7693
2023-11-04 20:11:05,663:INFO:  Epoch 260/500:  train Loss: 19.3238   val Loss: 23.8846   time: 243.45s   best: 23.3682
2023-11-04 20:11:53,049:INFO:  Epoch 30/500:  train Loss: 37.1435   val Loss: 39.9330   time: 169.67s   best: 39.7693
2023-11-04 20:14:42,646:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 20:14:42,667:INFO:  Epoch 31/500:  train Loss: 36.6243   val Loss: 37.9344   time: 169.58s   best: 37.9344
2023-11-04 20:15:09,317:INFO:  Epoch 261/500:  train Loss: 19.3579   val Loss: 23.9635   time: 243.64s   best: 23.3682
2023-11-04 20:17:30,698:INFO:  Epoch 32/500:  train Loss: 36.1010   val Loss: 38.8763   time: 168.03s   best: 37.9344
2023-11-04 20:20:19,519:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 20:20:19,540:INFO:  Epoch 33/500:  train Loss: 35.7552   val Loss: 37.6982   time: 168.76s   best: 37.6982
2023-11-04 20:20:34,111:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 20:20:34,254:INFO:  Epoch 262/500:  train Loss: 19.5235   val Loss: 23.1979   time: 324.78s   best: 23.1979
2023-11-04 20:23:08,285:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 20:23:08,328:INFO:  Epoch 34/500:  train Loss: 35.4312   val Loss: 36.3651   time: 168.74s   best: 36.3651
2023-11-04 20:25:56,473:INFO:  Epoch 35/500:  train Loss: 35.0102   val Loss: 38.5915   time: 168.13s   best: 36.3651
2023-11-04 20:26:15,745:INFO:  Epoch 263/500:  train Loss: 19.3261   val Loss: 24.2623   time: 341.46s   best: 23.1979
2023-11-04 20:28:44,427:INFO:  Epoch 36/500:  train Loss: 34.8665   val Loss: 37.7073   time: 167.95s   best: 36.3651
2023-11-04 20:31:33,540:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 20:31:33,561:INFO:  Epoch 37/500:  train Loss: 34.3526   val Loss: 36.0494   time: 169.07s   best: 36.0494
2023-11-04 20:32:00,081:INFO:  Epoch 264/500:  train Loss: 19.2472   val Loss: 23.7909   time: 344.33s   best: 23.1979
2023-11-04 20:34:21,917:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 20:34:21,944:INFO:  Epoch 38/500:  train Loss: 33.9521   val Loss: 35.5456   time: 168.34s   best: 35.5456
2023-11-04 20:37:11,711:INFO:  Epoch 39/500:  train Loss: 33.6253   val Loss: 41.5586   time: 169.76s   best: 35.5456
2023-11-04 20:37:26,327:INFO:  Epoch 265/500:  train Loss: 19.3037   val Loss: 23.7559   time: 326.24s   best: 23.1979
2023-11-04 20:39:59,854:INFO:  Epoch 40/500:  train Loss: 33.4570   val Loss: 40.3492   time: 168.13s   best: 35.5456
2023-11-04 20:42:47,113:INFO:  Epoch 266/500:  train Loss: 19.2745   val Loss: 24.2729   time: 320.78s   best: 23.1979
2023-11-04 20:42:48,692:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 20:42:48,775:INFO:  Epoch 41/500:  train Loss: 33.0837   val Loss: 35.1432   time: 168.81s   best: 35.1432
2023-11-04 20:45:36,988:INFO:  Epoch 42/500:  train Loss: 32.7188   val Loss: 37.1935   time: 168.20s   best: 35.1432
2023-11-04 20:48:25,503:INFO:  Epoch 43/500:  train Loss: 32.3624   val Loss: 35.3493   time: 168.50s   best: 35.1432
2023-11-04 20:48:31,421:INFO:  Epoch 267/500:  train Loss: 19.2505   val Loss: 23.6977   time: 344.13s   best: 23.1979
2023-11-04 20:51:13,510:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 20:51:13,552:INFO:  Epoch 44/500:  train Loss: 32.1396   val Loss: 34.7351   time: 167.98s   best: 34.7351
2023-11-04 20:54:02,635:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 20:54:02,656:INFO:  Epoch 45/500:  train Loss: 32.0140   val Loss: 34.6184   time: 169.07s   best: 34.6184
2023-11-04 20:54:16,112:INFO:  Epoch 268/500:  train Loss: 19.4489   val Loss: 24.0475   time: 344.69s   best: 23.1979
2023-11-04 20:56:52,001:INFO:  Epoch 46/500:  train Loss: 31.9269   val Loss: 37.9340   time: 169.34s   best: 34.6184
2023-11-04 20:59:41,466:INFO:  Epoch 47/500:  train Loss: 31.5308   val Loss: 37.8081   time: 169.44s   best: 34.6184
2023-11-04 21:00:00,217:INFO:  Epoch 269/500:  train Loss: 19.3051   val Loss: 24.1766   time: 344.10s   best: 23.1979
2023-11-04 21:02:30,863:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 21:02:30,901:INFO:  Epoch 48/500:  train Loss: 31.5260   val Loss: 33.1884   time: 169.37s   best: 33.1884
2023-11-04 21:04:50,188:INFO:  Epoch 270/500:  train Loss: 19.3310   val Loss: 23.7107   time: 289.96s   best: 23.1979
2023-11-04 21:05:19,255:INFO:  Epoch 49/500:  train Loss: 31.1120   val Loss: 36.0752   time: 168.35s   best: 33.1884
2023-11-04 21:08:08,087:INFO:  Epoch 50/500:  train Loss: 31.0655   val Loss: 34.5678   time: 168.81s   best: 33.1884
2023-11-04 21:08:51,822:INFO:  Epoch 271/500:  train Loss: 19.3471   val Loss: 24.1219   time: 241.62s   best: 23.1979
2023-11-04 21:10:56,534:INFO:  Epoch 51/500:  train Loss: 30.8207   val Loss: 34.4985   time: 168.45s   best: 33.1884
2023-11-04 21:13:10,488:INFO:  Epoch 272/500:  train Loss: 19.2016   val Loss: 24.1604   time: 258.58s   best: 23.1979
2023-11-04 21:13:44,256:INFO:  Epoch 52/500:  train Loss: 30.5706   val Loss: 33.9877   time: 167.70s   best: 33.1884
2023-11-04 21:16:33,993:INFO:  Epoch 53/500:  train Loss: 30.6326   val Loss: 33.7725   time: 169.70s   best: 33.1884
2023-11-04 21:17:15,111:INFO:  Epoch 273/500:  train Loss: 19.1739   val Loss: 24.6486   time: 244.62s   best: 23.1979
2023-11-04 21:19:23,056:INFO:  Epoch 54/500:  train Loss: 30.2310   val Loss: 34.8944   time: 169.06s   best: 33.1884
2023-11-04 21:21:29,951:INFO:  Epoch 274/500:  train Loss: 19.2926   val Loss: 24.0462   time: 254.82s   best: 23.1979
2023-11-04 21:22:11,258:INFO:  Epoch 55/500:  train Loss: 30.1431   val Loss: 37.7897   time: 168.18s   best: 33.1884
2023-11-04 21:24:59,599:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 21:24:59,619:INFO:  Epoch 56/500:  train Loss: 30.2051   val Loss: 33.0088   time: 168.30s   best: 33.0088
2023-11-04 21:25:33,969:INFO:  Epoch 275/500:  train Loss: 19.3437   val Loss: 24.7247   time: 244.01s   best: 23.1979
2023-11-04 21:27:48,709:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 21:27:48,746:INFO:  Epoch 57/500:  train Loss: 29.8580   val Loss: 32.1655   time: 169.09s   best: 32.1655
2023-11-04 21:29:37,844:INFO:  Epoch 276/500:  train Loss: 19.1666   val Loss: 24.0326   time: 243.85s   best: 23.1979
2023-11-04 21:30:38,335:INFO:  Epoch 58/500:  train Loss: 29.7134   val Loss: 32.6365   time: 169.58s   best: 32.1655
2023-11-04 21:33:26,650:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 21:33:26,671:INFO:  Epoch 59/500:  train Loss: 29.5493   val Loss: 31.4138   time: 168.29s   best: 31.4138
2023-11-04 21:33:43,367:INFO:  Epoch 277/500:  train Loss: 19.2323   val Loss: 23.8827   time: 245.51s   best: 23.1979
2023-11-04 21:36:14,989:INFO:  Epoch 60/500:  train Loss: 29.3721   val Loss: 31.5531   time: 168.31s   best: 31.4138
2023-11-04 21:37:55,556:INFO:  Epoch 278/500:  train Loss: 19.3393   val Loss: 24.1776   time: 252.19s   best: 23.1979
2023-11-04 21:39:03,260:INFO:  Epoch 61/500:  train Loss: 29.2521   val Loss: 31.5915   time: 168.26s   best: 31.4138
2023-11-04 21:41:52,945:INFO:  Epoch 62/500:  train Loss: 29.0963   val Loss: 32.2934   time: 169.65s   best: 31.4138
2023-11-04 21:42:01,229:INFO:  Epoch 279/500:  train Loss: 19.7756   val Loss: 23.4786   time: 245.66s   best: 23.1979
2023-11-04 21:44:41,455:INFO:  Epoch 63/500:  train Loss: 28.7799   val Loss: 37.3794   time: 168.50s   best: 31.4138
2023-11-04 21:46:07,649:INFO:  Epoch 280/500:  train Loss: 20.4753   val Loss: 24.4247   time: 246.42s   best: 23.1979
2023-11-04 21:47:31,041:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 21:47:31,078:INFO:  Epoch 64/500:  train Loss: 28.9692   val Loss: 30.7116   time: 169.37s   best: 30.7116
2023-11-04 21:50:19,810:INFO:  Epoch 281/500:  train Loss: 19.2979   val Loss: 24.7846   time: 252.15s   best: 23.1979
2023-11-04 21:50:20,455:INFO:  Epoch 65/500:  train Loss: 28.5984   val Loss: 33.0973   time: 169.38s   best: 30.7116
2023-11-04 21:53:08,583:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 21:53:08,604:INFO:  Epoch 66/500:  train Loss: 28.5702   val Loss: 30.6633   time: 168.08s   best: 30.6633
2023-11-04 21:54:33,966:INFO:  Epoch 282/500:  train Loss: 19.1282   val Loss: 24.4978   time: 254.15s   best: 23.1979
2023-11-04 21:55:57,555:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 21:55:57,587:INFO:  Epoch 67/500:  train Loss: 28.4406   val Loss: 30.4673   time: 168.94s   best: 30.4673
2023-11-04 21:58:44,579:INFO:  Epoch 283/500:  train Loss: 19.2033   val Loss: 23.7748   time: 250.60s   best: 23.1979
2023-11-04 21:58:46,460:INFO:  Epoch 68/500:  train Loss: 28.2748   val Loss: 32.1107   time: 168.86s   best: 30.4673
2023-11-04 22:01:35,041:INFO:  Epoch 69/500:  train Loss: 28.1799   val Loss: 30.9846   time: 168.49s   best: 30.4673
2023-11-04 22:02:53,188:INFO:  Epoch 284/500:  train Loss: 19.1968   val Loss: 23.7728   time: 248.58s   best: 23.1979
2023-11-04 22:04:23,868:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 22:04:23,916:INFO:  Epoch 70/500:  train Loss: 28.1233   val Loss: 30.4595   time: 168.80s   best: 30.4595
2023-11-04 22:07:07,633:INFO:  Epoch 285/500:  train Loss: 19.2462   val Loss: 23.6014   time: 254.43s   best: 23.1979
2023-11-04 22:07:12,662:INFO:  Epoch 71/500:  train Loss: 28.2149   val Loss: 32.2951   time: 168.73s   best: 30.4595
2023-11-04 22:10:00,558:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 22:10:00,578:INFO:  Epoch 72/500:  train Loss: 27.8623   val Loss: 30.1949   time: 167.87s   best: 30.1949
2023-11-04 22:12:21,793:INFO:  Epoch 286/500:  train Loss: 19.1845   val Loss: 24.0386   time: 314.14s   best: 23.1979
2023-11-04 22:12:48,940:INFO:  Epoch 73/500:  train Loss: 27.7704   val Loss: 32.1827   time: 168.36s   best: 30.1949
2023-11-04 22:15:36,890:INFO:  Epoch 74/500:  train Loss: 27.5660   val Loss: 30.7198   time: 167.92s   best: 30.1949
2023-11-04 22:18:05,619:INFO:  Epoch 287/500:  train Loss: 19.3335   val Loss: 23.3683   time: 343.81s   best: 23.1979
2023-11-04 22:18:25,047:INFO:  Epoch 75/500:  train Loss: 27.6288   val Loss: 31.9665   time: 168.14s   best: 30.1949
2023-11-04 22:21:12,982:INFO:  Epoch 76/500:  train Loss: 27.4086   val Loss: 30.4937   time: 167.91s   best: 30.1949
2023-11-04 22:22:30,574:INFO:  Epoch 288/500:  train Loss: 19.0877   val Loss: 24.0963   time: 264.94s   best: 23.1979
2023-11-04 22:24:02,378:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 22:24:02,403:INFO:  Epoch 77/500:  train Loss: 27.3850   val Loss: 30.0681   time: 169.39s   best: 30.0681
2023-11-04 22:26:51,422:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 22:26:51,443:INFO:  Epoch 78/500:  train Loss: 27.2269   val Loss: 29.7875   time: 169.00s   best: 29.7875
2023-11-04 22:28:11,535:INFO:  Epoch 289/500:  train Loss: 19.3497   val Loss: 23.5715   time: 340.94s   best: 23.1979
2023-11-04 22:29:40,323:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 22:29:40,353:INFO:  Epoch 79/500:  train Loss: 27.0991   val Loss: 29.2810   time: 168.86s   best: 29.2810
2023-11-04 22:32:29,847:INFO:  Epoch 80/500:  train Loss: 26.9990   val Loss: 32.7074   time: 169.49s   best: 29.2810
2023-11-04 22:33:54,658:INFO:  Epoch 290/500:  train Loss: 19.1343   val Loss: 28.7299   time: 343.12s   best: 23.1979
2023-11-04 22:35:18,581:INFO:  Epoch 81/500:  train Loss: 27.0348   val Loss: 29.2925   time: 168.72s   best: 29.2810
2023-11-04 22:38:06,839:INFO:  Epoch 82/500:  train Loss: 26.7340   val Loss: 29.4538   time: 168.23s   best: 29.2810
2023-11-04 22:39:37,461:INFO:  Epoch 291/500:  train Loss: 19.2448   val Loss: 23.6876   time: 342.78s   best: 23.1979
2023-11-04 22:40:56,324:INFO:  Epoch 83/500:  train Loss: 26.8524   val Loss: 31.1161   time: 169.47s   best: 29.2810
2023-11-04 22:43:44,378:INFO:  Epoch 84/500:  train Loss: 26.5915   val Loss: 29.4098   time: 168.01s   best: 29.2810
2023-11-04 22:45:21,790:INFO:  Epoch 292/500:  train Loss: 18.9531   val Loss: 23.7688   time: 344.32s   best: 23.1979
2023-11-04 22:46:33,658:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 22:46:33,692:INFO:  Epoch 85/500:  train Loss: 26.5569   val Loss: 29.0577   time: 169.26s   best: 29.0577
2023-11-04 22:49:21,822:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 22:49:21,842:INFO:  Epoch 86/500:  train Loss: 26.6036   val Loss: 28.6377   time: 168.11s   best: 28.6377
2023-11-04 22:49:23,237:INFO:  Epoch 293/500:  train Loss: 19.0637   val Loss: 23.4380   time: 241.28s   best: 23.1979
2023-11-04 22:52:10,094:INFO:  Epoch 87/500:  train Loss: 26.3544   val Loss: 28.6873   time: 168.25s   best: 28.6377
2023-11-04 22:53:49,947:INFO:  Epoch 294/500:  train Loss: 19.3509   val Loss: 23.5351   time: 266.65s   best: 23.1979
2023-11-04 22:54:59,329:INFO:  Epoch 88/500:  train Loss: 26.3124   val Loss: 28.8810   time: 169.23s   best: 28.6377
2023-11-04 22:57:47,465:INFO:  Epoch 89/500:  train Loss: 26.2848   val Loss: 29.1075   time: 168.11s   best: 28.6377
2023-11-04 22:57:54,226:INFO:  Epoch 295/500:  train Loss: 19.0933   val Loss: 23.9282   time: 244.09s   best: 23.1979
2023-11-04 23:00:35,387:INFO:  Epoch 90/500:  train Loss: 26.1185   val Loss: 29.7685   time: 167.92s   best: 28.6377
2023-11-04 23:03:15,230:INFO:  Epoch 296/500:  train Loss: 19.5331   val Loss: 23.3185   time: 320.98s   best: 23.1979
2023-11-04 23:03:23,521:INFO:  Epoch 91/500:  train Loss: 26.1255   val Loss: 30.8217   time: 168.09s   best: 28.6377
2023-11-04 23:06:12,779:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 23:06:12,799:INFO:  Epoch 92/500:  train Loss: 25.9844   val Loss: 28.0045   time: 169.23s   best: 28.0045
2023-11-04 23:08:57,685:INFO:  Epoch 297/500:  train Loss: 19.1561   val Loss: 23.6156   time: 342.42s   best: 23.1979
2023-11-04 23:09:01,313:INFO:  Epoch 93/500:  train Loss: 25.9854   val Loss: 29.0315   time: 168.50s   best: 28.0045
2023-11-04 23:11:49,133:INFO:  Epoch 94/500:  train Loss: 25.8276   val Loss: 29.5119   time: 167.82s   best: 28.0045
2023-11-04 23:14:36,763:INFO:  Epoch 298/500:  train Loss: 19.0933   val Loss: 23.4113   time: 339.07s   best: 23.1979
2023-11-04 23:14:37,273:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 23:14:37,343:INFO:  Epoch 95/500:  train Loss: 25.8049   val Loss: 27.6846   time: 168.12s   best: 27.6846
2023-11-04 23:17:25,082:INFO:  Epoch 96/500:  train Loss: 25.7891   val Loss: 29.1130   time: 167.72s   best: 27.6846
2023-11-04 23:20:10,062:INFO:  Epoch 299/500:  train Loss: 19.0826   val Loss: 23.7544   time: 333.27s   best: 23.1979
2023-11-04 23:20:13,469:INFO:  Epoch 97/500:  train Loss: 25.5957   val Loss: 28.6020   time: 168.38s   best: 27.6846
2023-11-04 23:23:02,519:INFO:  Epoch 98/500:  train Loss: 25.5083   val Loss: 28.0972   time: 169.00s   best: 27.6846
2023-11-04 23:24:27,146:INFO:  Epoch 300/500:  train Loss: 18.9950   val Loss: 23.2575   time: 257.06s   best: 23.1979
2023-11-04 23:25:50,436:INFO:  Epoch 99/500:  train Loss: 25.5453   val Loss: 28.4541   time: 167.91s   best: 27.6846
2023-11-04 23:28:39,924:INFO:  Epoch 100/500:  train Loss: 25.4368   val Loss: 28.2289   time: 169.46s   best: 27.6846
2023-11-04 23:28:52,004:INFO:  Epoch 301/500:  train Loss: 19.0466   val Loss: 23.4810   time: 264.84s   best: 23.1979
2023-11-04 23:31:28,449:INFO:  Epoch 101/500:  train Loss: 25.6829   val Loss: 27.9167   time: 168.51s   best: 27.6846
2023-11-04 23:32:58,159:INFO:  Epoch 302/500:  train Loss: 19.1660   val Loss: 23.7180   time: 246.15s   best: 23.1979
2023-11-04 23:34:17,192:INFO:  Epoch 102/500:  train Loss: 25.3489   val Loss: 28.9381   time: 168.73s   best: 27.6846
2023-11-04 23:37:06,359:INFO:  Epoch 103/500:  train Loss: 25.3030   val Loss: 28.6842   time: 169.12s   best: 27.6846
2023-11-04 23:37:19,529:INFO:  Epoch 303/500:  train Loss: 19.2958   val Loss: 23.5291   time: 261.35s   best: 23.1979
2023-11-04 23:39:54,028:INFO:  Epoch 104/500:  train Loss: 25.3844   val Loss: 27.7966   time: 167.65s   best: 27.6846
2023-11-04 23:41:33,724:INFO:  Epoch 304/500:  train Loss: 18.9302   val Loss: 23.5923   time: 254.19s   best: 23.1979
2023-11-04 23:42:43,152:INFO:  Epoch 105/500:  train Loss: 25.0527   val Loss: 27.7092   time: 169.09s   best: 27.6846
2023-11-04 23:45:31,463:INFO:  Epoch 106/500:  train Loss: 25.0370   val Loss: 28.3556   time: 168.15s   best: 27.6846
2023-11-04 23:45:38,352:INFO:  Epoch 305/500:  train Loss: 19.0578   val Loss: 23.6014   time: 244.62s   best: 23.1979
2023-11-04 23:48:20,734:INFO:  Epoch 107/500:  train Loss: 25.2646   val Loss: 29.0522   time: 169.25s   best: 27.6846
2023-11-04 23:49:39,661:INFO:  Epoch 306/500:  train Loss: 19.0407   val Loss: 23.7337   time: 241.31s   best: 23.1979
2023-11-04 23:51:08,414:INFO:  Epoch 108/500:  train Loss: 24.9628   val Loss: 28.3255   time: 167.65s   best: 27.6846
2023-11-04 23:53:50,256:INFO:  Epoch 307/500:  train Loss: 20.0366   val Loss: 23.5607   time: 250.58s   best: 23.1979
2023-11-04 23:53:57,878:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-04 23:53:57,904:INFO:  Epoch 109/500:  train Loss: 25.0503   val Loss: 27.5776   time: 169.44s   best: 27.5776
2023-11-04 23:56:45,503:INFO:  Epoch 110/500:  train Loss: 24.9265   val Loss: 28.6637   time: 167.59s   best: 27.5776
2023-11-04 23:57:53,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-04 23:57:54,028:INFO:  Epoch 308/500:  train Loss: 18.9512   val Loss: 23.1350   time: 243.71s   best: 23.1350
2023-11-04 23:59:34,661:INFO:  Epoch 111/500:  train Loss: 24.8192   val Loss: 31.4199   time: 169.13s   best: 27.5776
2023-11-05 00:01:59,474:INFO:  Epoch 309/500:  train Loss: 19.1558   val Loss: 23.8013   time: 245.45s   best: 23.1350
2023-11-05 00:02:22,752:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 00:02:22,774:INFO:  Epoch 112/500:  train Loss: 25.3697   val Loss: 27.5741   time: 168.08s   best: 27.5741
2023-11-05 00:05:12,009:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 00:05:12,050:INFO:  Epoch 113/500:  train Loss: 24.7695   val Loss: 27.4672   time: 169.22s   best: 27.4672
2023-11-05 00:06:10,618:INFO:  Epoch 310/500:  train Loss: 18.9796   val Loss: 23.1486   time: 251.13s   best: 23.1350
2023-11-05 00:08:00,772:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 00:08:00,805:INFO:  Epoch 114/500:  train Loss: 24.6230   val Loss: 27.3820   time: 168.70s   best: 27.3820
2023-11-05 00:10:16,777:INFO:  Epoch 311/500:  train Loss: 18.8495   val Loss: 29.2898   time: 246.14s   best: 23.1350
2023-11-05 00:10:48,673:INFO:  Epoch 115/500:  train Loss: 24.7148   val Loss: 28.7776   time: 167.86s   best: 27.3820
2023-11-05 00:13:36,356:INFO:  Epoch 116/500:  train Loss: 24.6431   val Loss: 28.2012   time: 167.67s   best: 27.3820
2023-11-05 00:15:08,429:INFO:  Epoch 312/500:  train Loss: 18.9270   val Loss: 24.1733   time: 291.64s   best: 23.1350
2023-11-05 00:16:25,564:INFO:  Epoch 117/500:  train Loss: 25.1615   val Loss: 27.8317   time: 169.18s   best: 27.3820
2023-11-05 00:19:14,577:INFO:  Epoch 118/500:  train Loss: 24.4515   val Loss: 28.1562   time: 168.97s   best: 27.3820
2023-11-05 00:19:42,482:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-05 00:19:42,515:INFO:  Epoch 313/500:  train Loss: 18.9388   val Loss: 23.0315   time: 274.04s   best: 23.0315
2023-11-05 00:22:02,477:INFO:  Epoch 119/500:  train Loss: 24.5204   val Loss: 29.4695   time: 167.88s   best: 27.3820
2023-11-05 00:24:04,296:INFO:  Epoch 314/500:  train Loss: 18.8431   val Loss: 23.4371   time: 261.78s   best: 23.0315
2023-11-05 00:24:50,201:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 00:24:50,228:INFO:  Epoch 120/500:  train Loss: 24.6488   val Loss: 27.2524   time: 167.71s   best: 27.2524
2023-11-05 00:27:39,597:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 00:27:39,617:INFO:  Epoch 121/500:  train Loss: 24.4858   val Loss: 27.2485   time: 169.36s   best: 27.2485
2023-11-05 00:28:08,295:INFO:  Epoch 315/500:  train Loss: 18.9067   val Loss: 24.1016   time: 243.98s   best: 23.0315
2023-11-05 00:30:27,507:INFO:  Epoch 122/500:  train Loss: 24.2075   val Loss: 28.3411   time: 167.89s   best: 27.2485
2023-11-05 00:32:16,076:INFO:  Epoch 316/500:  train Loss: 19.3355   val Loss: 23.4811   time: 247.72s   best: 23.0315
2023-11-05 00:33:15,685:INFO:  Epoch 123/500:  train Loss: 24.2342   val Loss: 28.4436   time: 168.14s   best: 27.2485
2023-11-05 00:36:03,634:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 00:36:03,652:INFO:  Epoch 124/500:  train Loss: 24.2482   val Loss: 26.6941   time: 167.94s   best: 26.6941
2023-11-05 00:36:24,001:INFO:  Epoch 317/500:  train Loss: 18.8934   val Loss: 23.7815   time: 247.88s   best: 23.0315
2023-11-05 00:38:51,722:INFO:  Epoch 125/500:  train Loss: 24.0456   val Loss: 26.8529   time: 168.06s   best: 26.6941
2023-11-05 00:40:39,412:INFO:  Epoch 318/500:  train Loss: 18.9195   val Loss: 23.2479   time: 255.40s   best: 23.0315
2023-11-05 00:41:39,724:INFO:  Epoch 126/500:  train Loss: 24.1651   val Loss: 30.4463   time: 167.97s   best: 26.6941
2023-11-05 00:44:29,112:INFO:  Epoch 127/500:  train Loss: 24.0245   val Loss: 29.7464   time: 169.38s   best: 26.6941
2023-11-05 00:44:46,590:INFO:  Epoch 319/500:  train Loss: 18.8628   val Loss: 24.4884   time: 247.17s   best: 23.0315
2023-11-05 00:47:16,652:INFO:  Epoch 128/500:  train Loss: 24.0935   val Loss: 29.2655   time: 167.53s   best: 26.6941
2023-11-05 00:49:04,155:INFO:  Epoch 320/500:  train Loss: 19.0141   val Loss: 23.8231   time: 257.55s   best: 23.0315
2023-11-05 00:50:05,470:INFO:  Epoch 129/500:  train Loss: 24.0736   val Loss: 27.0976   time: 168.79s   best: 26.6941
2023-11-05 00:52:53,821:INFO:  Epoch 130/500:  train Loss: 23.9423   val Loss: 28.2375   time: 168.31s   best: 26.6941
2023-11-05 00:53:07,990:INFO:  Epoch 321/500:  train Loss: 19.7175   val Loss: 24.1257   time: 243.82s   best: 23.0315
2023-11-05 00:55:42,631:INFO:  Epoch 131/500:  train Loss: 24.0258   val Loss: 27.4936   time: 168.80s   best: 26.6941
2023-11-05 00:57:13,787:INFO:  Epoch 322/500:  train Loss: 18.8795   val Loss: 23.3150   time: 245.78s   best: 23.0315
2023-11-05 00:58:31,700:INFO:  Epoch 132/500:  train Loss: 23.8233   val Loss: 27.2117   time: 169.05s   best: 26.6941
2023-11-05 01:01:18,027:INFO:  Epoch 323/500:  train Loss: 18.8391   val Loss: 23.6322   time: 244.22s   best: 23.0315
2023-11-05 01:01:19,430:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 01:01:19,458:INFO:  Epoch 133/500:  train Loss: 23.8548   val Loss: 26.5772   time: 167.69s   best: 26.5772
2023-11-05 01:04:07,187:INFO:  Epoch 134/500:  train Loss: 23.8698   val Loss: 27.1004   time: 167.73s   best: 26.5772
2023-11-05 01:06:55,293:INFO:  Epoch 135/500:  train Loss: 23.8014   val Loss: 30.2201   time: 168.11s   best: 26.5772
2023-11-05 01:06:58,932:INFO:  Epoch 324/500:  train Loss: 18.9037   val Loss: 23.2845   time: 340.89s   best: 23.0315
2023-11-05 01:09:43,838:INFO:  Epoch 136/500:  train Loss: 23.7636   val Loss: 29.0650   time: 168.53s   best: 26.5772
2023-11-05 01:12:31,979:INFO:  Epoch 137/500:  train Loss: 23.6058   val Loss: 28.1006   time: 168.11s   best: 26.5772
2023-11-05 01:12:40,206:INFO:  Epoch 325/500:  train Loss: 18.7557   val Loss: 23.3516   time: 341.27s   best: 23.0315
2023-11-05 01:15:19,783:INFO:  Epoch 138/500:  train Loss: 23.7258   val Loss: 28.8426   time: 167.78s   best: 26.5772
2023-11-05 01:18:09,361:INFO:  Epoch 139/500:  train Loss: 23.9455   val Loss: 26.8000   time: 169.55s   best: 26.5772
2023-11-05 01:18:23,062:INFO:  Epoch 326/500:  train Loss: 18.9209   val Loss: 23.6801   time: 342.85s   best: 23.0315
2023-11-05 01:20:56,890:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 01:20:56,932:INFO:  Epoch 140/500:  train Loss: 23.5806   val Loss: 26.5361   time: 167.50s   best: 26.5361
2023-11-05 01:23:44,662:INFO:  Epoch 141/500:  train Loss: 23.5353   val Loss: 28.0902   time: 167.72s   best: 26.5361
2023-11-05 01:24:03,754:INFO:  Epoch 327/500:  train Loss: 18.7427   val Loss: 23.9671   time: 340.69s   best: 23.0315
2023-11-05 01:26:32,551:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 01:26:32,584:INFO:  Epoch 142/500:  train Loss: 23.4574   val Loss: 26.3159   time: 167.85s   best: 26.3159
2023-11-05 01:28:09,598:INFO:  Epoch 328/500:  train Loss: 18.7669   val Loss: 23.9470   time: 245.84s   best: 23.0315
2023-11-05 01:29:20,537:INFO:  Epoch 143/500:  train Loss: 23.3933   val Loss: 28.1006   time: 167.94s   best: 26.3159
2023-11-05 01:32:08,233:INFO:  Epoch 144/500:  train Loss: 23.3946   val Loss: 26.6860   time: 167.65s   best: 26.3159
2023-11-05 01:33:26,601:INFO:  Epoch 329/500:  train Loss: 18.8622   val Loss: 23.3602   time: 316.99s   best: 23.0315
2023-11-05 01:34:56,423:INFO:  Epoch 145/500:  train Loss: 23.8681   val Loss: 26.8853   time: 168.19s   best: 26.3159
2023-11-05 01:37:44,642:INFO:  Epoch 146/500:  train Loss: 23.2719   val Loss: 26.5806   time: 168.19s   best: 26.3159
2023-11-05 01:39:09,683:INFO:  Epoch 330/500:  train Loss: 18.7663   val Loss: 23.4783   time: 343.06s   best: 23.0315
2023-11-05 01:40:32,538:INFO:  Epoch 147/500:  train Loss: 23.4752   val Loss: 26.6735   time: 167.85s   best: 26.3159
2023-11-05 01:43:21,784:INFO:  Epoch 148/500:  train Loss: 23.2080   val Loss: 26.9632   time: 169.22s   best: 26.3159
2023-11-05 01:44:43,378:INFO:  Epoch 331/500:  train Loss: 18.7555   val Loss: 24.8278   time: 333.64s   best: 23.0315
2023-11-05 01:46:09,684:INFO:  Epoch 149/500:  train Loss: 23.2167   val Loss: 26.3896   time: 167.87s   best: 26.3159
2023-11-05 01:48:57,080:INFO:  Epoch 150/500:  train Loss: 23.1754   val Loss: 26.9906   time: 167.37s   best: 26.3159
2023-11-05 01:50:26,769:INFO:  Epoch 332/500:  train Loss: 18.8249   val Loss: 23.3780   time: 343.38s   best: 23.0315
2023-11-05 01:51:45,964:INFO:  Epoch 151/500:  train Loss: 23.3105   val Loss: 28.9437   time: 168.85s   best: 26.3159
2023-11-05 01:54:30,237:INFO:  Epoch 333/500:  train Loss: 18.7975   val Loss: 23.5382   time: 243.33s   best: 23.0315
2023-11-05 01:54:34,887:INFO:  Epoch 152/500:  train Loss: 23.4235   val Loss: 26.4460   time: 168.91s   best: 26.3159
2023-11-05 01:57:24,150:INFO:  Epoch 153/500:  train Loss: 23.0971   val Loss: 26.3793   time: 169.17s   best: 26.3159
2023-11-05 01:58:36,109:INFO:  Epoch 334/500:  train Loss: 18.9564   val Loss: 23.6669   time: 245.86s   best: 23.0315
2023-11-05 02:00:12,441:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 02:00:12,489:INFO:  Epoch 154/500:  train Loss: 23.1266   val Loss: 26.1330   time: 168.28s   best: 26.1330
2023-11-05 02:03:00,280:INFO:  Epoch 155/500:  train Loss: 23.0759   val Loss: 26.4044   time: 167.78s   best: 26.1330
2023-11-05 02:04:07,374:INFO:  Epoch 335/500:  train Loss: 18.6967   val Loss: 23.7241   time: 331.25s   best: 23.0315
2023-11-05 02:05:47,794:INFO:  Epoch 156/500:  train Loss: 23.1551   val Loss: 26.9448   time: 167.50s   best: 26.1330
2023-11-05 02:08:36,961:INFO:  Epoch 157/500:  train Loss: 23.1221   val Loss: 26.6594   time: 169.14s   best: 26.1330
2023-11-05 02:09:05,533:INFO:  Epoch 336/500:  train Loss: 18.7676   val Loss: 23.6426   time: 298.14s   best: 23.0315
2023-11-05 02:11:24,622:INFO:  Epoch 158/500:  train Loss: 23.2153   val Loss: 26.8413   time: 167.63s   best: 26.1330
2023-11-05 02:13:11,685:INFO:  Epoch 337/500:  train Loss: 18.6736   val Loss: 23.2487   time: 246.14s   best: 23.0315
2023-11-05 02:14:13,627:INFO:  Epoch 159/500:  train Loss: 23.0028   val Loss: 28.5449   time: 168.96s   best: 26.1330
2023-11-05 02:17:02,783:INFO:  Epoch 160/500:  train Loss: 22.9770   val Loss: 26.3898   time: 169.13s   best: 26.1330
2023-11-05 02:17:33,796:INFO:  Epoch 338/500:  train Loss: 18.7683   val Loss: 23.2401   time: 262.09s   best: 23.0315
2023-11-05 02:19:50,614:INFO:  Epoch 161/500:  train Loss: 22.8237   val Loss: 27.0717   time: 167.81s   best: 26.1330
2023-11-05 02:22:39,787:INFO:  Epoch 162/500:  train Loss: 22.9866   val Loss: 26.3047   time: 169.15s   best: 26.1330
2023-11-05 02:23:05,994:INFO:  Epoch 339/500:  train Loss: 18.8039   val Loss: 23.7515   time: 332.19s   best: 23.0315
2023-11-05 02:25:29,137:INFO:  Epoch 163/500:  train Loss: 22.8450   val Loss: 28.7929   time: 169.32s   best: 26.1330
2023-11-05 02:28:17,664:INFO:  Epoch 164/500:  train Loss: 23.0452   val Loss: 27.7271   time: 168.49s   best: 26.1330
2023-11-05 02:28:48,518:INFO:  Epoch 340/500:  train Loss: 18.6554   val Loss: 23.2068   time: 342.52s   best: 23.0315
2023-11-05 02:31:06,721:INFO:  Epoch 165/500:  train Loss: 22.9839   val Loss: 26.3096   time: 169.03s   best: 26.1330
2023-11-05 02:33:54,326:INFO:  Epoch 166/500:  train Loss: 22.6631   val Loss: 26.8202   time: 167.56s   best: 26.1330
2023-11-05 02:34:32,115:INFO:  Epoch 341/500:  train Loss: 18.6605   val Loss: 23.3490   time: 343.58s   best: 23.0315
2023-11-05 02:36:43,516:INFO:  Epoch 167/500:  train Loss: 22.8434   val Loss: 26.1686   time: 169.16s   best: 26.1330
2023-11-05 02:39:05,234:INFO:  Epoch 342/500:  train Loss: 18.6095   val Loss: 24.1370   time: 273.11s   best: 23.0315
2023-11-05 02:39:32,001:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 02:39:32,104:INFO:  Epoch 168/500:  train Loss: 22.7376   val Loss: 25.8804   time: 168.46s   best: 25.8804
2023-11-05 02:42:22,083:INFO:  Epoch 169/500:  train Loss: 22.7428   val Loss: 27.0682   time: 169.96s   best: 25.8804
2023-11-05 02:43:10,946:INFO:  Epoch 343/500:  train Loss: 18.6853   val Loss: 23.1856   time: 245.70s   best: 23.0315
2023-11-05 02:45:09,813:INFO:  Epoch 170/500:  train Loss: 23.1634   val Loss: 27.0814   time: 167.73s   best: 25.8804
2023-11-05 02:47:57,501:INFO:  Epoch 171/500:  train Loss: 22.6362   val Loss: 25.9099   time: 167.65s   best: 25.8804
2023-11-05 02:48:13,502:INFO:  Epoch 344/500:  train Loss: 18.5925   val Loss: 23.6770   time: 302.54s   best: 23.0315
2023-11-05 02:50:45,269:INFO:  Epoch 172/500:  train Loss: 22.7103   val Loss: 28.2553   time: 167.74s   best: 25.8804
2023-11-05 02:53:34,053:INFO:  Epoch 173/500:  train Loss: 22.4430   val Loss: 26.6575   time: 168.77s   best: 25.8804
2023-11-05 02:53:53,549:INFO:  Epoch 345/500:  train Loss: 19.6686   val Loss: 24.6442   time: 340.04s   best: 23.0315
2023-11-05 02:56:23,320:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 02:56:23,353:INFO:  Epoch 174/500:  train Loss: 22.7389   val Loss: 25.7778   time: 169.25s   best: 25.7778
2023-11-05 02:59:12,318:INFO:  Epoch 175/500:  train Loss: 22.4876   val Loss: 26.0221   time: 168.95s   best: 25.7778
2023-11-05 02:59:36,497:INFO:  Epoch 346/500:  train Loss: 18.7234   val Loss: 24.2053   time: 342.94s   best: 23.0315
2023-11-05 03:02:00,032:INFO:  Epoch 176/500:  train Loss: 22.6866   val Loss: 26.4483   time: 167.70s   best: 25.7778
2023-11-05 03:04:48,898:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 03:04:48,918:INFO:  Epoch 177/500:  train Loss: 22.5210   val Loss: 25.5243   time: 168.83s   best: 25.5243
2023-11-05 03:05:20,767:INFO:  Epoch 347/500:  train Loss: 18.6755   val Loss: 23.1448   time: 344.07s   best: 23.0315
2023-11-05 03:07:38,083:INFO:  Epoch 178/500:  train Loss: 22.5363   val Loss: 25.6682   time: 169.15s   best: 25.5243
2023-11-05 03:10:25,678:INFO:  Epoch 179/500:  train Loss: 22.2756   val Loss: 26.1483   time: 167.58s   best: 25.5243
2023-11-05 03:10:49,817:INFO:  Epoch 348/500:  train Loss: 18.9430   val Loss: 23.4225   time: 329.03s   best: 23.0315
2023-11-05 03:13:14,329:INFO:  Epoch 180/500:  train Loss: 23.0624   val Loss: 26.1378   time: 168.64s   best: 25.5243
2023-11-05 03:14:55,240:INFO:  Epoch 349/500:  train Loss: 18.7514   val Loss: 24.7811   time: 245.38s   best: 23.0315
2023-11-05 03:16:02,560:INFO:  Epoch 181/500:  train Loss: 22.2896   val Loss: 25.7949   time: 168.23s   best: 25.5243
2023-11-05 03:18:50,895:INFO:  Epoch 182/500:  train Loss: 22.4553   val Loss: 26.3638   time: 168.30s   best: 25.5243
2023-11-05 03:20:10,803:INFO:  Epoch 350/500:  train Loss: 19.4279   val Loss: 25.7961   time: 315.55s   best: 23.0315
2023-11-05 03:21:38,682:INFO:  Epoch 183/500:  train Loss: 22.2365   val Loss: 25.6094   time: 167.78s   best: 25.5243
2023-11-05 03:24:26,349:INFO:  Epoch 184/500:  train Loss: 22.4166   val Loss: 26.5869   time: 167.63s   best: 25.5243
2023-11-05 03:25:52,999:INFO:  Epoch 351/500:  train Loss: 18.8512   val Loss: 23.2771   time: 342.19s   best: 23.0315
2023-11-05 03:27:14,114:INFO:  Epoch 185/500:  train Loss: 22.2134   val Loss: 26.3548   time: 167.75s   best: 25.5243
2023-11-05 03:30:01,571:INFO:  Epoch 186/500:  train Loss: 22.1867   val Loss: 26.2404   time: 167.43s   best: 25.5243
2023-11-05 03:31:37,593:INFO:  Epoch 352/500:  train Loss: 18.6498   val Loss: 23.5698   time: 344.26s   best: 23.0315
2023-11-05 03:32:49,465:INFO:  Epoch 187/500:  train Loss: 22.1795   val Loss: 25.8787   time: 167.88s   best: 25.5243
2023-11-05 03:35:37,210:INFO:  Epoch 188/500:  train Loss: 22.2655   val Loss: 27.2470   time: 167.66s   best: 25.5243
2023-11-05 03:37:21,340:INFO:  Epoch 353/500:  train Loss: 18.6141   val Loss: 24.1370   time: 343.73s   best: 23.0315
2023-11-05 03:38:24,916:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 03:38:24,949:INFO:  Epoch 189/500:  train Loss: 22.1458   val Loss: 24.9976   time: 167.70s   best: 24.9976
2023-11-05 03:41:13,026:INFO:  Epoch 190/500:  train Loss: 22.0486   val Loss: 26.0756   time: 168.07s   best: 24.9976
2023-11-05 03:43:05,456:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-05 03:43:05,502:INFO:  Epoch 354/500:  train Loss: 18.6960   val Loss: 22.9880   time: 344.10s   best: 22.9880
2023-11-05 03:44:00,966:INFO:  Epoch 191/500:  train Loss: 22.1221   val Loss: 28.6921   time: 167.93s   best: 24.9976
2023-11-05 03:46:48,432:INFO:  Epoch 192/500:  train Loss: 22.0400   val Loss: 25.3332   time: 167.43s   best: 24.9976
2023-11-05 03:48:21,011:INFO:  Epoch 355/500:  train Loss: 18.6667   val Loss: 23.3657   time: 315.51s   best: 22.9880
2023-11-05 03:49:36,769:INFO:  Epoch 193/500:  train Loss: 22.1816   val Loss: 28.8348   time: 168.31s   best: 24.9976
2023-11-05 03:52:25,294:INFO:  Epoch 194/500:  train Loss: 22.6190   val Loss: 26.9188   time: 168.48s   best: 24.9976
2023-11-05 03:54:04,703:INFO:  Epoch 356/500:  train Loss: 18.5880   val Loss: 25.9354   time: 343.67s   best: 22.9880
2023-11-05 03:55:14,529:INFO:  Epoch 195/500:  train Loss: 22.4867   val Loss: 28.0519   time: 169.22s   best: 24.9976
2023-11-05 03:58:02,306:INFO:  Epoch 196/500:  train Loss: 21.9351   val Loss: 26.3345   time: 167.75s   best: 24.9976
2023-11-05 03:59:48,844:INFO:  Epoch 357/500:  train Loss: 18.6267   val Loss: 23.3203   time: 344.13s   best: 22.9880
2023-11-05 04:00:50,598:INFO:  Epoch 197/500:  train Loss: 22.0522   val Loss: 25.9030   time: 168.12s   best: 24.9976
2023-11-05 04:03:38,299:INFO:  Epoch 198/500:  train Loss: 21.8308   val Loss: 25.8164   time: 167.67s   best: 24.9976
2023-11-05 04:05:28,917:INFO:  Epoch 358/500:  train Loss: 18.5547   val Loss: 23.3210   time: 340.03s   best: 22.9880
2023-11-05 04:06:25,982:INFO:  Epoch 199/500:  train Loss: 21.8533   val Loss: 27.3988   time: 167.67s   best: 24.9976
2023-11-05 04:09:14,821:INFO:  Epoch 200/500:  train Loss: 21.7984   val Loss: 25.9473   time: 168.80s   best: 24.9976
2023-11-05 04:11:13,146:INFO:  Epoch 359/500:  train Loss: 18.5654   val Loss: 23.5388   time: 344.16s   best: 22.9880
2023-11-05 04:12:02,664:INFO:  Epoch 201/500:  train Loss: 21.9367   val Loss: 25.7449   time: 167.83s   best: 24.9976
2023-11-05 04:14:51,807:INFO:  Epoch 202/500:  train Loss: 22.0097   val Loss: 25.9907   time: 169.11s   best: 24.9976
2023-11-05 04:15:29,273:INFO:  Epoch 360/500:  train Loss: 18.5419   val Loss: 23.5727   time: 256.11s   best: 22.9880
2023-11-05 04:17:39,506:INFO:  Epoch 203/500:  train Loss: 21.9358   val Loss: 26.0256   time: 167.67s   best: 24.9976
2023-11-05 04:20:15,518:INFO:  Epoch 361/500:  train Loss: 18.6473   val Loss: 23.5106   time: 286.23s   best: 22.9880
2023-11-05 04:20:27,486:INFO:  Epoch 204/500:  train Loss: 21.7464   val Loss: 25.4061   time: 167.95s   best: 24.9976
2023-11-05 04:23:15,373:INFO:  Epoch 205/500:  train Loss: 21.7685   val Loss: 28.1336   time: 167.89s   best: 24.9976
2023-11-05 04:25:24,780:INFO:  Epoch 362/500:  train Loss: 18.6663   val Loss: 23.2201   time: 309.25s   best: 22.9880
2023-11-05 04:26:04,401:INFO:  Epoch 206/500:  train Loss: 21.9622   val Loss: 25.5696   time: 168.91s   best: 24.9976
2023-11-05 04:28:52,157:INFO:  Epoch 207/500:  train Loss: 21.6712   val Loss: 25.8391   time: 167.73s   best: 24.9976
2023-11-05 04:31:09,077:INFO:  Epoch 363/500:  train Loss: 18.9590   val Loss: 23.3949   time: 344.27s   best: 22.9880
2023-11-05 04:31:39,684:INFO:  Epoch 208/500:  train Loss: 21.5873   val Loss: 25.9163   time: 167.49s   best: 24.9976
2023-11-05 04:34:27,580:INFO:  Epoch 209/500:  train Loss: 22.1256   val Loss: 26.2379   time: 167.87s   best: 24.9976
2023-11-05 04:35:48,106:INFO:  Epoch 364/500:  train Loss: 18.5047   val Loss: 23.5754   time: 279.02s   best: 22.9880
2023-11-05 04:37:15,123:INFO:  Epoch 210/500:  train Loss: 21.6840   val Loss: 25.9235   time: 167.32s   best: 24.9976
2023-11-05 04:40:03,010:INFO:  Epoch 211/500:  train Loss: 21.6371   val Loss: 29.1968   time: 167.88s   best: 24.9976
2023-11-05 04:41:12,701:INFO:  Epoch 365/500:  train Loss: 18.4935   val Loss: 23.3448   time: 324.59s   best: 22.9880
2023-11-05 04:42:51,609:INFO:  Epoch 212/500:  train Loss: 21.8477   val Loss: 26.4971   time: 168.60s   best: 24.9976
2023-11-05 04:45:39,770:INFO:  Epoch 213/500:  train Loss: 21.6249   val Loss: 26.4930   time: 168.12s   best: 24.9976
2023-11-05 04:46:02,228:INFO:  Epoch 366/500:  train Loss: 18.5684   val Loss: 23.6402   time: 289.51s   best: 22.9880
2023-11-05 04:48:27,256:INFO:  Epoch 214/500:  train Loss: 21.6898   val Loss: 26.9249   time: 167.48s   best: 24.9976
2023-11-05 04:50:25,674:INFO:  Epoch 367/500:  train Loss: 18.4903   val Loss: 23.4249   time: 263.44s   best: 22.9880
2023-11-05 04:51:15,116:INFO:  Epoch 215/500:  train Loss: 21.5316   val Loss: 27.5158   time: 167.85s   best: 24.9976
2023-11-05 04:54:02,760:INFO:  Epoch 216/500:  train Loss: 21.4932   val Loss: 26.3402   time: 167.64s   best: 24.9976
2023-11-05 04:54:28,822:INFO:  Epoch 368/500:  train Loss: 18.7225   val Loss: 23.2153   time: 243.14s   best: 22.9880
2023-11-05 04:56:52,089:INFO:  Epoch 217/500:  train Loss: 21.6559   val Loss: 25.6397   time: 169.28s   best: 24.9976
2023-11-05 04:58:34,893:INFO:  Epoch 369/500:  train Loss: 19.3924   val Loss: 23.6854   time: 246.06s   best: 22.9880
2023-11-05 04:59:39,738:INFO:  Epoch 218/500:  train Loss: 21.5010   val Loss: 25.0916   time: 167.38s   best: 24.9976
2023-11-05 05:02:27,688:INFO:  Epoch 219/500:  train Loss: 21.4522   val Loss: 25.1618   time: 167.92s   best: 24.9976
2023-11-05 05:02:40,171:INFO:  Epoch 370/500:  train Loss: 18.7535   val Loss: 23.6430   time: 245.23s   best: 22.9880
2023-11-05 05:05:16,460:INFO:  Epoch 220/500:  train Loss: 21.3861   val Loss: 25.6896   time: 168.75s   best: 24.9976
2023-11-05 05:07:59,552:INFO:  Epoch 371/500:  train Loss: 18.7411   val Loss: 23.6342   time: 319.38s   best: 22.9880
2023-11-05 05:08:04,429:INFO:  Epoch 221/500:  train Loss: 21.3432   val Loss: 25.7679   time: 167.75s   best: 24.9976
2023-11-05 05:10:51,885:INFO:  Epoch 222/500:  train Loss: 21.4640   val Loss: 25.9250   time: 167.43s   best: 24.9976
2023-11-05 05:12:12,779:INFO:  Epoch 372/500:  train Loss: 18.6341   val Loss: 23.8537   time: 253.20s   best: 22.9880
2023-11-05 05:13:39,627:INFO:  Epoch 223/500:  train Loss: 21.2778   val Loss: 25.7356   time: 167.72s   best: 24.9976
2023-11-05 05:16:28,703:INFO:  Epoch 224/500:  train Loss: 22.0675   val Loss: 25.8553   time: 169.04s   best: 24.9976
2023-11-05 05:16:40,615:INFO:  Epoch 373/500:  train Loss: 18.4836   val Loss: 24.2391   time: 267.74s   best: 22.9880
2023-11-05 05:19:16,760:INFO:  Epoch 225/500:  train Loss: 21.3776   val Loss: 26.0653   time: 168.03s   best: 24.9976
2023-11-05 05:20:45,886:INFO:  Epoch 374/500:  train Loss: 18.5212   val Loss: 23.2504   time: 245.27s   best: 22.9880
2023-11-05 05:22:04,692:INFO:  Epoch 226/500:  train Loss: 21.4143   val Loss: 25.6396   time: 167.92s   best: 24.9976
2023-11-05 05:24:52,104:INFO:  Epoch 227/500:  train Loss: 21.4538   val Loss: 25.4331   time: 167.39s   best: 24.9976
2023-11-05 05:25:59,312:INFO:  Epoch 375/500:  train Loss: 18.4188   val Loss: 23.4298   time: 313.40s   best: 22.9880
2023-11-05 05:27:41,046:INFO:  Epoch 228/500:  train Loss: 21.2773   val Loss: 26.2070   time: 168.92s   best: 24.9976
2023-11-05 05:30:28,866:INFO:  Epoch 229/500:  train Loss: 21.2724   val Loss: 26.0848   time: 167.66s   best: 24.9976
2023-11-05 05:31:38,069:INFO:  Epoch 376/500:  train Loss: 18.6168   val Loss: 24.3119   time: 338.74s   best: 22.9880
2023-11-05 05:33:16,422:INFO:  Epoch 230/500:  train Loss: 21.8784   val Loss: 25.8802   time: 167.54s   best: 24.9976
2023-11-05 05:35:54,636:INFO:  Epoch 377/500:  train Loss: 18.5881   val Loss: 23.4749   time: 256.56s   best: 22.9880
2023-11-05 05:36:04,681:INFO:  Epoch 231/500:  train Loss: 21.2144   val Loss: 26.3699   time: 168.22s   best: 24.9976
2023-11-05 05:38:53,616:INFO:  Epoch 232/500:  train Loss: 21.3009   val Loss: 25.9681   time: 168.92s   best: 24.9976
2023-11-05 05:39:58,919:INFO:  Epoch 378/500:  train Loss: 18.4518   val Loss: 23.8949   time: 244.27s   best: 22.9880
2023-11-05 05:41:41,762:INFO:  Epoch 233/500:  train Loss: 21.4788   val Loss: 25.7381   time: 168.14s   best: 24.9976
2023-11-05 05:44:28,151:INFO:  Epoch 379/500:  train Loss: 18.7459   val Loss: 23.7409   time: 269.21s   best: 22.9880
2023-11-05 05:44:29,159:INFO:  Epoch 234/500:  train Loss: 21.4989   val Loss: 27.0267   time: 167.37s   best: 24.9976
2023-11-05 05:47:17,096:INFO:  Epoch 235/500:  train Loss: 21.1562   val Loss: 25.4643   time: 167.93s   best: 24.9976
2023-11-05 05:48:32,277:INFO:  Epoch 380/500:  train Loss: 18.4150   val Loss: 23.4566   time: 244.10s   best: 22.9880
2023-11-05 05:50:05,391:INFO:  Epoch 236/500:  train Loss: 21.1011   val Loss: 25.6988   time: 168.29s   best: 24.9976
2023-11-05 05:52:52,991:INFO:  Epoch 237/500:  train Loss: 21.3293   val Loss: 25.5374   time: 167.57s   best: 24.9976
2023-11-05 05:53:54,573:INFO:  Epoch 381/500:  train Loss: 18.4482   val Loss: 23.1263   time: 322.28s   best: 22.9880
2023-11-05 05:55:41,410:INFO:  Epoch 238/500:  train Loss: 21.6308   val Loss: 26.6182   time: 168.42s   best: 24.9976
2023-11-05 05:58:29,134:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 05:58:29,153:INFO:  Epoch 239/500:  train Loss: 21.0382   val Loss: 24.9498   time: 167.70s   best: 24.9498
2023-11-05 05:59:33,017:INFO:  Epoch 382/500:  train Loss: 18.4187   val Loss: 23.5148   time: 338.42s   best: 22.9880
2023-11-05 06:01:16,811:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 06:01:16,850:INFO:  Epoch 240/500:  train Loss: 21.0854   val Loss: 24.9442   time: 167.65s   best: 24.9442
2023-11-05 06:04:05,953:INFO:  Epoch 241/500:  train Loss: 21.1466   val Loss: 26.5709   time: 169.09s   best: 24.9442
2023-11-05 06:05:17,213:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-05 06:05:17,380:INFO:  Epoch 383/500:  train Loss: 18.4036   val Loss: 22.7572   time: 344.18s   best: 22.7572
2023-11-05 06:06:54,406:INFO:  Epoch 242/500:  train Loss: 21.0073   val Loss: 26.1089   time: 168.45s   best: 24.9442
2023-11-05 06:09:43,079:INFO:  Epoch 243/500:  train Loss: 21.0641   val Loss: 26.9909   time: 168.64s   best: 24.9442
2023-11-05 06:11:00,446:INFO:  Epoch 384/500:  train Loss: 18.4346   val Loss: 23.3031   time: 343.06s   best: 22.7572
2023-11-05 06:12:30,695:INFO:  Epoch 244/500:  train Loss: 20.9633   val Loss: 25.4159   time: 167.60s   best: 24.9442
2023-11-05 06:15:19,770:INFO:  Epoch 245/500:  train Loss: 21.0875   val Loss: 25.6181   time: 169.03s   best: 24.9442
2023-11-05 06:16:32,267:INFO:  Epoch 385/500:  train Loss: 18.4483   val Loss: 26.0709   time: 331.79s   best: 22.7572
2023-11-05 06:18:06,928:INFO:  Epoch 246/500:  train Loss: 21.4806   val Loss: 25.4691   time: 167.16s   best: 24.9442
2023-11-05 06:20:55,620:INFO:  Epoch 247/500:  train Loss: 21.0902   val Loss: 26.5354   time: 168.67s   best: 24.9442
2023-11-05 06:22:15,333:INFO:  Epoch 386/500:  train Loss: 18.7352   val Loss: 23.0942   time: 343.04s   best: 22.7572
2023-11-05 06:23:42,756:INFO:  Epoch 248/500:  train Loss: 21.0650   val Loss: 25.3898   time: 167.13s   best: 24.9442
2023-11-05 06:26:30,181:INFO:  Epoch 249/500:  train Loss: 20.9772   val Loss: 25.7841   time: 167.39s   best: 24.9442
2023-11-05 06:27:58,845:INFO:  Epoch 387/500:  train Loss: 18.5018   val Loss: 24.5818   time: 343.51s   best: 22.7572
2023-11-05 06:29:17,448:INFO:  Epoch 250/500:  train Loss: 21.1381   val Loss: 26.6616   time: 167.25s   best: 24.9442
2023-11-05 06:32:05,401:INFO:  Epoch 251/500:  train Loss: 20.8853   val Loss: 26.1646   time: 167.94s   best: 24.9442
2023-11-05 06:33:40,427:INFO:  Epoch 388/500:  train Loss: 18.4957   val Loss: 22.9737   time: 341.57s   best: 22.7572
2023-11-05 06:34:52,698:INFO:  Epoch 252/500:  train Loss: 21.0010   val Loss: 27.4593   time: 167.27s   best: 24.9442
2023-11-05 06:37:41,456:INFO:  Epoch 253/500:  train Loss: 20.8514   val Loss: 26.0430   time: 168.74s   best: 24.9442
2023-11-05 06:39:21,527:INFO:  Epoch 389/500:  train Loss: 18.3586   val Loss: 23.4885   time: 341.09s   best: 22.7572
2023-11-05 06:40:30,059:INFO:  Epoch 254/500:  train Loss: 20.9321   val Loss: 25.3702   time: 168.59s   best: 24.9442
2023-11-05 06:43:18,571:INFO:  Epoch 255/500:  train Loss: 20.8381   val Loss: 27.4809   time: 168.47s   best: 24.9442
2023-11-05 06:45:03,792:INFO:  Epoch 390/500:  train Loss: 18.5593   val Loss: 23.5844   time: 342.25s   best: 22.7572
2023-11-05 06:46:07,259:INFO:  Epoch 256/500:  train Loss: 21.0135   val Loss: 27.4110   time: 168.68s   best: 24.9442
2023-11-05 06:48:54,692:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 06:48:54,711:INFO:  Epoch 257/500:  train Loss: 21.2126   val Loss: 24.7294   time: 167.41s   best: 24.7294
2023-11-05 06:49:10,238:INFO:  Epoch 391/500:  train Loss: 18.3822   val Loss: 27.1884   time: 246.28s   best: 22.7572
2023-11-05 06:51:43,438:INFO:  Epoch 258/500:  train Loss: 20.8200   val Loss: 24.9834   time: 168.71s   best: 24.7294
2023-11-05 06:53:31,981:INFO:  Epoch 392/500:  train Loss: 18.5979   val Loss: 23.0678   time: 261.74s   best: 22.7572
2023-11-05 06:54:31,426:INFO:  Epoch 259/500:  train Loss: 20.9338   val Loss: 25.1516   time: 167.97s   best: 24.7294
2023-11-05 06:57:18,762:INFO:  Epoch 260/500:  train Loss: 20.7367   val Loss: 25.6317   time: 167.29s   best: 24.7294
2023-11-05 06:57:46,256:INFO:  Epoch 393/500:  train Loss: 18.8049   val Loss: 23.3617   time: 254.26s   best: 22.7572
2023-11-05 07:00:06,549:INFO:  Epoch 261/500:  train Loss: 21.1332   val Loss: 26.8595   time: 167.76s   best: 24.7294
2023-11-05 07:02:14,621:INFO:  Epoch 394/500:  train Loss: 18.5673   val Loss: 22.9099   time: 268.36s   best: 22.7572
2023-11-05 07:02:54,116:INFO:  Epoch 262/500:  train Loss: 20.7531   val Loss: 26.1833   time: 167.55s   best: 24.7294
2023-11-05 07:05:41,815:INFO:  Epoch 263/500:  train Loss: 20.7864   val Loss: 25.9688   time: 167.66s   best: 24.7294
2023-11-05 07:06:20,985:INFO:  Epoch 395/500:  train Loss: 18.3410   val Loss: 23.4957   time: 246.35s   best: 22.7572
2023-11-05 07:08:29,276:INFO:  Epoch 264/500:  train Loss: 20.6273   val Loss: 26.0517   time: 167.44s   best: 24.7294
2023-11-05 07:10:54,031:INFO:  Epoch 396/500:  train Loss: 18.3749   val Loss: 23.2767   time: 273.03s   best: 22.7572
2023-11-05 07:11:17,455:INFO:  Epoch 265/500:  train Loss: 20.8312   val Loss: 25.5382   time: 168.15s   best: 24.7294
2023-11-05 07:14:04,883:INFO:  Epoch 266/500:  train Loss: 20.7945   val Loss: 27.8766   time: 167.42s   best: 24.7294
2023-11-05 07:15:00,208:INFO:  Epoch 397/500:  train Loss: 18.4556   val Loss: 23.6429   time: 246.17s   best: 22.7572
2023-11-05 07:16:52,346:INFO:  Epoch 267/500:  train Loss: 20.6589   val Loss: 25.4006   time: 167.46s   best: 24.7294
2023-11-05 07:19:12,669:INFO:  Epoch 398/500:  train Loss: 18.7696   val Loss: 23.1134   time: 252.44s   best: 22.7572
2023-11-05 07:19:39,946:INFO:  Epoch 268/500:  train Loss: 20.6518   val Loss: 25.0014   time: 167.55s   best: 24.7294
2023-11-05 07:22:29,035:INFO:  Epoch 269/500:  train Loss: 20.5871   val Loss: 25.3957   time: 168.90s   best: 24.7294
2023-11-05 07:23:25,720:INFO:  Epoch 399/500:  train Loss: 18.3897   val Loss: 23.5545   time: 253.02s   best: 22.7572
2023-11-05 07:25:16,314:INFO:  Epoch 270/500:  train Loss: 20.7539   val Loss: 25.2373   time: 167.28s   best: 24.7294
2023-11-05 07:28:03,973:INFO:  Epoch 271/500:  train Loss: 20.7685   val Loss: 26.3634   time: 167.62s   best: 24.7294
2023-11-05 07:28:29,730:INFO:  Epoch 400/500:  train Loss: 18.2620   val Loss: 22.9059   time: 303.98s   best: 22.7572
2023-11-05 07:30:51,168:INFO:  Epoch 272/500:  train Loss: 20.7930   val Loss: 25.9403   time: 167.17s   best: 24.7294
2023-11-05 07:33:38,744:INFO:  Epoch 273/500:  train Loss: 20.9620   val Loss: 25.0900   time: 167.54s   best: 24.7294
2023-11-05 07:34:14,353:INFO:  Epoch 401/500:  train Loss: 18.3409   val Loss: 23.7396   time: 344.62s   best: 22.7572
2023-11-05 07:36:26,195:INFO:  Epoch 274/500:  train Loss: 21.6861   val Loss: 25.3306   time: 167.43s   best: 24.7294
2023-11-05 07:39:13,862:INFO:  Epoch 275/500:  train Loss: 20.8299   val Loss: 24.7928   time: 167.65s   best: 24.7294
2023-11-05 07:39:57,293:INFO:  Epoch 402/500:  train Loss: 18.3182   val Loss: 23.6477   time: 342.93s   best: 22.7572
2023-11-05 07:42:02,436:INFO:  Epoch 276/500:  train Loss: 20.7120   val Loss: 24.8060   time: 168.57s   best: 24.7294
2023-11-05 07:44:51,501:INFO:  Epoch 277/500:  train Loss: 20.5314   val Loss: 25.9266   time: 169.02s   best: 24.7294
2023-11-05 07:45:40,476:INFO:  Epoch 403/500:  train Loss: 18.3285   val Loss: 23.5396   time: 343.17s   best: 22.7572
2023-11-05 07:47:38,845:INFO:  Epoch 278/500:  train Loss: 20.5106   val Loss: 25.2901   time: 167.33s   best: 24.7294
2023-11-05 07:50:26,990:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 07:50:27,010:INFO:  Epoch 279/500:  train Loss: 20.6528   val Loss: 24.7248   time: 168.11s   best: 24.7248
2023-11-05 07:51:23,985:INFO:  Epoch 404/500:  train Loss: 18.4071   val Loss: 23.5642   time: 343.50s   best: 22.7572
2023-11-05 07:53:14,595:INFO:  Epoch 280/500:  train Loss: 20.4576   val Loss: 26.4065   time: 167.57s   best: 24.7248
2023-11-05 07:56:02,099:INFO:  Epoch 281/500:  train Loss: 20.4961   val Loss: 26.2653   time: 167.48s   best: 24.7248
2023-11-05 07:56:53,703:INFO:  Epoch 405/500:  train Loss: 18.4288   val Loss: 23.3369   time: 329.71s   best: 22.7572
2023-11-05 07:58:49,268:INFO:  Epoch 282/500:  train Loss: 20.6587   val Loss: 25.1937   time: 167.17s   best: 24.7248
2023-11-05 08:01:20,516:INFO:  Epoch 406/500:  train Loss: 18.2518   val Loss: 24.4034   time: 266.80s   best: 22.7572
2023-11-05 08:01:36,620:INFO:  Epoch 283/500:  train Loss: 20.7823   val Loss: 25.4763   time: 167.29s   best: 24.7248
2023-11-05 08:04:25,253:INFO:  Epoch 284/500:  train Loss: 20.5241   val Loss: 24.9898   time: 168.61s   best: 24.7248
2023-11-05 08:05:44,254:INFO:  Epoch 407/500:  train Loss: 18.3782   val Loss: 23.2816   time: 263.71s   best: 22.7572
2023-11-05 08:07:13,812:INFO:  Epoch 285/500:  train Loss: 20.3476   val Loss: 25.9317   time: 168.56s   best: 24.7248
2023-11-05 08:09:49,069:INFO:  Epoch 408/500:  train Loss: 19.1199   val Loss: 25.3267   time: 244.80s   best: 22.7572
2023-11-05 08:10:01,044:INFO:  Epoch 286/500:  train Loss: 20.7177   val Loss: 25.7574   time: 167.21s   best: 24.7248
2023-11-05 08:12:49,259:INFO:  Epoch 287/500:  train Loss: 20.5986   val Loss: 25.8575   time: 168.21s   best: 24.7248
2023-11-05 08:14:10,280:INFO:  Epoch 409/500:  train Loss: 18.6248   val Loss: 24.1310   time: 261.20s   best: 22.7572
2023-11-05 08:15:37,027:INFO:  Epoch 288/500:  train Loss: 20.5449   val Loss: 27.6424   time: 167.76s   best: 24.7248
2023-11-05 08:18:25,043:INFO:  Epoch 289/500:  train Loss: 20.5228   val Loss: 25.8747   time: 167.97s   best: 24.7248
2023-11-05 08:18:35,130:INFO:  Epoch 410/500:  train Loss: 18.3533   val Loss: 23.3999   time: 264.82s   best: 22.7572
2023-11-05 08:21:11,894:INFO:  Epoch 290/500:  train Loss: 20.6985   val Loss: 25.7619   time: 166.84s   best: 24.7248
2023-11-05 08:22:43,675:INFO:  Epoch 411/500:  train Loss: 18.8749   val Loss: 23.5708   time: 248.54s   best: 22.7572
2023-11-05 08:23:59,887:INFO:  Epoch 291/500:  train Loss: 20.3531   val Loss: 25.4154   time: 167.97s   best: 24.7248
2023-11-05 08:26:46,961:INFO:  Epoch 292/500:  train Loss: 20.7270   val Loss: 25.9737   time: 167.06s   best: 24.7248
2023-11-05 08:27:16,559:INFO:  Epoch 412/500:  train Loss: 18.3896   val Loss: 23.2345   time: 272.86s   best: 22.7572
2023-11-05 08:29:34,023:INFO:  Epoch 293/500:  train Loss: 20.2976   val Loss: 25.4358   time: 167.06s   best: 24.7248
2023-11-05 08:31:23,552:INFO:  Epoch 413/500:  train Loss: 19.6882   val Loss: 24.2142   time: 246.98s   best: 22.7572
2023-11-05 08:32:21,874:INFO:  Epoch 294/500:  train Loss: 20.3116   val Loss: 26.7134   time: 167.62s   best: 24.7248
2023-11-05 08:35:09,366:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 08:35:09,386:INFO:  Epoch 295/500:  train Loss: 20.5876   val Loss: 24.0560   time: 167.45s   best: 24.0560
2023-11-05 08:35:40,321:INFO:  Epoch 414/500:  train Loss: 18.3503   val Loss: 23.5236   time: 256.76s   best: 22.7572
2023-11-05 08:37:56,430:INFO:  Epoch 296/500:  train Loss: 20.5223   val Loss: 24.8832   time: 167.03s   best: 24.0560
2023-11-05 08:39:44,008:INFO:  Epoch 415/500:  train Loss: 18.4863   val Loss: 23.5494   time: 243.66s   best: 22.7572
2023-11-05 08:40:44,869:INFO:  Epoch 297/500:  train Loss: 20.2277   val Loss: 25.4340   time: 168.39s   best: 24.0560
2023-11-05 08:43:32,254:INFO:  Epoch 298/500:  train Loss: 20.2564   val Loss: 25.1344   time: 167.33s   best: 24.0560
2023-11-05 08:43:48,381:INFO:  Epoch 416/500:  train Loss: 18.2337   val Loss: 22.8920   time: 244.36s   best: 22.7572
2023-11-05 08:46:19,089:INFO:  Epoch 299/500:  train Loss: 20.6342   val Loss: 26.5705   time: 166.82s   best: 24.0560
2023-11-05 08:47:53,898:INFO:  Epoch 417/500:  train Loss: 18.2590   val Loss: 23.2874   time: 245.51s   best: 22.7572
2023-11-05 08:49:05,832:INFO:  Epoch 300/500:  train Loss: 20.9483   val Loss: 26.9187   time: 166.71s   best: 24.0560
2023-11-05 08:51:53,194:INFO:  Epoch 301/500:  train Loss: 21.2998   val Loss: 25.7118   time: 167.32s   best: 24.0560
2023-11-05 08:51:59,401:INFO:  Epoch 418/500:  train Loss: 18.3852   val Loss: 23.5914   time: 245.49s   best: 22.7572
2023-11-05 08:54:40,286:INFO:  Epoch 302/500:  train Loss: 20.4012   val Loss: 25.0580   time: 167.09s   best: 24.0560
2023-11-05 08:56:12,415:INFO:  Epoch 419/500:  train Loss: 18.2335   val Loss: 23.4633   time: 253.01s   best: 22.7572
2023-11-05 08:57:27,412:INFO:  Epoch 303/500:  train Loss: 20.2862   val Loss: 25.3914   time: 167.10s   best: 24.0560
2023-11-05 09:00:14,416:INFO:  Epoch 304/500:  train Loss: 20.4570   val Loss: 25.4775   time: 166.97s   best: 24.0560
2023-11-05 09:00:35,510:INFO:  Epoch 420/500:  train Loss: 18.2252   val Loss: 23.0614   time: 263.07s   best: 22.7572
2023-11-05 09:03:02,509:INFO:  Epoch 305/500:  train Loss: 20.3963   val Loss: 25.2592   time: 168.08s   best: 24.0560
2023-11-05 09:04:52,779:INFO:  Epoch 421/500:  train Loss: 18.3590   val Loss: 23.2832   time: 257.27s   best: 22.7572
2023-11-05 09:05:50,009:INFO:  Epoch 306/500:  train Loss: 20.2434   val Loss: 25.3316   time: 167.49s   best: 24.0560
2023-11-05 09:08:38,460:INFO:  Epoch 307/500:  train Loss: 20.1879   val Loss: 25.1112   time: 168.43s   best: 24.0560
2023-11-05 09:08:54,415:INFO:  Epoch 422/500:  train Loss: 18.2050   val Loss: 23.1700   time: 241.61s   best: 22.7572
2023-11-05 09:11:25,456:INFO:  Epoch 308/500:  train Loss: 20.3555   val Loss: 24.3643   time: 166.99s   best: 24.0560
2023-11-05 09:13:11,378:INFO:  Epoch 423/500:  train Loss: 18.4492   val Loss: 23.6082   time: 256.95s   best: 22.7572
2023-11-05 09:14:13,147:INFO:  Epoch 309/500:  train Loss: 20.4541   val Loss: 25.1007   time: 167.66s   best: 24.0560
2023-11-05 09:17:01,223:INFO:  Epoch 310/500:  train Loss: 21.0968   val Loss: 25.6017   time: 168.06s   best: 24.0560
2023-11-05 09:17:30,170:INFO:  Epoch 424/500:  train Loss: 18.2665   val Loss: 23.3539   time: 258.72s   best: 22.7572
2023-11-05 09:19:48,328:INFO:  Epoch 311/500:  train Loss: 20.2683   val Loss: 25.5567   time: 167.09s   best: 24.0560
2023-11-05 09:22:11,284:INFO:  Epoch 425/500:  train Loss: 18.1189   val Loss: 23.5935   time: 281.09s   best: 22.7572
2023-11-05 09:22:36,389:INFO:  Epoch 312/500:  train Loss: 20.1258   val Loss: 25.5865   time: 168.03s   best: 24.0560
2023-11-05 09:25:23,398:INFO:  Epoch 313/500:  train Loss: 20.4517   val Loss: 27.4020   time: 166.99s   best: 24.0560
2023-11-05 09:26:12,764:INFO:  Epoch 426/500:  train Loss: 18.5166   val Loss: 23.3108   time: 241.47s   best: 22.7572
2023-11-05 09:28:10,647:INFO:  Epoch 314/500:  train Loss: 20.2847   val Loss: 25.9030   time: 167.24s   best: 24.0560
2023-11-05 09:30:17,367:INFO:  Epoch 427/500:  train Loss: 18.5813   val Loss: 24.6317   time: 244.58s   best: 22.7572
2023-11-05 09:30:57,700:INFO:  Epoch 315/500:  train Loss: 20.2750   val Loss: 25.6265   time: 167.04s   best: 24.0560
2023-11-05 09:33:44,694:INFO:  Epoch 316/500:  train Loss: 20.3297   val Loss: 26.2641   time: 166.95s   best: 24.0560
2023-11-05 09:34:18,486:INFO:  Epoch 428/500:  train Loss: 18.2514   val Loss: 23.3926   time: 241.11s   best: 22.7572
2023-11-05 09:36:31,713:INFO:  Epoch 317/500:  train Loss: 20.1142   val Loss: 26.0825   time: 167.02s   best: 24.0560
2023-11-05 09:38:19,588:INFO:  Epoch 429/500:  train Loss: 18.4024   val Loss: 22.9587   time: 241.09s   best: 22.7572
2023-11-05 09:39:19,778:INFO:  Epoch 318/500:  train Loss: 20.2786   val Loss: 24.8768   time: 168.03s   best: 24.0560
2023-11-05 09:42:07,136:INFO:  Epoch 319/500:  train Loss: 20.1103   val Loss: 25.5351   time: 167.32s   best: 24.0560
2023-11-05 09:42:21,904:INFO:  Epoch 430/500:  train Loss: 18.3850   val Loss: 22.7605   time: 242.31s   best: 22.7572
2023-11-05 09:44:54,122:INFO:  Epoch 320/500:  train Loss: 20.1618   val Loss: 25.1133   time: 166.99s   best: 24.0560
2023-11-05 09:46:25,923:INFO:  Epoch 431/500:  train Loss: 18.2706   val Loss: 23.2964   time: 244.02s   best: 22.7572
2023-11-05 09:47:41,363:INFO:  Epoch 321/500:  train Loss: 20.1660   val Loss: 24.7696   time: 167.23s   best: 24.0560
2023-11-05 09:50:29,629:INFO:  Epoch 322/500:  train Loss: 20.1742   val Loss: 24.8828   time: 168.25s   best: 24.0560
2023-11-05 09:52:06,984:INFO:  Epoch 432/500:  train Loss: 18.2098   val Loss: 22.9458   time: 341.05s   best: 22.7572
2023-11-05 09:53:17,173:INFO:  Epoch 323/500:  train Loss: 19.9687   val Loss: 25.0874   time: 167.54s   best: 24.0560
2023-11-05 09:56:04,361:INFO:  Epoch 324/500:  train Loss: 20.3629   val Loss: 25.1581   time: 167.17s   best: 24.0560
2023-11-05 09:56:40,770:INFO:  Epoch 433/500:  train Loss: 18.6839   val Loss: 23.3129   time: 273.78s   best: 22.7572
2023-11-05 09:58:51,783:INFO:  Epoch 325/500:  train Loss: 19.9410   val Loss: 24.8638   time: 167.42s   best: 24.0560
2023-11-05 10:01:38,685:INFO:  Epoch 326/500:  train Loss: 21.1673   val Loss: 24.8143   time: 166.87s   best: 24.0560
2023-11-05 10:02:17,353:INFO:  Epoch 434/500:  train Loss: 18.1643   val Loss: 23.0029   time: 336.57s   best: 22.7572
2023-11-05 10:04:27,176:INFO:  Epoch 327/500:  train Loss: 20.8638   val Loss: 24.2522   time: 168.49s   best: 24.0560
2023-11-05 10:07:14,970:INFO:  Epoch 328/500:  train Loss: 20.0037   val Loss: 25.1774   time: 167.77s   best: 24.0560
2023-11-05 10:07:58,268:INFO:  Epoch 435/500:  train Loss: 18.2440   val Loss: 23.2720   time: 340.89s   best: 22.7572
2023-11-05 10:10:03,371:INFO:  Epoch 329/500:  train Loss: 19.9772   val Loss: 25.2851   time: 168.40s   best: 24.0560
2023-11-05 10:12:30,568:INFO:  Epoch 436/500:  train Loss: 18.3055   val Loss: 24.6334   time: 272.29s   best: 22.7572
2023-11-05 10:12:51,683:INFO:  Epoch 330/500:  train Loss: 19.9917   val Loss: 25.4056   time: 168.29s   best: 24.0560
2023-11-05 10:15:38,905:INFO:  Epoch 331/500:  train Loss: 20.1308   val Loss: 24.9067   time: 167.20s   best: 24.0560
2023-11-05 10:17:28,479:INFO:  Epoch 437/500:  train Loss: 19.8241   val Loss: 22.9096   time: 297.90s   best: 22.7572
2023-11-05 10:18:26,691:INFO:  Epoch 332/500:  train Loss: 19.8895   val Loss: 25.1400   time: 167.77s   best: 24.0560
2023-11-05 10:21:14,734:INFO:  Epoch 333/500:  train Loss: 20.9863   val Loss: 25.6830   time: 168.02s   best: 24.0560
2023-11-05 10:23:11,249:INFO:  Epoch 438/500:  train Loss: 18.2822   val Loss: 25.5220   time: 342.76s   best: 22.7572
2023-11-05 10:24:03,204:INFO:  Epoch 334/500:  train Loss: 19.9557   val Loss: 25.2741   time: 168.47s   best: 24.0560
2023-11-05 10:26:50,140:INFO:  Epoch 335/500:  train Loss: 20.0861   val Loss: 26.8076   time: 166.87s   best: 24.0560
2023-11-05 10:28:55,889:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.5 dataset (0.1 dropout)_804d.pt
2023-11-05 10:28:55,932:INFO:  Epoch 439/500:  train Loss: 18.1784   val Loss: 22.6527   time: 344.62s   best: 22.6527
2023-11-05 10:29:36,994:INFO:  Epoch 336/500:  train Loss: 20.0576   val Loss: 25.9946   time: 166.85s   best: 24.0560
2023-11-05 10:32:25,089:INFO:  Epoch 337/500:  train Loss: 20.3279   val Loss: 24.9306   time: 167.95s   best: 24.0560
2023-11-05 10:34:39,740:INFO:  Epoch 440/500:  train Loss: 18.2137   val Loss: 23.0818   time: 343.81s   best: 22.6527
2023-11-05 10:35:12,104:INFO:  Epoch 338/500:  train Loss: 20.1055   val Loss: 25.1828   time: 167.00s   best: 24.0560
2023-11-05 10:38:00,707:INFO:  Epoch 339/500:  train Loss: 19.8526   val Loss: 25.5801   time: 168.57s   best: 24.0560
2023-11-05 10:40:23,779:INFO:  Epoch 441/500:  train Loss: 18.2758   val Loss: 25.5911   time: 344.03s   best: 22.6527
2023-11-05 10:40:47,955:INFO:  Epoch 340/500:  train Loss: 20.0260   val Loss: 25.3078   time: 167.24s   best: 24.0560
2023-11-05 10:43:36,317:INFO:  Epoch 341/500:  train Loss: 19.9053   val Loss: 25.0095   time: 168.32s   best: 24.0560
2023-11-05 10:46:05,501:INFO:  Epoch 442/500:  train Loss: 18.1980   val Loss: 23.0089   time: 341.71s   best: 22.6527
2023-11-05 10:46:23,364:INFO:  Epoch 342/500:  train Loss: 20.1566   val Loss: 25.6591   time: 167.05s   best: 24.0560
2023-11-05 10:49:09,732:INFO:  Epoch 343/500:  train Loss: 20.5801   val Loss: 25.6186   time: 166.34s   best: 24.0560
2023-11-05 10:50:11,083:INFO:  Epoch 443/500:  train Loss: 18.1200   val Loss: 23.3408   time: 245.50s   best: 22.6527
2023-11-05 10:51:55,439:INFO:  Epoch 344/500:  train Loss: 20.1122   val Loss: 26.2534   time: 165.69s   best: 24.0560
2023-11-05 10:54:17,562:INFO:  Epoch 444/500:  train Loss: 18.3047   val Loss: 23.2800   time: 246.47s   best: 22.6527
2023-11-05 10:54:42,470:INFO:  Epoch 345/500:  train Loss: 19.8169   val Loss: 25.1966   time: 167.01s   best: 24.0560
2023-11-05 10:57:28,341:INFO:  Epoch 346/500:  train Loss: 19.8897   val Loss: 25.2077   time: 165.85s   best: 24.0560
2023-11-05 10:58:21,065:INFO:  Epoch 445/500:  train Loss: 18.1119   val Loss: 23.7458   time: 243.32s   best: 22.6527
2023-11-05 11:00:14,377:INFO:  Epoch 347/500:  train Loss: 19.8703   val Loss: 28.5916   time: 166.04s   best: 24.0560
2023-11-05 11:02:21,369:INFO:  Epoch 446/500:  train Loss: 18.4077   val Loss: 23.0425   time: 240.30s   best: 22.6527
2023-11-05 11:03:00,187:INFO:  Epoch 348/500:  train Loss: 20.2732   val Loss: 25.4474   time: 165.79s   best: 24.0560
2023-11-05 11:05:46,173:INFO:  Epoch 349/500:  train Loss: 19.7439   val Loss: 26.1523   time: 165.97s   best: 24.0560
2023-11-05 11:06:22,005:INFO:  Epoch 447/500:  train Loss: 18.2680   val Loss: 23.5405   time: 240.62s   best: 22.6527
2023-11-05 11:08:32,279:INFO:  Epoch 350/500:  train Loss: 19.7645   val Loss: 25.5462   time: 166.08s   best: 24.0560
2023-11-05 11:10:31,235:INFO:  Epoch 448/500:  train Loss: 18.2909   val Loss: 22.8092   time: 249.22s   best: 22.6527
2023-11-05 11:11:19,659:INFO:  Epoch 351/500:  train Loss: 19.8192   val Loss: 24.8656   time: 167.36s   best: 24.0560
2023-11-05 11:14:05,962:INFO:  Epoch 352/500:  train Loss: 20.0016   val Loss: 37.3418   time: 166.27s   best: 24.0560
2023-11-05 11:14:48,178:INFO:  Epoch 449/500:  train Loss: 18.0987   val Loss: 22.8869   time: 256.93s   best: 22.6527
2023-11-05 11:16:51,853:INFO:  Epoch 353/500:  train Loss: 20.2219   val Loss: 25.3741   time: 165.62s   best: 24.0560
2023-11-05 11:18:54,267:INFO:  Epoch 450/500:  train Loss: 18.2940   val Loss: 23.8242   time: 246.08s   best: 22.6527
2023-11-05 11:19:38,625:INFO:  Epoch 354/500:  train Loss: 19.7786   val Loss: 25.1915   time: 166.75s   best: 24.0560
2023-11-05 11:22:26,113:INFO:  Epoch 355/500:  train Loss: 20.0797   val Loss: 25.1174   time: 167.45s   best: 24.0560
2023-11-05 11:23:00,316:INFO:  Epoch 451/500:  train Loss: 18.0301   val Loss: 23.0840   time: 246.04s   best: 22.6527
2023-11-05 11:25:13,509:INFO:  Epoch 356/500:  train Loss: 19.7828   val Loss: 25.4479   time: 167.09s   best: 24.0560
2023-11-05 11:27:31,841:INFO:  Epoch 452/500:  train Loss: 18.2047   val Loss: 23.0331   time: 271.52s   best: 22.6527
2023-11-05 11:27:59,578:INFO:  Epoch 357/500:  train Loss: 20.4868   val Loss: 24.9326   time: 166.05s   best: 24.0560
2023-11-05 11:30:45,645:INFO:  Epoch 358/500:  train Loss: 19.6620   val Loss: 24.8963   time: 166.04s   best: 24.0560
2023-11-05 11:31:34,753:INFO:  Epoch 453/500:  train Loss: 18.1729   val Loss: 22.9252   time: 242.90s   best: 22.6527
2023-11-05 11:33:31,272:INFO:  Epoch 359/500:  train Loss: 19.7326   val Loss: 25.2542   time: 165.63s   best: 24.0560
2023-11-05 11:35:40,286:INFO:  Epoch 454/500:  train Loss: 18.1462   val Loss: 23.4141   time: 245.52s   best: 22.6527
2023-11-05 11:36:18,076:INFO:  Epoch 360/500:  train Loss: 19.7207   val Loss: 25.2860   time: 166.78s   best: 24.0560
2023-11-05 11:39:03,890:INFO:  Epoch 361/500:  train Loss: 19.7097   val Loss: 25.2555   time: 165.78s   best: 24.0560
2023-11-05 11:39:44,101:INFO:  Epoch 455/500:  train Loss: 18.0299   val Loss: 22.9040   time: 243.80s   best: 22.6527
2023-11-05 11:41:49,356:INFO:  Epoch 362/500:  train Loss: 19.6865   val Loss: 24.8822   time: 165.45s   best: 24.0560
2023-11-05 11:43:49,448:INFO:  Epoch 456/500:  train Loss: 18.0143   val Loss: 23.6994   time: 245.34s   best: 22.6527
2023-11-05 11:44:35,027:INFO:  Epoch 363/500:  train Loss: 19.7149   val Loss: 25.6413   time: 165.64s   best: 24.0560
2023-11-05 11:47:20,444:INFO:  Epoch 364/500:  train Loss: 19.8232   val Loss: 25.4619   time: 165.41s   best: 24.0560
2023-11-05 11:48:01,262:INFO:  Epoch 457/500:  train Loss: 18.1864   val Loss: 23.3512   time: 251.80s   best: 22.6527
2023-11-05 11:50:06,216:INFO:  Epoch 365/500:  train Loss: 19.9660   val Loss: 24.3743   time: 165.76s   best: 24.0560
2023-11-05 11:52:09,606:INFO:  Epoch 458/500:  train Loss: 18.1172   val Loss: 23.2793   time: 248.33s   best: 22.6527
2023-11-05 11:52:53,159:INFO:  Epoch 366/500:  train Loss: 19.7157   val Loss: 24.6826   time: 166.91s   best: 24.0560
2023-11-05 11:55:38,816:INFO:  Epoch 367/500:  train Loss: 19.6832   val Loss: 24.5126   time: 165.62s   best: 24.0560
2023-11-05 11:56:11,423:INFO:  Epoch 459/500:  train Loss: 18.0415   val Loss: 23.6845   time: 241.81s   best: 22.6527
2023-11-05 11:58:25,349:INFO:  Epoch 368/500:  train Loss: 19.6099   val Loss: 25.1118   time: 166.45s   best: 24.0560
2023-11-05 12:00:42,441:INFO:  Epoch 460/500:  train Loss: 18.1500   val Loss: 30.4811   time: 270.97s   best: 22.6527
2023-11-05 12:01:12,046:INFO:  Epoch 369/500:  train Loss: 19.5942   val Loss: 25.3224   time: 166.68s   best: 24.0560
2023-11-05 12:03:58,495:INFO:  Epoch 370/500:  train Loss: 19.6189   val Loss: 24.8624   time: 166.44s   best: 24.0560
2023-11-05 12:04:48,230:INFO:  Epoch 461/500:  train Loss: 18.5155   val Loss: 26.9657   time: 245.77s   best: 22.6527
2023-11-05 12:06:44,071:INFO:  Epoch 371/500:  train Loss: 19.7691   val Loss: 25.1379   time: 165.56s   best: 24.0560
2023-11-05 12:08:50,578:INFO:  Epoch 462/500:  train Loss: 18.2801   val Loss: 23.2543   time: 242.33s   best: 22.6527
2023-11-05 12:09:29,656:INFO:  Epoch 372/500:  train Loss: 20.1088   val Loss: 25.6975   time: 165.58s   best: 24.0560
2023-11-05 12:12:16,829:INFO:  Epoch 373/500:  train Loss: 19.6439   val Loss: 25.6150   time: 167.14s   best: 24.0560
2023-11-05 12:13:09,529:INFO:  Epoch 463/500:  train Loss: 18.2565   val Loss: 22.8708   time: 258.94s   best: 22.6527
2023-11-05 12:15:03,816:INFO:  Epoch 374/500:  train Loss: 19.7371   val Loss: 25.5956   time: 166.99s   best: 24.0560
2023-11-05 12:17:49,697:INFO:  Epoch 375/500:  train Loss: 19.5722   val Loss: 24.8087   time: 165.87s   best: 24.0560
2023-11-05 12:18:48,213:INFO:  Epoch 464/500:  train Loss: 18.5226   val Loss: 23.2524   time: 338.67s   best: 22.6527
2023-11-05 12:20:36,340:INFO:  Epoch 376/500:  train Loss: 19.7424   val Loss: 30.6390   time: 166.63s   best: 24.0560
2023-11-05 12:22:50,745:INFO:  Epoch 465/500:  train Loss: 18.0554   val Loss: 23.4362   time: 242.50s   best: 22.6527
2023-11-05 12:23:22,203:INFO:  Epoch 377/500:  train Loss: 19.6294   val Loss: 24.7864   time: 165.85s   best: 24.0560
2023-11-05 12:26:08,041:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_1a3e.pt
2023-11-05 12:26:08,071:INFO:  Epoch 378/500:  train Loss: 19.6592   val Loss: 24.0513   time: 165.80s   best: 24.0513
2023-11-05 12:26:52,984:INFO:  Epoch 466/500:  train Loss: 18.1142   val Loss: 23.3914   time: 242.23s   best: 22.6527
2023-11-05 12:28:53,875:INFO:  Epoch 379/500:  train Loss: 19.5733   val Loss: 25.3026   time: 165.80s   best: 24.0513
2023-11-05 12:31:39,937:INFO:  Epoch 380/500:  train Loss: 19.5020   val Loss: 25.9626   time: 166.01s   best: 24.0513
2023-11-05 12:31:45,921:INFO:  Epoch 467/500:  train Loss: 18.0550   val Loss: 22.8955   time: 292.93s   best: 22.6527
2023-11-05 12:34:25,784:INFO:  Epoch 381/500:  train Loss: 19.5372   val Loss: 24.8604   time: 165.82s   best: 24.0513
2023-11-05 12:37:09,962:INFO:  Epoch 468/500:  train Loss: 18.2222   val Loss: 23.3491   time: 324.04s   best: 22.6527
2023-11-05 12:37:11,696:INFO:  Epoch 382/500:  train Loss: 19.5044   val Loss: 27.9633   time: 165.90s   best: 24.0513
2023-11-05 12:39:58,314:INFO:  Epoch 383/500:  train Loss: 19.6729   val Loss: 25.4041   time: 166.47s   best: 24.0513
2023-11-05 12:42:45,322:INFO:  Epoch 384/500:  train Loss: 19.6066   val Loss: 26.3306   time: 167.00s   best: 24.0513
2023-11-05 12:42:51,637:INFO:  Epoch 469/500:  train Loss: 18.1618   val Loss: 23.0824   time: 341.66s   best: 22.6527
2023-11-05 12:45:31,135:INFO:  Epoch 385/500:  train Loss: 19.4806   val Loss: 25.2227   time: 165.79s   best: 24.0513
2023-11-05 12:48:09,563:INFO:  Epoch 470/500:  train Loss: 17.9859   val Loss: 23.0818   time: 317.86s   best: 22.6527
2023-11-05 12:48:17,393:INFO:  Epoch 386/500:  train Loss: 19.6666   val Loss: 26.1190   time: 166.23s   best: 24.0513
2023-11-05 12:51:03,201:INFO:  Epoch 387/500:  train Loss: 19.5001   val Loss: 24.9491   time: 165.78s   best: 24.0513
2023-11-05 12:53:16,846:INFO:  Epoch 471/500:  train Loss: 18.1521   val Loss: 23.1454   time: 307.27s   best: 22.6527
2023-11-05 12:53:48,875:INFO:  Epoch 388/500:  train Loss: 20.2503   val Loss: 25.7051   time: 165.67s   best: 24.0513
2023-11-05 12:56:35,059:INFO:  Epoch 389/500:  train Loss: 19.4300   val Loss: 27.1502   time: 166.16s   best: 24.0513
2023-11-05 12:57:37,806:INFO:  Epoch 472/500:  train Loss: 18.0191   val Loss: 23.1191   time: 260.94s   best: 22.6527
2023-11-05 12:59:20,881:INFO:  Epoch 390/500:  train Loss: 19.5254   val Loss: 25.1504   time: 165.80s   best: 24.0513
2023-11-05 13:01:41,391:INFO:  Epoch 473/500:  train Loss: 17.9722   val Loss: 22.8923   time: 243.57s   best: 22.6527
2023-11-05 13:02:06,585:INFO:  Epoch 391/500:  train Loss: 19.5481   val Loss: 24.8493   time: 165.69s   best: 24.0513
2023-11-05 13:04:52,270:INFO:  Epoch 392/500:  train Loss: 19.5409   val Loss: 26.0040   time: 165.65s   best: 24.0513
2023-11-05 13:05:44,701:INFO:  Epoch 474/500:  train Loss: 18.1911   val Loss: 23.2625   time: 243.29s   best: 22.6527
2023-11-05 13:07:38,758:INFO:  Epoch 393/500:  train Loss: 19.3837   val Loss: 26.8479   time: 166.45s   best: 24.0513
2023-11-05 13:09:49,974:INFO:  Epoch 475/500:  train Loss: 17.9831   val Loss: 25.0217   time: 245.26s   best: 22.6527
2023-11-05 13:10:24,764:INFO:  Epoch 394/500:  train Loss: 19.6357   val Loss: 25.4458   time: 165.99s   best: 24.0513
2023-11-05 13:12:40,296:INFO:  Starting experiment lstm autoencoder with 0.3 dataset (0.1 dropout)
2023-11-05 13:12:40,476:INFO:  Defining the model
2023-11-05 13:12:40,539:INFO:  Reading the dataset
2023-11-05 13:13:10,717:INFO:  Epoch 395/500:  train Loss: 19.3636   val Loss: 25.3018   time: 165.92s   best: 24.0513
2023-11-05 13:14:14,107:INFO:  Epoch 476/500:  train Loss: 18.0642   val Loss: 22.8617   time: 264.13s   best: 22.6527
2023-11-05 13:15:57,879:INFO:  Epoch 396/500:  train Loss: 19.7476   val Loss: 25.5854   time: 167.14s   best: 24.0513
2023-11-05 13:18:20,205:INFO:  Epoch 477/500:  train Loss: 18.1379   val Loss: 23.1186   time: 246.09s   best: 22.6527
2023-11-05 13:18:44,479:INFO:  Epoch 397/500:  train Loss: 19.8663   val Loss: 29.3292   time: 166.57s   best: 24.0513
2023-11-05 13:21:30,891:INFO:  Epoch 398/500:  train Loss: 19.5738   val Loss: 26.7110   time: 166.29s   best: 24.0513
2023-11-05 13:22:26,341:INFO:  Epoch 478/500:  train Loss: 18.1474   val Loss: 26.6863   time: 246.11s   best: 22.6527
2023-11-05 13:24:16,460:INFO:  Epoch 399/500:  train Loss: 19.4990   val Loss: 25.0106   time: 165.54s   best: 24.0513
2023-11-05 13:26:31,928:INFO:  Epoch 479/500:  train Loss: 18.1273   val Loss: 23.3775   time: 245.56s   best: 22.6527
2023-11-05 13:27:03,130:INFO:  Epoch 400/500:  train Loss: 19.4810   val Loss: 24.5633   time: 166.40s   best: 24.0513
2023-11-05 13:29:48,919:INFO:  Epoch 401/500:  train Loss: 19.4636   val Loss: 25.7846   time: 165.76s   best: 24.0513
2023-11-05 13:30:38,236:INFO:  Epoch 480/500:  train Loss: 18.3333   val Loss: 23.9803   time: 246.30s   best: 22.6527
2023-11-05 13:32:34,777:INFO:  Epoch 402/500:  train Loss: 19.4916   val Loss: 25.3178   time: 165.85s   best: 24.0513
2023-11-05 13:34:39,289:INFO:  Epoch 481/500:  train Loss: 17.9829   val Loss: 23.3374   time: 240.99s   best: 22.6527
2023-11-05 13:35:21,734:INFO:  Epoch 403/500:  train Loss: 19.4603   val Loss: 30.2962   time: 166.91s   best: 24.0513
2023-11-05 13:38:07,621:INFO:  Epoch 404/500:  train Loss: 19.5611   val Loss: 25.8093   time: 165.86s   best: 24.0513
2023-11-05 13:39:48,749:INFO:  Epoch 482/500:  train Loss: 18.2286   val Loss: 22.8844   time: 309.44s   best: 22.6527
2023-11-05 13:40:54,569:INFO:  Epoch 405/500:  train Loss: 19.3636   val Loss: 27.1306   time: 166.92s   best: 24.0513
2023-11-05 13:43:41,207:INFO:  Epoch 406/500:  train Loss: 19.5588   val Loss: 25.2687   time: 166.60s   best: 24.0513
2023-11-05 13:43:54,990:INFO:  Epoch 483/500:  train Loss: 18.1026   val Loss: 29.0839   time: 246.23s   best: 22.6527
2023-11-05 13:46:03,597:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 13:46:03,628:INFO:  Epoch 1/500:  train Loss: 84.5425   val Loss: 79.3135   time: 166.50s   best: 79.3135
2023-11-05 13:46:28,103:INFO:  Epoch 407/500:  train Loss: 19.3418   val Loss: 25.3311   time: 166.89s   best: 24.0513
2023-11-05 13:47:58,527:INFO:  Epoch 484/500:  train Loss: 18.2150   val Loss: 23.0609   time: 243.52s   best: 22.6527
2023-11-05 13:48:48,503:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 13:48:48,532:INFO:  Epoch 2/500:  train Loss: 74.6846   val Loss: 72.0146   time: 164.87s   best: 72.0146
2023-11-05 13:49:13,994:INFO:  Epoch 408/500:  train Loss: 19.3913   val Loss: 25.5345   time: 165.86s   best: 24.0513
2023-11-05 13:51:32,683:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 13:51:32,715:INFO:  Epoch 3/500:  train Loss: 71.2269   val Loss: 70.8712   time: 164.15s   best: 70.8712
2023-11-05 13:51:59,853:INFO:  Epoch 409/500:  train Loss: 19.5264   val Loss: 25.2308   time: 165.84s   best: 24.0513
2023-11-05 13:52:04,346:INFO:  Epoch 485/500:  train Loss: 17.9991   val Loss: 22.9245   time: 245.81s   best: 22.6527
2023-11-05 13:54:16,908:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 13:54:16,938:INFO:  Epoch 4/500:  train Loss: 68.2046   val Loss: 67.4598   time: 164.18s   best: 67.4598
2023-11-05 13:54:45,488:INFO:  Epoch 410/500:  train Loss: 19.2982   val Loss: 25.4243   time: 165.63s   best: 24.0513
2023-11-05 13:56:17,758:INFO:  Epoch 486/500:  train Loss: 18.2810   val Loss: 30.3264   time: 253.41s   best: 22.6527
2023-11-05 13:57:02,365:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 13:57:02,393:INFO:  Epoch 5/500:  train Loss: 65.9406   val Loss: 64.4473   time: 165.42s   best: 64.4473
2023-11-05 13:57:32,249:INFO:  Epoch 411/500:  train Loss: 19.4168   val Loss: 25.6366   time: 166.73s   best: 24.0513
2023-11-05 13:59:47,676:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 13:59:47,707:INFO:  Epoch 6/500:  train Loss: 64.1846   val Loss: 62.4572   time: 165.28s   best: 62.4572
2023-11-05 14:00:18,202:INFO:  Epoch 412/500:  train Loss: 19.3527   val Loss: 32.1789   time: 165.94s   best: 24.0513
2023-11-05 14:00:20,977:INFO:  Epoch 487/500:  train Loss: 18.1022   val Loss: 23.1957   time: 243.20s   best: 22.6527
2023-11-05 14:02:32,061:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:02:32,102:INFO:  Epoch 7/500:  train Loss: 62.5492   val Loss: 62.0559   time: 164.34s   best: 62.0559
2023-11-05 14:03:04,324:INFO:  Epoch 413/500:  train Loss: 19.5445   val Loss: 25.0952   time: 166.09s   best: 24.0513
2023-11-05 14:04:27,692:INFO:  Epoch 488/500:  train Loss: 18.1414   val Loss: 22.7788   time: 246.71s   best: 22.6527
2023-11-05 14:05:16,547:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:05:16,577:INFO:  Epoch 8/500:  train Loss: 61.2189   val Loss: 59.9493   time: 164.43s   best: 59.9493
2023-11-05 14:05:51,331:INFO:  Epoch 414/500:  train Loss: 19.3807   val Loss: 27.2217   time: 166.98s   best: 24.0513
2023-11-05 14:08:00,719:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:08:00,750:INFO:  Epoch 9/500:  train Loss: 60.0355   val Loss: 58.8536   time: 164.13s   best: 58.8536
2023-11-05 14:08:37,654:INFO:  Epoch 415/500:  train Loss: 19.2434   val Loss: 25.1775   time: 166.30s   best: 24.0513
2023-11-05 14:08:42,156:INFO:  Epoch 489/500:  train Loss: 17.9542   val Loss: 23.6324   time: 254.46s   best: 22.6527
2023-11-05 14:10:45,090:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:10:45,122:INFO:  Epoch 10/500:  train Loss: 58.6750   val Loss: 57.1747   time: 164.34s   best: 57.1747
2023-11-05 14:11:24,352:INFO:  Epoch 416/500:  train Loss: 19.8709   val Loss: 24.4622   time: 166.62s   best: 24.0513
2023-11-05 14:12:56,364:INFO:  Epoch 490/500:  train Loss: 18.0111   val Loss: 23.2115   time: 254.20s   best: 22.6527
2023-11-05 14:13:30,667:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:13:30,695:INFO:  Epoch 11/500:  train Loss: 57.0439   val Loss: 56.5068   time: 165.54s   best: 56.5068
2023-11-05 14:14:10,666:INFO:  Epoch 417/500:  train Loss: 19.7647   val Loss: 28.8634   time: 166.29s   best: 24.0513
2023-11-05 14:16:15,161:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:16:15,192:INFO:  Epoch 12/500:  train Loss: 55.4943   val Loss: 54.3964   time: 164.45s   best: 54.3964
2023-11-05 14:16:57,106:INFO:  Epoch 418/500:  train Loss: 19.3971   val Loss: 25.5166   time: 166.40s   best: 24.0513
2023-11-05 14:16:59,938:INFO:  Epoch 491/500:  train Loss: 18.1264   val Loss: 22.6750   time: 243.57s   best: 22.6527
2023-11-05 14:18:59,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:18:59,473:INFO:  Epoch 13/500:  train Loss: 54.0342   val Loss: 54.1379   time: 164.25s   best: 54.1379
2023-11-05 14:19:44,007:INFO:  Epoch 419/500:  train Loss: 19.5932   val Loss: 24.8261   time: 166.87s   best: 24.0513
2023-11-05 14:21:06,172:INFO:  Epoch 492/500:  train Loss: 17.8901   val Loss: 23.1043   time: 246.13s   best: 22.6527
2023-11-05 14:21:43,978:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:21:44,021:INFO:  Epoch 14/500:  train Loss: 52.3197   val Loss: 52.3653   time: 164.50s   best: 52.3653
2023-11-05 14:22:29,945:INFO:  Epoch 420/500:  train Loss: 19.4132   val Loss: 25.2804   time: 165.92s   best: 24.0513
2023-11-05 14:24:29,534:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:24:29,568:INFO:  Epoch 15/500:  train Loss: 50.7649   val Loss: 49.6151   time: 165.51s   best: 49.6151
2023-11-05 14:25:11,407:INFO:  Epoch 493/500:  train Loss: 18.0666   val Loss: 23.4506   time: 245.22s   best: 22.6527
2023-11-05 14:25:16,233:INFO:  Epoch 421/500:  train Loss: 19.2606   val Loss: 24.8049   time: 166.17s   best: 24.0513
2023-11-05 14:27:14,014:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:27:14,082:INFO:  Epoch 16/500:  train Loss: 49.5895   val Loss: 48.7909   time: 164.44s   best: 48.7909
2023-11-05 14:28:03,340:INFO:  Epoch 422/500:  train Loss: 19.2188   val Loss: 24.7532   time: 167.07s   best: 24.0513
2023-11-05 14:29:13,306:INFO:  Epoch 494/500:  train Loss: 17.9909   val Loss: 23.5983   time: 241.88s   best: 22.6527
2023-11-05 14:29:59,843:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:29:59,872:INFO:  Epoch 17/500:  train Loss: 48.4598   val Loss: 48.1489   time: 165.73s   best: 48.1489
2023-11-05 14:30:50,720:INFO:  Epoch 423/500:  train Loss: 19.2201   val Loss: 28.3576   time: 167.35s   best: 24.0513
2023-11-05 14:32:44,439:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:32:44,486:INFO:  Epoch 18/500:  train Loss: 47.3109   val Loss: 47.2196   time: 164.56s   best: 47.2196
2023-11-05 14:33:14,067:INFO:  Epoch 495/500:  train Loss: 17.8657   val Loss: 23.6463   time: 240.75s   best: 22.6527
2023-11-05 14:33:36,526:INFO:  Epoch 424/500:  train Loss: 19.3213   val Loss: 25.1752   time: 165.77s   best: 24.0513
2023-11-05 14:35:29,087:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:35:29,118:INFO:  Epoch 19/500:  train Loss: 46.3435   val Loss: 46.2411   time: 164.60s   best: 46.2411
2023-11-05 14:36:22,491:INFO:  Epoch 425/500:  train Loss: 19.3841   val Loss: 25.3150   time: 165.95s   best: 24.0513
2023-11-05 14:37:43,255:INFO:  Epoch 496/500:  train Loss: 18.0600   val Loss: 23.0306   time: 269.00s   best: 22.6527
2023-11-05 14:38:13,480:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:38:13,506:INFO:  Epoch 20/500:  train Loss: 45.6021   val Loss: 45.5611   time: 164.36s   best: 45.5611
2023-11-05 14:39:09,501:INFO:  Epoch 426/500:  train Loss: 19.2185   val Loss: 25.0435   time: 166.99s   best: 24.0513
2023-11-05 14:40:58,449:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:40:58,496:INFO:  Epoch 21/500:  train Loss: 44.7351   val Loss: 45.0568   time: 164.94s   best: 45.0568
2023-11-05 14:41:43,669:INFO:  Epoch 497/500:  train Loss: 18.1385   val Loss: 23.2623   time: 240.40s   best: 22.6527
2023-11-05 14:41:56,554:INFO:  Epoch 427/500:  train Loss: 19.4674   val Loss: 25.2012   time: 167.03s   best: 24.0513
2023-11-05 14:43:43,246:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:43:43,276:INFO:  Epoch 22/500:  train Loss: 43.7407   val Loss: 43.4954   time: 164.75s   best: 43.4954
2023-11-05 14:44:42,220:INFO:  Epoch 428/500:  train Loss: 19.1907   val Loss: 25.2901   time: 165.64s   best: 24.0513
2023-11-05 14:45:46,678:INFO:  Epoch 498/500:  train Loss: 18.0070   val Loss: 23.4463   time: 243.00s   best: 22.6527
2023-11-05 14:46:27,569:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:46:27,598:INFO:  Epoch 23/500:  train Loss: 42.9801   val Loss: 43.0968   time: 164.28s   best: 43.0968
2023-11-05 14:47:29,265:INFO:  Epoch 429/500:  train Loss: 19.1642   val Loss: 25.7472   time: 166.76s   best: 24.0513
2023-11-05 14:49:13,827:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:49:13,861:INFO:  Epoch 24/500:  train Loss: 42.0555   val Loss: 42.1793   time: 166.21s   best: 42.1793
2023-11-05 14:49:53,089:INFO:  Epoch 499/500:  train Loss: 17.9963   val Loss: 23.5087   time: 246.40s   best: 22.6527
2023-11-05 14:50:15,145:INFO:  Epoch 430/500:  train Loss: 20.1212   val Loss: 29.3245   time: 165.85s   best: 24.0513
2023-11-05 14:51:59,818:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:51:59,851:INFO:  Epoch 25/500:  train Loss: 41.2230   val Loss: 41.4264   time: 165.94s   best: 41.4264
2023-11-05 14:53:02,321:INFO:  Epoch 431/500:  train Loss: 19.4144   val Loss: 24.9388   time: 167.15s   best: 24.0513
2023-11-05 14:53:58,603:INFO:  Epoch 500/500:  train Loss: 17.8887   val Loss: 23.4388   time: 245.51s   best: 22.6527
2023-11-05 14:53:58,611:INFO:  -----> Training complete in 2293m 48s   best validation loss: 22.6527
 
2023-11-05 14:54:45,866:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 14:54:45,899:INFO:  Epoch 26/500:  train Loss: 40.8017   val Loss: 40.7601   time: 165.99s   best: 40.7601
2023-11-05 14:55:48,952:INFO:  Epoch 432/500:  train Loss: 19.3457   val Loss: 24.8359   time: 166.32s   best: 24.0513
2023-11-05 14:57:30,580:INFO:  Epoch 27/500:  train Loss: 40.4095   val Loss: 41.1908   time: 164.68s   best: 40.7601
2023-11-05 14:58:34,906:INFO:  Epoch 433/500:  train Loss: 19.2528   val Loss: 25.5008   time: 165.93s   best: 24.0513
2023-11-05 15:00:15,730:INFO:  Epoch 28/500:  train Loss: 39.3521   val Loss: 42.4852   time: 164.96s   best: 40.7601
2023-11-05 15:01:21,894:INFO:  Epoch 434/500:  train Loss: 19.2235   val Loss: 31.6091   time: 166.98s   best: 24.0513
2023-11-05 15:03:00,421:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:03:00,449:INFO:  Epoch 29/500:  train Loss: 38.9701   val Loss: 40.6481   time: 164.66s   best: 40.6481
2023-11-05 15:04:08,996:INFO:  Epoch 435/500:  train Loss: 19.2518   val Loss: 25.4383   time: 167.07s   best: 24.0513
2023-11-05 15:05:45,580:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:05:45,603:INFO:  Epoch 30/500:  train Loss: 38.3988   val Loss: 39.5695   time: 165.11s   best: 39.5695
2023-11-05 15:06:54,572:INFO:  Epoch 436/500:  train Loss: 19.1563   val Loss: 27.8792   time: 165.56s   best: 24.0513
2023-11-05 15:08:30,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:08:30,484:INFO:  Epoch 31/500:  train Loss: 37.7154   val Loss: 38.6903   time: 164.83s   best: 38.6903
2023-11-05 15:09:41,269:INFO:  Epoch 437/500:  train Loss: 19.2934   val Loss: 24.8259   time: 166.36s   best: 24.0513
2023-11-05 15:11:14,684:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:11:14,908:INFO:  Epoch 32/500:  train Loss: 37.4281   val Loss: 38.5904   time: 164.20s   best: 38.5904
2023-11-05 15:12:28,333:INFO:  Epoch 438/500:  train Loss: 19.1865   val Loss: 25.1962   time: 167.03s   best: 24.0513
2023-11-05 15:14:00,087:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:14:00,122:INFO:  Epoch 33/500:  train Loss: 36.7279   val Loss: 37.7745   time: 165.17s   best: 37.7745
2023-11-05 15:15:15,942:INFO:  Epoch 439/500:  train Loss: 19.3485   val Loss: 24.5626   time: 167.60s   best: 24.0513
2023-11-05 15:16:44,548:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:16:44,578:INFO:  Epoch 34/500:  train Loss: 36.3925   val Loss: 36.9137   time: 164.41s   best: 36.9137
2023-11-05 15:18:01,789:INFO:  Epoch 440/500:  train Loss: 19.1100   val Loss: 25.1702   time: 165.82s   best: 24.0513
2023-11-05 15:19:29,635:INFO:  Epoch 35/500:  train Loss: 36.0142   val Loss: 37.3380   time: 165.06s   best: 36.9137
2023-11-05 15:20:49,279:INFO:  Epoch 441/500:  train Loss: 19.2781   val Loss: 25.2552   time: 167.48s   best: 24.0513
2023-11-05 15:22:13,990:INFO:  Epoch 36/500:  train Loss: 35.6350   val Loss: 36.9968   time: 164.35s   best: 36.9137
2023-11-05 15:23:36,097:INFO:  Epoch 442/500:  train Loss: 19.0831   val Loss: 25.9546   time: 166.79s   best: 24.0513
2023-11-05 15:24:59,579:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:24:59,612:INFO:  Epoch 37/500:  train Loss: 35.2202   val Loss: 35.7708   time: 165.57s   best: 35.7708
2023-11-05 15:26:22,176:INFO:  Epoch 443/500:  train Loss: 19.1917   val Loss: 27.0685   time: 166.04s   best: 24.0513
2023-11-05 15:27:44,228:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:27:44,260:INFO:  Epoch 38/500:  train Loss: 34.8827   val Loss: 35.0951   time: 164.61s   best: 35.0951
2023-11-05 15:29:08,932:INFO:  Epoch 444/500:  train Loss: 19.0664   val Loss: 25.8634   time: 166.72s   best: 24.0513
2023-11-05 15:30:29,880:INFO:  Epoch 39/500:  train Loss: 34.5964   val Loss: 35.1601   time: 165.62s   best: 35.0951
2023-11-05 15:31:56,305:INFO:  Epoch 445/500:  train Loss: 19.2222   val Loss: 25.3111   time: 167.35s   best: 24.0513
2023-11-05 15:33:14,425:INFO:  Epoch 40/500:  train Loss: 34.3087   val Loss: 35.1434   time: 164.54s   best: 35.0951
2023-11-05 15:34:42,890:INFO:  Epoch 446/500:  train Loss: 19.1124   val Loss: 25.0798   time: 166.56s   best: 24.0513
2023-11-05 15:35:59,841:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:35:59,878:INFO:  Epoch 41/500:  train Loss: 34.1314   val Loss: 34.2197   time: 165.40s   best: 34.2197
2023-11-05 15:37:28,820:INFO:  Epoch 447/500:  train Loss: 20.2111   val Loss: 28.2183   time: 165.92s   best: 24.0513
2023-11-05 15:38:44,698:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:38:44,733:INFO:  Epoch 42/500:  train Loss: 33.6851   val Loss: 34.1721   time: 164.81s   best: 34.1721
2023-11-05 15:40:14,608:INFO:  Epoch 448/500:  train Loss: 20.1217   val Loss: 25.3386   time: 165.75s   best: 24.0513
2023-11-05 15:41:29,399:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:41:29,439:INFO:  Epoch 43/500:  train Loss: 33.2612   val Loss: 33.9717   time: 164.66s   best: 33.9717
2023-11-05 15:43:00,426:INFO:  Epoch 449/500:  train Loss: 19.0280   val Loss: 25.2253   time: 165.80s   best: 24.0513
2023-11-05 15:44:13,657:INFO:  Epoch 44/500:  train Loss: 33.6123   val Loss: 35.0434   time: 164.22s   best: 33.9717
2023-11-05 15:45:47,395:INFO:  Epoch 450/500:  train Loss: 19.3486   val Loss: 29.7829   time: 166.94s   best: 24.0513
2023-11-05 15:46:59,188:INFO:  Epoch 45/500:  train Loss: 32.9876   val Loss: 33.9966   time: 165.52s   best: 33.9717
2023-11-05 15:48:34,985:INFO:  Epoch 451/500:  train Loss: 19.4955   val Loss: 24.6214   time: 167.30s   best: 24.0513
2023-11-05 15:49:43,836:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:49:43,880:INFO:  Epoch 46/500:  train Loss: 32.7470   val Loss: 33.3845   time: 164.63s   best: 33.3845
2023-11-05 15:51:22,333:INFO:  Epoch 452/500:  train Loss: 21.5862   val Loss: 26.0727   time: 167.32s   best: 24.0513
2023-11-05 15:52:29,418:INFO:  Epoch 47/500:  train Loss: 32.4219   val Loss: 33.9711   time: 165.52s   best: 33.3845
2023-11-05 15:54:08,317:INFO:  Epoch 453/500:  train Loss: 19.3275   val Loss: 26.5077   time: 165.95s   best: 24.0513
2023-11-05 15:55:13,969:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 15:55:14,001:INFO:  Epoch 48/500:  train Loss: 32.0820   val Loss: 32.9676   time: 164.34s   best: 32.9676
2023-11-05 15:56:54,695:INFO:  Epoch 454/500:  train Loss: 19.0416   val Loss: 25.5311   time: 166.35s   best: 24.0513
2023-11-05 15:57:58,311:INFO:  Epoch 49/500:  train Loss: 31.9698   val Loss: 33.5071   time: 164.31s   best: 32.9676
2023-11-05 15:59:40,731:INFO:  Epoch 455/500:  train Loss: 19.2657   val Loss: 25.3858   time: 166.01s   best: 24.0513
2023-11-05 16:00:42,518:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:00:42,545:INFO:  Epoch 50/500:  train Loss: 31.6292   val Loss: 32.3018   time: 164.19s   best: 32.3018
2023-11-05 16:02:26,540:INFO:  Epoch 456/500:  train Loss: 19.1006   val Loss: 24.8833   time: 165.78s   best: 24.0513
2023-11-05 16:03:28,021:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:03:28,092:INFO:  Epoch 51/500:  train Loss: 31.5703   val Loss: 32.1711   time: 165.46s   best: 32.1711
2023-11-05 16:05:12,557:INFO:  Epoch 457/500:  train Loss: 19.1005   val Loss: 25.2710   time: 166.01s   best: 24.0513
2023-11-05 16:06:12,406:INFO:  Epoch 52/500:  train Loss: 31.1568   val Loss: 32.2739   time: 164.30s   best: 32.1711
2023-11-05 16:07:58,229:INFO:  Epoch 458/500:  train Loss: 19.0323   val Loss: 26.4416   time: 165.64s   best: 24.0513
2023-11-05 16:08:56,842:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:08:56,870:INFO:  Epoch 53/500:  train Loss: 31.0445   val Loss: 31.9144   time: 164.41s   best: 31.9144
2023-11-05 16:10:44,241:INFO:  Epoch 459/500:  train Loss: 19.2833   val Loss: 24.8480   time: 166.00s   best: 24.0513
2023-11-05 16:11:40,959:INFO:  Epoch 54/500:  train Loss: 30.8430   val Loss: 32.1155   time: 164.08s   best: 31.9144
2023-11-05 16:13:30,068:INFO:  Epoch 460/500:  train Loss: 19.2168   val Loss: 26.1103   time: 165.80s   best: 24.0513
2023-11-05 16:14:25,388:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:14:25,415:INFO:  Epoch 55/500:  train Loss: 30.6472   val Loss: 31.5959   time: 164.39s   best: 31.5959
2023-11-05 16:16:15,996:INFO:  Epoch 461/500:  train Loss: 19.1400   val Loss: 24.9601   time: 165.90s   best: 24.0513
2023-11-05 16:17:11,251:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:17:11,279:INFO:  Epoch 56/500:  train Loss: 30.5531   val Loss: 31.3385   time: 165.82s   best: 31.3385
2023-11-05 16:19:03,252:INFO:  Epoch 462/500:  train Loss: 19.0208   val Loss: 25.9261   time: 166.99s   best: 24.0513
2023-11-05 16:19:56,353:INFO:  Epoch 57/500:  train Loss: 30.4775   val Loss: 31.7656   time: 165.07s   best: 31.3385
2023-11-05 16:21:49,117:INFO:  Epoch 463/500:  train Loss: 19.1969   val Loss: 25.1069   time: 165.83s   best: 24.0513
2023-11-05 16:22:40,933:INFO:  Epoch 58/500:  train Loss: 30.3453   val Loss: 31.5111   time: 164.55s   best: 31.3385
2023-11-05 16:24:34,804:INFO:  Epoch 464/500:  train Loss: 19.2940   val Loss: 25.0979   time: 165.67s   best: 24.0513
2023-11-05 16:25:26,394:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:25:26,419:INFO:  Epoch 59/500:  train Loss: 30.0061   val Loss: 31.1803   time: 165.42s   best: 31.1803
2023-11-05 16:27:20,765:INFO:  Epoch 465/500:  train Loss: 19.0384   val Loss: 25.3823   time: 165.94s   best: 24.0513
2023-11-05 16:28:12,040:INFO:  Epoch 60/500:  train Loss: 29.8999   val Loss: 31.7537   time: 165.61s   best: 31.1803
2023-11-05 16:30:06,402:INFO:  Epoch 466/500:  train Loss: 19.3805   val Loss: 24.4791   time: 165.60s   best: 24.0513
2023-11-05 16:30:56,528:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:30:56,599:INFO:  Epoch 61/500:  train Loss: 29.7801   val Loss: 30.9274   time: 164.45s   best: 30.9274
2023-11-05 16:32:52,628:INFO:  Epoch 467/500:  train Loss: 18.9229   val Loss: 25.2807   time: 166.20s   best: 24.0513
2023-11-05 16:33:41,928:INFO:  Epoch 62/500:  train Loss: 29.5544   val Loss: 30.9634   time: 165.32s   best: 30.9274
2023-11-05 16:35:39,690:INFO:  Epoch 468/500:  train Loss: 19.0940   val Loss: 25.3205   time: 167.03s   best: 24.0513
2023-11-05 16:36:26,340:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:36:26,369:INFO:  Epoch 63/500:  train Loss: 29.4641   val Loss: 30.8860   time: 164.37s   best: 30.8860
2023-11-05 16:38:25,643:INFO:  Epoch 469/500:  train Loss: 19.1787   val Loss: 25.4264   time: 165.93s   best: 24.0513
2023-11-05 16:39:11,001:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:39:11,032:INFO:  Epoch 64/500:  train Loss: 29.3208   val Loss: 30.4384   time: 164.61s   best: 30.4384
2023-11-05 16:41:11,549:INFO:  Epoch 470/500:  train Loss: 18.9377   val Loss: 25.3851   time: 165.87s   best: 24.0513
2023-11-05 16:41:56,753:INFO:  Epoch 65/500:  train Loss: 29.1665   val Loss: 30.5950   time: 165.70s   best: 30.4384
2023-11-05 16:43:58,615:INFO:  Epoch 471/500:  train Loss: 18.9661   val Loss: 25.9023   time: 167.05s   best: 24.0513
2023-11-05 16:44:41,620:INFO:  Epoch 66/500:  train Loss: 28.9953   val Loss: 31.2105   time: 164.79s   best: 30.4384
2023-11-05 16:46:44,287:INFO:  Epoch 472/500:  train Loss: 19.2151   val Loss: 31.0260   time: 165.62s   best: 24.0513
2023-11-05 16:47:25,880:INFO:  Epoch 67/500:  train Loss: 29.0309   val Loss: 30.4407   time: 164.24s   best: 30.4384
2023-11-05 16:49:30,317:INFO:  Epoch 473/500:  train Loss: 19.1361   val Loss: 24.5796   time: 166.00s   best: 24.0513
2023-11-05 16:50:10,121:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:50:10,155:INFO:  Epoch 68/500:  train Loss: 28.6206   val Loss: 30.2121   time: 164.23s   best: 30.2121
2023-11-05 16:52:17,287:INFO:  Epoch 474/500:  train Loss: 18.9094   val Loss: 26.6273   time: 166.94s   best: 24.0513
2023-11-05 16:52:55,495:INFO:  Epoch 69/500:  train Loss: 28.6982   val Loss: 30.2757   time: 165.33s   best: 30.2121
2023-11-05 16:55:04,135:INFO:  Epoch 475/500:  train Loss: 18.9181   val Loss: 25.6392   time: 166.83s   best: 24.0513
2023-11-05 16:55:41,172:INFO:  Epoch 70/500:  train Loss: 28.5847   val Loss: 30.5408   time: 165.65s   best: 30.2121
2023-11-05 16:57:49,887:INFO:  Epoch 476/500:  train Loss: 18.9597   val Loss: 26.1849   time: 165.72s   best: 24.0513
2023-11-05 16:58:25,805:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 16:58:25,832:INFO:  Epoch 71/500:  train Loss: 28.4659   val Loss: 30.0057   time: 164.60s   best: 30.0057
2023-11-05 17:00:35,807:INFO:  Epoch 477/500:  train Loss: 19.4046   val Loss: 25.0854   time: 165.89s   best: 24.0513
2023-11-05 17:01:10,968:INFO:  Epoch 72/500:  train Loss: 28.1084   val Loss: 30.6274   time: 165.12s   best: 30.0057
2023-11-05 17:03:21,847:INFO:  Epoch 478/500:  train Loss: 18.8545   val Loss: 26.3639   time: 166.01s   best: 24.0513
2023-11-05 17:03:55,404:INFO:  Epoch 73/500:  train Loss: 28.1927   val Loss: 30.1465   time: 164.39s   best: 30.0057
2023-11-05 17:06:07,986:INFO:  Epoch 479/500:  train Loss: 19.0338   val Loss: 25.8850   time: 166.12s   best: 24.0513
2023-11-05 17:06:40,145:INFO:  Epoch 74/500:  train Loss: 28.2338   val Loss: 30.0538   time: 164.70s   best: 30.0057
2023-11-05 17:08:55,326:INFO:  Epoch 480/500:  train Loss: 19.7357   val Loss: 25.3804   time: 167.30s   best: 24.0513
2023-11-05 17:09:25,191:INFO:  Epoch 75/500:  train Loss: 27.9600   val Loss: 30.3158   time: 165.01s   best: 30.0057
2023-11-05 17:11:41,252:INFO:  Epoch 481/500:  train Loss: 19.0536   val Loss: 25.3827   time: 165.90s   best: 24.0513
2023-11-05 17:12:09,906:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 17:12:10,670:INFO:  Epoch 76/500:  train Loss: 27.7543   val Loss: 29.5094   time: 164.61s   best: 29.5094
2023-11-05 17:14:28,323:INFO:  Epoch 482/500:  train Loss: 18.9732   val Loss: 25.0124   time: 166.83s   best: 24.0513
2023-11-05 17:14:55,234:INFO:  Epoch 77/500:  train Loss: 27.6081   val Loss: 29.7909   time: 164.56s   best: 29.5094
2023-11-05 17:17:15,483:INFO:  Epoch 483/500:  train Loss: 19.2812   val Loss: 25.2435   time: 167.13s   best: 24.0513
2023-11-05 17:17:39,856:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 17:17:39,890:INFO:  Epoch 78/500:  train Loss: 27.4870   val Loss: 29.4410   time: 164.60s   best: 29.4410
2023-11-05 17:20:02,006:INFO:  Epoch 484/500:  train Loss: 18.9250   val Loss: 26.1019   time: 166.48s   best: 24.0513
2023-11-05 17:20:25,414:INFO:  Epoch 79/500:  train Loss: 27.5241   val Loss: 29.4908   time: 165.52s   best: 29.4410
2023-11-05 17:22:49,756:INFO:  Epoch 485/500:  train Loss: 18.9935   val Loss: 24.8745   time: 167.74s   best: 24.0513
2023-11-05 17:23:10,558:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 17:23:10,583:INFO:  Epoch 80/500:  train Loss: 27.2999   val Loss: 28.9695   time: 164.84s   best: 28.9695
2023-11-05 17:25:36,252:INFO:  Epoch 486/500:  train Loss: 18.9535   val Loss: 24.9128   time: 166.46s   best: 24.0513
2023-11-05 17:25:56,354:INFO:  Epoch 81/500:  train Loss: 27.2022   val Loss: 30.0844   time: 165.75s   best: 28.9695
2023-11-05 17:28:23,335:INFO:  Epoch 487/500:  train Loss: 19.3622   val Loss: 25.0878   time: 167.04s   best: 24.0513
2023-11-05 17:28:40,626:INFO:  Epoch 82/500:  train Loss: 27.2642   val Loss: 29.0057   time: 164.24s   best: 28.9695
2023-11-05 17:31:08,977:INFO:  Epoch 488/500:  train Loss: 18.8788   val Loss: 25.8889   time: 165.63s   best: 24.0513
2023-11-05 17:31:25,103:INFO:  Epoch 83/500:  train Loss: 27.2837   val Loss: 29.0954   time: 164.47s   best: 28.9695
2023-11-05 17:33:55,486:INFO:  Epoch 489/500:  train Loss: 18.8760   val Loss: 25.2590   time: 166.48s   best: 24.0513
2023-11-05 17:34:10,954:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 17:34:10,981:INFO:  Epoch 84/500:  train Loss: 26.9545   val Loss: 28.5275   time: 165.82s   best: 28.5275
2023-11-05 17:36:41,958:INFO:  Epoch 490/500:  train Loss: 19.0214   val Loss: 25.4450   time: 166.46s   best: 24.0513
2023-11-05 17:36:55,500:INFO:  Epoch 85/500:  train Loss: 26.8028   val Loss: 28.6928   time: 164.51s   best: 28.5275
2023-11-05 17:39:29,840:INFO:  Epoch 491/500:  train Loss: 19.1687   val Loss: 25.7562   time: 167.69s   best: 24.0513
2023-11-05 17:39:39,758:INFO:  Epoch 86/500:  train Loss: 26.8099   val Loss: 29.1838   time: 164.25s   best: 28.5275
2023-11-05 17:42:16,178:INFO:  Epoch 492/500:  train Loss: 18.8578   val Loss: 25.6257   time: 166.02s   best: 24.0513
2023-11-05 17:42:24,611:INFO:  Epoch 87/500:  train Loss: 26.7240   val Loss: 28.7821   time: 164.85s   best: 28.5275
2023-11-05 17:45:03,074:INFO:  Epoch 493/500:  train Loss: 18.9271   val Loss: 25.9620   time: 166.86s   best: 24.0513
2023-11-05 17:45:09,200:INFO:  Epoch 88/500:  train Loss: 26.7202   val Loss: 29.1938   time: 164.58s   best: 28.5275
2023-11-05 17:47:49,666:INFO:  Epoch 494/500:  train Loss: 19.2238   val Loss: 24.5162   time: 166.56s   best: 24.0513
2023-11-05 17:47:54,332:INFO:  Epoch 89/500:  train Loss: 26.6638   val Loss: 28.7367   time: 165.11s   best: 28.5275
2023-11-05 17:50:36,955:INFO:  Epoch 495/500:  train Loss: 18.8071   val Loss: 28.3448   time: 167.28s   best: 24.0513
2023-11-05 17:50:39,519:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 17:50:39,679:INFO:  Epoch 90/500:  train Loss: 26.4111   val Loss: 28.5068   time: 165.10s   best: 28.5068
2023-11-05 17:53:23,763:INFO:  Epoch 496/500:  train Loss: 19.0174   val Loss: 29.9210   time: 166.78s   best: 24.0513
2023-11-05 17:53:25,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 17:53:25,525:INFO:  Epoch 91/500:  train Loss: 26.3919   val Loss: 28.3791   time: 165.73s   best: 28.3791
2023-11-05 17:56:10,183:INFO:  Epoch 92/500:  train Loss: 26.2121   val Loss: 28.6021   time: 164.65s   best: 28.3791
2023-11-05 17:56:10,807:INFO:  Epoch 497/500:  train Loss: 18.9604   val Loss: 25.1599   time: 167.01s   best: 24.0513
2023-11-05 17:58:55,668:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 17:58:55,700:INFO:  Epoch 93/500:  train Loss: 26.1685   val Loss: 28.1666   time: 165.46s   best: 28.1666
2023-11-05 17:58:58,065:INFO:  Epoch 498/500:  train Loss: 18.8256   val Loss: 25.1446   time: 167.21s   best: 24.0513
2023-11-05 18:01:40,335:INFO:  Epoch 94/500:  train Loss: 26.1392   val Loss: 28.9134   time: 164.62s   best: 28.1666
2023-11-05 18:01:44,179:INFO:  Epoch 499/500:  train Loss: 19.0881   val Loss: 25.0776   time: 166.11s   best: 24.0513
2023-11-05 18:04:25,859:INFO:  Epoch 95/500:  train Loss: 25.9606   val Loss: 29.1272   time: 165.50s   best: 28.1666
2023-11-05 18:04:31,138:INFO:  Epoch 500/500:  train Loss: 18.8779   val Loss: 25.1458   time: 166.96s   best: 24.0513
2023-11-05 18:04:31,168:INFO:  -----> Training complete in 1397m 8s   best validation loss: 24.0513
 
2023-11-05 18:07:10,271:INFO:  Epoch 96/500:  train Loss: 25.9374   val Loss: 28.7374   time: 164.40s   best: 28.1666
2023-11-05 18:09:54,600:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 18:09:54,618:INFO:  Epoch 97/500:  train Loss: 25.8904   val Loss: 27.9588   time: 164.31s   best: 27.9588
2023-11-05 18:12:39,327:INFO:  Epoch 98/500:  train Loss: 26.0788   val Loss: 28.5310   time: 164.70s   best: 27.9588
2023-11-05 18:15:24,952:INFO:  Epoch 99/500:  train Loss: 25.7897   val Loss: 28.3861   time: 165.62s   best: 27.9588
2023-11-05 18:18:09,322:INFO:  Epoch 100/500:  train Loss: 25.6124   val Loss: 28.5005   time: 164.35s   best: 27.9588
2023-11-05 18:20:54,186:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 18:20:54,205:INFO:  Epoch 101/500:  train Loss: 25.6321   val Loss: 27.6617   time: 164.85s   best: 27.6617
2023-11-05 18:23:38,805:INFO:  Epoch 102/500:  train Loss: 25.4420   val Loss: 32.0008   time: 164.59s   best: 27.6617
2023-11-05 18:26:23,950:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 18:26:23,970:INFO:  Epoch 103/500:  train Loss: 26.0464   val Loss: 27.6129   time: 165.13s   best: 27.6129
2023-11-05 18:29:09,261:INFO:  Epoch 104/500:  train Loss: 25.6162   val Loss: 28.3529   time: 165.28s   best: 27.6129
2023-11-05 18:31:54,298:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 18:31:54,318:INFO:  Epoch 105/500:  train Loss: 25.4576   val Loss: 27.5611   time: 165.02s   best: 27.5611
2023-11-05 18:34:39,075:INFO:  Epoch 106/500:  train Loss: 25.2759   val Loss: 27.6491   time: 164.74s   best: 27.5611
2023-11-05 18:37:23,567:INFO:  Epoch 107/500:  train Loss: 25.6227   val Loss: 27.6277   time: 164.48s   best: 27.5611
2023-11-05 18:40:07,654:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 18:40:07,673:INFO:  Epoch 108/500:  train Loss: 25.1362   val Loss: 27.4112   time: 164.08s   best: 27.4112
2023-11-05 18:42:52,533:INFO:  Epoch 109/500:  train Loss: 25.1445   val Loss: 28.0568   time: 164.86s   best: 27.4112
2023-11-05 18:45:37,642:INFO:  Epoch 110/500:  train Loss: 24.9487   val Loss: 27.6850   time: 165.10s   best: 27.4112
2023-11-05 18:48:22,309:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 18:48:22,328:INFO:  Epoch 111/500:  train Loss: 25.1230   val Loss: 27.3085   time: 164.65s   best: 27.3085
2023-11-05 18:51:06,823:INFO:  Epoch 112/500:  train Loss: 24.9912   val Loss: 27.3245   time: 164.48s   best: 27.3085
2023-11-05 18:53:51,199:INFO:  Epoch 113/500:  train Loss: 24.8863   val Loss: 27.3114   time: 164.36s   best: 27.3085
2023-11-05 18:56:35,540:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 18:56:35,686:INFO:  Epoch 114/500:  train Loss: 24.8670   val Loss: 27.2259   time: 164.31s   best: 27.2259
2023-11-05 18:59:20,086:INFO:  Epoch 115/500:  train Loss: 24.7140   val Loss: 27.3917   time: 164.38s   best: 27.2259
2023-11-05 19:02:05,481:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 19:02:05,500:INFO:  Epoch 116/500:  train Loss: 24.7895   val Loss: 27.0587   time: 165.39s   best: 27.0587
2023-11-05 19:04:49,880:INFO:  Epoch 117/500:  train Loss: 24.5628   val Loss: 27.8698   time: 164.37s   best: 27.0587
2023-11-05 19:07:34,531:INFO:  Epoch 118/500:  train Loss: 24.5532   val Loss: 27.3406   time: 164.65s   best: 27.0587
2023-11-05 19:10:18,958:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 19:10:18,978:INFO:  Epoch 119/500:  train Loss: 24.5569   val Loss: 27.0369   time: 164.41s   best: 27.0369
2023-11-05 19:13:03,328:INFO:  Epoch 120/500:  train Loss: 24.3698   val Loss: 27.4510   time: 164.34s   best: 27.0369
2023-11-05 19:15:47,815:INFO:  Epoch 121/500:  train Loss: 24.4802   val Loss: 27.2935   time: 164.48s   best: 27.0369
2023-11-05 19:18:32,548:INFO:  Epoch 122/500:  train Loss: 24.2474   val Loss: 27.2497   time: 164.72s   best: 27.0369
2023-11-05 19:21:17,211:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 19:21:17,231:INFO:  Epoch 123/500:  train Loss: 24.4106   val Loss: 26.5970   time: 164.64s   best: 26.5970
2023-11-05 19:24:01,392:INFO:  Epoch 124/500:  train Loss: 24.3904   val Loss: 26.6259   time: 164.16s   best: 26.5970
2023-11-05 19:26:45,662:INFO:  Epoch 125/500:  train Loss: 24.2609   val Loss: 26.7240   time: 164.25s   best: 26.5970
2023-11-05 19:29:29,937:INFO:  Epoch 126/500:  train Loss: 24.1809   val Loss: 26.8253   time: 164.27s   best: 26.5970
2023-11-05 19:32:15,056:INFO:  Epoch 127/500:  train Loss: 24.1677   val Loss: 27.0579   time: 165.11s   best: 26.5970
2023-11-05 19:34:59,346:INFO:  Epoch 128/500:  train Loss: 23.9696   val Loss: 26.9522   time: 164.28s   best: 26.5970
2023-11-05 19:37:43,909:INFO:  Epoch 129/500:  train Loss: 24.1059   val Loss: 27.0078   time: 164.53s   best: 26.5970
2023-11-05 19:40:30,258:INFO:  Epoch 130/500:  train Loss: 23.9808   val Loss: 26.7928   time: 166.34s   best: 26.5970
2023-11-05 19:43:14,592:INFO:  Epoch 131/500:  train Loss: 23.9661   val Loss: 26.6354   time: 164.32s   best: 26.5970
2023-11-05 19:45:59,792:INFO:  Epoch 132/500:  train Loss: 23.8197   val Loss: 26.8781   time: 165.20s   best: 26.5970
2023-11-05 19:48:44,092:INFO:  Epoch 133/500:  train Loss: 23.9246   val Loss: 27.3083   time: 164.30s   best: 26.5970
2023-11-05 19:51:29,681:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 19:51:29,701:INFO:  Epoch 134/500:  train Loss: 23.7817   val Loss: 26.5704   time: 165.57s   best: 26.5704
2023-11-05 19:54:14,388:INFO:  Epoch 135/500:  train Loss: 23.6598   val Loss: 26.6321   time: 164.69s   best: 26.5704
2023-11-05 19:56:58,718:INFO:  Epoch 136/500:  train Loss: 23.8326   val Loss: 27.6362   time: 164.33s   best: 26.5704
2023-11-05 19:59:43,233:INFO:  Epoch 137/500:  train Loss: 24.2200   val Loss: 27.6299   time: 164.51s   best: 26.5704
2023-11-05 20:02:27,271:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 20:02:27,290:INFO:  Epoch 138/500:  train Loss: 23.8492   val Loss: 26.5074   time: 164.01s   best: 26.5074
2023-11-05 20:05:11,384:INFO:  Epoch 139/500:  train Loss: 23.7665   val Loss: 26.6400   time: 164.08s   best: 26.5074
2023-11-05 20:07:55,914:INFO:  Epoch 140/500:  train Loss: 23.5306   val Loss: 26.9559   time: 164.51s   best: 26.5074
2023-11-05 20:10:40,212:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 20:10:40,233:INFO:  Epoch 141/500:  train Loss: 23.7756   val Loss: 26.4244   time: 164.28s   best: 26.4244
2023-11-05 20:13:25,551:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 20:13:25,571:INFO:  Epoch 142/500:  train Loss: 23.3940   val Loss: 26.3391   time: 165.30s   best: 26.3391
2023-11-05 20:16:09,748:INFO:  Epoch 143/500:  train Loss: 23.3650   val Loss: 26.6139   time: 164.16s   best: 26.3391
2023-11-05 20:18:54,727:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 20:18:54,756:INFO:  Epoch 144/500:  train Loss: 23.5015   val Loss: 26.2118   time: 164.96s   best: 26.2118
2023-11-05 20:21:40,066:INFO:  Epoch 145/500:  train Loss: 23.3778   val Loss: 26.3487   time: 165.30s   best: 26.2118
2023-11-05 20:24:24,704:INFO:  Epoch 146/500:  train Loss: 23.4296   val Loss: 26.3364   time: 164.63s   best: 26.2118
2023-11-05 20:27:08,926:INFO:  Epoch 147/500:  train Loss: 23.2911   val Loss: 26.3175   time: 164.22s   best: 26.2118
2023-11-05 20:29:52,779:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 20:29:52,799:INFO:  Epoch 148/500:  train Loss: 23.1990   val Loss: 26.1174   time: 163.83s   best: 26.1174
2023-11-05 20:32:36,788:INFO:  Epoch 149/500:  train Loss: 23.3541   val Loss: 26.1577   time: 163.99s   best: 26.1174
2023-11-05 20:35:21,092:INFO:  Epoch 150/500:  train Loss: 23.2100   val Loss: 26.2304   time: 164.30s   best: 26.1174
2023-11-05 20:38:05,054:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 20:38:05,074:INFO:  Epoch 151/500:  train Loss: 23.1289   val Loss: 26.0599   time: 163.95s   best: 26.0599
2023-11-05 20:40:50,463:INFO:  Epoch 152/500:  train Loss: 23.0319   val Loss: 26.2015   time: 165.39s   best: 26.0599
2023-11-05 20:43:34,116:INFO:  Epoch 153/500:  train Loss: 22.9925   val Loss: 26.2498   time: 163.65s   best: 26.0599
2023-11-05 20:46:18,871:INFO:  Epoch 154/500:  train Loss: 23.1117   val Loss: 26.2748   time: 164.75s   best: 26.0599
2023-11-05 20:49:02,843:INFO:  Epoch 155/500:  train Loss: 23.6074   val Loss: 26.2567   time: 163.97s   best: 26.0599
2023-11-05 20:51:46,776:INFO:  Epoch 156/500:  train Loss: 23.0656   val Loss: 26.4834   time: 163.93s   best: 26.0599
2023-11-05 20:54:31,233:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 20:54:31,253:INFO:  Epoch 157/500:  train Loss: 23.0745   val Loss: 25.9239   time: 164.45s   best: 25.9239
2023-11-05 20:57:15,657:INFO:  Epoch 158/500:  train Loss: 22.8925   val Loss: 26.3469   time: 164.40s   best: 25.9239
2023-11-05 21:00:00,786:INFO:  Epoch 159/500:  train Loss: 22.9037   val Loss: 26.0343   time: 165.13s   best: 25.9239
2023-11-05 21:02:45,724:INFO:  Epoch 160/500:  train Loss: 22.7821   val Loss: 26.3327   time: 164.94s   best: 25.9239
2023-11-05 21:05:29,910:INFO:  Epoch 161/500:  train Loss: 22.9607   val Loss: 26.2969   time: 164.19s   best: 25.9239
2023-11-05 21:08:14,653:INFO:  Epoch 162/500:  train Loss: 22.9032   val Loss: 26.3782   time: 164.74s   best: 25.9239
2023-11-05 21:10:58,683:INFO:  Epoch 163/500:  train Loss: 23.2386   val Loss: 26.1551   time: 164.02s   best: 25.9239
2023-11-05 21:13:42,677:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 21:13:42,696:INFO:  Epoch 164/500:  train Loss: 22.7223   val Loss: 25.9000   time: 163.99s   best: 25.9000
2023-11-05 21:16:26,669:INFO:  Epoch 165/500:  train Loss: 22.6907   val Loss: 26.3394   time: 163.95s   best: 25.9000
2023-11-05 21:19:10,519:INFO:  Epoch 166/500:  train Loss: 22.8227   val Loss: 26.0783   time: 163.84s   best: 25.9000
2023-11-05 21:21:55,557:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 21:21:55,577:INFO:  Epoch 167/500:  train Loss: 22.5702   val Loss: 25.6705   time: 165.03s   best: 25.6705
2023-11-05 21:24:39,789:INFO:  Epoch 168/500:  train Loss: 22.7668   val Loss: 26.1376   time: 164.20s   best: 25.6705
2023-11-05 21:27:23,948:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 21:27:23,968:INFO:  Epoch 169/500:  train Loss: 22.5973   val Loss: 25.6700   time: 164.14s   best: 25.6700
2023-11-05 21:30:09,299:INFO:  Epoch 170/500:  train Loss: 22.5574   val Loss: 28.5991   time: 165.32s   best: 25.6700
2023-11-05 21:32:53,114:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 21:32:53,142:INFO:  Epoch 171/500:  train Loss: 22.6037   val Loss: 25.6056   time: 163.81s   best: 25.6056
2023-11-05 21:35:37,865:INFO:  Epoch 172/500:  train Loss: 22.4245   val Loss: 25.8358   time: 164.72s   best: 25.6056
2023-11-05 21:38:22,762:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 21:38:22,782:INFO:  Epoch 173/500:  train Loss: 22.5908   val Loss: 25.4977   time: 164.88s   best: 25.4977
2023-11-05 21:41:07,059:INFO:  Epoch 174/500:  train Loss: 22.5693   val Loss: 25.9321   time: 164.27s   best: 25.4977
2023-11-05 21:43:50,979:INFO:  Epoch 175/500:  train Loss: 22.4265   val Loss: 25.5718   time: 163.91s   best: 25.4977
2023-11-05 21:46:35,837:INFO:  Epoch 176/500:  train Loss: 22.5665   val Loss: 25.5700   time: 164.85s   best: 25.4977
2023-11-05 21:49:19,969:INFO:  Epoch 177/500:  train Loss: 22.5142   val Loss: 25.5057   time: 164.12s   best: 25.4977
2023-11-05 21:52:04,166:INFO:  Epoch 178/500:  train Loss: 22.2634   val Loss: 25.7575   time: 164.18s   best: 25.4977
2023-11-05 21:54:48,725:INFO:  Epoch 179/500:  train Loss: 22.2767   val Loss: 25.8332   time: 164.55s   best: 25.4977
2023-11-05 21:57:33,931:INFO:  Epoch 180/500:  train Loss: 22.2057   val Loss: 25.6858   time: 165.19s   best: 25.4977
2023-11-05 22:00:19,038:INFO:  Epoch 181/500:  train Loss: 22.1988   val Loss: 25.9420   time: 165.11s   best: 25.4977
2023-11-05 22:03:03,419:INFO:  Epoch 182/500:  train Loss: 22.9039   val Loss: 25.7244   time: 164.38s   best: 25.4977
2023-11-05 22:05:47,188:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 22:05:47,208:INFO:  Epoch 183/500:  train Loss: 22.1691   val Loss: 25.4438   time: 163.76s   best: 25.4438
2023-11-05 22:08:32,122:INFO:  Epoch 184/500:  train Loss: 22.0995   val Loss: 25.9823   time: 164.91s   best: 25.4438
2023-11-05 22:11:16,300:INFO:  Epoch 185/500:  train Loss: 22.2587   val Loss: 25.7384   time: 164.16s   best: 25.4438
2023-11-05 22:14:00,983:INFO:  Epoch 186/500:  train Loss: 22.0994   val Loss: 25.8191   time: 164.67s   best: 25.4438
2023-11-05 22:16:44,964:INFO:  Epoch 187/500:  train Loss: 22.0079   val Loss: 25.5404   time: 163.98s   best: 25.4438
2023-11-05 22:19:29,676:INFO:  Epoch 188/500:  train Loss: 22.0791   val Loss: 27.3906   time: 164.71s   best: 25.4438
2023-11-05 22:22:13,640:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 22:22:13,660:INFO:  Epoch 189/500:  train Loss: 22.2066   val Loss: 25.4369   time: 163.95s   best: 25.4369
2023-11-05 22:24:57,725:INFO:  Epoch 190/500:  train Loss: 22.0468   val Loss: 25.5918   time: 164.06s   best: 25.4369
2023-11-05 22:27:43,028:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 22:27:43,048:INFO:  Epoch 191/500:  train Loss: 22.0374   val Loss: 25.3885   time: 165.30s   best: 25.3885
2023-11-05 22:30:27,066:INFO:  Epoch 192/500:  train Loss: 21.8983   val Loss: 25.6415   time: 164.02s   best: 25.3885
2023-11-05 22:33:11,220:INFO:  Epoch 193/500:  train Loss: 22.0307   val Loss: 25.5386   time: 164.14s   best: 25.3885
2023-11-05 22:35:56,117:INFO:  Epoch 194/500:  train Loss: 21.8398   val Loss: 25.8098   time: 164.90s   best: 25.3885
2023-11-05 22:38:40,345:INFO:  Epoch 195/500:  train Loss: 21.8286   val Loss: 25.7957   time: 164.22s   best: 25.3885
2023-11-05 22:41:24,535:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 22:41:24,555:INFO:  Epoch 196/500:  train Loss: 21.8150   val Loss: 25.3862   time: 164.16s   best: 25.3862
2023-11-05 22:44:08,439:INFO:  Epoch 197/500:  train Loss: 21.8394   val Loss: 26.3646   time: 163.87s   best: 25.3862
2023-11-05 22:46:52,496:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 22:46:52,516:INFO:  Epoch 198/500:  train Loss: 21.7728   val Loss: 25.1339   time: 164.03s   best: 25.1339
2023-11-05 22:49:36,415:INFO:  Epoch 199/500:  train Loss: 22.1178   val Loss: 25.4804   time: 163.89s   best: 25.1339
2023-11-05 22:52:20,258:INFO:  Epoch 200/500:  train Loss: 21.7667   val Loss: 25.4278   time: 163.83s   best: 25.1339
2023-11-05 22:55:05,379:INFO:  Epoch 201/500:  train Loss: 21.8003   val Loss: 26.7773   time: 165.11s   best: 25.1339
2023-11-05 22:57:49,250:INFO:  Epoch 202/500:  train Loss: 21.6224   val Loss: 25.6158   time: 163.86s   best: 25.1339
2023-11-05 23:00:34,090:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 23:00:34,110:INFO:  Epoch 203/500:  train Loss: 21.8623   val Loss: 24.9958   time: 164.82s   best: 24.9958
2023-11-05 23:03:17,943:INFO:  Epoch 204/500:  train Loss: 22.1516   val Loss: 25.2405   time: 163.83s   best: 24.9958
2023-11-05 23:06:01,667:INFO:  Epoch 205/500:  train Loss: 21.6536   val Loss: 25.0649   time: 163.72s   best: 24.9958
2023-11-05 23:08:45,549:INFO:  Epoch 206/500:  train Loss: 21.8234   val Loss: 25.1601   time: 163.87s   best: 24.9958
2023-11-05 23:11:29,253:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-05 23:11:29,275:INFO:  Epoch 207/500:  train Loss: 21.8136   val Loss: 24.8527   time: 163.69s   best: 24.8527
2023-11-05 23:14:13,591:INFO:  Epoch 208/500:  train Loss: 21.9730   val Loss: 25.0979   time: 164.31s   best: 24.8527
2023-11-05 23:16:58,617:INFO:  Epoch 209/500:  train Loss: 21.6083   val Loss: 25.3037   time: 165.03s   best: 24.8527
2023-11-05 23:19:42,568:INFO:  Epoch 210/500:  train Loss: 21.7027   val Loss: 25.1739   time: 163.94s   best: 24.8527
2023-11-05 23:22:27,364:INFO:  Epoch 211/500:  train Loss: 21.8474   val Loss: 25.3232   time: 164.78s   best: 24.8527
2023-11-05 23:25:11,205:INFO:  Epoch 212/500:  train Loss: 21.5805   val Loss: 25.3104   time: 163.83s   best: 24.8527
2023-11-05 23:27:54,832:INFO:  Epoch 213/500:  train Loss: 21.5119   val Loss: 25.2312   time: 163.63s   best: 24.8527
2023-11-05 23:30:39,734:INFO:  Epoch 214/500:  train Loss: 21.4831   val Loss: 25.2693   time: 164.89s   best: 24.8527
2023-11-05 23:33:24,131:INFO:  Epoch 215/500:  train Loss: 21.6088   val Loss: 25.8393   time: 164.40s   best: 24.8527
2023-11-05 23:36:07,928:INFO:  Epoch 216/500:  train Loss: 21.4186   val Loss: 25.5365   time: 163.77s   best: 24.8527
2023-11-05 23:38:53,046:INFO:  Epoch 217/500:  train Loss: 21.4042   val Loss: 25.1832   time: 165.09s   best: 24.8527
2023-11-05 23:41:37,362:INFO:  Epoch 218/500:  train Loss: 21.5623   val Loss: 25.2585   time: 164.30s   best: 24.8527
2023-11-05 23:44:22,476:INFO:  Epoch 219/500:  train Loss: 21.3740   val Loss: 25.0847   time: 165.11s   best: 24.8527
2023-11-05 23:47:06,222:INFO:  Epoch 220/500:  train Loss: 21.5929   val Loss: 27.4364   time: 163.75s   best: 24.8527
2023-11-05 23:49:51,579:INFO:  Epoch 221/500:  train Loss: 21.2917   val Loss: 24.9315   time: 165.34s   best: 24.8527
2023-11-05 23:52:37,190:INFO:  Epoch 222/500:  train Loss: 21.3276   val Loss: 25.1175   time: 165.60s   best: 24.8527
2023-11-05 23:55:21,754:INFO:  Epoch 223/500:  train Loss: 21.3499   val Loss: 25.1430   time: 164.56s   best: 24.8527
2023-11-05 23:58:06,117:INFO:  Epoch 224/500:  train Loss: 21.2110   val Loss: 25.5675   time: 164.35s   best: 24.8527
2023-11-06 00:00:50,836:INFO:  Epoch 225/500:  train Loss: 21.1916   val Loss: 25.2654   time: 164.71s   best: 24.8527
2023-11-06 00:03:35,361:INFO:  Epoch 226/500:  train Loss: 21.5750   val Loss: 26.2543   time: 164.51s   best: 24.8527
2023-11-06 00:06:18,945:INFO:  Epoch 227/500:  train Loss: 21.4910   val Loss: 25.3194   time: 163.56s   best: 24.8527
2023-11-06 00:09:02,584:INFO:  Epoch 228/500:  train Loss: 21.2992   val Loss: 25.0057   time: 163.63s   best: 24.8527
2023-11-06 00:11:46,026:INFO:  Epoch 229/500:  train Loss: 21.4093   val Loss: 25.7265   time: 163.43s   best: 24.8527
2023-11-06 00:14:29,457:INFO:  Epoch 230/500:  train Loss: 21.3337   val Loss: 25.1296   time: 163.42s   best: 24.8527
2023-11-06 00:17:12,774:INFO:  Epoch 231/500:  train Loss: 21.3516   val Loss: 24.9215   time: 163.32s   best: 24.8527
2023-11-06 00:19:56,139:INFO:  Epoch 232/500:  train Loss: 21.2428   val Loss: 25.1649   time: 163.35s   best: 24.8527
2023-11-06 00:22:40,055:INFO:  Epoch 233/500:  train Loss: 21.3070   val Loss: 25.4122   time: 163.90s   best: 24.8527
2023-11-06 00:25:23,667:INFO:  Epoch 234/500:  train Loss: 21.4747   val Loss: 25.5023   time: 163.61s   best: 24.8527
2023-11-06 00:28:07,086:INFO:  Epoch 235/500:  train Loss: 21.4458   val Loss: 25.2472   time: 163.42s   best: 24.8527
2023-11-06 00:30:50,778:INFO:  Epoch 236/500:  train Loss: 21.1675   val Loss: 25.0418   time: 163.69s   best: 24.8527
2023-11-06 00:33:34,284:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-06 00:33:34,302:INFO:  Epoch 237/500:  train Loss: 21.1823   val Loss: 24.7985   time: 163.49s   best: 24.7985
2023-11-06 00:36:19,214:INFO:  Epoch 238/500:  train Loss: 21.0227   val Loss: 25.0337   time: 164.90s   best: 24.7985
2023-11-06 00:39:02,859:INFO:  Epoch 239/500:  train Loss: 21.1056   val Loss: 24.8272   time: 163.63s   best: 24.7985
2023-11-06 00:41:46,191:INFO:  Epoch 240/500:  train Loss: 21.1294   val Loss: 24.8045   time: 163.33s   best: 24.7985
2023-11-06 00:44:30,654:INFO:  Epoch 241/500:  train Loss: 20.9682   val Loss: 25.1519   time: 164.45s   best: 24.7985
2023-11-06 00:47:14,248:INFO:  Epoch 242/500:  train Loss: 20.9577   val Loss: 25.1737   time: 163.59s   best: 24.7985
2023-11-06 00:49:58,415:INFO:  Epoch 243/500:  train Loss: 21.1443   val Loss: 25.9323   time: 164.15s   best: 24.7985
2023-11-06 00:52:42,064:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-06 00:52:42,083:INFO:  Epoch 244/500:  train Loss: 20.9304   val Loss: 24.7407   time: 163.63s   best: 24.7407
2023-11-06 00:55:25,709:INFO:  Epoch 245/500:  train Loss: 20.8721   val Loss: 24.8251   time: 163.61s   best: 24.7407
2023-11-06 00:58:09,154:INFO:  Epoch 246/500:  train Loss: 21.0395   val Loss: 24.9021   time: 163.43s   best: 24.7407
2023-11-06 01:00:52,965:INFO:  Epoch 247/500:  train Loss: 21.4093   val Loss: 25.9306   time: 163.80s   best: 24.7407
2023-11-06 01:03:37,593:INFO:  Epoch 248/500:  train Loss: 21.3419   val Loss: 25.0309   time: 164.62s   best: 24.7407
2023-11-06 01:06:22,298:INFO:  Epoch 249/500:  train Loss: 20.8971   val Loss: 24.8959   time: 164.70s   best: 24.7407
2023-11-06 01:09:06,785:INFO:  Epoch 250/500:  train Loss: 20.9649   val Loss: 25.0005   time: 164.49s   best: 24.7407
2023-11-06 01:11:50,238:INFO:  Epoch 251/500:  train Loss: 20.8657   val Loss: 26.0892   time: 163.44s   best: 24.7407
2023-11-06 01:14:33,889:INFO:  Epoch 252/500:  train Loss: 20.9576   val Loss: 24.9621   time: 163.64s   best: 24.7407
2023-11-06 01:17:17,138:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-06 01:17:17,158:INFO:  Epoch 253/500:  train Loss: 20.8662   val Loss: 24.7237   time: 163.24s   best: 24.7237
2023-11-06 01:20:00,878:INFO:  Epoch 254/500:  train Loss: 20.8014   val Loss: 24.9647   time: 163.72s   best: 24.7237
2023-11-06 01:22:45,648:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-06 01:22:45,668:INFO:  Epoch 255/500:  train Loss: 21.2727   val Loss: 24.5626   time: 164.77s   best: 24.5626
2023-11-06 01:25:29,274:INFO:  Epoch 256/500:  train Loss: 21.5508   val Loss: 25.0755   time: 163.59s   best: 24.5626
2023-11-06 01:28:13,568:INFO:  Epoch 257/500:  train Loss: 20.7760   val Loss: 24.6892   time: 164.28s   best: 24.5626
2023-11-06 01:30:56,999:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-06 01:30:57,019:INFO:  Epoch 258/500:  train Loss: 21.0891   val Loss: 24.5308   time: 163.43s   best: 24.5308
2023-11-06 01:33:40,472:INFO:  Epoch 259/500:  train Loss: 20.8865   val Loss: 25.1995   time: 163.44s   best: 24.5308
2023-11-06 01:36:24,391:INFO:  Epoch 260/500:  train Loss: 20.8111   val Loss: 24.5484   time: 163.92s   best: 24.5308
2023-11-06 01:39:08,069:INFO:  Epoch 261/500:  train Loss: 20.7253   val Loss: 24.8450   time: 163.68s   best: 24.5308
2023-11-06 01:41:52,762:INFO:  Epoch 262/500:  train Loss: 20.6134   val Loss: 24.7543   time: 164.68s   best: 24.5308
2023-11-06 01:44:37,267:INFO:  Epoch 263/500:  train Loss: 20.7525   val Loss: 24.7581   time: 164.49s   best: 24.5308
2023-11-06 01:47:20,772:INFO:  Epoch 264/500:  train Loss: 20.7408   val Loss: 30.8772   time: 163.50s   best: 24.5308
2023-11-06 01:50:05,157:INFO:  Epoch 265/500:  train Loss: 20.7145   val Loss: 24.8648   time: 164.37s   best: 24.5308
2023-11-06 01:52:48,480:INFO:  Epoch 266/500:  train Loss: 20.8146   val Loss: 24.8248   time: 163.31s   best: 24.5308
2023-11-06 01:55:32,076:INFO:  Epoch 267/500:  train Loss: 20.5764   val Loss: 24.9080   time: 163.58s   best: 24.5308
2023-11-06 01:58:15,390:INFO:  Epoch 268/500:  train Loss: 20.6730   val Loss: 24.6280   time: 163.31s   best: 24.5308
2023-11-06 02:00:59,023:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-06 02:00:59,042:INFO:  Epoch 269/500:  train Loss: 20.7839   val Loss: 24.4215   time: 163.63s   best: 24.4215
2023-11-06 02:03:42,100:INFO:  Epoch 270/500:  train Loss: 20.4412   val Loss: 24.9001   time: 163.05s   best: 24.4215
2023-11-06 02:06:25,634:INFO:  Epoch 271/500:  train Loss: 21.0402   val Loss: 25.9126   time: 163.53s   best: 24.4215
2023-11-06 02:09:08,854:INFO:  Epoch 272/500:  train Loss: 20.6565   val Loss: 24.5018   time: 163.21s   best: 24.4215
2023-11-06 02:11:51,106:INFO:  Epoch 273/500:  train Loss: 20.5115   val Loss: 25.1828   time: 162.24s   best: 24.4215
2023-11-06 02:14:34,095:INFO:  Epoch 274/500:  train Loss: 20.5488   val Loss: 24.4744   time: 162.98s   best: 24.4215
2023-11-06 02:17:16,445:INFO:  Epoch 275/500:  train Loss: 20.5116   val Loss: 24.6864   time: 162.34s   best: 24.4215
2023-11-06 02:19:59,305:INFO:  Epoch 276/500:  train Loss: 20.7245   val Loss: 24.7586   time: 162.86s   best: 24.4215
2023-11-06 02:22:41,717:INFO:  Epoch 277/500:  train Loss: 20.5159   val Loss: 24.5010   time: 162.41s   best: 24.4215
2023-11-06 02:25:24,298:INFO:  Epoch 278/500:  train Loss: 20.5568   val Loss: 25.3301   time: 162.58s   best: 24.4215
2023-11-06 02:28:06,448:INFO:  Epoch 279/500:  train Loss: 20.7324   val Loss: 26.9592   time: 162.14s   best: 24.4215
2023-11-06 02:30:49,578:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-06 02:30:49,597:INFO:  Epoch 280/500:  train Loss: 20.7439   val Loss: 24.3392   time: 163.11s   best: 24.3392
2023-11-06 02:33:31,794:INFO:  Epoch 281/500:  train Loss: 20.4624   val Loss: 24.6707   time: 162.18s   best: 24.3392
2023-11-06 02:36:14,201:INFO:  Epoch 282/500:  train Loss: 20.5527   val Loss: 24.7276   time: 162.41s   best: 24.3392
2023-11-06 02:38:56,565:INFO:  Epoch 283/500:  train Loss: 20.5173   val Loss: 25.2401   time: 162.36s   best: 24.3392
2023-11-06 02:41:38,812:INFO:  Epoch 284/500:  train Loss: 20.4467   val Loss: 25.0300   time: 162.23s   best: 24.3392
2023-11-06 02:44:21,781:INFO:  Epoch 285/500:  train Loss: 20.4048   val Loss: 24.7637   time: 162.96s   best: 24.3392
2023-11-06 02:47:03,950:INFO:  Epoch 286/500:  train Loss: 20.5005   val Loss: 25.1190   time: 162.17s   best: 24.3392
2023-11-06 02:49:46,111:INFO:  Epoch 287/500:  train Loss: 20.6347   val Loss: 24.8625   time: 162.15s   best: 24.3392
2023-11-06 02:52:28,209:INFO:  Epoch 288/500:  train Loss: 20.2835   val Loss: 24.7815   time: 162.09s   best: 24.3392
2023-11-06 02:55:10,874:INFO:  Epoch 289/500:  train Loss: 20.5099   val Loss: 24.5193   time: 162.64s   best: 24.3392
2023-11-06 02:57:54,273:INFO:  Epoch 290/500:  train Loss: 20.2980   val Loss: 24.7182   time: 163.39s   best: 24.3392
2023-11-06 03:00:37,750:INFO:  Epoch 291/500:  train Loss: 20.2966   val Loss: 25.2828   time: 163.48s   best: 24.3392
2023-11-06 03:03:20,173:INFO:  Epoch 292/500:  train Loss: 20.4456   val Loss: 24.9029   time: 162.41s   best: 24.3392
2023-11-06 03:06:02,782:INFO:  Epoch 293/500:  train Loss: 20.3172   val Loss: 24.7514   time: 162.60s   best: 24.3392
2023-11-06 03:08:45,305:INFO:  Epoch 294/500:  train Loss: 20.3215   val Loss: 24.6051   time: 162.51s   best: 24.3392
2023-11-06 03:11:27,733:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-06 03:11:27,766:INFO:  Epoch 295/500:  train Loss: 20.3311   val Loss: 24.3177   time: 162.39s   best: 24.3177
2023-11-06 03:14:11,162:INFO:  Epoch 296/500:  train Loss: 20.7834   val Loss: 27.2095   time: 163.38s   best: 24.3177
2023-11-06 03:16:53,597:INFO:  Epoch 297/500:  train Loss: 20.8872   val Loss: 24.4597   time: 162.34s   best: 24.3177
2023-11-06 03:19:36,806:INFO:  Epoch 298/500:  train Loss: 20.4644   val Loss: 25.1974   time: 163.20s   best: 24.3177
2023-11-06 03:22:19,926:INFO:  Epoch 299/500:  train Loss: 20.2762   val Loss: 24.8857   time: 163.11s   best: 24.3177
2023-11-06 03:25:02,387:INFO:  Epoch 300/500:  train Loss: 20.4032   val Loss: 24.4093   time: 162.45s   best: 24.3177
2023-11-06 03:27:44,365:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-06 03:27:44,383:INFO:  Epoch 301/500:  train Loss: 20.3710   val Loss: 24.0880   time: 161.96s   best: 24.0880
2023-11-06 03:30:26,398:INFO:  Epoch 302/500:  train Loss: 20.2568   val Loss: 24.5778   time: 162.01s   best: 24.0880
2023-11-06 03:33:09,175:INFO:  Epoch 303/500:  train Loss: 20.2271   val Loss: 24.4623   time: 162.78s   best: 24.0880
2023-11-06 03:35:52,143:INFO:  Epoch 304/500:  train Loss: 20.1754   val Loss: 24.7178   time: 162.97s   best: 24.0880
2023-11-06 03:38:34,145:INFO:  Epoch 305/500:  train Loss: 20.1441   val Loss: 24.8794   time: 161.99s   best: 24.0880
2023-11-06 03:41:17,252:INFO:  Epoch 306/500:  train Loss: 20.5506   val Loss: 26.8244   time: 163.10s   best: 24.0880
2023-11-06 03:44:00,373:INFO:  Epoch 307/500:  train Loss: 20.1816   val Loss: 24.6370   time: 163.11s   best: 24.0880
2023-11-06 03:46:42,546:INFO:  Epoch 308/500:  train Loss: 20.5085   val Loss: 24.9276   time: 162.17s   best: 24.0880
2023-11-06 03:49:24,790:INFO:  Epoch 309/500:  train Loss: 20.1846   val Loss: 24.8412   time: 162.23s   best: 24.0880
2023-11-06 03:52:06,665:INFO:  Epoch 310/500:  train Loss: 20.1972   val Loss: 24.4676   time: 161.87s   best: 24.0880
2023-11-06 03:54:48,879:INFO:  Epoch 311/500:  train Loss: 20.2200   val Loss: 24.6615   time: 162.20s   best: 24.0880
2023-11-06 03:57:30,861:INFO:  Epoch 312/500:  train Loss: 20.1028   val Loss: 24.6150   time: 161.97s   best: 24.0880
2023-11-06 04:00:13,737:INFO:  Epoch 313/500:  train Loss: 20.1921   val Loss: 24.2680   time: 162.87s   best: 24.0880
2023-11-06 04:02:56,351:INFO:  Epoch 314/500:  train Loss: 20.6661   val Loss: 24.8116   time: 162.59s   best: 24.0880
2023-11-06 04:05:38,232:INFO:  Epoch 315/500:  train Loss: 20.2538   val Loss: 25.3044   time: 161.88s   best: 24.0880
2023-11-06 04:08:20,110:INFO:  Epoch 316/500:  train Loss: 20.0713   val Loss: 24.9117   time: 161.88s   best: 24.0880
2023-11-06 04:11:03,059:INFO:  Epoch 317/500:  train Loss: 20.2650   val Loss: 25.2838   time: 162.94s   best: 24.0880
2023-11-06 04:13:44,895:INFO:  Epoch 318/500:  train Loss: 20.0003   val Loss: 24.7941   time: 161.82s   best: 24.0880
2023-11-06 04:16:26,938:INFO:  Epoch 319/500:  train Loss: 20.0107   val Loss: 24.7618   time: 162.03s   best: 24.0880
2023-11-06 04:19:09,084:INFO:  Epoch 320/500:  train Loss: 20.1320   val Loss: 24.7870   time: 162.14s   best: 24.0880
2023-11-06 04:21:51,151:INFO:  Epoch 321/500:  train Loss: 20.0390   val Loss: 24.7901   time: 162.07s   best: 24.0880
2023-11-06 04:24:34,021:INFO:  Epoch 322/500:  train Loss: 20.1080   val Loss: 24.7250   time: 162.86s   best: 24.0880
2023-11-06 04:27:16,413:INFO:  Epoch 323/500:  train Loss: 20.0212   val Loss: 24.6969   time: 162.38s   best: 24.0880
2023-11-06 04:29:58,608:INFO:  Epoch 324/500:  train Loss: 20.1143   val Loss: 24.8619   time: 162.18s   best: 24.0880
2023-11-06 04:32:40,714:INFO:  Epoch 325/500:  train Loss: 20.1002   val Loss: 24.4896   time: 162.09s   best: 24.0880
2023-11-06 04:35:22,995:INFO:  Epoch 326/500:  train Loss: 19.9532   val Loss: 24.9189   time: 162.26s   best: 24.0880
2023-11-06 04:38:05,766:INFO:  Epoch 327/500:  train Loss: 20.0464   val Loss: 24.8349   time: 162.75s   best: 24.0880
2023-11-06 04:40:47,697:INFO:  Epoch 328/500:  train Loss: 20.0041   val Loss: 24.5056   time: 161.92s   best: 24.0880
2023-11-06 04:43:29,730:INFO:  Epoch 329/500:  train Loss: 19.9211   val Loss: 24.9966   time: 162.03s   best: 24.0880
2023-11-06 04:46:11,685:INFO:  Epoch 330/500:  train Loss: 20.0666   val Loss: 26.2640   time: 161.94s   best: 24.0880
2023-11-06 04:48:54,514:INFO:  Epoch 331/500:  train Loss: 19.9602   val Loss: 24.8571   time: 162.82s   best: 24.0880
2023-11-06 04:51:36,515:INFO:  Epoch 332/500:  train Loss: 20.2332   val Loss: 26.6346   time: 162.00s   best: 24.0880
2023-11-06 04:54:18,604:INFO:  Epoch 333/500:  train Loss: 20.0645   val Loss: 25.5355   time: 162.08s   best: 24.0880
2023-11-06 04:57:00,630:INFO:  Epoch 334/500:  train Loss: 19.9463   val Loss: 24.5283   time: 162.01s   best: 24.0880
2023-11-06 04:59:43,831:INFO:  Epoch 335/500:  train Loss: 19.9120   val Loss: 24.3541   time: 163.19s   best: 24.0880
2023-11-06 05:02:25,718:INFO:  Epoch 336/500:  train Loss: 19.9809   val Loss: 24.7114   time: 161.89s   best: 24.0880
2023-11-06 05:05:07,733:INFO:  Epoch 337/500:  train Loss: 19.8771   val Loss: 24.8211   time: 162.00s   best: 24.0880
2023-11-06 05:07:50,465:INFO:  Epoch 338/500:  train Loss: 20.3195   val Loss: 25.0713   time: 162.72s   best: 24.0880
2023-11-06 05:10:32,461:INFO:  Epoch 339/500:  train Loss: 19.8418   val Loss: 24.5189   time: 161.99s   best: 24.0880
2023-11-06 05:13:14,434:INFO:  Epoch 340/500:  train Loss: 19.7471   val Loss: 25.0999   time: 161.96s   best: 24.0880
2023-11-06 05:15:57,617:INFO:  Epoch 341/500:  train Loss: 20.0261   val Loss: 27.2327   time: 162.94s   best: 24.0880
2023-11-06 05:18:40,030:INFO:  Epoch 342/500:  train Loss: 19.7860   val Loss: 25.0539   time: 162.41s   best: 24.0880
2023-11-06 05:21:23,288:INFO:  Epoch 343/500:  train Loss: 19.7432   val Loss: 24.8023   time: 163.25s   best: 24.0880
2023-11-06 05:24:05,275:INFO:  Epoch 344/500:  train Loss: 19.7584   val Loss: 24.7155   time: 161.98s   best: 24.0880
2023-11-06 05:26:47,161:INFO:  Epoch 345/500:  train Loss: 19.9455   val Loss: 24.5773   time: 161.88s   best: 24.0880
2023-11-06 05:29:29,245:INFO:  Epoch 346/500:  train Loss: 19.9977   val Loss: 24.7169   time: 162.06s   best: 24.0880
2023-11-06 05:32:11,124:INFO:  Epoch 347/500:  train Loss: 19.7457   val Loss: 24.8610   time: 161.87s   best: 24.0880
2023-11-06 05:34:53,169:INFO:  Epoch 348/500:  train Loss: 20.0249   val Loss: 24.8256   time: 162.02s   best: 24.0880
2023-11-06 05:37:35,671:INFO:  Epoch 349/500:  train Loss: 19.9399   val Loss: 24.8641   time: 162.48s   best: 24.0880
2023-11-06 05:40:17,471:INFO:  Epoch 350/500:  train Loss: 19.7507   val Loss: 24.9843   time: 161.79s   best: 24.0880
2023-11-06 05:43:00,089:INFO:  Epoch 351/500:  train Loss: 19.9439   val Loss: 25.5073   time: 162.61s   best: 24.0880
2023-11-06 05:45:43,165:INFO:  Epoch 352/500:  train Loss: 20.0191   val Loss: 24.5306   time: 163.07s   best: 24.0880
2023-11-06 05:48:25,574:INFO:  Epoch 353/500:  train Loss: 19.7702   val Loss: 24.6207   time: 162.24s   best: 24.0880
2023-11-06 05:51:07,500:INFO:  Epoch 354/500:  train Loss: 19.8144   val Loss: 24.3584   time: 161.91s   best: 24.0880
2023-11-06 05:53:49,781:INFO:  Epoch 355/500:  train Loss: 19.7317   val Loss: 24.4871   time: 162.11s   best: 24.0880
2023-11-06 05:56:31,491:INFO:  Epoch 356/500:  train Loss: 19.7805   val Loss: 24.5047   time: 161.69s   best: 24.0880
2023-11-06 05:59:13,786:INFO:  Epoch 357/500:  train Loss: 19.7366   val Loss: 24.4502   time: 161.91s   best: 24.0880
2023-11-06 06:01:56,374:INFO:  Epoch 358/500:  train Loss: 19.7757   val Loss: 24.2081   time: 162.57s   best: 24.0880
2023-11-06 06:04:38,546:INFO:  Epoch 359/500:  train Loss: 19.7277   val Loss: 24.4522   time: 162.15s   best: 24.0880
2023-11-06 06:07:21,032:INFO:  Epoch 360/500:  train Loss: 20.4827   val Loss: 26.5665   time: 162.47s   best: 24.0880
2023-11-06 06:10:04,073:INFO:  Epoch 361/500:  train Loss: 20.2348   val Loss: 24.4032   time: 163.04s   best: 24.0880
2023-11-06 06:12:46,560:INFO:  Epoch 362/500:  train Loss: 19.6316   val Loss: 24.8837   time: 162.49s   best: 24.0880
2023-11-06 06:15:30,033:INFO:  Epoch 363/500:  train Loss: 19.7311   val Loss: 24.4304   time: 163.46s   best: 24.0880
2023-11-06 06:18:12,392:INFO:  Epoch 364/500:  train Loss: 19.6911   val Loss: 24.6445   time: 162.36s   best: 24.0880
2023-11-06 06:20:54,550:INFO:  Epoch 365/500:  train Loss: 19.7338   val Loss: 24.4961   time: 162.16s   best: 24.0880
2023-11-06 06:23:36,644:INFO:  Epoch 366/500:  train Loss: 19.7066   val Loss: 24.4107   time: 162.09s   best: 24.0880
2023-11-06 06:26:18,393:INFO:  Epoch 367/500:  train Loss: 19.5673   val Loss: 25.3085   time: 161.74s   best: 24.0880
2023-11-06 06:29:01,109:INFO:  Epoch 368/500:  train Loss: 19.6122   val Loss: 24.4444   time: 162.70s   best: 24.0880
2023-11-06 06:31:43,014:INFO:  Epoch 369/500:  train Loss: 19.9494   val Loss: 24.8255   time: 161.89s   best: 24.0880
2023-11-06 06:34:25,130:INFO:  Epoch 370/500:  train Loss: 19.4797   val Loss: 24.7983   time: 162.12s   best: 24.0880
2023-11-06 06:37:07,189:INFO:  Epoch 371/500:  train Loss: 19.5673   val Loss: 25.0457   time: 162.04s   best: 24.0880
2023-11-06 06:39:49,203:INFO:  Epoch 372/500:  train Loss: 20.0462   val Loss: 24.6156   time: 162.00s   best: 24.0880
2023-11-06 06:42:31,331:INFO:  Epoch 373/500:  train Loss: 19.5770   val Loss: 24.5405   time: 162.13s   best: 24.0880
2023-11-06 06:45:13,342:INFO:  Epoch 374/500:  train Loss: 19.4461   val Loss: 24.5367   time: 162.00s   best: 24.0880
2023-11-06 06:47:55,201:INFO:  Epoch 375/500:  train Loss: 19.7960   val Loss: 24.4811   time: 161.86s   best: 24.0880
2023-11-06 06:50:37,234:INFO:  Epoch 376/500:  train Loss: 19.7787   val Loss: 24.6661   time: 162.02s   best: 24.0880
2023-11-06 06:53:20,196:INFO:  Epoch 377/500:  train Loss: 19.6472   val Loss: 24.8863   time: 162.95s   best: 24.0880
2023-11-06 06:56:02,059:INFO:  Epoch 378/500:  train Loss: 19.4612   val Loss: 25.5087   time: 161.85s   best: 24.0880
2023-11-06 06:58:43,853:INFO:  Epoch 379/500:  train Loss: 19.5059   val Loss: 24.8764   time: 161.78s   best: 24.0880
2023-11-06 07:01:25,852:INFO:  Epoch 380/500:  train Loss: 19.7938   val Loss: 24.8822   time: 161.99s   best: 24.0880
2023-11-06 07:04:08,395:INFO:  Epoch 381/500:  train Loss: 19.4963   val Loss: 24.6647   time: 162.53s   best: 24.0880
2023-11-06 07:06:51,080:INFO:  Epoch 382/500:  train Loss: 19.5610   val Loss: 24.7795   time: 162.67s   best: 24.0880
2023-11-06 07:09:33,043:INFO:  Epoch 383/500:  train Loss: 19.3920   val Loss: 24.9109   time: 161.96s   best: 24.0880
2023-11-06 07:12:15,361:INFO:  Epoch 384/500:  train Loss: 19.5290   val Loss: 24.7352   time: 162.31s   best: 24.0880
2023-11-06 07:14:57,497:INFO:  Epoch 385/500:  train Loss: 19.9349   val Loss: 27.0307   time: 162.12s   best: 24.0880
2023-11-06 07:17:39,205:INFO:  Epoch 386/500:  train Loss: 20.1466   val Loss: 24.5546   time: 161.71s   best: 24.0880
2023-11-06 07:20:20,939:INFO:  Epoch 387/500:  train Loss: 19.4189   val Loss: 24.6111   time: 161.73s   best: 24.0880
2023-11-06 07:23:02,951:INFO:  Epoch 388/500:  train Loss: 19.6309   val Loss: 24.9285   time: 162.00s   best: 24.0880
2023-11-06 07:25:45,647:INFO:  Epoch 389/500:  train Loss: 19.5169   val Loss: 24.8623   time: 162.70s   best: 24.0880
2023-11-06 07:28:27,420:INFO:  Epoch 390/500:  train Loss: 19.3833   val Loss: 24.4274   time: 161.77s   best: 24.0880
2023-11-06 07:31:09,000:INFO:  Epoch 391/500:  train Loss: 19.4403   val Loss: 24.5314   time: 161.58s   best: 24.0880
2023-11-06 07:33:50,872:INFO:  Epoch 392/500:  train Loss: 19.6028   val Loss: 24.6615   time: 161.86s   best: 24.0880
2023-11-06 07:36:33,099:INFO:  Epoch 393/500:  train Loss: 19.3474   val Loss: 24.7741   time: 162.22s   best: 24.0880
2023-11-06 07:39:16,462:INFO:  Epoch 394/500:  train Loss: 19.6306   val Loss: 24.4155   time: 163.35s   best: 24.0880
2023-11-06 07:41:58,510:INFO:  Epoch 395/500:  train Loss: 19.5147   val Loss: 24.6017   time: 162.03s   best: 24.0880
2023-11-06 07:44:41,363:INFO:  Epoch 396/500:  train Loss: 19.5076   val Loss: 24.9905   time: 162.85s   best: 24.0880
2023-11-06 07:47:23,416:INFO:  Epoch 397/500:  train Loss: 19.3466   val Loss: 24.5442   time: 162.04s   best: 24.0880
2023-11-06 07:50:06,004:INFO:  Epoch 398/500:  train Loss: 19.6742   val Loss: 24.5300   time: 162.59s   best: 24.0880
2023-11-06 07:52:48,817:INFO:  Epoch 399/500:  train Loss: 19.3405   val Loss: 24.5703   time: 162.80s   best: 24.0880
2023-11-06 07:55:30,853:INFO:  Epoch 400/500:  train Loss: 19.8233   val Loss: 24.6057   time: 162.02s   best: 24.0880
2023-11-06 07:58:12,628:INFO:  Epoch 401/500:  train Loss: 19.6806   val Loss: 24.4920   time: 161.76s   best: 24.0880
2023-11-06 08:00:55,213:INFO:  Epoch 402/500:  train Loss: 19.5421   val Loss: 25.2130   time: 162.58s   best: 24.0880
2023-11-06 08:03:36,949:INFO:  Epoch 403/500:  train Loss: 19.4837   val Loss: 24.6154   time: 161.73s   best: 24.0880
2023-11-06 08:06:19,004:INFO:  Epoch 404/500:  train Loss: 19.4332   val Loss: 24.6423   time: 162.05s   best: 24.0880
2023-11-06 08:09:01,062:INFO:  Epoch 405/500:  train Loss: 19.7920   val Loss: 24.5383   time: 162.05s   best: 24.0880
2023-11-06 08:11:42,886:INFO:  Epoch 406/500:  train Loss: 19.3705   val Loss: 26.4638   time: 161.81s   best: 24.0880
2023-11-06 08:14:25,103:INFO:  Epoch 407/500:  train Loss: 19.3108   val Loss: 24.5764   time: 162.21s   best: 24.0880
2023-11-06 08:17:07,315:INFO:  Epoch 408/500:  train Loss: 19.3996   val Loss: 24.5070   time: 162.21s   best: 24.0880
2023-11-06 08:19:49,312:INFO:  Epoch 409/500:  train Loss: 19.3238   val Loss: 24.4657   time: 161.98s   best: 24.0880
2023-11-06 08:22:31,227:INFO:  Epoch 410/500:  train Loss: 19.4009   val Loss: 24.6733   time: 161.89s   best: 24.0880
2023-11-06 08:25:13,125:INFO:  Epoch 411/500:  train Loss: 19.2750   val Loss: 25.6425   time: 161.90s   best: 24.0880
2023-11-06 08:27:55,078:INFO:  Epoch 412/500:  train Loss: 19.4367   val Loss: 24.6940   time: 161.94s   best: 24.0880
2023-11-06 08:30:37,116:INFO:  Epoch 413/500:  train Loss: 19.2621   val Loss: 24.2156   time: 162.03s   best: 24.0880
2023-11-06 08:33:19,204:INFO:  Epoch 414/500:  train Loss: 19.3768   val Loss: 24.8899   time: 162.06s   best: 24.0880
2023-11-06 08:36:01,361:INFO:  Epoch 415/500:  train Loss: 19.1961   val Loss: 24.5284   time: 162.14s   best: 24.0880
2023-11-06 08:38:43,614:INFO:  Epoch 416/500:  train Loss: 19.7726   val Loss: 24.7056   time: 162.23s   best: 24.0880
2023-11-06 08:41:25,843:INFO:  Epoch 417/500:  train Loss: 19.2857   val Loss: 24.2533   time: 162.23s   best: 24.0880
2023-11-06 08:44:09,156:INFO:  Epoch 418/500:  train Loss: 19.9885   val Loss: 25.7823   time: 163.31s   best: 24.0880
2023-11-06 08:46:51,343:INFO:  Epoch 419/500:  train Loss: 19.3617   val Loss: 25.1202   time: 162.19s   best: 24.0880
2023-11-06 08:49:33,071:INFO:  Epoch 420/500:  train Loss: 19.3922   val Loss: 24.7466   time: 161.72s   best: 24.0880
2023-11-06 08:52:14,852:INFO:  Epoch 421/500:  train Loss: 19.2333   val Loss: 24.7055   time: 161.78s   best: 24.0880
2023-11-06 08:54:56,935:INFO:  Epoch 422/500:  train Loss: 19.2865   val Loss: 24.6132   time: 162.08s   best: 24.0880
2023-11-06 08:57:39,433:INFO:  Epoch 423/500:  train Loss: 19.2061   val Loss: 24.5799   time: 162.50s   best: 24.0880
2023-11-06 09:00:21,234:INFO:  Epoch 424/500:  train Loss: 19.2728   val Loss: 24.4522   time: 161.80s   best: 24.0880
2023-11-06 09:03:03,141:INFO:  Epoch 425/500:  train Loss: 20.5668   val Loss: 25.4824   time: 161.90s   best: 24.0880
2023-11-06 09:05:45,797:INFO:  Epoch 426/500:  train Loss: 19.5088   val Loss: 24.6434   time: 162.64s   best: 24.0880
2023-11-06 09:08:28,527:INFO:  Epoch 427/500:  train Loss: 19.4176   val Loss: 24.5717   time: 162.73s   best: 24.0880
2023-11-06 09:11:10,688:INFO:  Epoch 428/500:  train Loss: 19.2871   val Loss: 24.6558   time: 162.16s   best: 24.0880
2023-11-06 09:13:52,553:INFO:  Epoch 429/500:  train Loss: 19.1478   val Loss: 24.1130   time: 161.86s   best: 24.0880
2023-11-06 09:16:34,496:INFO:  Epoch 430/500:  train Loss: 19.1541   val Loss: 24.2372   time: 161.93s   best: 24.0880
2023-11-06 09:19:16,478:INFO:  Epoch 431/500:  train Loss: 19.4542   val Loss: 25.7326   time: 161.97s   best: 24.0880
2023-11-06 09:21:58,452:INFO:  Epoch 432/500:  train Loss: 19.1536   val Loss: 24.4835   time: 161.97s   best: 24.0880
2023-11-06 09:24:41,116:INFO:  Epoch 433/500:  train Loss: 19.1056   val Loss: 24.5656   time: 162.64s   best: 24.0880
2023-11-06 09:27:22,997:INFO:  Epoch 434/500:  train Loss: 19.3131   val Loss: 25.4660   time: 161.88s   best: 24.0880
2023-11-06 09:30:05,905:INFO:  Epoch 435/500:  train Loss: 19.1278   val Loss: 24.4163   time: 162.90s   best: 24.0880
2023-11-06 09:32:48,111:INFO:  Epoch 436/500:  train Loss: 19.2293   val Loss: 24.1962   time: 162.21s   best: 24.0880
2023-11-06 09:35:31,683:INFO:  Epoch 437/500:  train Loss: 19.2403   val Loss: 26.3617   time: 163.57s   best: 24.0880
2023-11-06 09:38:14,790:INFO:  Epoch 438/500:  train Loss: 19.7160   val Loss: 25.1631   time: 163.10s   best: 24.0880
2023-11-06 09:40:57,798:INFO:  Epoch 439/500:  train Loss: 19.3224   val Loss: 25.3672   time: 162.98s   best: 24.0880
2023-11-06 09:43:40,422:INFO:  Epoch 440/500:  train Loss: 19.0787   val Loss: 24.3699   time: 162.61s   best: 24.0880
2023-11-06 09:46:22,189:INFO:  Epoch 441/500:  train Loss: 19.6239   val Loss: 24.8389   time: 161.77s   best: 24.0880
2023-11-06 09:49:04,625:INFO:  Epoch 442/500:  train Loss: 19.2890   val Loss: 24.3225   time: 162.41s   best: 24.0880
2023-11-06 09:51:47,528:INFO:  Epoch 443/500:  train Loss: 19.1809   val Loss: 24.4016   time: 162.90s   best: 24.0880
2023-11-06 09:54:29,881:INFO:  Epoch 444/500:  train Loss: 19.2843   val Loss: 24.7304   time: 162.34s   best: 24.0880
2023-11-06 09:57:11,990:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.3 dataset (0.1 dropout)_653b.pt
2023-11-06 09:57:12,019:INFO:  Epoch 445/500:  train Loss: 19.4831   val Loss: 24.0087   time: 162.10s   best: 24.0087
2023-11-06 09:59:53,811:INFO:  Epoch 446/500:  train Loss: 19.0324   val Loss: 24.8428   time: 161.78s   best: 24.0087
2023-11-06 10:02:35,333:INFO:  Epoch 447/500:  train Loss: 19.1169   val Loss: 24.5568   time: 161.51s   best: 24.0087
2023-11-06 10:05:17,163:INFO:  Epoch 448/500:  train Loss: 19.3234   val Loss: 24.1516   time: 161.83s   best: 24.0087
2023-11-06 10:07:59,334:INFO:  Epoch 449/500:  train Loss: 19.7165   val Loss: 24.5258   time: 162.15s   best: 24.0087
2023-11-06 10:10:41,282:INFO:  Epoch 450/500:  train Loss: 19.2143   val Loss: 24.3190   time: 161.94s   best: 24.0087
2023-11-06 10:13:23,550:INFO:  Epoch 451/500:  train Loss: 19.1786   val Loss: 24.8023   time: 162.26s   best: 24.0087
2023-11-06 10:16:05,708:INFO:  Epoch 452/500:  train Loss: 19.0597   val Loss: 24.5712   time: 162.16s   best: 24.0087
2023-11-06 10:18:48,540:INFO:  Epoch 453/500:  train Loss: 19.2902   val Loss: 24.1902   time: 162.82s   best: 24.0087
2023-11-06 10:21:30,270:INFO:  Epoch 454/500:  train Loss: 18.9402   val Loss: 24.0861   time: 161.73s   best: 24.0087
2023-11-06 10:24:12,078:INFO:  Epoch 455/500:  train Loss: 19.2033   val Loss: 24.3075   time: 161.79s   best: 24.0087
2023-11-06 10:26:54,189:INFO:  Epoch 456/500:  train Loss: 19.2348   val Loss: 24.6601   time: 162.11s   best: 24.0087
2023-11-06 10:29:36,204:INFO:  Epoch 457/500:  train Loss: 19.3349   val Loss: 24.8948   time: 162.00s   best: 24.0087
2023-11-06 10:32:18,149:INFO:  Epoch 458/500:  train Loss: 19.1644   val Loss: 24.9672   time: 161.93s   best: 24.0087
2023-11-06 10:35:00,065:INFO:  Epoch 459/500:  train Loss: 18.9530   val Loss: 24.4606   time: 161.92s   best: 24.0087
2023-11-06 10:37:41,984:INFO:  Epoch 460/500:  train Loss: 19.3122   val Loss: 24.9327   time: 161.92s   best: 24.0087
2023-11-06 10:40:23,927:INFO:  Epoch 461/500:  train Loss: 19.0298   val Loss: 24.4913   time: 161.94s   best: 24.0087
2023-11-06 10:43:06,117:INFO:  Epoch 462/500:  train Loss: 18.9742   val Loss: 24.6737   time: 162.18s   best: 24.0087
2023-11-06 10:45:47,896:INFO:  Epoch 463/500:  train Loss: 19.0167   val Loss: 24.8792   time: 161.78s   best: 24.0087
2023-11-06 10:48:29,518:INFO:  Epoch 464/500:  train Loss: 18.9230   val Loss: 24.4637   time: 161.62s   best: 24.0087
2023-11-06 10:51:11,389:INFO:  Epoch 465/500:  train Loss: 19.2248   val Loss: 24.3213   time: 161.86s   best: 24.0087
2023-11-06 10:53:53,272:INFO:  Epoch 466/500:  train Loss: 18.9602   val Loss: 24.5708   time: 161.86s   best: 24.0087
2023-11-06 10:56:36,299:INFO:  Epoch 467/500:  train Loss: 18.9361   val Loss: 24.7312   time: 163.03s   best: 24.0087
2023-11-06 10:59:17,958:INFO:  Epoch 468/500:  train Loss: 18.9776   val Loss: 24.2252   time: 161.66s   best: 24.0087
2023-11-06 11:01:59,914:INFO:  Epoch 469/500:  train Loss: 18.9859   val Loss: 24.5448   time: 161.94s   best: 24.0087
2023-11-06 11:04:42,665:INFO:  Epoch 470/500:  train Loss: 18.9985   val Loss: 24.3191   time: 162.75s   best: 24.0087
2023-11-06 11:07:25,573:INFO:  Epoch 471/500:  train Loss: 18.8817   val Loss: 24.6461   time: 162.91s   best: 24.0087
2023-11-06 11:10:08,272:INFO:  Epoch 472/500:  train Loss: 19.1391   val Loss: 27.5559   time: 162.69s   best: 24.0087
2023-11-06 11:12:50,708:INFO:  Epoch 473/500:  train Loss: 19.0878   val Loss: 24.4624   time: 162.43s   best: 24.0087
2023-11-06 11:15:32,767:INFO:  Epoch 474/500:  train Loss: 19.0297   val Loss: 24.9558   time: 162.05s   best: 24.0087
2023-11-06 11:18:14,581:INFO:  Epoch 475/500:  train Loss: 18.9228   val Loss: 24.6535   time: 161.80s   best: 24.0087
2023-11-06 11:20:56,465:INFO:  Epoch 476/500:  train Loss: 19.1865   val Loss: 24.5471   time: 161.87s   best: 24.0087
2023-11-06 11:22:53,283:INFO:  Starting experiment lstm autoencoder with 0.2 dataset (0.1 dropout)
2023-11-06 11:22:53,296:INFO:  Defining the model
2023-11-06 11:22:53,349:INFO:  Reading the dataset
2023-11-06 11:23:39,425:INFO:  Epoch 477/500:  train Loss: 19.1526   val Loss: 24.6090   time: 162.96s   best: 24.0087
2023-11-06 11:26:22,281:INFO:  Epoch 478/500:  train Loss: 19.0262   val Loss: 24.1649   time: 162.83s   best: 24.0087
2023-11-06 11:29:04,243:INFO:  Epoch 479/500:  train Loss: 18.8302   val Loss: 24.8766   time: 161.96s   best: 24.0087
2023-11-06 11:31:46,767:INFO:  Epoch 480/500:  train Loss: 19.0366   val Loss: 24.5311   time: 162.51s   best: 24.0087
2023-11-06 11:34:29,034:INFO:  Epoch 481/500:  train Loss: 19.0280   val Loss: 24.3324   time: 162.26s   best: 24.0087
2023-11-06 11:37:10,925:INFO:  Epoch 482/500:  train Loss: 18.9101   val Loss: 24.5481   time: 161.88s   best: 24.0087
2023-11-06 11:39:53,479:INFO:  Epoch 483/500:  train Loss: 18.9973   val Loss: 24.4185   time: 162.54s   best: 24.0087
2023-11-06 11:42:36,910:INFO:  Epoch 484/500:  train Loss: 18.8251   val Loss: 24.6926   time: 163.42s   best: 24.0087
2023-11-06 11:45:18,970:INFO:  Epoch 485/500:  train Loss: 19.0447   val Loss: 24.3056   time: 162.05s   best: 24.0087
2023-11-06 11:48:02,427:INFO:  Epoch 486/500:  train Loss: 19.7762   val Loss: 24.6042   time: 163.44s   best: 24.0087
2023-11-06 11:50:45,429:INFO:  Epoch 487/500:  train Loss: 18.8812   val Loss: 24.1323   time: 163.00s   best: 24.0087
2023-11-06 11:53:27,239:INFO:  Epoch 488/500:  train Loss: 18.8051   val Loss: 24.0211   time: 161.80s   best: 24.0087
2023-11-06 11:56:09,052:INFO:  Epoch 489/500:  train Loss: 19.0352   val Loss: 24.1519   time: 161.80s   best: 24.0087
2023-11-06 11:57:08,744:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 11:57:09,033:INFO:  Epoch 1/500:  train Loss: 87.5238   val Loss: 84.9970   time: 130.73s   best: 84.9970
2023-11-06 11:58:50,853:INFO:  Epoch 490/500:  train Loss: 18.7852   val Loss: 24.4739   time: 161.80s   best: 24.0087
2023-11-06 11:59:18,657:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 11:59:18,860:INFO:  Epoch 2/500:  train Loss: 78.9472   val Loss: 75.0892   time: 129.62s   best: 75.0892
2023-11-06 12:01:28,240:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:01:28,259:INFO:  Epoch 3/500:  train Loss: 73.2944   val Loss: 71.3939   time: 129.38s   best: 71.3939
2023-11-06 12:01:32,681:INFO:  Epoch 491/500:  train Loss: 19.0626   val Loss: 24.9963   time: 161.82s   best: 24.0087
2023-11-06 12:03:37,802:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:03:37,832:INFO:  Epoch 4/500:  train Loss: 70.3812   val Loss: 68.8711   time: 129.54s   best: 68.8711
2023-11-06 12:04:15,212:INFO:  Epoch 492/500:  train Loss: 19.1193   val Loss: 24.8674   time: 162.24s   best: 24.0087
2023-11-06 12:05:47,101:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:05:47,137:INFO:  Epoch 5/500:  train Loss: 68.6150   val Loss: 67.2743   time: 129.25s   best: 67.2743
2023-11-06 12:06:57,693:INFO:  Epoch 493/500:  train Loss: 18.9418   val Loss: 24.7225   time: 162.44s   best: 24.0087
2023-11-06 12:07:56,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:07:57,204:INFO:  Epoch 6/500:  train Loss: 67.2693   val Loss: 65.6390   time: 129.85s   best: 65.6390
2023-11-06 12:09:40,956:INFO:  Epoch 494/500:  train Loss: 19.1261   val Loss: 24.8059   time: 163.25s   best: 24.0087
2023-11-06 12:10:06,675:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:10:06,698:INFO:  Epoch 7/500:  train Loss: 65.9661   val Loss: 65.0609   time: 129.47s   best: 65.0609
2023-11-06 12:12:16,082:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:12:16,101:INFO:  Epoch 8/500:  train Loss: 64.7782   val Loss: 64.6364   time: 129.36s   best: 64.6364
2023-11-06 12:12:24,442:INFO:  Epoch 495/500:  train Loss: 18.8212   val Loss: 24.5189   time: 163.17s   best: 24.0087
2023-11-06 12:14:25,769:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:14:25,799:INFO:  Epoch 9/500:  train Loss: 63.6637   val Loss: 62.2936   time: 129.65s   best: 62.2936
2023-11-06 12:15:06,841:INFO:  Epoch 496/500:  train Loss: 18.7210   val Loss: 24.1303   time: 162.38s   best: 24.0087
2023-11-06 12:16:35,763:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:16:35,797:INFO:  Epoch 10/500:  train Loss: 62.3882   val Loss: 60.9295   time: 129.96s   best: 60.9295
2023-11-06 12:17:49,607:INFO:  Epoch 497/500:  train Loss: 18.7588   val Loss: 27.1920   time: 162.75s   best: 24.0087
2023-11-06 12:18:45,513:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:18:45,550:INFO:  Epoch 11/500:  train Loss: 61.0575   val Loss: 59.6837   time: 129.71s   best: 59.6837
2023-11-06 12:20:32,676:INFO:  Epoch 498/500:  train Loss: 18.9217   val Loss: 33.1893   time: 163.04s   best: 24.0087
2023-11-06 12:20:55,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:20:55,119:INFO:  Epoch 12/500:  train Loss: 59.8626   val Loss: 58.1826   time: 129.53s   best: 58.1826
2023-11-06 12:23:03,974:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:23:03,993:INFO:  Epoch 13/500:  train Loss: 58.4646   val Loss: 57.5928   time: 128.83s   best: 57.5928
2023-11-06 12:23:15,379:INFO:  Epoch 499/500:  train Loss: 19.8197   val Loss: 24.4834   time: 162.67s   best: 24.0087
2023-11-06 12:25:13,587:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:25:13,674:INFO:  Epoch 14/500:  train Loss: 57.1016   val Loss: 56.0743   time: 129.57s   best: 56.0743
2023-11-06 12:25:57,824:INFO:  Epoch 500/500:  train Loss: 19.0633   val Loss: 24.6378   time: 162.44s   best: 24.0087
2023-11-06 12:25:57,864:INFO:  -----> Training complete in 1362m 41s   best validation loss: 24.0087
 
2023-11-06 12:27:22,829:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:27:22,860:INFO:  Epoch 15/500:  train Loss: 56.3178   val Loss: 55.2231   time: 129.13s   best: 55.2231
2023-11-06 12:29:32,712:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:29:32,737:INFO:  Epoch 16/500:  train Loss: 54.8890   val Loss: 54.1088   time: 129.85s   best: 54.1088
2023-11-06 12:31:41,821:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:31:41,840:INFO:  Epoch 17/500:  train Loss: 53.8174   val Loss: 53.8798   time: 129.07s   best: 53.8798
2023-11-06 12:33:51,862:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:33:51,881:INFO:  Epoch 18/500:  train Loss: 52.5682   val Loss: 51.6623   time: 130.01s   best: 51.6623
2023-11-06 12:36:02,651:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:36:02,671:INFO:  Epoch 19/500:  train Loss: 51.6025   val Loss: 51.5716   time: 130.77s   best: 51.5716
2023-11-06 12:38:12,965:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:38:12,985:INFO:  Epoch 20/500:  train Loss: 50.5392   val Loss: 50.0537   time: 130.29s   best: 50.0537
2023-11-06 12:40:22,567:INFO:  Epoch 21/500:  train Loss: 49.6662   val Loss: 50.0986   time: 129.57s   best: 50.0537
2023-11-06 12:42:32,258:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:42:32,277:INFO:  Epoch 22/500:  train Loss: 48.7696   val Loss: 49.4862   time: 129.68s   best: 49.4862
2023-11-06 12:44:41,747:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:44:41,766:INFO:  Epoch 23/500:  train Loss: 48.2669   val Loss: 47.8926   time: 129.45s   best: 47.8926
2023-11-06 12:46:51,593:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:46:51,612:INFO:  Epoch 24/500:  train Loss: 47.3408   val Loss: 47.1505   time: 129.82s   best: 47.1505
2023-11-06 12:49:01,959:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:49:01,988:INFO:  Epoch 25/500:  train Loss: 46.4313   val Loss: 46.1279   time: 130.33s   best: 46.1279
2023-11-06 12:51:11,889:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:51:11,908:INFO:  Epoch 26/500:  train Loss: 45.6104   val Loss: 45.5454   time: 129.89s   best: 45.5454
2023-11-06 12:53:21,768:INFO:  Epoch 27/500:  train Loss: 45.2172   val Loss: 45.7842   time: 129.86s   best: 45.5454
2023-11-06 12:55:31,182:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:55:31,201:INFO:  Epoch 28/500:  train Loss: 44.3981   val Loss: 44.3810   time: 129.40s   best: 44.3810
2023-11-06 12:57:40,612:INFO:  Epoch 29/500:  train Loss: 43.6701   val Loss: 44.8440   time: 129.40s   best: 44.3810
2023-11-06 12:59:50,058:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 12:59:50,077:INFO:  Epoch 30/500:  train Loss: 43.0286   val Loss: 43.7422   time: 129.43s   best: 43.7422
2023-11-06 13:02:00,057:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:02:00,077:INFO:  Epoch 31/500:  train Loss: 42.3515   val Loss: 43.3938   time: 129.98s   best: 43.3938
2023-11-06 13:04:10,210:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:04:10,229:INFO:  Epoch 32/500:  train Loss: 41.7731   val Loss: 42.6697   time: 130.13s   best: 42.6697
2023-11-06 13:06:20,179:INFO:  Epoch 33/500:  train Loss: 41.2069   val Loss: 43.0301   time: 129.94s   best: 42.6697
2023-11-06 13:08:29,814:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:08:29,834:INFO:  Epoch 34/500:  train Loss: 40.6445   val Loss: 42.2676   time: 129.62s   best: 42.2676
2023-11-06 13:10:38,912:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:10:38,932:INFO:  Epoch 35/500:  train Loss: 40.1580   val Loss: 41.5182   time: 129.07s   best: 41.5182
2023-11-06 13:12:48,181:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:12:48,200:INFO:  Epoch 36/500:  train Loss: 39.7892   val Loss: 40.9794   time: 129.23s   best: 40.9794
2023-11-06 13:14:57,300:INFO:  Epoch 37/500:  train Loss: 39.2424   val Loss: 41.1949   time: 129.09s   best: 40.9794
2023-11-06 13:17:07,615:INFO:  Epoch 38/500:  train Loss: 38.7413   val Loss: 41.1984   time: 130.30s   best: 40.9794
2023-11-06 13:19:16,688:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:19:17,050:INFO:  Epoch 39/500:  train Loss: 38.4005   val Loss: 39.3517   time: 129.05s   best: 39.3517
2023-11-06 13:21:27,033:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:21:27,178:INFO:  Epoch 40/500:  train Loss: 38.4840   val Loss: 38.5204   time: 129.98s   best: 38.5204
2023-11-06 13:23:37,049:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:23:37,069:INFO:  Epoch 41/500:  train Loss: 37.5111   val Loss: 38.4741   time: 129.84s   best: 38.4741
2023-11-06 13:25:46,792:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:25:46,811:INFO:  Epoch 42/500:  train Loss: 37.2278   val Loss: 37.2764   time: 129.72s   best: 37.2764
2023-11-06 13:27:55,448:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:27:55,468:INFO:  Epoch 43/500:  train Loss: 36.8157   val Loss: 37.0360   time: 128.63s   best: 37.0360
2023-11-06 13:30:04,796:INFO:  Epoch 44/500:  train Loss: 36.5291   val Loss: 37.4989   time: 129.32s   best: 37.0360
2023-11-06 13:32:14,026:INFO:  Epoch 45/500:  train Loss: 36.1250   val Loss: 38.0873   time: 129.22s   best: 37.0360
2023-11-06 13:34:23,426:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:34:23,454:INFO:  Epoch 46/500:  train Loss: 35.7669   val Loss: 36.2817   time: 129.38s   best: 36.2817
2023-11-06 13:36:32,796:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:36:32,824:INFO:  Epoch 47/500:  train Loss: 35.4166   val Loss: 35.9300   time: 129.31s   best: 35.9300
2023-11-06 13:38:43,047:INFO:  Epoch 48/500:  train Loss: 35.3297   val Loss: 36.4388   time: 130.21s   best: 35.9300
2023-11-06 13:40:51,931:INFO:  Epoch 49/500:  train Loss: 34.9248   val Loss: 36.3223   time: 128.88s   best: 35.9300
2023-11-06 13:43:01,150:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:43:01,170:INFO:  Epoch 50/500:  train Loss: 34.6314   val Loss: 35.6002   time: 129.21s   best: 35.6002
2023-11-06 13:45:10,027:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:45:10,047:INFO:  Epoch 51/500:  train Loss: 34.4177   val Loss: 34.7210   time: 128.84s   best: 34.7210
2023-11-06 13:47:19,239:INFO:  Epoch 52/500:  train Loss: 34.0534   val Loss: 35.1386   time: 129.18s   best: 34.7210
2023-11-06 13:49:28,296:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:49:28,316:INFO:  Epoch 53/500:  train Loss: 33.8707   val Loss: 34.6398   time: 129.05s   best: 34.6398
2023-11-06 13:51:37,511:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:51:37,534:INFO:  Epoch 54/500:  train Loss: 33.7046   val Loss: 34.4773   time: 129.18s   best: 34.4773
2023-11-06 13:53:47,329:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:53:47,348:INFO:  Epoch 55/500:  train Loss: 33.4887   val Loss: 33.9518   time: 129.79s   best: 33.9518
2023-11-06 13:55:56,421:INFO:  Epoch 56/500:  train Loss: 33.2878   val Loss: 34.3269   time: 129.07s   best: 33.9518
2023-11-06 13:58:05,485:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 13:58:05,505:INFO:  Epoch 57/500:  train Loss: 33.0165   val Loss: 33.5528   time: 129.05s   best: 33.5528
2023-11-06 14:00:14,555:INFO:  Epoch 58/500:  train Loss: 32.8791   val Loss: 33.6792   time: 129.05s   best: 33.5528
2023-11-06 14:02:23,473:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:02:23,492:INFO:  Epoch 59/500:  train Loss: 32.7694   val Loss: 33.1762   time: 128.91s   best: 33.1762
2023-11-06 14:04:32,570:INFO:  Epoch 60/500:  train Loss: 32.5494   val Loss: 33.4102   time: 129.08s   best: 33.1762
2023-11-06 14:06:41,646:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:06:41,667:INFO:  Epoch 61/500:  train Loss: 32.2831   val Loss: 33.1356   time: 129.06s   best: 33.1356
2023-11-06 14:08:50,945:INFO:  Epoch 62/500:  train Loss: 32.2577   val Loss: 33.2771   time: 129.27s   best: 33.1356
2023-11-06 14:10:59,968:INFO:  Epoch 63/500:  train Loss: 32.0815   val Loss: 33.7313   time: 129.00s   best: 33.1356
2023-11-06 14:13:09,135:INFO:  Epoch 64/500:  train Loss: 32.1620   val Loss: 33.5240   time: 129.15s   best: 33.1356
2023-11-06 14:15:18,928:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:15:18,947:INFO:  Epoch 65/500:  train Loss: 31.7572   val Loss: 32.4501   time: 129.77s   best: 32.4501
2023-11-06 14:17:28,936:INFO:  Epoch 66/500:  train Loss: 31.4337   val Loss: 32.4838   time: 129.98s   best: 32.4501
2023-11-06 14:19:38,533:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:19:38,553:INFO:  Epoch 67/500:  train Loss: 31.4976   val Loss: 32.1336   time: 129.59s   best: 32.1336
2023-11-06 14:21:48,350:INFO:  Epoch 68/500:  train Loss: 31.2200   val Loss: 32.4123   time: 129.79s   best: 32.1336
2023-11-06 14:23:58,300:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:23:58,319:INFO:  Epoch 69/500:  train Loss: 31.1884   val Loss: 31.9186   time: 129.93s   best: 31.9186
2023-11-06 14:26:08,615:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:26:08,750:INFO:  Epoch 70/500:  train Loss: 31.0227   val Loss: 31.8418   time: 130.29s   best: 31.8418
2023-11-06 14:28:18,185:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:28:18,204:INFO:  Epoch 71/500:  train Loss: 30.8844   val Loss: 31.7865   time: 129.41s   best: 31.7865
2023-11-06 14:30:27,818:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:30:27,838:INFO:  Epoch 72/500:  train Loss: 30.6011   val Loss: 31.5477   time: 129.60s   best: 31.5477
2023-11-06 14:32:36,791:INFO:  Epoch 73/500:  train Loss: 30.4779   val Loss: 32.0509   time: 128.95s   best: 31.5477
2023-11-06 14:34:45,694:INFO:  Epoch 74/500:  train Loss: 30.5987   val Loss: 32.2202   time: 128.90s   best: 31.5477
2023-11-06 14:36:54,435:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:36:54,455:INFO:  Epoch 75/500:  train Loss: 31.0105   val Loss: 31.4362   time: 128.74s   best: 31.4362
2023-11-06 14:39:04,014:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:39:04,033:INFO:  Epoch 76/500:  train Loss: 30.2602   val Loss: 31.2349   time: 129.54s   best: 31.2349
2023-11-06 14:41:13,847:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:41:13,866:INFO:  Epoch 77/500:  train Loss: 30.0845   val Loss: 31.2320   time: 129.80s   best: 31.2320
2023-11-06 14:43:22,939:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:43:22,958:INFO:  Epoch 78/500:  train Loss: 30.1439   val Loss: 30.7434   time: 129.07s   best: 30.7434
2023-11-06 14:45:31,865:INFO:  Epoch 79/500:  train Loss: 29.9607   val Loss: 31.1055   time: 128.91s   best: 30.7434
2023-11-06 14:47:41,610:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:47:41,629:INFO:  Epoch 80/500:  train Loss: 29.9087   val Loss: 30.4076   time: 129.72s   best: 30.4076
2023-11-06 14:49:51,180:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 14:49:51,206:INFO:  Epoch 81/500:  train Loss: 29.5957   val Loss: 30.3427   time: 129.54s   best: 30.3427
2023-11-06 14:52:00,802:INFO:  Epoch 82/500:  train Loss: 29.5681   val Loss: 30.5825   time: 129.59s   best: 30.3427
2023-11-06 14:54:11,113:INFO:  Epoch 83/500:  train Loss: 29.4930   val Loss: 30.7096   time: 130.30s   best: 30.3427
2023-11-06 14:56:20,887:INFO:  Epoch 84/500:  train Loss: 29.3929   val Loss: 30.5414   time: 129.76s   best: 30.3427
2023-11-06 14:58:30,190:INFO:  Epoch 85/500:  train Loss: 29.3742   val Loss: 30.3958   time: 129.29s   best: 30.3427
2023-11-06 15:00:39,921:INFO:  Epoch 86/500:  train Loss: 29.2192   val Loss: 31.2646   time: 129.73s   best: 30.3427
2023-11-06 15:02:49,194:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 15:02:49,214:INFO:  Epoch 87/500:  train Loss: 29.2665   val Loss: 30.3148   time: 129.26s   best: 30.3148
2023-11-06 15:04:59,090:INFO:  Epoch 88/500:  train Loss: 29.0799   val Loss: 30.5760   time: 129.88s   best: 30.3148
2023-11-06 15:07:09,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 15:07:09,107:INFO:  Epoch 89/500:  train Loss: 28.8465   val Loss: 29.9747   time: 129.98s   best: 29.9747
2023-11-06 15:09:18,397:INFO:  Epoch 90/500:  train Loss: 28.8296   val Loss: 30.0392   time: 129.28s   best: 29.9747
2023-11-06 15:11:27,353:INFO:  Epoch 91/500:  train Loss: 28.7567   val Loss: 30.3422   time: 128.94s   best: 29.9747
2023-11-06 15:13:36,656:INFO:  Epoch 92/500:  train Loss: 28.6777   val Loss: 30.3681   time: 129.29s   best: 29.9747
2023-11-06 15:15:45,858:INFO:  Epoch 93/500:  train Loss: 28.6519   val Loss: 30.2750   time: 129.20s   best: 29.9747
2023-11-06 15:17:54,959:INFO:  Epoch 94/500:  train Loss: 28.6497   val Loss: 31.1824   time: 129.09s   best: 29.9747
2023-11-06 15:20:04,211:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 15:20:04,230:INFO:  Epoch 95/500:  train Loss: 28.3728   val Loss: 29.8024   time: 129.24s   best: 29.8024
2023-11-06 15:22:13,530:INFO:  Epoch 96/500:  train Loss: 28.3970   val Loss: 29.8904   time: 129.30s   best: 29.8024
2023-11-06 15:24:22,406:INFO:  Epoch 97/500:  train Loss: 28.1699   val Loss: 30.2885   time: 128.87s   best: 29.8024
2023-11-06 15:26:31,296:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 15:26:31,316:INFO:  Epoch 98/500:  train Loss: 28.2501   val Loss: 29.6729   time: 128.89s   best: 29.6729
2023-11-06 15:28:40,307:INFO:  Epoch 99/500:  train Loss: 28.0371   val Loss: 29.6783   time: 128.98s   best: 29.6729
2023-11-06 15:30:50,103:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 15:30:50,123:INFO:  Epoch 100/500:  train Loss: 28.2205   val Loss: 29.5762   time: 129.77s   best: 29.5762
2023-11-06 15:32:59,367:INFO:  Epoch 101/500:  train Loss: 28.3132   val Loss: 30.3485   time: 129.23s   best: 29.5762
2023-11-06 15:35:08,779:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 15:35:08,798:INFO:  Epoch 102/500:  train Loss: 27.9763   val Loss: 29.3936   time: 129.40s   best: 29.3936
2023-11-06 15:37:19,231:INFO:  Epoch 103/500:  train Loss: 27.9066   val Loss: 29.4308   time: 130.42s   best: 29.3936
2023-11-06 15:39:29,372:INFO:  Epoch 104/500:  train Loss: 27.6942   val Loss: 29.4137   time: 130.14s   best: 29.3936
2023-11-06 15:41:39,608:INFO:  Epoch 105/500:  train Loss: 27.8222   val Loss: 29.5777   time: 130.22s   best: 29.3936
2023-11-06 15:43:49,399:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 15:43:49,419:INFO:  Epoch 106/500:  train Loss: 27.5779   val Loss: 29.2775   time: 129.78s   best: 29.2775
2023-11-06 15:45:58,931:INFO:  Epoch 107/500:  train Loss: 27.6820   val Loss: 29.4457   time: 129.51s   best: 29.2775
2023-11-06 15:48:09,371:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 15:48:09,389:INFO:  Epoch 108/500:  train Loss: 27.4030   val Loss: 28.7998   time: 130.40s   best: 28.7998
2023-11-06 15:50:19,356:INFO:  Epoch 109/500:  train Loss: 27.3395   val Loss: 29.0696   time: 129.96s   best: 28.7998
2023-11-06 15:52:28,710:INFO:  Epoch 110/500:  train Loss: 27.4050   val Loss: 29.2444   time: 129.33s   best: 28.7998
2023-11-06 15:54:37,597:INFO:  Epoch 111/500:  train Loss: 27.5187   val Loss: 29.1230   time: 128.88s   best: 28.7998
2023-11-06 15:56:47,376:INFO:  Epoch 112/500:  train Loss: 27.1909   val Loss: 29.1495   time: 129.78s   best: 28.7998
2023-11-06 15:58:56,797:INFO:  Epoch 113/500:  train Loss: 27.2423   val Loss: 29.2228   time: 129.42s   best: 28.7998
2023-11-06 16:01:06,294:INFO:  Epoch 114/500:  train Loss: 27.2214   val Loss: 29.0075   time: 129.49s   best: 28.7998
2023-11-06 16:03:15,492:INFO:  Epoch 115/500:  train Loss: 27.0002   val Loss: 29.0241   time: 129.20s   best: 28.7998
2023-11-06 16:05:24,955:INFO:  Epoch 116/500:  train Loss: 26.9426   val Loss: 29.2398   time: 129.46s   best: 28.7998
2023-11-06 16:07:35,028:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 16:07:35,047:INFO:  Epoch 117/500:  train Loss: 27.0095   val Loss: 28.7467   time: 130.06s   best: 28.7467
2023-11-06 16:09:45,173:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 16:09:45,192:INFO:  Epoch 118/500:  train Loss: 26.8917   val Loss: 28.7222   time: 130.11s   best: 28.7222
2023-11-06 16:11:55,400:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 16:11:55,419:INFO:  Epoch 119/500:  train Loss: 26.7870   val Loss: 28.6731   time: 130.19s   best: 28.6731
2023-11-06 16:14:05,340:INFO:  Epoch 120/500:  train Loss: 26.8307   val Loss: 28.7930   time: 129.91s   best: 28.6731
2023-11-06 16:16:14,563:INFO:  Epoch 121/500:  train Loss: 26.6145   val Loss: 29.2594   time: 129.21s   best: 28.6731
2023-11-06 16:18:23,859:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 16:18:23,878:INFO:  Epoch 122/500:  train Loss: 26.9687   val Loss: 28.5136   time: 129.27s   best: 28.5136
2023-11-06 16:20:32,720:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 16:20:32,959:INFO:  Epoch 123/500:  train Loss: 26.7185   val Loss: 28.5031   time: 128.84s   best: 28.5031
2023-11-06 16:22:42,737:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 16:22:42,756:INFO:  Epoch 124/500:  train Loss: 26.5399   val Loss: 28.3422   time: 129.77s   best: 28.3422
2023-11-06 16:24:51,773:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 16:24:51,996:INFO:  Epoch 125/500:  train Loss: 26.5928   val Loss: 28.0293   time: 129.00s   best: 28.0293
2023-11-06 16:27:01,959:INFO:  Epoch 126/500:  train Loss: 26.6840   val Loss: 28.5272   time: 129.96s   best: 28.0293
2023-11-06 16:29:11,106:INFO:  Epoch 127/500:  train Loss: 26.3662   val Loss: 28.1102   time: 129.13s   best: 28.0293
2023-11-06 16:31:20,547:INFO:  Epoch 128/500:  train Loss: 26.4727   val Loss: 28.5642   time: 129.43s   best: 28.0293
2023-11-06 16:33:30,420:INFO:  Epoch 129/500:  train Loss: 26.2439   val Loss: 28.2986   time: 129.86s   best: 28.0293
2023-11-06 16:35:39,503:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 16:35:39,523:INFO:  Epoch 130/500:  train Loss: 26.1774   val Loss: 27.8833   time: 129.07s   best: 27.8833
2023-11-06 16:37:48,450:INFO:  Epoch 131/500:  train Loss: 26.0503   val Loss: 28.1230   time: 128.92s   best: 27.8833
2023-11-06 16:39:57,636:INFO:  Epoch 132/500:  train Loss: 26.2461   val Loss: 28.3289   time: 129.17s   best: 27.8833
2023-11-06 16:42:06,757:INFO:  Epoch 133/500:  train Loss: 26.1374   val Loss: 28.0132   time: 129.11s   best: 27.8833
2023-11-06 16:44:15,809:INFO:  Epoch 134/500:  train Loss: 25.9735   val Loss: 27.9420   time: 129.03s   best: 27.8833
2023-11-06 16:46:24,723:INFO:  Epoch 135/500:  train Loss: 26.0371   val Loss: 27.9273   time: 128.91s   best: 27.8833
2023-11-06 16:48:33,559:INFO:  Epoch 136/500:  train Loss: 25.7945   val Loss: 28.0913   time: 128.82s   best: 27.8833
2023-11-06 16:50:43,413:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 16:50:43,440:INFO:  Epoch 137/500:  train Loss: 25.8841   val Loss: 27.7506   time: 129.84s   best: 27.7506
2023-11-06 16:52:53,929:INFO:  Epoch 138/500:  train Loss: 25.7911   val Loss: 27.7759   time: 130.49s   best: 27.7506
2023-11-06 16:55:03,314:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 16:55:03,334:INFO:  Epoch 139/500:  train Loss: 25.7787   val Loss: 27.7428   time: 129.35s   best: 27.7428
2023-11-06 16:57:12,505:INFO:  Epoch 140/500:  train Loss: 25.7337   val Loss: 28.0611   time: 129.17s   best: 27.7428
2023-11-06 16:59:21,520:INFO:  Epoch 141/500:  train Loss: 25.6235   val Loss: 27.9319   time: 129.00s   best: 27.7428
2023-11-06 17:01:30,718:INFO:  Epoch 142/500:  train Loss: 25.6175   val Loss: 27.8297   time: 129.20s   best: 27.7428
2023-11-06 17:03:39,663:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 17:03:39,691:INFO:  Epoch 143/500:  train Loss: 25.6970   val Loss: 27.7412   time: 128.93s   best: 27.7412
2023-11-06 17:05:48,868:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 17:05:48,888:INFO:  Epoch 144/500:  train Loss: 25.5665   val Loss: 27.5846   time: 129.15s   best: 27.5846
2023-11-06 17:07:58,116:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 17:07:58,135:INFO:  Epoch 145/500:  train Loss: 25.4446   val Loss: 27.4201   time: 129.20s   best: 27.4201
2023-11-06 17:10:07,480:INFO:  Epoch 146/500:  train Loss: 25.3861   val Loss: 27.6224   time: 129.34s   best: 27.4201
2023-11-06 17:12:16,247:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 17:12:16,267:INFO:  Epoch 147/500:  train Loss: 25.5356   val Loss: 27.2933   time: 128.75s   best: 27.2933
2023-11-06 17:14:25,143:INFO:  Epoch 148/500:  train Loss: 25.4030   val Loss: 27.5592   time: 128.87s   best: 27.2933
2023-11-06 17:16:33,833:INFO:  Epoch 149/500:  train Loss: 25.3206   val Loss: 27.6107   time: 128.68s   best: 27.2933
2023-11-06 17:18:42,782:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 17:18:42,801:INFO:  Epoch 150/500:  train Loss: 25.3038   val Loss: 27.1175   time: 128.94s   best: 27.1175
2023-11-06 17:20:51,888:INFO:  Epoch 151/500:  train Loss: 25.2002   val Loss: 27.1416   time: 129.09s   best: 27.1175
2023-11-06 17:23:01,293:INFO:  Epoch 152/500:  train Loss: 25.2744   val Loss: 27.2898   time: 129.39s   best: 27.1175
2023-11-06 17:25:10,539:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 17:25:10,558:INFO:  Epoch 153/500:  train Loss: 25.1432   val Loss: 27.0083   time: 129.22s   best: 27.0083
2023-11-06 17:27:20,278:INFO:  Epoch 154/500:  train Loss: 25.2922   val Loss: 27.5584   time: 129.72s   best: 27.0083
2023-11-06 17:29:29,531:INFO:  Epoch 155/500:  train Loss: 24.9537   val Loss: 28.4427   time: 129.25s   best: 27.0083
2023-11-06 17:31:39,212:INFO:  Epoch 156/500:  train Loss: 25.9506   val Loss: 27.5679   time: 129.68s   best: 27.0083
2023-11-06 17:33:48,947:INFO:  Epoch 157/500:  train Loss: 25.0715   val Loss: 28.5119   time: 129.73s   best: 27.0083
2023-11-06 17:35:58,227:INFO:  Epoch 158/500:  train Loss: 24.9888   val Loss: 27.7154   time: 129.26s   best: 27.0083
2023-11-06 17:38:07,460:INFO:  Epoch 159/500:  train Loss: 24.9489   val Loss: 27.1114   time: 129.23s   best: 27.0083
2023-11-06 17:40:16,453:INFO:  Epoch 160/500:  train Loss: 24.9792   val Loss: 27.3029   time: 128.99s   best: 27.0083
2023-11-06 17:42:25,721:INFO:  Epoch 161/500:  train Loss: 24.8036   val Loss: 27.1472   time: 129.26s   best: 27.0083
2023-11-06 17:44:35,172:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 17:44:35,198:INFO:  Epoch 162/500:  train Loss: 24.8200   val Loss: 26.8745   time: 129.42s   best: 26.8745
2023-11-06 17:46:43,915:INFO:  Epoch 163/500:  train Loss: 24.8670   val Loss: 27.4070   time: 128.71s   best: 26.8745
2023-11-06 17:48:52,946:INFO:  Epoch 164/500:  train Loss: 24.6631   val Loss: 27.3385   time: 129.02s   best: 26.8745
2023-11-06 17:51:01,759:INFO:  Epoch 165/500:  train Loss: 24.7207   val Loss: 26.8803   time: 128.81s   best: 26.8745
2023-11-06 17:53:12,425:INFO:  Epoch 166/500:  train Loss: 25.1236   val Loss: 27.4144   time: 130.65s   best: 26.8745
2023-11-06 17:55:22,282:INFO:  Epoch 167/500:  train Loss: 25.6521   val Loss: 28.5248   time: 129.83s   best: 26.8745
2023-11-06 17:57:32,128:INFO:  Epoch 168/500:  train Loss: 24.8730   val Loss: 26.9580   time: 129.85s   best: 26.8745
2023-11-06 17:59:41,758:INFO:  Epoch 169/500:  train Loss: 24.8978   val Loss: 27.7938   time: 129.62s   best: 26.8745
2023-11-06 18:01:51,174:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 18:01:51,193:INFO:  Epoch 170/500:  train Loss: 24.6274   val Loss: 26.8232   time: 129.40s   best: 26.8232
2023-11-06 18:04:00,322:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 18:04:00,341:INFO:  Epoch 171/500:  train Loss: 24.5586   val Loss: 26.7289   time: 129.11s   best: 26.7289
2023-11-06 18:06:09,448:INFO:  Epoch 172/500:  train Loss: 24.6048   val Loss: 26.8117   time: 129.11s   best: 26.7289
2023-11-06 18:08:18,609:INFO:  Epoch 173/500:  train Loss: 24.3906   val Loss: 27.2858   time: 129.15s   best: 26.7289
2023-11-06 18:10:28,194:INFO:  Epoch 174/500:  train Loss: 24.8076   val Loss: 28.1787   time: 129.58s   best: 26.7289
2023-11-06 18:12:37,701:INFO:  Epoch 175/500:  train Loss: 24.4055   val Loss: 27.1111   time: 129.49s   best: 26.7289
2023-11-06 18:14:46,579:INFO:  Epoch 176/500:  train Loss: 24.2855   val Loss: 27.0257   time: 128.88s   best: 26.7289
2023-11-06 18:16:55,157:INFO:  Epoch 177/500:  train Loss: 24.4388   val Loss: 27.5679   time: 128.57s   best: 26.7289
2023-11-06 18:19:04,310:INFO:  Epoch 178/500:  train Loss: 24.2008   val Loss: 27.2206   time: 129.14s   best: 26.7289
2023-11-06 18:21:14,127:INFO:  Epoch 179/500:  train Loss: 24.2516   val Loss: 27.0576   time: 129.82s   best: 26.7289
2023-11-06 18:23:23,265:INFO:  Epoch 180/500:  train Loss: 24.1221   val Loss: 26.8769   time: 129.14s   best: 26.7289
2023-11-06 18:25:32,082:INFO:  Epoch 181/500:  train Loss: 24.2703   val Loss: 26.9201   time: 128.82s   best: 26.7289
2023-11-06 18:27:41,011:INFO:  Epoch 182/500:  train Loss: 25.0608   val Loss: 27.4445   time: 128.90s   best: 26.7289
2023-11-06 18:29:49,621:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 18:29:49,639:INFO:  Epoch 183/500:  train Loss: 24.7739   val Loss: 26.6110   time: 128.59s   best: 26.6110
2023-11-06 18:31:58,358:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 18:31:58,377:INFO:  Epoch 184/500:  train Loss: 24.0852   val Loss: 26.5674   time: 128.70s   best: 26.5674
2023-11-06 18:34:06,884:INFO:  Epoch 185/500:  train Loss: 24.1198   val Loss: 26.6119   time: 128.49s   best: 26.5674
2023-11-06 18:36:15,567:INFO:  Epoch 186/500:  train Loss: 24.2658   val Loss: 26.9583   time: 128.66s   best: 26.5674
2023-11-06 18:38:24,757:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 18:38:24,776:INFO:  Epoch 187/500:  train Loss: 23.9468   val Loss: 26.5141   time: 129.17s   best: 26.5141
2023-11-06 18:40:33,869:INFO:  Epoch 188/500:  train Loss: 24.0411   val Loss: 26.7926   time: 129.09s   best: 26.5141
2023-11-06 18:42:42,458:INFO:  Epoch 189/500:  train Loss: 24.0063   val Loss: 26.7531   time: 128.58s   best: 26.5141
2023-11-06 18:44:51,210:INFO:  Epoch 190/500:  train Loss: 24.2570   val Loss: 26.6573   time: 128.75s   best: 26.5141
2023-11-06 18:46:59,781:INFO:  Epoch 191/500:  train Loss: 23.7976   val Loss: 26.5893   time: 128.56s   best: 26.5141
2023-11-06 18:49:08,610:INFO:  Epoch 192/500:  train Loss: 23.8517   val Loss: 26.9558   time: 128.82s   best: 26.5141
2023-11-06 18:51:17,288:INFO:  Epoch 193/500:  train Loss: 23.7336   val Loss: 27.4042   time: 128.68s   best: 26.5141
2023-11-06 18:53:26,054:INFO:  Epoch 194/500:  train Loss: 23.9997   val Loss: 26.6042   time: 128.76s   best: 26.5141
2023-11-06 18:55:34,953:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 18:55:34,971:INFO:  Epoch 195/500:  train Loss: 23.6756   val Loss: 26.4082   time: 128.86s   best: 26.4082
2023-11-06 18:57:43,709:INFO:  Epoch 196/500:  train Loss: 23.7875   val Loss: 26.6275   time: 128.74s   best: 26.4082
2023-11-06 18:59:52,476:INFO:  Epoch 197/500:  train Loss: 23.7116   val Loss: 26.7030   time: 128.76s   best: 26.4082
2023-11-06 19:02:01,164:INFO:  Epoch 198/500:  train Loss: 23.7139   val Loss: 26.6251   time: 128.68s   best: 26.4082
2023-11-06 19:04:09,532:INFO:  Epoch 199/500:  train Loss: 23.6815   val Loss: 26.6972   time: 128.37s   best: 26.4082
2023-11-06 19:06:18,165:INFO:  Epoch 200/500:  train Loss: 23.6601   val Loss: 26.8627   time: 128.63s   best: 26.4082
2023-11-06 19:08:26,868:INFO:  Epoch 201/500:  train Loss: 23.7257   val Loss: 27.4605   time: 128.69s   best: 26.4082
2023-11-06 19:10:37,055:INFO:  Epoch 202/500:  train Loss: 23.5587   val Loss: 26.5754   time: 130.19s   best: 26.4082
2023-11-06 19:12:46,876:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 19:12:46,938:INFO:  Epoch 203/500:  train Loss: 23.5034   val Loss: 26.3415   time: 129.80s   best: 26.3415
2023-11-06 19:14:56,126:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 19:14:56,145:INFO:  Epoch 204/500:  train Loss: 23.5032   val Loss: 26.2725   time: 129.17s   best: 26.2725
2023-11-06 19:17:05,036:INFO:  Epoch 205/500:  train Loss: 23.6216   val Loss: 26.5646   time: 128.89s   best: 26.2725
2023-11-06 19:19:14,121:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 19:19:14,140:INFO:  Epoch 206/500:  train Loss: 23.3630   val Loss: 26.1485   time: 129.07s   best: 26.1485
2023-11-06 19:21:23,189:INFO:  Epoch 207/500:  train Loss: 23.3474   val Loss: 26.6810   time: 129.04s   best: 26.1485
2023-11-06 19:23:33,320:INFO:  Epoch 208/500:  train Loss: 23.6373   val Loss: 26.4037   time: 130.11s   best: 26.1485
2023-11-06 19:25:43,121:INFO:  Epoch 209/500:  train Loss: 23.2582   val Loss: 26.2174   time: 129.78s   best: 26.1485
2023-11-06 19:27:52,770:INFO:  Epoch 210/500:  train Loss: 23.5693   val Loss: 26.3191   time: 129.63s   best: 26.1485
2023-11-06 19:30:01,809:INFO:  Epoch 211/500:  train Loss: 23.2085   val Loss: 26.4093   time: 129.02s   best: 26.1485
2023-11-06 19:32:10,504:INFO:  Epoch 212/500:  train Loss: 23.3889   val Loss: 26.3204   time: 128.68s   best: 26.1485
2023-11-06 19:34:19,180:INFO:  Epoch 213/500:  train Loss: 23.4357   val Loss: 26.2348   time: 128.68s   best: 26.1485
2023-11-06 19:36:28,590:INFO:  Epoch 214/500:  train Loss: 23.3420   val Loss: 26.4782   time: 129.40s   best: 26.1485
2023-11-06 19:38:37,275:INFO:  Epoch 215/500:  train Loss: 23.4004   val Loss: 26.6298   time: 128.67s   best: 26.1485
2023-11-06 19:40:46,310:INFO:  Epoch 216/500:  train Loss: 23.8261   val Loss: 26.4657   time: 129.02s   best: 26.1485
2023-11-06 19:42:55,730:INFO:  Epoch 217/500:  train Loss: 23.2159   val Loss: 26.5280   time: 129.42s   best: 26.1485
2023-11-06 19:45:05,322:INFO:  Epoch 218/500:  train Loss: 23.1897   val Loss: 26.2341   time: 129.57s   best: 26.1485
2023-11-06 19:47:14,172:INFO:  Epoch 219/500:  train Loss: 23.1059   val Loss: 27.4846   time: 128.84s   best: 26.1485
2023-11-06 19:49:22,939:INFO:  Epoch 220/500:  train Loss: 23.0927   val Loss: 26.4709   time: 128.74s   best: 26.1485
2023-11-06 19:51:32,218:INFO:  Epoch 221/500:  train Loss: 23.0561   val Loss: 26.1896   time: 129.28s   best: 26.1485
2023-11-06 19:53:41,460:INFO:  Epoch 222/500:  train Loss: 23.2043   val Loss: 27.2981   time: 129.24s   best: 26.1485
2023-11-06 19:55:50,631:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 19:55:50,651:INFO:  Epoch 223/500:  train Loss: 23.0595   val Loss: 25.9665   time: 129.17s   best: 25.9665
2023-11-06 19:57:59,370:INFO:  Epoch 224/500:  train Loss: 22.9774   val Loss: 26.2464   time: 128.71s   best: 25.9665
2023-11-06 20:00:08,875:INFO:  Epoch 225/500:  train Loss: 22.9037   val Loss: 26.2621   time: 129.49s   best: 25.9665
2023-11-06 20:02:17,519:INFO:  Epoch 226/500:  train Loss: 22.9001   val Loss: 26.1296   time: 128.63s   best: 25.9665
2023-11-06 20:04:26,434:INFO:  Epoch 227/500:  train Loss: 23.0605   val Loss: 26.1135   time: 128.90s   best: 25.9665
2023-11-06 20:06:35,421:INFO:  Epoch 228/500:  train Loss: 22.7784   val Loss: 25.9768   time: 128.99s   best: 25.9665
2023-11-06 20:08:44,387:INFO:  Epoch 229/500:  train Loss: 22.8576   val Loss: 26.1910   time: 128.95s   best: 25.9665
2023-11-06 20:10:53,500:INFO:  Epoch 230/500:  train Loss: 22.9936   val Loss: 26.2227   time: 129.11s   best: 25.9665
2023-11-06 20:13:02,632:INFO:  Epoch 231/500:  train Loss: 22.6929   val Loss: 26.2356   time: 129.12s   best: 25.9665
2023-11-06 20:15:11,555:INFO:  Epoch 232/500:  train Loss: 22.9235   val Loss: 27.5776   time: 128.90s   best: 25.9665
2023-11-06 20:17:20,529:INFO:  Epoch 233/500:  train Loss: 23.4634   val Loss: 26.2943   time: 128.96s   best: 25.9665
2023-11-06 20:19:30,055:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 20:19:30,077:INFO:  Epoch 234/500:  train Loss: 22.8111   val Loss: 25.9614   time: 129.52s   best: 25.9614
2023-11-06 20:21:39,969:INFO:  Epoch 235/500:  train Loss: 22.8423   val Loss: 26.0629   time: 129.89s   best: 25.9614
2023-11-06 20:23:49,976:INFO:  Epoch 236/500:  train Loss: 22.7291   val Loss: 26.3381   time: 130.00s   best: 25.9614
2023-11-06 20:26:00,379:INFO:  Epoch 237/500:  train Loss: 22.6596   val Loss: 26.2541   time: 130.39s   best: 25.9614
2023-11-06 20:28:09,993:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 20:28:10,012:INFO:  Epoch 238/500:  train Loss: 22.6678   val Loss: 25.9128   time: 129.61s   best: 25.9128
2023-11-06 20:30:18,700:INFO:  Epoch 239/500:  train Loss: 23.0015   val Loss: 26.0770   time: 128.68s   best: 25.9128
2023-11-06 20:32:28,454:INFO:  Epoch 240/500:  train Loss: 22.5247   val Loss: 26.2249   time: 129.74s   best: 25.9128
2023-11-06 20:34:37,003:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 20:34:37,022:INFO:  Epoch 241/500:  train Loss: 22.6293   val Loss: 25.6794   time: 128.53s   best: 25.6794
2023-11-06 20:36:45,995:INFO:  Epoch 242/500:  train Loss: 22.5551   val Loss: 26.1035   time: 128.97s   best: 25.6794
2023-11-06 20:38:55,388:INFO:  Epoch 243/500:  train Loss: 22.4805   val Loss: 26.5202   time: 129.38s   best: 25.6794
2023-11-06 20:41:05,165:INFO:  Epoch 244/500:  train Loss: 22.7223   val Loss: 26.1728   time: 129.75s   best: 25.6794
2023-11-06 20:43:14,036:INFO:  Epoch 245/500:  train Loss: 22.5393   val Loss: 26.2897   time: 128.87s   best: 25.6794
2023-11-06 20:45:23,546:INFO:  Epoch 246/500:  train Loss: 22.4106   val Loss: 26.1626   time: 129.51s   best: 25.6794
2023-11-06 20:47:32,593:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 20:47:32,612:INFO:  Epoch 247/500:  train Loss: 22.8147   val Loss: 25.5619   time: 129.04s   best: 25.5619
2023-11-06 20:49:41,875:INFO:  Epoch 248/500:  train Loss: 22.4033   val Loss: 25.9215   time: 129.24s   best: 25.5619
2023-11-06 20:51:50,820:INFO:  Epoch 249/500:  train Loss: 22.4390   val Loss: 26.0780   time: 128.94s   best: 25.5619
2023-11-06 20:53:59,588:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 20:53:59,616:INFO:  Epoch 250/500:  train Loss: 22.3917   val Loss: 25.5387   time: 128.76s   best: 25.5387
2023-11-06 20:56:08,300:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 20:56:08,319:INFO:  Epoch 251/500:  train Loss: 22.3349   val Loss: 25.4293   time: 128.67s   best: 25.4293
2023-11-06 20:58:16,999:INFO:  Epoch 252/500:  train Loss: 22.5207   val Loss: 25.7964   time: 128.67s   best: 25.4293
2023-11-06 21:00:25,606:INFO:  Epoch 253/500:  train Loss: 22.3026   val Loss: 25.7332   time: 128.61s   best: 25.4293
2023-11-06 21:02:34,787:INFO:  Epoch 254/500:  train Loss: 22.4064   val Loss: 25.4697   time: 129.18s   best: 25.4293
2023-11-06 21:04:43,604:INFO:  Epoch 255/500:  train Loss: 22.2648   val Loss: 25.8090   time: 128.79s   best: 25.4293
2023-11-06 21:06:52,659:INFO:  Epoch 256/500:  train Loss: 22.3618   val Loss: 25.8016   time: 129.05s   best: 25.4293
2023-11-06 21:09:01,395:INFO:  Epoch 257/500:  train Loss: 22.2750   val Loss: 25.6722   time: 128.74s   best: 25.4293
2023-11-06 21:11:10,957:INFO:  Epoch 258/500:  train Loss: 22.3938   val Loss: 26.2323   time: 129.56s   best: 25.4293
2023-11-06 21:13:19,612:INFO:  Epoch 259/500:  train Loss: 22.1569   val Loss: 25.6588   time: 128.65s   best: 25.4293
2023-11-06 21:15:29,934:INFO:  Epoch 260/500:  train Loss: 22.1999   val Loss: 25.6902   time: 130.32s   best: 25.4293
2023-11-06 21:17:39,440:INFO:  Epoch 261/500:  train Loss: 22.4111   val Loss: 25.8721   time: 129.51s   best: 25.4293
2023-11-06 21:19:48,220:INFO:  Epoch 262/500:  train Loss: 22.3077   val Loss: 25.9689   time: 128.77s   best: 25.4293
2023-11-06 21:21:56,649:INFO:  Epoch 263/500:  train Loss: 22.0949   val Loss: 25.7615   time: 128.42s   best: 25.4293
2023-11-06 21:24:05,801:INFO:  Epoch 264/500:  train Loss: 22.2330   val Loss: 26.7891   time: 129.12s   best: 25.4293
2023-11-06 21:26:14,407:INFO:  Epoch 265/500:  train Loss: 22.1355   val Loss: 25.9189   time: 128.60s   best: 25.4293
2023-11-06 21:28:23,568:INFO:  Epoch 266/500:  train Loss: 22.0686   val Loss: 25.9440   time: 129.15s   best: 25.4293
2023-11-06 21:30:32,707:INFO:  Epoch 267/500:  train Loss: 22.1107   val Loss: 25.7772   time: 129.13s   best: 25.4293
2023-11-06 21:32:41,574:INFO:  Epoch 268/500:  train Loss: 22.1273   val Loss: 25.6401   time: 128.86s   best: 25.4293
2023-11-06 21:34:50,317:INFO:  Epoch 269/500:  train Loss: 22.0528   val Loss: 25.7975   time: 128.74s   best: 25.4293
2023-11-06 21:36:59,567:INFO:  Epoch 270/500:  train Loss: 21.9879   val Loss: 27.1144   time: 129.24s   best: 25.4293
2023-11-06 21:39:08,420:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 21:39:08,439:INFO:  Epoch 271/500:  train Loss: 22.0045   val Loss: 25.3500   time: 128.84s   best: 25.3500
2023-11-06 21:41:17,325:INFO:  Epoch 272/500:  train Loss: 22.4033   val Loss: 25.9044   time: 128.88s   best: 25.3500
2023-11-06 21:43:26,262:INFO:  Epoch 273/500:  train Loss: 21.9568   val Loss: 25.6348   time: 128.93s   best: 25.3500
2023-11-06 21:45:35,060:INFO:  Epoch 274/500:  train Loss: 22.2364   val Loss: 26.6375   time: 128.79s   best: 25.3500
2023-11-06 21:47:44,086:INFO:  Epoch 275/500:  train Loss: 22.0210   val Loss: 25.9202   time: 129.00s   best: 25.3500
2023-11-06 21:49:53,786:INFO:  Epoch 276/500:  train Loss: 21.8792   val Loss: 25.6025   time: 129.69s   best: 25.3500
2023-11-06 21:52:02,867:INFO:  Epoch 277/500:  train Loss: 22.7475   val Loss: 26.1284   time: 129.08s   best: 25.3500
2023-11-06 21:54:12,738:INFO:  Epoch 278/500:  train Loss: 22.0022   val Loss: 25.7996   time: 129.87s   best: 25.3500
2023-11-06 21:56:21,675:INFO:  Epoch 279/500:  train Loss: 21.9830   val Loss: 25.6593   time: 128.94s   best: 25.3500
2023-11-06 21:58:30,757:INFO:  Epoch 280/500:  train Loss: 21.8362   val Loss: 25.8183   time: 129.07s   best: 25.3500
2023-11-06 22:00:39,957:INFO:  Epoch 281/500:  train Loss: 21.9024   val Loss: 25.6076   time: 129.19s   best: 25.3500
2023-11-06 22:02:49,096:INFO:  Epoch 282/500:  train Loss: 21.7919   val Loss: 26.0502   time: 129.12s   best: 25.3500
2023-11-06 22:04:58,853:INFO:  Epoch 283/500:  train Loss: 21.9580   val Loss: 25.8723   time: 129.74s   best: 25.3500
2023-11-06 22:07:08,784:INFO:  Epoch 284/500:  train Loss: 22.0143   val Loss: 25.3950   time: 129.93s   best: 25.3500
2023-11-06 22:09:17,895:INFO:  Epoch 285/500:  train Loss: 22.3739   val Loss: 25.9548   time: 129.11s   best: 25.3500
2023-11-06 22:11:26,905:INFO:  Epoch 286/500:  train Loss: 21.7074   val Loss: 25.7394   time: 129.00s   best: 25.3500
2023-11-06 22:13:35,826:INFO:  Epoch 287/500:  train Loss: 21.7587   val Loss: 25.9729   time: 128.92s   best: 25.3500
2023-11-06 22:15:45,032:INFO:  Epoch 288/500:  train Loss: 22.2285   val Loss: 25.6503   time: 129.18s   best: 25.3500
2023-11-06 22:17:53,942:INFO:  Epoch 289/500:  train Loss: 21.7693   val Loss: 25.4110   time: 128.90s   best: 25.3500
2023-11-06 22:20:03,092:INFO:  Epoch 290/500:  train Loss: 21.7041   val Loss: 25.5837   time: 129.13s   best: 25.3500
2023-11-06 22:22:12,182:INFO:  Epoch 291/500:  train Loss: 21.6957   val Loss: 25.5215   time: 129.08s   best: 25.3500
2023-11-06 22:24:20,877:INFO:  Epoch 292/500:  train Loss: 21.7741   val Loss: 25.6980   time: 128.67s   best: 25.3500
2023-11-06 22:26:30,101:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 22:26:30,121:INFO:  Epoch 293/500:  train Loss: 21.7331   val Loss: 25.2952   time: 129.21s   best: 25.2952
2023-11-06 22:28:40,134:INFO:  Epoch 294/500:  train Loss: 21.5832   val Loss: 25.7563   time: 130.00s   best: 25.2952
2023-11-06 22:30:48,640:INFO:  Epoch 295/500:  train Loss: 21.7296   val Loss: 25.9995   time: 128.49s   best: 25.2952
2023-11-06 22:32:58,211:INFO:  Epoch 296/500:  train Loss: 22.0041   val Loss: 25.5999   time: 129.57s   best: 25.2952
2023-11-06 22:35:06,890:INFO:  Epoch 297/500:  train Loss: 21.6424   val Loss: 25.9225   time: 128.68s   best: 25.2952
2023-11-06 22:37:15,912:INFO:  Epoch 298/500:  train Loss: 21.7825   val Loss: 25.6810   time: 129.02s   best: 25.2952
2023-11-06 22:39:25,566:INFO:  Epoch 299/500:  train Loss: 21.6138   val Loss: 25.7911   time: 129.64s   best: 25.2952
2023-11-06 22:41:35,486:INFO:  Epoch 300/500:  train Loss: 21.6327   val Loss: 25.6099   time: 129.92s   best: 25.2952
2023-11-06 22:43:44,299:INFO:  Epoch 301/500:  train Loss: 21.5189   val Loss: 25.6980   time: 128.79s   best: 25.2952
2023-11-06 22:45:53,165:INFO:  Epoch 302/500:  train Loss: 21.5128   val Loss: 25.7352   time: 128.85s   best: 25.2952
2023-11-06 22:48:02,523:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 22:48:02,543:INFO:  Epoch 303/500:  train Loss: 21.7790   val Loss: 25.2728   time: 129.34s   best: 25.2728
2023-11-06 22:50:11,168:INFO:  Epoch 304/500:  train Loss: 21.6153   val Loss: 25.9245   time: 128.62s   best: 25.2728
2023-11-06 22:52:19,866:INFO:  Epoch 305/500:  train Loss: 21.5206   val Loss: 25.7159   time: 128.70s   best: 25.2728
2023-11-06 22:54:28,905:INFO:  Epoch 306/500:  train Loss: 21.7676   val Loss: 25.9224   time: 129.03s   best: 25.2728
2023-11-06 22:56:38,073:INFO:  Epoch 307/500:  train Loss: 21.4619   val Loss: 26.0282   time: 129.16s   best: 25.2728
2023-11-06 22:58:46,586:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 22:58:46,606:INFO:  Epoch 308/500:  train Loss: 21.4298   val Loss: 25.0265   time: 128.50s   best: 25.0265
2023-11-06 23:00:55,119:INFO:  Epoch 309/500:  train Loss: 21.4708   val Loss: 25.4397   time: 128.50s   best: 25.0265
2023-11-06 23:03:03,977:INFO:  Epoch 310/500:  train Loss: 21.4385   val Loss: 25.7555   time: 128.83s   best: 25.0265
2023-11-06 23:05:12,461:INFO:  Epoch 311/500:  train Loss: 21.4097   val Loss: 25.4458   time: 128.46s   best: 25.0265
2023-11-06 23:07:22,439:INFO:  Epoch 312/500:  train Loss: 21.3383   val Loss: 26.5053   time: 129.97s   best: 25.0265
2023-11-06 23:09:32,029:INFO:  Epoch 313/500:  train Loss: 21.5170   val Loss: 25.3067   time: 129.58s   best: 25.0265
2023-11-06 23:11:41,785:INFO:  Epoch 314/500:  train Loss: 21.6205   val Loss: 25.3870   time: 129.74s   best: 25.0265
2023-11-06 23:13:50,391:INFO:  Epoch 315/500:  train Loss: 21.5265   val Loss: 25.3981   time: 128.59s   best: 25.0265
2023-11-06 23:16:00,127:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 23:16:00,147:INFO:  Epoch 316/500:  train Loss: 21.3217   val Loss: 24.9984   time: 129.73s   best: 24.9984
2023-11-06 23:18:08,482:INFO:  Epoch 317/500:  train Loss: 21.4518   val Loss: 25.3943   time: 128.33s   best: 24.9984
2023-11-06 23:20:17,064:INFO:  Epoch 318/500:  train Loss: 21.3416   val Loss: 26.0275   time: 128.57s   best: 24.9984
2023-11-06 23:22:25,657:INFO:  Epoch 319/500:  train Loss: 21.5238   val Loss: 25.5961   time: 128.56s   best: 24.9984
2023-11-06 23:24:34,321:INFO:  Epoch 320/500:  train Loss: 21.2116   val Loss: 25.4549   time: 128.64s   best: 24.9984
2023-11-06 23:26:42,622:INFO:  Epoch 321/500:  train Loss: 21.4312   val Loss: 25.2605   time: 128.30s   best: 24.9984
2023-11-06 23:28:51,237:INFO:  Epoch 322/500:  train Loss: 21.3399   val Loss: 25.0882   time: 128.60s   best: 24.9984
2023-11-06 23:31:00,637:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-06 23:31:00,656:INFO:  Epoch 323/500:  train Loss: 21.2590   val Loss: 24.6692   time: 129.38s   best: 24.6692
2023-11-06 23:33:09,432:INFO:  Epoch 324/500:  train Loss: 21.2210   val Loss: 25.1611   time: 128.76s   best: 24.6692
2023-11-06 23:35:17,641:INFO:  Epoch 325/500:  train Loss: 21.3485   val Loss: 25.7205   time: 128.21s   best: 24.6692
2023-11-06 23:37:25,798:INFO:  Epoch 326/500:  train Loss: 21.3242   val Loss: 25.1731   time: 128.14s   best: 24.6692
2023-11-06 23:39:33,859:INFO:  Epoch 327/500:  train Loss: 21.4520   val Loss: 25.2865   time: 128.04s   best: 24.6692
2023-11-06 23:41:41,575:INFO:  Epoch 328/500:  train Loss: 21.4765   val Loss: 25.0711   time: 127.72s   best: 24.6692
2023-11-06 23:43:49,403:INFO:  Epoch 329/500:  train Loss: 21.2643   val Loss: 26.2298   time: 127.82s   best: 24.6692
2023-11-06 23:45:58,033:INFO:  Epoch 330/500:  train Loss: 21.2617   val Loss: 25.2454   time: 128.62s   best: 24.6692
2023-11-06 23:48:05,575:INFO:  Epoch 331/500:  train Loss: 21.1035   val Loss: 25.3422   time: 127.54s   best: 24.6692
2023-11-06 23:50:13,227:INFO:  Epoch 332/500:  train Loss: 21.3247   val Loss: 25.0109   time: 127.64s   best: 24.6692
2023-11-06 23:52:20,573:INFO:  Epoch 333/500:  train Loss: 21.1283   val Loss: 25.0593   time: 127.35s   best: 24.6692
2023-11-06 23:54:28,678:INFO:  Epoch 334/500:  train Loss: 21.1330   val Loss: 25.3189   time: 128.09s   best: 24.6692
2023-11-06 23:56:36,385:INFO:  Epoch 335/500:  train Loss: 21.0885   val Loss: 25.5544   time: 127.71s   best: 24.6692
2023-11-06 23:58:45,061:INFO:  Epoch 336/500:  train Loss: 21.2943   val Loss: 25.2836   time: 128.66s   best: 24.6692
2023-11-07 00:00:52,972:INFO:  Epoch 337/500:  train Loss: 21.2627   val Loss: 25.4332   time: 127.91s   best: 24.6692
2023-11-07 00:03:01,626:INFO:  Epoch 338/500:  train Loss: 21.1541   val Loss: 25.4636   time: 128.65s   best: 24.6692
2023-11-07 00:05:09,067:INFO:  Epoch 339/500:  train Loss: 21.0957   val Loss: 25.1578   time: 127.42s   best: 24.6692
2023-11-07 00:07:16,553:INFO:  Epoch 340/500:  train Loss: 21.1549   val Loss: 25.4222   time: 127.47s   best: 24.6692
2023-11-07 00:09:24,302:INFO:  Epoch 341/500:  train Loss: 21.7686   val Loss: 25.8003   time: 127.74s   best: 24.6692
2023-11-07 00:11:32,836:INFO:  Epoch 342/500:  train Loss: 21.1565   val Loss: 25.7322   time: 128.53s   best: 24.6692
2023-11-07 00:13:41,187:INFO:  Epoch 343/500:  train Loss: 21.0217   val Loss: 25.3617   time: 128.34s   best: 24.6692
2023-11-07 00:15:49,503:INFO:  Epoch 344/500:  train Loss: 21.0078   val Loss: 25.7963   time: 128.29s   best: 24.6692
2023-11-07 00:17:56,934:INFO:  Epoch 345/500:  train Loss: 20.9782   val Loss: 25.0060   time: 127.42s   best: 24.6692
2023-11-07 00:20:04,597:INFO:  Epoch 346/500:  train Loss: 21.0262   val Loss: 25.4019   time: 127.64s   best: 24.6692
2023-11-07 00:22:12,230:INFO:  Epoch 347/500:  train Loss: 20.9809   val Loss: 24.7584   time: 127.62s   best: 24.6692
2023-11-07 00:24:19,859:INFO:  Epoch 348/500:  train Loss: 20.9295   val Loss: 25.3279   time: 127.62s   best: 24.6692
2023-11-07 00:26:27,175:INFO:  Epoch 349/500:  train Loss: 21.1763   val Loss: 25.0968   time: 127.29s   best: 24.6692
2023-11-07 00:28:34,632:INFO:  Epoch 350/500:  train Loss: 21.9440   val Loss: 25.2159   time: 127.45s   best: 24.6692
2023-11-07 00:30:41,997:INFO:  Epoch 351/500:  train Loss: 20.9732   val Loss: 25.3202   time: 127.35s   best: 24.6692
2023-11-07 00:32:49,780:INFO:  Epoch 352/500:  train Loss: 20.9839   val Loss: 24.7655   time: 127.78s   best: 24.6692
2023-11-07 00:34:57,274:INFO:  Epoch 353/500:  train Loss: 20.8893   val Loss: 25.4201   time: 127.48s   best: 24.6692
2023-11-07 00:37:05,692:INFO:  Epoch 354/500:  train Loss: 20.9277   val Loss: 25.1263   time: 128.41s   best: 24.6692
2023-11-07 00:39:13,274:INFO:  Epoch 355/500:  train Loss: 21.8022   val Loss: 25.7391   time: 127.58s   best: 24.6692
2023-11-07 00:41:20,913:INFO:  Epoch 356/500:  train Loss: 21.0226   val Loss: 24.7856   time: 127.62s   best: 24.6692
2023-11-07 00:43:28,668:INFO:  Epoch 357/500:  train Loss: 20.9850   val Loss: 25.5716   time: 127.75s   best: 24.6692
2023-11-07 00:45:36,729:INFO:  Epoch 358/500:  train Loss: 20.9129   val Loss: 24.9703   time: 128.06s   best: 24.6692
2023-11-07 00:47:44,374:INFO:  Epoch 359/500:  train Loss: 20.8105   val Loss: 25.4352   time: 127.64s   best: 24.6692
2023-11-07 00:49:52,317:INFO:  Epoch 360/500:  train Loss: 20.9217   val Loss: 25.3091   time: 127.93s   best: 24.6692
2023-11-07 00:52:00,661:INFO:  Epoch 361/500:  train Loss: 21.0419   val Loss: 25.0207   time: 128.32s   best: 24.6692
2023-11-07 00:54:08,717:INFO:  Epoch 362/500:  train Loss: 21.1000   val Loss: 25.0131   time: 128.04s   best: 24.6692
2023-11-07 00:56:17,160:INFO:  Epoch 363/500:  train Loss: 20.8173   val Loss: 25.9193   time: 128.43s   best: 24.6692
2023-11-07 00:58:25,807:INFO:  Epoch 364/500:  train Loss: 20.7883   val Loss: 24.9766   time: 128.64s   best: 24.6692
2023-11-07 01:00:33,603:INFO:  Epoch 365/500:  train Loss: 21.1941   val Loss: 27.5335   time: 127.78s   best: 24.6692
2023-11-07 01:02:41,100:INFO:  Epoch 366/500:  train Loss: 20.9318   val Loss: 25.1035   time: 127.48s   best: 24.6692
2023-11-07 01:04:48,469:INFO:  Epoch 367/500:  train Loss: 20.7643   val Loss: 25.2109   time: 127.36s   best: 24.6692
2023-11-07 01:06:56,325:INFO:  Epoch 368/500:  train Loss: 21.0446   val Loss: 24.9207   time: 127.86s   best: 24.6692
2023-11-07 01:09:03,725:INFO:  Epoch 369/500:  train Loss: 20.7098   val Loss: 24.8687   time: 127.38s   best: 24.6692
2023-11-07 01:11:11,280:INFO:  Epoch 370/500:  train Loss: 20.8456   val Loss: 24.9940   time: 127.55s   best: 24.6692
2023-11-07 01:13:18,751:INFO:  Epoch 371/500:  train Loss: 20.8711   val Loss: 25.4935   time: 127.46s   best: 24.6692
2023-11-07 01:15:26,276:INFO:  Epoch 372/500:  train Loss: 20.7379   val Loss: 25.0560   time: 127.51s   best: 24.6692
2023-11-07 01:17:33,917:INFO:  Epoch 373/500:  train Loss: 20.7577   val Loss: 24.9590   time: 127.63s   best: 24.6692
2023-11-07 01:19:42,661:INFO:  Epoch 374/500:  train Loss: 20.7216   val Loss: 24.7289   time: 128.74s   best: 24.6692
2023-11-07 01:21:50,409:INFO:  Epoch 375/500:  train Loss: 20.9786   val Loss: 25.3892   time: 127.75s   best: 24.6692
2023-11-07 01:23:58,498:INFO:  Epoch 376/500:  train Loss: 20.6966   val Loss: 25.3610   time: 128.09s   best: 24.6692
2023-11-07 01:26:06,154:INFO:  Epoch 377/500:  train Loss: 21.0249   val Loss: 24.8674   time: 127.64s   best: 24.6692
2023-11-07 01:28:14,123:INFO:  Epoch 378/500:  train Loss: 21.7028   val Loss: 27.1483   time: 127.96s   best: 24.6692
2023-11-07 01:30:22,714:INFO:  Epoch 379/500:  train Loss: 21.5382   val Loss: 24.8891   time: 128.58s   best: 24.6692
2023-11-07 01:32:31,681:INFO:  Epoch 380/500:  train Loss: 20.7211   val Loss: 24.9241   time: 128.96s   best: 24.6692
2023-11-07 01:34:39,025:INFO:  Epoch 381/500:  train Loss: 20.7826   val Loss: 25.0372   time: 127.33s   best: 24.6692
2023-11-07 01:36:46,421:INFO:  Epoch 382/500:  train Loss: 20.6559   val Loss: 24.8699   time: 127.38s   best: 24.6692
2023-11-07 01:38:54,975:INFO:  Epoch 383/500:  train Loss: 20.7403   val Loss: 25.1683   time: 128.54s   best: 24.6692
2023-11-07 01:41:02,855:INFO:  Epoch 384/500:  train Loss: 21.0045   val Loss: 24.7654   time: 127.86s   best: 24.6692
2023-11-07 01:43:10,612:INFO:  Epoch 385/500:  train Loss: 20.7041   val Loss: 25.3662   time: 127.76s   best: 24.6692
2023-11-07 01:45:18,403:INFO:  Epoch 386/500:  train Loss: 20.7197   val Loss: 25.1157   time: 127.79s   best: 24.6692
2023-11-07 01:47:26,067:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-07 01:47:26,099:INFO:  Epoch 387/500:  train Loss: 20.7664   val Loss: 24.5559   time: 127.65s   best: 24.5559
2023-11-07 01:49:33,812:INFO:  Epoch 388/500:  train Loss: 20.8072   val Loss: 24.8606   time: 127.70s   best: 24.5559
2023-11-07 01:51:41,582:INFO:  Epoch 389/500:  train Loss: 20.5958   val Loss: 25.3458   time: 127.77s   best: 24.5559
2023-11-07 01:53:49,437:INFO:  Epoch 390/500:  train Loss: 20.6382   val Loss: 24.9643   time: 127.85s   best: 24.5559
2023-11-07 01:55:57,064:INFO:  Epoch 391/500:  train Loss: 20.8245   val Loss: 25.0433   time: 127.60s   best: 24.5559
2023-11-07 01:58:05,046:INFO:  Epoch 392/500:  train Loss: 20.7389   val Loss: 26.5594   time: 127.97s   best: 24.5559
2023-11-07 02:00:12,623:INFO:  Epoch 393/500:  train Loss: 20.5860   val Loss: 25.0088   time: 127.56s   best: 24.5559
2023-11-07 02:02:20,182:INFO:  Epoch 394/500:  train Loss: 20.8281   val Loss: 24.9593   time: 127.56s   best: 24.5559
2023-11-07 02:04:27,493:INFO:  Epoch 395/500:  train Loss: 20.4768   val Loss: 24.8978   time: 127.30s   best: 24.5559
2023-11-07 02:06:35,048:INFO:  Epoch 396/500:  train Loss: 20.6028   val Loss: 25.1067   time: 127.55s   best: 24.5559
2023-11-07 02:08:42,549:INFO:  Epoch 397/500:  train Loss: 20.6025   val Loss: 24.6727   time: 127.49s   best: 24.5559
2023-11-07 02:10:50,634:INFO:  Epoch 398/500:  train Loss: 20.4914   val Loss: 25.2674   time: 128.07s   best: 24.5559
2023-11-07 02:12:58,025:INFO:  Epoch 399/500:  train Loss: 20.5259   val Loss: 24.9978   time: 127.39s   best: 24.5559
2023-11-07 02:15:06,088:INFO:  Epoch 400/500:  train Loss: 20.5324   val Loss: 24.6780   time: 128.04s   best: 24.5559
2023-11-07 02:17:13,595:INFO:  Epoch 401/500:  train Loss: 20.5491   val Loss: 24.8460   time: 127.48s   best: 24.5559
2023-11-07 02:19:21,735:INFO:  Epoch 402/500:  train Loss: 20.4330   val Loss: 24.9797   time: 128.13s   best: 24.5559
2023-11-07 02:21:29,332:INFO:  Epoch 403/500:  train Loss: 20.7444   val Loss: 24.7730   time: 127.57s   best: 24.5559
2023-11-07 02:23:36,957:INFO:  Epoch 404/500:  train Loss: 20.3536   val Loss: 25.3452   time: 127.61s   best: 24.5559
2023-11-07 02:25:44,183:INFO:  Epoch 405/500:  train Loss: 20.6869   val Loss: 24.9634   time: 127.21s   best: 24.5559
2023-11-07 02:27:52,683:INFO:  Epoch 406/500:  train Loss: 20.3224   val Loss: 25.1851   time: 128.49s   best: 24.5559
2023-11-07 02:30:00,102:INFO:  Epoch 407/500:  train Loss: 20.5741   val Loss: 24.9372   time: 127.42s   best: 24.5559
2023-11-07 02:32:07,682:INFO:  Epoch 408/500:  train Loss: 20.5323   val Loss: 25.2134   time: 127.57s   best: 24.5559
2023-11-07 02:34:15,025:INFO:  Epoch 409/500:  train Loss: 20.3285   val Loss: 24.7586   time: 127.33s   best: 24.5559
2023-11-07 02:36:23,038:INFO:  Epoch 410/500:  train Loss: 20.4408   val Loss: 25.0563   time: 128.01s   best: 24.5559
2023-11-07 02:38:31,279:INFO:  Epoch 411/500:  train Loss: 21.0810   val Loss: 25.1318   time: 128.23s   best: 24.5559
2023-11-07 02:40:39,649:INFO:  Epoch 412/500:  train Loss: 20.3433   val Loss: 25.2793   time: 128.35s   best: 24.5559
2023-11-07 02:42:46,983:INFO:  Epoch 413/500:  train Loss: 20.5834   val Loss: 25.3430   time: 127.32s   best: 24.5559
2023-11-07 02:44:54,624:INFO:  Epoch 414/500:  train Loss: 20.3697   val Loss: 24.9362   time: 127.63s   best: 24.5559
2023-11-07 02:47:02,081:INFO:  Epoch 415/500:  train Loss: 20.3597   val Loss: 24.8253   time: 127.44s   best: 24.5559
2023-11-07 02:49:09,542:INFO:  Epoch 416/500:  train Loss: 20.8431   val Loss: 25.1098   time: 127.46s   best: 24.5559
2023-11-07 02:51:17,119:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-07 02:51:17,139:INFO:  Epoch 417/500:  train Loss: 20.2472   val Loss: 24.4241   time: 127.56s   best: 24.4241
2023-11-07 02:53:25,106:INFO:  Epoch 418/500:  train Loss: 20.3614   val Loss: 24.7874   time: 127.94s   best: 24.4241
2023-11-07 02:55:32,315:INFO:  Epoch 419/500:  train Loss: 20.3334   val Loss: 24.7832   time: 127.21s   best: 24.4241
2023-11-07 02:57:39,844:INFO:  Epoch 420/500:  train Loss: 20.2637   val Loss: 25.2344   time: 127.53s   best: 24.4241
2023-11-07 02:59:47,616:INFO:  Epoch 421/500:  train Loss: 20.3154   val Loss: 25.2160   time: 127.75s   best: 24.4241
2023-11-07 03:01:55,548:INFO:  Epoch 422/500:  train Loss: 20.3041   val Loss: 25.1716   time: 127.91s   best: 24.4241
2023-11-07 03:04:03,615:INFO:  Epoch 423/500:  train Loss: 20.4984   val Loss: 24.7787   time: 128.06s   best: 24.4241
2023-11-07 03:06:11,106:INFO:  Epoch 424/500:  train Loss: 20.8528   val Loss: 24.5929   time: 127.47s   best: 24.4241
2023-11-07 03:08:18,364:INFO:  Epoch 425/500:  train Loss: 20.5633   val Loss: 25.9065   time: 127.25s   best: 24.4241
2023-11-07 03:10:26,480:INFO:  Epoch 426/500:  train Loss: 20.5195   val Loss: 25.2002   time: 128.10s   best: 24.4241
2023-11-07 03:12:33,735:INFO:  Epoch 427/500:  train Loss: 20.2620   val Loss: 24.7291   time: 127.24s   best: 24.4241
2023-11-07 03:14:41,203:INFO:  Epoch 428/500:  train Loss: 20.3162   val Loss: 25.0394   time: 127.46s   best: 24.4241
2023-11-07 03:16:48,349:INFO:  Epoch 429/500:  train Loss: 20.2012   val Loss: 24.7661   time: 127.12s   best: 24.4241
2023-11-07 03:18:55,802:INFO:  Epoch 430/500:  train Loss: 20.2811   val Loss: 24.9045   time: 127.44s   best: 24.4241
2023-11-07 03:21:03,247:INFO:  Epoch 431/500:  train Loss: 20.1759   val Loss: 25.1013   time: 127.42s   best: 24.4241
2023-11-07 03:23:10,831:INFO:  Epoch 432/500:  train Loss: 20.2209   val Loss: 24.9627   time: 127.56s   best: 24.4241
2023-11-07 03:25:18,289:INFO:  Epoch 433/500:  train Loss: 20.1390   val Loss: 25.0276   time: 127.45s   best: 24.4241
2023-11-07 03:27:25,778:INFO:  Epoch 434/500:  train Loss: 20.4158   val Loss: 24.7604   time: 127.48s   best: 24.4241
2023-11-07 03:29:33,053:INFO:  Epoch 435/500:  train Loss: 20.5463   val Loss: 25.2162   time: 127.27s   best: 24.4241
2023-11-07 03:31:40,571:INFO:  Epoch 436/500:  train Loss: 20.3850   val Loss: 25.5837   time: 127.52s   best: 24.4241
2023-11-07 03:33:48,644:INFO:  Epoch 437/500:  train Loss: 20.2803   val Loss: 25.1684   time: 128.06s   best: 24.4241
2023-11-07 03:35:56,359:INFO:  Epoch 438/500:  train Loss: 20.2190   val Loss: 25.1408   time: 127.71s   best: 24.4241
2023-11-07 03:38:04,123:INFO:  Epoch 439/500:  train Loss: 20.5808   val Loss: 24.7360   time: 127.76s   best: 24.4241
2023-11-07 03:40:11,888:INFO:  Epoch 440/500:  train Loss: 20.1010   val Loss: 24.8280   time: 127.76s   best: 24.4241
2023-11-07 03:42:19,522:INFO:  Epoch 441/500:  train Loss: 20.1323   val Loss: 24.7470   time: 127.63s   best: 24.4241
2023-11-07 03:44:27,415:INFO:  Epoch 442/500:  train Loss: 20.3151   val Loss: 24.7752   time: 127.88s   best: 24.4241
2023-11-07 03:46:34,853:INFO:  Epoch 443/500:  train Loss: 20.1251   val Loss: 24.5380   time: 127.44s   best: 24.4241
2023-11-07 03:48:42,769:INFO:  Epoch 444/500:  train Loss: 20.1301   val Loss: 24.5655   time: 127.92s   best: 24.4241
2023-11-07 03:50:50,455:INFO:  Epoch 445/500:  train Loss: 20.1392   val Loss: 25.1318   time: 127.68s   best: 24.4241
2023-11-07 03:52:58,097:INFO:  Epoch 446/500:  train Loss: 20.0549   val Loss: 24.8163   time: 127.64s   best: 24.4241
2023-11-07 03:55:07,098:INFO:  Epoch 447/500:  train Loss: 20.0891   val Loss: 25.1553   time: 128.99s   best: 24.4241
2023-11-07 03:57:15,442:INFO:  Epoch 448/500:  train Loss: 20.0653   val Loss: 24.8950   time: 128.33s   best: 24.4241
2023-11-07 03:59:23,094:INFO:  Epoch 449/500:  train Loss: 20.0630   val Loss: 24.9856   time: 127.64s   best: 24.4241
2023-11-07 04:01:30,676:INFO:  Epoch 450/500:  train Loss: 20.0491   val Loss: 24.4904   time: 127.57s   best: 24.4241
2023-11-07 04:03:39,602:INFO:  Epoch 451/500:  train Loss: 20.1433   val Loss: 25.0129   time: 128.91s   best: 24.4241
2023-11-07 04:05:47,443:INFO:  Epoch 452/500:  train Loss: 19.9857   val Loss: 24.8298   time: 127.84s   best: 24.4241
2023-11-07 04:07:56,087:INFO:  Epoch 453/500:  train Loss: 20.7862   val Loss: 25.2151   time: 128.63s   best: 24.4241
2023-11-07 04:10:03,947:INFO:  Epoch 454/500:  train Loss: 19.9667   val Loss: 24.8880   time: 127.86s   best: 24.4241
2023-11-07 04:12:11,649:INFO:  Epoch 455/500:  train Loss: 20.3774   val Loss: 24.6349   time: 127.68s   best: 24.4241
2023-11-07 04:14:19,614:INFO:  Epoch 456/500:  train Loss: 20.3094   val Loss: 25.0240   time: 127.96s   best: 24.4241
2023-11-07 04:16:27,272:INFO:  Epoch 457/500:  train Loss: 19.9408   val Loss: 25.1405   time: 127.65s   best: 24.4241
2023-11-07 04:18:35,586:INFO:  Epoch 458/500:  train Loss: 19.9717   val Loss: 24.9922   time: 128.30s   best: 24.4241
2023-11-07 04:20:43,479:INFO:  Epoch 459/500:  train Loss: 19.9489   val Loss: 25.1392   time: 127.88s   best: 24.4241
2023-11-07 04:22:51,773:INFO:  Epoch 460/500:  train Loss: 20.0740   val Loss: 25.0817   time: 128.29s   best: 24.4241
2023-11-07 04:24:59,991:INFO:  Epoch 461/500:  train Loss: 20.0006   val Loss: 25.0651   time: 128.22s   best: 24.4241
2023-11-07 04:27:07,768:INFO:  Epoch 462/500:  train Loss: 20.0044   val Loss: 25.6381   time: 127.78s   best: 24.4241
2023-11-07 04:29:15,779:INFO:  Epoch 463/500:  train Loss: 19.9745   val Loss: 24.9544   time: 128.00s   best: 24.4241
2023-11-07 04:31:23,565:INFO:  Epoch 464/500:  train Loss: 20.6323   val Loss: 25.6673   time: 127.77s   best: 24.4241
2023-11-07 04:33:32,225:INFO:  Epoch 465/500:  train Loss: 20.6066   val Loss: 25.3161   time: 128.65s   best: 24.4241
2023-11-07 04:35:40,639:INFO:  Epoch 466/500:  train Loss: 19.9556   val Loss: 24.7733   time: 128.41s   best: 24.4241
2023-11-07 04:37:48,122:INFO:  Epoch 467/500:  train Loss: 19.9465   val Loss: 25.0450   time: 127.47s   best: 24.4241
2023-11-07 04:39:56,054:INFO:  Epoch 468/500:  train Loss: 20.1418   val Loss: 24.6001   time: 127.93s   best: 24.4241
2023-11-07 04:42:04,592:INFO:  Epoch 469/500:  train Loss: 19.8639   val Loss: 24.9597   time: 128.52s   best: 24.4241
2023-11-07 04:44:12,374:INFO:  Epoch 470/500:  train Loss: 19.9234   val Loss: 24.6029   time: 127.77s   best: 24.4241
2023-11-07 04:46:19,855:INFO:  Epoch 471/500:  train Loss: 20.1916   val Loss: 25.2457   time: 127.48s   best: 24.4241
2023-11-07 04:48:28,266:INFO:  Epoch 472/500:  train Loss: 19.8761   val Loss: 24.7469   time: 128.40s   best: 24.4241
2023-11-07 04:50:36,376:INFO:  Epoch 473/500:  train Loss: 20.0833   val Loss: 24.9457   time: 128.10s   best: 24.4241
2023-11-07 04:52:44,822:INFO:  Epoch 474/500:  train Loss: 19.9144   val Loss: 24.8060   time: 128.43s   best: 24.4241
2023-11-07 04:54:52,818:INFO:  Epoch 475/500:  train Loss: 20.3339   val Loss: 25.2233   time: 127.97s   best: 24.4241
2023-11-07 04:57:01,566:INFO:  Epoch 476/500:  train Loss: 19.8491   val Loss: 25.1437   time: 128.74s   best: 24.4241
2023-11-07 04:59:09,745:INFO:  Epoch 477/500:  train Loss: 19.8584   val Loss: 24.8908   time: 128.17s   best: 24.4241
2023-11-07 05:01:18,447:INFO:  Epoch 478/500:  train Loss: 19.8544   val Loss: 25.1282   time: 128.69s   best: 24.4241
2023-11-07 05:03:25,750:INFO:  Epoch 479/500:  train Loss: 19.9891   val Loss: 24.7960   time: 127.30s   best: 24.4241
2023-11-07 05:05:33,284:INFO:  Epoch 480/500:  train Loss: 19.8584   val Loss: 24.5748   time: 127.53s   best: 24.4241
2023-11-07 05:07:41,248:INFO:  Epoch 481/500:  train Loss: 19.7630   val Loss: 25.1840   time: 127.94s   best: 24.4241
2023-11-07 05:09:49,143:INFO:  Epoch 482/500:  train Loss: 20.0392   val Loss: 25.0037   time: 127.88s   best: 24.4241
2023-11-07 05:11:56,728:INFO:  Epoch 483/500:  train Loss: 19.8485   val Loss: 24.6261   time: 127.59s   best: 24.4241
2023-11-07 05:14:04,620:INFO:  Epoch 484/500:  train Loss: 19.7842   val Loss: 25.0043   time: 127.88s   best: 24.4241
2023-11-07 05:16:12,642:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_3113.pt
2023-11-07 05:16:12,659:INFO:  Epoch 485/500:  train Loss: 19.8837   val Loss: 24.3396   time: 128.01s   best: 24.3396
2023-11-07 05:18:20,598:INFO:  Epoch 486/500:  train Loss: 19.7858   val Loss: 24.7926   time: 127.94s   best: 24.3396
2023-11-07 05:20:29,228:INFO:  Epoch 487/500:  train Loss: 19.7490   val Loss: 25.0201   time: 128.62s   best: 24.3396
2023-11-07 05:22:37,497:INFO:  Epoch 488/500:  train Loss: 19.8128   val Loss: 25.2081   time: 128.26s   best: 24.3396
2023-11-07 05:24:45,141:INFO:  Epoch 489/500:  train Loss: 19.7752   val Loss: 24.8403   time: 127.64s   best: 24.3396
2023-11-07 05:26:52,941:INFO:  Epoch 490/500:  train Loss: 19.8118   val Loss: 25.0196   time: 127.79s   best: 24.3396
2023-11-07 05:29:01,328:INFO:  Epoch 491/500:  train Loss: 19.7388   val Loss: 25.1202   time: 128.37s   best: 24.3396
2023-11-07 05:31:09,381:INFO:  Epoch 492/500:  train Loss: 19.9269   val Loss: 24.8413   time: 128.04s   best: 24.3396
2023-11-07 05:33:17,233:INFO:  Epoch 493/500:  train Loss: 19.6974   val Loss: 24.9107   time: 127.84s   best: 24.3396
2023-11-07 05:35:25,271:INFO:  Epoch 494/500:  train Loss: 19.9723   val Loss: 25.0144   time: 128.04s   best: 24.3396
2023-11-07 05:37:33,251:INFO:  Epoch 495/500:  train Loss: 19.6716   val Loss: 25.2267   time: 127.97s   best: 24.3396
2023-11-07 05:39:40,945:INFO:  Epoch 496/500:  train Loss: 19.7285   val Loss: 24.7916   time: 127.69s   best: 24.3396
2023-11-07 05:41:48,673:INFO:  Epoch 497/500:  train Loss: 19.7451   val Loss: 24.6383   time: 127.72s   best: 24.3396
2023-11-07 05:43:56,230:INFO:  Epoch 498/500:  train Loss: 19.6502   val Loss: 24.4747   time: 127.55s   best: 24.3396
2023-11-07 05:46:03,593:INFO:  Epoch 499/500:  train Loss: 19.8752   val Loss: 24.9382   time: 127.35s   best: 24.3396
2023-11-07 05:48:11,858:INFO:  Epoch 500/500:  train Loss: 19.6294   val Loss: 24.6207   time: 128.25s   best: 24.3396
2023-11-07 05:48:11,870:INFO:  -----> Training complete in 1073m 14s   best validation loss: 24.3396
 
2023-11-07 09:11:30,377:INFO:  Starting experiment lstm autoencoder with 0.2 dataset (0.1 dropout)
2023-11-07 09:11:30,382:INFO:  Defining the model
2023-11-07 09:11:30,443:INFO:  Reading the dataset
2023-11-07 09:37:06,885:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:37:06,926:INFO:  Epoch 1/500:  train Loss: 87.4613   val Loss: 84.8412   time: 130.91s   best: 84.8412
2023-11-07 09:39:16,142:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:39:16,161:INFO:  Epoch 2/500:  train Loss: 80.4956   val Loss: 76.4690   time: 129.20s   best: 76.4690
2023-11-07 09:41:25,376:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:41:25,395:INFO:  Epoch 3/500:  train Loss: 73.7778   val Loss: 71.8913   time: 129.20s   best: 71.8913
2023-11-07 09:43:34,786:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:43:34,805:INFO:  Epoch 4/500:  train Loss: 71.0684   val Loss: 69.1607   time: 129.39s   best: 69.1607
2023-11-07 09:45:44,916:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:45:44,936:INFO:  Epoch 5/500:  train Loss: 68.8402   val Loss: 66.9759   time: 130.10s   best: 66.9759
2023-11-07 09:47:53,931:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:47:53,950:INFO:  Epoch 6/500:  train Loss: 67.3452   val Loss: 65.8036   time: 128.99s   best: 65.8036
2023-11-07 09:50:03,226:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:50:03,245:INFO:  Epoch 7/500:  train Loss: 66.0165   val Loss: 65.0293   time: 129.26s   best: 65.0293
2023-11-07 09:52:12,323:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:52:12,342:INFO:  Epoch 8/500:  train Loss: 64.4982   val Loss: 63.9975   time: 129.06s   best: 63.9975
2023-11-07 09:54:21,122:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:54:21,142:INFO:  Epoch 9/500:  train Loss: 63.1738   val Loss: 62.6764   time: 128.76s   best: 62.6764
2023-11-07 09:56:30,990:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:56:31,009:INFO:  Epoch 10/500:  train Loss: 62.2960   val Loss: 61.9919   time: 129.83s   best: 61.9919
2023-11-07 09:58:40,331:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 09:58:40,351:INFO:  Epoch 11/500:  train Loss: 61.4079   val Loss: 60.7838   time: 129.32s   best: 60.7838
2023-11-07 10:00:49,324:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:00:49,344:INFO:  Epoch 12/500:  train Loss: 60.4194   val Loss: 60.1037   time: 128.96s   best: 60.1037
2023-11-07 10:02:58,026:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:02:58,046:INFO:  Epoch 13/500:  train Loss: 59.6080   val Loss: 59.6341   time: 128.68s   best: 59.6341
2023-11-07 10:05:07,101:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:05:07,121:INFO:  Epoch 14/500:  train Loss: 58.5710   val Loss: 57.7856   time: 129.04s   best: 57.7856
2023-11-07 10:07:16,582:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:07:16,602:INFO:  Epoch 15/500:  train Loss: 57.5205   val Loss: 56.8819   time: 129.45s   best: 56.8819
2023-11-07 10:09:25,324:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:09:25,344:INFO:  Epoch 16/500:  train Loss: 56.4749   val Loss: 55.5345   time: 128.71s   best: 55.5345
2023-11-07 10:11:34,480:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:11:34,500:INFO:  Epoch 17/500:  train Loss: 55.4622   val Loss: 54.7679   time: 129.13s   best: 54.7679
2023-11-07 10:13:43,433:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:13:43,452:INFO:  Epoch 18/500:  train Loss: 54.3734   val Loss: 53.6360   time: 128.92s   best: 53.6360
2023-11-07 10:15:52,388:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:15:52,407:INFO:  Epoch 19/500:  train Loss: 53.3068   val Loss: 52.4426   time: 128.91s   best: 52.4426
2023-11-07 10:18:01,161:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:18:01,191:INFO:  Epoch 20/500:  train Loss: 52.2001   val Loss: 51.3669   time: 128.75s   best: 51.3669
2023-11-07 10:20:10,696:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:20:10,716:INFO:  Epoch 21/500:  train Loss: 51.2010   val Loss: 50.3862   time: 129.48s   best: 50.3862
2023-11-07 10:22:19,701:INFO:  Epoch 22/500:  train Loss: 50.3249   val Loss: 50.6220   time: 128.97s   best: 50.3862
2023-11-07 10:24:28,480:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:24:28,500:INFO:  Epoch 23/500:  train Loss: 49.5502   val Loss: 50.2860   time: 128.76s   best: 50.2860
2023-11-07 10:26:37,147:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:26:37,167:INFO:  Epoch 24/500:  train Loss: 48.8248   val Loss: 48.2849   time: 128.63s   best: 48.2849
2023-11-07 10:28:46,958:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:28:46,978:INFO:  Epoch 25/500:  train Loss: 47.9398   val Loss: 47.5716   time: 129.78s   best: 47.5716
2023-11-07 10:30:55,808:INFO:  Epoch 26/500:  train Loss: 47.3306   val Loss: 47.6895   time: 128.81s   best: 47.5716
2023-11-07 10:33:04,778:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:33:04,798:INFO:  Epoch 27/500:  train Loss: 46.4960   val Loss: 47.4790   time: 128.95s   best: 47.4790
2023-11-07 10:35:14,364:INFO:  Epoch 28/500:  train Loss: 45.8257   val Loss: 48.4613   time: 129.55s   best: 47.4790
2023-11-07 10:37:23,675:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:37:23,695:INFO:  Epoch 29/500:  train Loss: 45.3306   val Loss: 46.2293   time: 129.30s   best: 46.2293
2023-11-07 10:39:33,260:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:39:33,279:INFO:  Epoch 30/500:  train Loss: 44.6509   val Loss: 45.6131   time: 129.55s   best: 45.6131
2023-11-07 10:41:43,573:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:41:43,593:INFO:  Epoch 31/500:  train Loss: 44.2309   val Loss: 45.3493   time: 130.28s   best: 45.3493
2023-11-07 10:43:52,841:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:43:52,861:INFO:  Epoch 32/500:  train Loss: 43.6453   val Loss: 44.7033   time: 129.24s   best: 44.7033
2023-11-07 10:46:02,367:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:46:02,386:INFO:  Epoch 33/500:  train Loss: 43.2027   val Loss: 44.3923   time: 129.48s   best: 44.3923
2023-11-07 10:48:11,450:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:48:11,492:INFO:  Epoch 34/500:  train Loss: 42.8737   val Loss: 44.0613   time: 129.05s   best: 44.0613
2023-11-07 10:50:21,191:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:50:21,211:INFO:  Epoch 35/500:  train Loss: 42.3493   val Loss: 43.3571   time: 129.69s   best: 43.3571
2023-11-07 10:52:30,426:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:52:30,446:INFO:  Epoch 36/500:  train Loss: 41.8031   val Loss: 42.4676   time: 129.19s   best: 42.4676
2023-11-07 10:54:39,647:INFO:  Epoch 37/500:  train Loss: 41.5393   val Loss: 44.0578   time: 129.19s   best: 42.4676
2023-11-07 10:56:48,549:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 10:56:48,578:INFO:  Epoch 38/500:  train Loss: 41.0771   val Loss: 42.1702   time: 128.89s   best: 42.1702
2023-11-07 10:58:58,314:INFO:  Epoch 39/500:  train Loss: 40.6902   val Loss: 42.5517   time: 129.72s   best: 42.1702
2023-11-07 11:01:07,470:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:01:07,490:INFO:  Epoch 40/500:  train Loss: 40.1052   val Loss: 41.3073   time: 129.14s   best: 41.3073
2023-11-07 11:03:16,581:INFO:  Epoch 41/500:  train Loss: 40.0622   val Loss: 41.5080   time: 129.09s   best: 41.3073
2023-11-07 11:05:25,488:INFO:  Epoch 42/500:  train Loss: 39.4497   val Loss: 41.3526   time: 128.90s   best: 41.3073
2023-11-07 11:07:34,757:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:07:34,777:INFO:  Epoch 43/500:  train Loss: 39.0267   val Loss: 40.8379   time: 129.26s   best: 40.8379
2023-11-07 11:09:43,743:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:09:43,763:INFO:  Epoch 44/500:  train Loss: 38.5189   val Loss: 39.5236   time: 128.96s   best: 39.5236
2023-11-07 11:11:53,649:INFO:  Epoch 45/500:  train Loss: 38.1722   val Loss: 40.7615   time: 129.87s   best: 39.5236
2023-11-07 11:14:03,089:INFO:  Epoch 46/500:  train Loss: 37.8325   val Loss: 39.9161   time: 129.43s   best: 39.5236
2023-11-07 11:16:13,480:INFO:  Epoch 47/500:  train Loss: 37.3800   val Loss: 40.4169   time: 130.38s   best: 39.5236
2023-11-07 11:18:23,258:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:18:23,385:INFO:  Epoch 48/500:  train Loss: 37.1232   val Loss: 38.1151   time: 129.70s   best: 38.1151
2023-11-07 11:20:34,194:INFO:  Epoch 49/500:  train Loss: 36.5521   val Loss: 38.2468   time: 130.80s   best: 38.1151
2023-11-07 11:22:43,622:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:22:43,643:INFO:  Epoch 50/500:  train Loss: 36.3635   val Loss: 37.7059   time: 129.41s   best: 37.7059
2023-11-07 11:24:53,898:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:24:53,934:INFO:  Epoch 51/500:  train Loss: 36.0752   val Loss: 37.2740   time: 130.24s   best: 37.2740
2023-11-07 11:27:03,254:INFO:  Epoch 52/500:  train Loss: 36.1471   val Loss: 38.3434   time: 129.31s   best: 37.2740
2023-11-07 11:29:13,117:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:29:13,190:INFO:  Epoch 53/500:  train Loss: 35.4878   val Loss: 36.7090   time: 129.85s   best: 36.7090
2023-11-07 11:31:22,359:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:31:22,382:INFO:  Epoch 54/500:  train Loss: 35.1416   val Loss: 36.2905   time: 129.15s   best: 36.2905
2023-11-07 11:33:31,803:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:33:31,823:INFO:  Epoch 55/500:  train Loss: 34.8304   val Loss: 36.2433   time: 129.39s   best: 36.2433
2023-11-07 11:35:41,085:INFO:  Epoch 56/500:  train Loss: 34.5866   val Loss: 36.7294   time: 129.25s   best: 36.2433
2023-11-07 11:37:50,027:INFO:  Epoch 57/500:  train Loss: 34.6048   val Loss: 38.8965   time: 128.93s   best: 36.2433
2023-11-07 11:39:59,098:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:39:59,118:INFO:  Epoch 58/500:  train Loss: 34.1781   val Loss: 35.4081   time: 129.06s   best: 35.4081
2023-11-07 11:42:10,027:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:42:10,047:INFO:  Epoch 59/500:  train Loss: 33.9846   val Loss: 35.3211   time: 130.89s   best: 35.3211
2023-11-07 11:44:19,555:INFO:  Epoch 60/500:  train Loss: 33.8495   val Loss: 36.8605   time: 129.50s   best: 35.3211
2023-11-07 11:46:29,896:INFO:  Epoch 61/500:  train Loss: 33.5597   val Loss: 35.9346   time: 130.33s   best: 35.3211
2023-11-07 11:48:39,019:INFO:  Epoch 62/500:  train Loss: 34.1169   val Loss: 35.7834   time: 129.12s   best: 35.3211
2023-11-07 11:50:49,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:50:49,314:INFO:  Epoch 63/500:  train Loss: 33.6171   val Loss: 34.6622   time: 130.26s   best: 34.6622
2023-11-07 11:52:58,360:INFO:  Epoch 64/500:  train Loss: 33.0665   val Loss: 35.5966   time: 129.05s   best: 34.6622
2023-11-07 11:55:08,641:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:55:08,661:INFO:  Epoch 65/500:  train Loss: 32.8666   val Loss: 34.5159   time: 130.27s   best: 34.5159
2023-11-07 11:57:18,366:INFO:  Epoch 66/500:  train Loss: 32.7983   val Loss: 35.4100   time: 129.70s   best: 34.5159
2023-11-07 11:59:27,629:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 11:59:27,649:INFO:  Epoch 67/500:  train Loss: 32.5791   val Loss: 33.5895   time: 129.24s   best: 33.5895
2023-11-07 12:01:37,106:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 12:01:37,126:INFO:  Epoch 68/500:  train Loss: 32.4569   val Loss: 33.2480   time: 129.44s   best: 33.2480
2023-11-07 12:03:46,784:INFO:  Epoch 69/500:  train Loss: 32.3305   val Loss: 33.3693   time: 129.65s   best: 33.2480
2023-11-07 12:05:56,423:INFO:  Epoch 70/500:  train Loss: 31.8773   val Loss: 33.5640   time: 129.62s   best: 33.2480
2023-11-07 12:08:06,628:INFO:  Epoch 71/500:  train Loss: 31.8975   val Loss: 33.5319   time: 130.19s   best: 33.2480
2023-11-07 12:10:16,275:INFO:  Epoch 72/500:  train Loss: 31.6941   val Loss: 33.6952   time: 129.63s   best: 33.2480
2023-11-07 12:12:26,100:INFO:  Epoch 73/500:  train Loss: 31.8547   val Loss: 33.4633   time: 129.81s   best: 33.2480
2023-11-07 12:14:35,625:INFO:  Epoch 74/500:  train Loss: 32.0833   val Loss: 33.8598   time: 129.50s   best: 33.2480
2023-11-07 12:16:45,052:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 12:16:45,072:INFO:  Epoch 75/500:  train Loss: 31.3543   val Loss: 32.9229   time: 129.41s   best: 32.9229
2023-11-07 12:18:54,323:INFO:  Epoch 76/500:  train Loss: 31.6562   val Loss: 33.3213   time: 129.24s   best: 32.9229
2023-11-07 12:21:03,924:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 12:21:03,943:INFO:  Epoch 77/500:  train Loss: 31.0180   val Loss: 32.3935   time: 129.59s   best: 32.3935
2023-11-07 12:23:13,888:INFO:  Epoch 78/500:  train Loss: 31.0897   val Loss: 32.5218   time: 129.93s   best: 32.3935
2023-11-07 12:25:23,020:INFO:  Epoch 79/500:  train Loss: 30.7902   val Loss: 33.1626   time: 129.11s   best: 32.3935
2023-11-07 12:27:32,872:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 12:27:32,891:INFO:  Epoch 80/500:  train Loss: 30.6489   val Loss: 31.9503   time: 129.85s   best: 31.9503
2023-11-07 12:29:42,401:INFO:  Epoch 81/500:  train Loss: 30.8436   val Loss: 32.5315   time: 129.50s   best: 31.9503
2023-11-07 12:31:51,493:INFO:  Epoch 82/500:  train Loss: 30.5749   val Loss: 32.6408   time: 129.09s   best: 31.9503
2023-11-07 12:34:00,705:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 12:34:00,725:INFO:  Epoch 83/500:  train Loss: 30.6114   val Loss: 31.7463   time: 129.20s   best: 31.7463
2023-11-07 12:36:10,270:INFO:  Epoch 84/500:  train Loss: 30.4323   val Loss: 34.0325   time: 129.53s   best: 31.7463
2023-11-07 12:38:19,415:INFO:  Epoch 85/500:  train Loss: 30.3138   val Loss: 31.7650   time: 129.12s   best: 31.7463
2023-11-07 12:40:28,701:INFO:  Epoch 86/500:  train Loss: 29.9608   val Loss: 32.3141   time: 129.27s   best: 31.7463
2023-11-07 12:42:39,204:INFO:  Epoch 87/500:  train Loss: 30.1653   val Loss: 32.7341   time: 130.49s   best: 31.7463
2023-11-07 12:44:49,206:INFO:  Epoch 88/500:  train Loss: 29.8887   val Loss: 32.7902   time: 129.99s   best: 31.7463
2023-11-07 12:46:58,606:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 12:46:58,625:INFO:  Epoch 89/500:  train Loss: 29.7286   val Loss: 31.7271   time: 129.40s   best: 31.7271
2023-11-07 12:49:07,530:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 12:49:07,549:INFO:  Epoch 90/500:  train Loss: 29.6107   val Loss: 31.6485   time: 128.89s   best: 31.6485
2023-11-07 12:51:16,365:INFO:  Epoch 91/500:  train Loss: 29.8994   val Loss: 31.8199   time: 128.82s   best: 31.6485
2023-11-07 12:53:25,270:INFO:  Epoch 92/500:  train Loss: 29.4552   val Loss: 32.7816   time: 128.89s   best: 31.6485
2023-11-07 12:55:34,467:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 12:55:34,725:INFO:  Epoch 93/500:  train Loss: 29.6259   val Loss: 31.3227   time: 129.18s   best: 31.3227
2023-11-07 12:57:43,991:INFO:  Epoch 94/500:  train Loss: 29.1817   val Loss: 31.8922   time: 129.26s   best: 31.3227
2023-11-07 12:59:52,986:INFO:  Epoch 95/500:  train Loss: 29.1754   val Loss: 31.5503   time: 128.98s   best: 31.3227
2023-11-07 13:02:01,799:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 13:02:01,818:INFO:  Epoch 96/500:  train Loss: 29.3591   val Loss: 30.7445   time: 128.81s   best: 30.7445
2023-11-07 13:04:10,970:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 13:04:11,005:INFO:  Epoch 97/500:  train Loss: 28.8819   val Loss: 30.5967   time: 129.15s   best: 30.5967
2023-11-07 13:06:19,979:INFO:  Epoch 98/500:  train Loss: 29.1300   val Loss: 31.4619   time: 128.97s   best: 30.5967
2023-11-07 13:08:29,831:INFO:  Epoch 99/500:  train Loss: 29.1774   val Loss: 32.3222   time: 129.84s   best: 30.5967
2023-11-07 13:10:38,729:INFO:  Epoch 100/500:  train Loss: 29.0754   val Loss: 31.4683   time: 128.89s   best: 30.5967
2023-11-07 13:12:47,591:INFO:  Epoch 101/500:  train Loss: 28.8454   val Loss: 31.9201   time: 128.85s   best: 30.5967
2023-11-07 13:14:56,452:INFO:  Epoch 102/500:  train Loss: 28.6281   val Loss: 30.6926   time: 128.86s   best: 30.5967
2023-11-07 13:17:05,573:INFO:  Epoch 103/500:  train Loss: 28.6296   val Loss: 32.0683   time: 129.11s   best: 30.5967
2023-11-07 13:19:15,462:INFO:  Epoch 104/500:  train Loss: 29.2061   val Loss: 32.1105   time: 129.87s   best: 30.5967
2023-11-07 13:21:24,745:INFO:  Epoch 105/500:  train Loss: 28.5928   val Loss: 33.0731   time: 129.27s   best: 30.5967
2023-11-07 13:23:33,811:INFO:  Epoch 106/500:  train Loss: 28.4700   val Loss: 30.6637   time: 129.05s   best: 30.5967
2023-11-07 13:25:42,907:INFO:  Epoch 107/500:  train Loss: 28.6023   val Loss: 30.8126   time: 129.08s   best: 30.5967
2023-11-07 13:27:51,758:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 13:27:51,800:INFO:  Epoch 108/500:  train Loss: 28.1110   val Loss: 30.3495   time: 128.83s   best: 30.3495
2023-11-07 13:30:00,906:INFO:  Epoch 109/500:  train Loss: 28.2356   val Loss: 30.4520   time: 129.09s   best: 30.3495
2023-11-07 13:32:10,042:INFO:  Epoch 110/500:  train Loss: 27.9785   val Loss: 30.5492   time: 129.12s   best: 30.3495
2023-11-07 13:34:19,740:INFO:  Epoch 111/500:  train Loss: 27.8720   val Loss: 30.4983   time: 129.70s   best: 30.3495
2023-11-07 13:36:28,908:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 13:36:28,927:INFO:  Epoch 112/500:  train Loss: 27.9396   val Loss: 30.1039   time: 129.15s   best: 30.1039
2023-11-07 13:38:37,896:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 13:38:37,916:INFO:  Epoch 113/500:  train Loss: 28.0033   val Loss: 29.8356   time: 128.94s   best: 29.8356
2023-11-07 13:40:47,751:INFO:  Epoch 114/500:  train Loss: 27.7085   val Loss: 30.5264   time: 129.83s   best: 29.8356
2023-11-07 13:42:57,776:INFO:  Epoch 115/500:  train Loss: 27.6367   val Loss: 29.9518   time: 130.01s   best: 29.8356
2023-11-07 13:45:06,757:INFO:  Epoch 116/500:  train Loss: 27.5061   val Loss: 31.1041   time: 128.98s   best: 29.8356
2023-11-07 13:47:15,412:INFO:  Epoch 117/500:  train Loss: 27.8139   val Loss: 32.2885   time: 128.63s   best: 29.8356
2023-11-07 13:49:24,423:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 13:49:24,443:INFO:  Epoch 118/500:  train Loss: 27.6120   val Loss: 29.8329   time: 129.00s   best: 29.8329
2023-11-07 13:51:33,927:INFO:  Epoch 119/500:  train Loss: 27.2872   val Loss: 30.8991   time: 129.47s   best: 29.8329
2023-11-07 13:53:44,307:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 13:53:44,336:INFO:  Epoch 120/500:  train Loss: 27.5373   val Loss: 29.8175   time: 130.36s   best: 29.8175
2023-11-07 13:55:53,482:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 13:55:53,502:INFO:  Epoch 121/500:  train Loss: 27.5628   val Loss: 29.6768   time: 129.13s   best: 29.6768
2023-11-07 13:58:03,340:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 13:58:03,359:INFO:  Epoch 122/500:  train Loss: 27.1261   val Loss: 29.3299   time: 129.82s   best: 29.3299
2023-11-07 14:00:12,615:INFO:  Epoch 123/500:  train Loss: 27.1131   val Loss: 30.0275   time: 129.24s   best: 29.3299
2023-11-07 14:02:21,673:INFO:  Epoch 124/500:  train Loss: 27.2454   val Loss: 36.8667   time: 129.05s   best: 29.3299
2023-11-07 14:04:30,707:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 14:04:30,726:INFO:  Epoch 125/500:  train Loss: 27.2188   val Loss: 29.2536   time: 129.03s   best: 29.2536
2023-11-07 14:06:40,084:INFO:  Epoch 126/500:  train Loss: 26.9979   val Loss: 29.8391   time: 129.35s   best: 29.2536
2023-11-07 14:08:49,985:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 14:08:50,004:INFO:  Epoch 127/500:  train Loss: 26.8842   val Loss: 29.0822   time: 129.89s   best: 29.0822
2023-11-07 14:10:59,266:INFO:  Epoch 128/500:  train Loss: 27.0216   val Loss: 29.5018   time: 129.25s   best: 29.0822
2023-11-07 14:13:08,331:INFO:  Epoch 129/500:  train Loss: 26.8975   val Loss: 29.2412   time: 129.06s   best: 29.0822
2023-11-07 14:15:18,132:INFO:  Epoch 130/500:  train Loss: 26.8968   val Loss: 29.3674   time: 129.80s   best: 29.0822
2023-11-07 14:17:27,987:INFO:  Epoch 131/500:  train Loss: 26.5149   val Loss: 29.7253   time: 129.84s   best: 29.0822
2023-11-07 14:19:38,070:INFO:  Epoch 132/500:  train Loss: 27.0996   val Loss: 30.8243   time: 130.07s   best: 29.0822
2023-11-07 14:21:47,664:INFO:  Epoch 133/500:  train Loss: 26.5138   val Loss: 30.2382   time: 129.59s   best: 29.0822
2023-11-07 14:23:57,309:INFO:  Epoch 134/500:  train Loss: 26.6119   val Loss: 31.1433   time: 129.64s   best: 29.0822
2023-11-07 14:26:06,496:INFO:  Epoch 135/500:  train Loss: 26.5109   val Loss: 29.2342   time: 129.19s   best: 29.0822
2023-11-07 14:28:15,880:INFO:  Epoch 136/500:  train Loss: 26.5474   val Loss: 29.1553   time: 129.37s   best: 29.0822
2023-11-07 14:30:24,988:INFO:  Epoch 137/500:  train Loss: 26.2969   val Loss: 29.3918   time: 129.10s   best: 29.0822
2023-11-07 14:32:35,247:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 14:32:35,268:INFO:  Epoch 138/500:  train Loss: 26.5017   val Loss: 28.7239   time: 130.24s   best: 28.7239
2023-11-07 14:34:44,568:INFO:  Epoch 139/500:  train Loss: 26.1069   val Loss: 29.6650   time: 129.29s   best: 28.7239
2023-11-07 14:36:53,896:INFO:  Epoch 140/500:  train Loss: 26.1909   val Loss: 31.3446   time: 129.32s   best: 28.7239
2023-11-07 14:39:04,125:INFO:  Epoch 141/500:  train Loss: 26.1289   val Loss: 28.7480   time: 130.22s   best: 28.7239
2023-11-07 14:41:13,456:INFO:  Epoch 142/500:  train Loss: 26.2151   val Loss: 29.3767   time: 129.33s   best: 28.7239
2023-11-07 14:43:22,598:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 14:43:22,619:INFO:  Epoch 143/500:  train Loss: 26.0834   val Loss: 28.1775   time: 129.13s   best: 28.1775
2023-11-07 14:45:31,949:INFO:  Epoch 144/500:  train Loss: 25.9872   val Loss: 29.2405   time: 129.32s   best: 28.1775
2023-11-07 14:47:42,368:INFO:  Epoch 145/500:  train Loss: 25.9622   val Loss: 30.6984   time: 130.41s   best: 28.1775
2023-11-07 14:49:52,459:INFO:  Epoch 146/500:  train Loss: 26.4279   val Loss: 28.2523   time: 130.08s   best: 28.1775
2023-11-07 14:52:02,322:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 14:52:02,342:INFO:  Epoch 147/500:  train Loss: 25.8422   val Loss: 28.0419   time: 129.85s   best: 28.0419
2023-11-07 14:54:12,129:INFO:  Epoch 148/500:  train Loss: 26.7211   val Loss: 28.3711   time: 129.78s   best: 28.0419
2023-11-07 14:56:20,934:INFO:  Epoch 149/500:  train Loss: 25.6535   val Loss: 28.3592   time: 128.79s   best: 28.0419
2023-11-07 14:58:29,765:INFO:  Epoch 150/500:  train Loss: 25.8048   val Loss: 28.3279   time: 128.82s   best: 28.0419
2023-11-07 15:00:38,887:INFO:  Epoch 151/500:  train Loss: 25.6960   val Loss: 28.7759   time: 129.11s   best: 28.0419
2023-11-07 15:02:47,686:INFO:  Epoch 152/500:  train Loss: 25.7197   val Loss: 28.7516   time: 128.80s   best: 28.0419
2023-11-07 15:04:56,641:INFO:  Epoch 153/500:  train Loss: 26.0385   val Loss: 29.9859   time: 128.94s   best: 28.0419
2023-11-07 15:07:06,187:INFO:  Epoch 154/500:  train Loss: 26.2278   val Loss: 28.1980   time: 129.53s   best: 28.0419
2023-11-07 15:09:15,150:INFO:  Epoch 155/500:  train Loss: 25.5747   val Loss: 28.6865   time: 128.95s   best: 28.0419
2023-11-07 15:11:24,043:INFO:  Epoch 156/500:  train Loss: 25.5300   val Loss: 28.1354   time: 128.89s   best: 28.0419
2023-11-07 15:13:33,010:INFO:  Epoch 157/500:  train Loss: 25.4401   val Loss: 28.6539   time: 128.94s   best: 28.0419
2023-11-07 15:15:42,222:INFO:  Epoch 158/500:  train Loss: 25.4432   val Loss: 29.1166   time: 129.20s   best: 28.0419
2023-11-07 15:17:51,877:INFO:  Epoch 159/500:  train Loss: 25.4581   val Loss: 28.5193   time: 129.65s   best: 28.0419
2023-11-07 15:20:01,665:INFO:  Epoch 160/500:  train Loss: 25.3756   val Loss: 29.6511   time: 129.79s   best: 28.0419
2023-11-07 15:22:10,455:INFO:  Epoch 161/500:  train Loss: 25.9014   val Loss: 29.5543   time: 128.77s   best: 28.0419
2023-11-07 15:24:19,235:INFO:  Epoch 162/500:  train Loss: 25.3451   val Loss: 28.5121   time: 128.78s   best: 28.0419
2023-11-07 15:26:28,295:INFO:  Epoch 163/500:  train Loss: 25.1582   val Loss: 29.4263   time: 129.05s   best: 28.0419
2023-11-07 15:28:36,969:INFO:  Epoch 164/500:  train Loss: 25.5363   val Loss: 28.1857   time: 128.66s   best: 28.0419
2023-11-07 15:30:46,027:INFO:  Epoch 165/500:  train Loss: 25.3884   val Loss: 28.2770   time: 129.05s   best: 28.0419
2023-11-07 15:32:55,040:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 15:32:55,059:INFO:  Epoch 166/500:  train Loss: 25.0349   val Loss: 27.8813   time: 129.00s   best: 27.8813
2023-11-07 15:35:04,989:INFO:  Epoch 167/500:  train Loss: 24.9452   val Loss: 28.0564   time: 129.93s   best: 27.8813
2023-11-07 15:37:14,798:INFO:  Epoch 168/500:  train Loss: 25.3623   val Loss: 29.4937   time: 129.81s   best: 27.8813
2023-11-07 15:39:24,286:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 15:39:24,314:INFO:  Epoch 169/500:  train Loss: 25.1188   val Loss: 27.8402   time: 129.47s   best: 27.8402
2023-11-07 15:41:33,562:INFO:  Epoch 170/500:  train Loss: 25.3812   val Loss: 30.8802   time: 129.24s   best: 27.8402
2023-11-07 15:43:43,477:INFO:  Epoch 171/500:  train Loss: 25.2529   val Loss: 29.1547   time: 129.90s   best: 27.8402
2023-11-07 15:45:53,579:INFO:  Epoch 172/500:  train Loss: 25.0317   val Loss: 27.9149   time: 130.09s   best: 27.8402
2023-11-07 15:48:03,812:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 15:48:03,831:INFO:  Epoch 173/500:  train Loss: 24.8660   val Loss: 27.7183   time: 130.22s   best: 27.7183
2023-11-07 15:50:13,960:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 15:50:13,980:INFO:  Epoch 174/500:  train Loss: 24.9111   val Loss: 27.5053   time: 130.11s   best: 27.5053
2023-11-07 15:52:23,963:INFO:  Epoch 175/500:  train Loss: 25.3038   val Loss: 31.2484   time: 129.96s   best: 27.5053
2023-11-07 15:54:34,344:INFO:  Epoch 176/500:  train Loss: 25.1063   val Loss: 28.3411   time: 130.37s   best: 27.5053
2023-11-07 15:56:43,668:INFO:  Epoch 177/500:  train Loss: 24.6196   val Loss: 28.4670   time: 129.30s   best: 27.5053
2023-11-07 15:58:53,120:INFO:  Epoch 178/500:  train Loss: 24.7969   val Loss: 28.6226   time: 129.44s   best: 27.5053
2023-11-07 16:01:02,645:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 16:01:02,664:INFO:  Epoch 179/500:  train Loss: 24.9183   val Loss: 27.5052   time: 129.51s   best: 27.5052
2023-11-07 16:03:12,780:INFO:  Epoch 180/500:  train Loss: 24.5941   val Loss: 29.2335   time: 130.11s   best: 27.5052
2023-11-07 16:05:22,033:INFO:  Epoch 181/500:  train Loss: 25.2244   val Loss: 28.6753   time: 129.24s   best: 27.5052
2023-11-07 16:07:31,162:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 16:07:31,182:INFO:  Epoch 182/500:  train Loss: 24.9541   val Loss: 27.4079   time: 129.12s   best: 27.4079
2023-11-07 16:09:41,594:INFO:  Epoch 183/500:  train Loss: 24.5560   val Loss: 27.9085   time: 130.39s   best: 27.4079
2023-11-07 16:11:50,744:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 16:11:50,763:INFO:  Epoch 184/500:  train Loss: 24.6737   val Loss: 27.3749   time: 129.12s   best: 27.3749
2023-11-07 16:13:59,847:INFO:  Epoch 185/500:  train Loss: 24.4479   val Loss: 27.7863   time: 129.07s   best: 27.3749
2023-11-07 16:16:09,809:INFO:  Epoch 186/500:  train Loss: 25.0372   val Loss: 27.4847   time: 129.95s   best: 27.3749
2023-11-07 16:18:19,143:INFO:  Epoch 187/500:  train Loss: 24.4176   val Loss: 29.4882   time: 129.31s   best: 27.3749
2023-11-07 16:20:28,453:INFO:  Epoch 188/500:  train Loss: 24.4925   val Loss: 30.2024   time: 129.28s   best: 27.3749
2023-11-07 16:22:38,026:INFO:  Epoch 189/500:  train Loss: 24.5425   val Loss: 27.6308   time: 129.55s   best: 27.3749
2023-11-07 16:24:47,270:INFO:  Epoch 190/500:  train Loss: 24.6702   val Loss: 28.9293   time: 129.23s   best: 27.3749
2023-11-07 16:26:56,014:INFO:  Epoch 191/500:  train Loss: 24.3647   val Loss: 27.9471   time: 128.73s   best: 27.3749
2023-11-07 16:29:04,794:INFO:  Epoch 192/500:  train Loss: 24.2689   val Loss: 27.9225   time: 128.77s   best: 27.3749
2023-11-07 16:31:14,504:INFO:  Epoch 193/500:  train Loss: 24.4208   val Loss: 27.4710   time: 129.70s   best: 27.3749
2023-11-07 16:33:23,430:INFO:  Epoch 194/500:  train Loss: 24.4723   val Loss: 27.7250   time: 128.89s   best: 27.3749
2023-11-07 16:35:32,512:INFO:  Epoch 195/500:  train Loss: 24.3280   val Loss: 28.1771   time: 129.08s   best: 27.3749
2023-11-07 16:37:41,650:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 16:37:41,689:INFO:  Epoch 196/500:  train Loss: 24.2680   val Loss: 27.1295   time: 129.13s   best: 27.1295
2023-11-07 16:39:50,931:INFO:  Epoch 197/500:  train Loss: 24.1537   val Loss: 27.4606   time: 129.23s   best: 27.1295
2023-11-07 16:42:00,714:INFO:  Epoch 198/500:  train Loss: 24.3834   val Loss: 27.6652   time: 129.77s   best: 27.1295
2023-11-07 16:44:09,978:INFO:  Epoch 199/500:  train Loss: 24.1647   val Loss: 27.3530   time: 129.26s   best: 27.1295
2023-11-07 16:46:19,043:INFO:  Epoch 200/500:  train Loss: 24.1297   val Loss: 27.7529   time: 129.06s   best: 27.1295
2023-11-07 16:48:28,320:INFO:  Epoch 201/500:  train Loss: 24.0526   val Loss: 27.8079   time: 129.26s   best: 27.1295
2023-11-07 16:50:37,838:INFO:  Epoch 202/500:  train Loss: 23.9292   val Loss: 27.5486   time: 129.52s   best: 27.1295
2023-11-07 16:52:47,280:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 16:52:47,384:INFO:  Epoch 203/500:  train Loss: 23.8742   val Loss: 26.7332   time: 129.42s   best: 26.7332
2023-11-07 16:54:56,902:INFO:  Epoch 204/500:  train Loss: 23.9624   val Loss: 27.2294   time: 129.49s   best: 26.7332
2023-11-07 16:57:05,665:INFO:  Epoch 205/500:  train Loss: 24.2316   val Loss: 26.9031   time: 128.75s   best: 26.7332
2023-11-07 16:59:14,859:INFO:  Epoch 206/500:  train Loss: 23.9630   val Loss: 29.4930   time: 129.18s   best: 26.7332
2023-11-07 17:01:23,437:INFO:  Epoch 207/500:  train Loss: 24.0696   val Loss: 27.0710   time: 128.55s   best: 26.7332
2023-11-07 17:03:32,092:INFO:  Epoch 208/500:  train Loss: 23.8230   val Loss: 27.3289   time: 128.65s   best: 26.7332
2023-11-07 17:05:40,957:INFO:  Epoch 209/500:  train Loss: 23.7301   val Loss: 29.0885   time: 128.85s   best: 26.7332
2023-11-07 17:07:50,269:INFO:  Epoch 210/500:  train Loss: 24.0604   val Loss: 28.3853   time: 129.30s   best: 26.7332
2023-11-07 17:10:00,387:INFO:  Epoch 211/500:  train Loss: 23.8301   val Loss: 26.7374   time: 130.09s   best: 26.7332
2023-11-07 17:12:09,419:INFO:  Epoch 212/500:  train Loss: 23.6434   val Loss: 27.0895   time: 129.02s   best: 26.7332
2023-11-07 17:14:18,426:INFO:  Epoch 213/500:  train Loss: 23.8645   val Loss: 27.6020   time: 129.00s   best: 26.7332
2023-11-07 17:16:27,483:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 17:16:27,511:INFO:  Epoch 214/500:  train Loss: 23.5049   val Loss: 26.6832   time: 129.03s   best: 26.6832
2023-11-07 17:18:36,491:INFO:  Epoch 215/500:  train Loss: 24.2929   val Loss: 27.3225   time: 128.98s   best: 26.6832
2023-11-07 17:20:45,087:INFO:  Epoch 216/500:  train Loss: 23.8184   val Loss: 26.8912   time: 128.59s   best: 26.6832
2023-11-07 17:22:53,873:INFO:  Epoch 217/500:  train Loss: 23.7417   val Loss: 26.7727   time: 128.78s   best: 26.6832
2023-11-07 17:25:02,568:INFO:  Epoch 218/500:  train Loss: 23.5603   val Loss: 27.2717   time: 128.67s   best: 26.6832
2023-11-07 17:27:11,341:INFO:  Epoch 219/500:  train Loss: 23.6749   val Loss: 28.2941   time: 128.76s   best: 26.6832
2023-11-07 17:29:20,254:INFO:  Epoch 220/500:  train Loss: 23.7553   val Loss: 27.6463   time: 128.91s   best: 26.6832
2023-11-07 17:31:28,769:INFO:  Epoch 221/500:  train Loss: 23.4383   val Loss: 27.9146   time: 128.51s   best: 26.6832
2023-11-07 17:33:38,666:INFO:  Epoch 222/500:  train Loss: 23.5762   val Loss: 27.4502   time: 129.89s   best: 26.6832
2023-11-07 17:35:48,075:INFO:  Epoch 223/500:  train Loss: 23.6857   val Loss: 27.4730   time: 129.40s   best: 26.6832
2023-11-07 17:37:56,969:INFO:  Epoch 224/500:  train Loss: 23.5145   val Loss: 26.8604   time: 128.88s   best: 26.6832
2023-11-07 17:40:05,656:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 17:40:05,674:INFO:  Epoch 225/500:  train Loss: 23.3546   val Loss: 26.6501   time: 128.67s   best: 26.6501
2023-11-07 17:42:14,270:INFO:  Epoch 226/500:  train Loss: 23.5237   val Loss: 28.4534   time: 128.58s   best: 26.6501
2023-11-07 17:44:22,698:INFO:  Epoch 227/500:  train Loss: 23.4020   val Loss: 26.7001   time: 128.40s   best: 26.6501
2023-11-07 17:46:31,503:INFO:  Epoch 228/500:  train Loss: 23.2401   val Loss: 28.3277   time: 128.80s   best: 26.6501
2023-11-07 17:48:40,396:INFO:  Epoch 229/500:  train Loss: 23.4288   val Loss: 28.1944   time: 128.89s   best: 26.6501
2023-11-07 17:50:49,960:INFO:  Epoch 230/500:  train Loss: 23.3166   val Loss: 29.2157   time: 129.55s   best: 26.6501
2023-11-07 17:52:59,745:INFO:  Epoch 231/500:  train Loss: 23.5700   val Loss: 27.2225   time: 129.78s   best: 26.6501
2023-11-07 17:55:08,557:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 17:55:08,576:INFO:  Epoch 232/500:  train Loss: 23.2474   val Loss: 26.5636   time: 128.80s   best: 26.5636
2023-11-07 17:57:17,432:INFO:  Epoch 233/500:  train Loss: 23.1400   val Loss: 26.6587   time: 128.84s   best: 26.5636
2023-11-07 17:59:25,962:INFO:  Epoch 234/500:  train Loss: 23.2220   val Loss: 27.0097   time: 128.52s   best: 26.5636
2023-11-07 18:01:34,673:INFO:  Epoch 235/500:  train Loss: 23.2977   val Loss: 28.9014   time: 128.70s   best: 26.5636
2023-11-07 18:03:44,041:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 18:03:44,061:INFO:  Epoch 236/500:  train Loss: 23.2577   val Loss: 26.5039   time: 129.36s   best: 26.5039
2023-11-07 18:05:52,613:INFO:  Epoch 237/500:  train Loss: 23.1157   val Loss: 26.6813   time: 128.54s   best: 26.5039
2023-11-07 18:08:01,421:INFO:  Epoch 238/500:  train Loss: 23.0241   val Loss: 26.8353   time: 128.79s   best: 26.5039
2023-11-07 18:10:11,352:INFO:  Epoch 239/500:  train Loss: 23.1544   val Loss: 26.8553   time: 129.92s   best: 26.5039
2023-11-07 18:12:20,212:INFO:  Epoch 240/500:  train Loss: 23.5182   val Loss: 28.4274   time: 128.86s   best: 26.5039
2023-11-07 18:14:29,068:INFO:  Epoch 241/500:  train Loss: 23.1747   val Loss: 26.5739   time: 128.84s   best: 26.5039
2023-11-07 18:16:38,932:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 18:16:38,952:INFO:  Epoch 242/500:  train Loss: 23.0754   val Loss: 26.4148   time: 129.85s   best: 26.4148
2023-11-07 18:18:48,279:INFO:  Epoch 243/500:  train Loss: 23.0254   val Loss: 27.0324   time: 129.33s   best: 26.4148
2023-11-07 18:20:57,083:INFO:  Epoch 244/500:  train Loss: 22.9816   val Loss: 26.6271   time: 128.80s   best: 26.4148
2023-11-07 18:23:05,977:INFO:  Epoch 245/500:  train Loss: 22.9047   val Loss: 27.7663   time: 128.88s   best: 26.4148
2023-11-07 18:25:14,942:INFO:  Epoch 246/500:  train Loss: 22.8770   val Loss: 26.6400   time: 128.95s   best: 26.4148
2023-11-07 18:27:25,342:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 18:27:25,361:INFO:  Epoch 247/500:  train Loss: 22.9722   val Loss: 26.0783   time: 130.38s   best: 26.0783
2023-11-07 18:29:35,244:INFO:  Epoch 248/500:  train Loss: 22.9107   val Loss: 26.7243   time: 129.87s   best: 26.0783
2023-11-07 18:31:44,253:INFO:  Epoch 249/500:  train Loss: 22.8601   val Loss: 40.6249   time: 129.00s   best: 26.0783
2023-11-07 18:33:53,860:INFO:  Epoch 250/500:  train Loss: 23.1982   val Loss: 27.1390   time: 129.59s   best: 26.0783
2023-11-07 18:36:03,939:INFO:  Epoch 251/500:  train Loss: 22.6966   val Loss: 26.2649   time: 130.07s   best: 26.0783
2023-11-07 18:38:14,317:INFO:  Epoch 252/500:  train Loss: 22.8131   val Loss: 28.2479   time: 130.37s   best: 26.0783
2023-11-07 18:40:23,592:INFO:  Epoch 253/500:  train Loss: 23.3736   val Loss: 28.6042   time: 129.27s   best: 26.0783
2023-11-07 18:42:32,075:INFO:  Epoch 254/500:  train Loss: 22.9880   val Loss: 26.4711   time: 128.48s   best: 26.0783
2023-11-07 18:44:40,587:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 18:44:40,606:INFO:  Epoch 255/500:  train Loss: 22.6935   val Loss: 25.9682   time: 128.48s   best: 25.9682
2023-11-07 18:46:49,108:INFO:  Epoch 256/500:  train Loss: 22.8762   val Loss: 26.8510   time: 128.48s   best: 25.9682
2023-11-07 18:48:58,745:INFO:  Epoch 257/500:  train Loss: 22.6147   val Loss: 26.8366   time: 129.62s   best: 25.9682
2023-11-07 18:51:07,153:INFO:  Epoch 258/500:  train Loss: 22.5814   val Loss: 26.8757   time: 128.40s   best: 25.9682
2023-11-07 18:53:15,613:INFO:  Epoch 259/500:  train Loss: 22.6856   val Loss: 26.6121   time: 128.44s   best: 25.9682
2023-11-07 18:55:24,139:INFO:  Epoch 260/500:  train Loss: 22.5429   val Loss: 26.5630   time: 128.52s   best: 25.9682
2023-11-07 18:57:33,486:INFO:  Epoch 261/500:  train Loss: 22.7331   val Loss: 27.6672   time: 129.35s   best: 25.9682
2023-11-07 18:59:42,768:INFO:  Epoch 262/500:  train Loss: 22.5276   val Loss: 26.3580   time: 129.27s   best: 25.9682
2023-11-07 19:01:51,358:INFO:  Epoch 263/500:  train Loss: 22.6325   val Loss: 26.7862   time: 128.59s   best: 25.9682
2023-11-07 19:04:00,347:INFO:  Epoch 264/500:  train Loss: 22.6733   val Loss: 26.2859   time: 128.99s   best: 25.9682
2023-11-07 19:06:09,382:INFO:  Epoch 265/500:  train Loss: 22.5532   val Loss: 26.7267   time: 129.02s   best: 25.9682
2023-11-07 19:08:18,902:INFO:  Epoch 266/500:  train Loss: 22.5067   val Loss: 27.4627   time: 129.51s   best: 25.9682
2023-11-07 19:10:27,682:INFO:  Epoch 267/500:  train Loss: 22.5047   val Loss: 26.1062   time: 128.77s   best: 25.9682
2023-11-07 19:12:36,990:INFO:  Epoch 268/500:  train Loss: 22.8615   val Loss: 26.4730   time: 129.30s   best: 25.9682
2023-11-07 19:14:46,620:INFO:  Epoch 269/500:  train Loss: 22.4365   val Loss: 26.4849   time: 129.62s   best: 25.9682
2023-11-07 19:16:55,197:INFO:  Epoch 270/500:  train Loss: 22.5212   val Loss: 26.7190   time: 128.57s   best: 25.9682
2023-11-07 19:19:03,997:INFO:  Epoch 271/500:  train Loss: 22.5083   val Loss: 26.0545   time: 128.79s   best: 25.9682
2023-11-07 19:21:12,521:INFO:  Epoch 272/500:  train Loss: 22.5465   val Loss: 26.2041   time: 128.51s   best: 25.9682
2023-11-07 19:23:21,431:INFO:  Epoch 273/500:  train Loss: 22.4217   val Loss: 26.5727   time: 128.89s   best: 25.9682
2023-11-07 19:25:30,730:INFO:  Epoch 274/500:  train Loss: 22.3820   val Loss: 26.2684   time: 129.29s   best: 25.9682
2023-11-07 19:27:39,254:INFO:  Epoch 275/500:  train Loss: 22.4784   val Loss: 27.6343   time: 128.50s   best: 25.9682
2023-11-07 19:29:47,575:INFO:  Epoch 276/500:  train Loss: 22.5249   val Loss: 26.1745   time: 128.31s   best: 25.9682
2023-11-07 19:31:56,096:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 19:31:56,144:INFO:  Epoch 277/500:  train Loss: 22.2611   val Loss: 25.8776   time: 128.51s   best: 25.8776
2023-11-07 19:34:04,953:INFO:  Epoch 278/500:  train Loss: 24.2665   val Loss: 26.3418   time: 128.80s   best: 25.8776
2023-11-07 19:36:14,826:INFO:  Epoch 279/500:  train Loss: 22.5709   val Loss: 26.5165   time: 129.86s   best: 25.8776
2023-11-07 19:38:23,992:INFO:  Epoch 280/500:  train Loss: 22.2331   val Loss: 26.6892   time: 129.16s   best: 25.8776
2023-11-07 19:40:32,798:INFO:  Epoch 281/500:  train Loss: 22.3542   val Loss: 26.4938   time: 128.80s   best: 25.8776
2023-11-07 19:42:41,481:INFO:  Epoch 282/500:  train Loss: 22.3541   val Loss: 26.0144   time: 128.68s   best: 25.8776
2023-11-07 19:44:50,544:INFO:  Epoch 283/500:  train Loss: 22.2798   val Loss: 26.5537   time: 129.06s   best: 25.8776
2023-11-07 19:46:59,694:INFO:  Epoch 284/500:  train Loss: 22.2930   val Loss: 26.3071   time: 129.14s   best: 25.8776
2023-11-07 19:49:08,986:INFO:  Epoch 285/500:  train Loss: 22.5850   val Loss: 26.6909   time: 129.28s   best: 25.8776
2023-11-07 19:51:17,752:INFO:  Epoch 286/500:  train Loss: 22.2982   val Loss: 26.1256   time: 128.77s   best: 25.8776
2023-11-07 19:53:26,725:INFO:  Epoch 287/500:  train Loss: 22.2548   val Loss: 26.3487   time: 128.96s   best: 25.8776
2023-11-07 19:55:35,348:INFO:  Epoch 288/500:  train Loss: 22.2209   val Loss: 26.1156   time: 128.62s   best: 25.8776
2023-11-07 19:57:44,164:INFO:  Epoch 289/500:  train Loss: 22.1166   val Loss: 26.3080   time: 128.80s   best: 25.8776
2023-11-07 19:59:53,066:INFO:  Epoch 290/500:  train Loss: 21.9927   val Loss: 26.2807   time: 128.89s   best: 25.8776
2023-11-07 20:02:01,484:INFO:  Epoch 291/500:  train Loss: 22.0439   val Loss: 26.4941   time: 128.42s   best: 25.8776
2023-11-07 20:04:10,041:INFO:  Epoch 292/500:  train Loss: 22.1140   val Loss: 26.1827   time: 128.55s   best: 25.8776
2023-11-07 20:06:18,735:INFO:  Epoch 293/500:  train Loss: 22.1457   val Loss: 26.3851   time: 128.69s   best: 25.8776
2023-11-07 20:08:27,005:INFO:  Epoch 294/500:  train Loss: 22.0361   val Loss: 26.2904   time: 128.27s   best: 25.8776
2023-11-07 20:10:35,298:INFO:  Epoch 295/500:  train Loss: 21.9276   val Loss: 26.1685   time: 128.28s   best: 25.8776
2023-11-07 20:12:43,620:INFO:  Epoch 296/500:  train Loss: 21.9554   val Loss: 26.3569   time: 128.31s   best: 25.8776
2023-11-07 20:14:52,436:INFO:  Epoch 297/500:  train Loss: 22.7164   val Loss: 27.0853   time: 128.82s   best: 25.8776
2023-11-07 20:17:01,107:INFO:  Epoch 298/500:  train Loss: 22.1865   val Loss: 26.0164   time: 128.67s   best: 25.8776
2023-11-07 20:19:09,689:INFO:  Epoch 299/500:  train Loss: 22.0289   val Loss: 26.2692   time: 128.57s   best: 25.8776
2023-11-07 20:21:18,610:INFO:  Epoch 300/500:  train Loss: 22.0707   val Loss: 25.9786   time: 128.91s   best: 25.8776
2023-11-07 20:23:27,945:INFO:  Epoch 301/500:  train Loss: 22.1102   val Loss: 26.3946   time: 129.32s   best: 25.8776
2023-11-07 20:25:36,424:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 20:25:36,443:INFO:  Epoch 302/500:  train Loss: 21.9082   val Loss: 25.5458   time: 128.45s   best: 25.5458
2023-11-07 20:27:45,198:INFO:  Epoch 303/500:  train Loss: 22.0415   val Loss: 25.9589   time: 128.74s   best: 25.5458
2023-11-07 20:29:53,741:INFO:  Epoch 304/500:  train Loss: 21.8055   val Loss: 25.9246   time: 128.54s   best: 25.5458
2023-11-07 20:32:02,374:INFO:  Epoch 305/500:  train Loss: 22.0972   val Loss: 25.9120   time: 128.63s   best: 25.5458
2023-11-07 20:34:11,019:INFO:  Epoch 306/500:  train Loss: 22.0948   val Loss: 35.5848   time: 128.64s   best: 25.5458
2023-11-07 20:36:19,450:INFO:  Epoch 307/500:  train Loss: 22.6399   val Loss: 26.3047   time: 128.42s   best: 25.5458
2023-11-07 20:38:29,401:INFO:  Epoch 308/500:  train Loss: 21.8614   val Loss: 26.9044   time: 129.94s   best: 25.5458
2023-11-07 20:40:38,082:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 20:40:38,104:INFO:  Epoch 309/500:  train Loss: 21.9834   val Loss: 25.3437   time: 128.65s   best: 25.3437
2023-11-07 20:42:46,760:INFO:  Epoch 310/500:  train Loss: 21.7534   val Loss: 26.0540   time: 128.64s   best: 25.3437
2023-11-07 20:44:55,773:INFO:  Epoch 311/500:  train Loss: 21.8723   val Loss: 25.8160   time: 129.00s   best: 25.3437
2023-11-07 20:47:04,360:INFO:  Epoch 312/500:  train Loss: 21.7417   val Loss: 26.0958   time: 128.56s   best: 25.3437
2023-11-07 20:49:13,045:INFO:  Epoch 313/500:  train Loss: 21.7603   val Loss: 26.3630   time: 128.67s   best: 25.3437
2023-11-07 20:51:21,115:INFO:  Epoch 314/500:  train Loss: 21.7368   val Loss: 26.0113   time: 128.07s   best: 25.3437
2023-11-07 20:53:29,046:INFO:  Epoch 315/500:  train Loss: 21.7841   val Loss: 25.8133   time: 127.93s   best: 25.3437
2023-11-07 20:55:37,636:INFO:  Epoch 316/500:  train Loss: 21.6909   val Loss: 25.7584   time: 128.58s   best: 25.3437
2023-11-07 20:57:46,779:INFO:  Epoch 317/500:  train Loss: 21.7165   val Loss: 25.7365   time: 129.13s   best: 25.3437
2023-11-07 20:59:55,115:INFO:  Epoch 318/500:  train Loss: 21.6108   val Loss: 26.0369   time: 128.32s   best: 25.3437
2023-11-07 21:02:02,603:INFO:  Epoch 319/500:  train Loss: 21.6209   val Loss: 26.4267   time: 127.48s   best: 25.3437
2023-11-07 21:04:11,250:INFO:  Epoch 320/500:  train Loss: 21.7587   val Loss: 26.4567   time: 128.65s   best: 25.3437
2023-11-07 21:06:19,020:INFO:  Epoch 321/500:  train Loss: 21.8073   val Loss: 25.5593   time: 127.76s   best: 25.3437
2023-11-07 21:08:27,432:INFO:  Epoch 322/500:  train Loss: 21.6988   val Loss: 26.4552   time: 128.39s   best: 25.3437
2023-11-07 21:10:35,157:INFO:  Epoch 323/500:  train Loss: 21.6076   val Loss: 25.3689   time: 127.71s   best: 25.3437
2023-11-07 21:12:44,366:INFO:  Epoch 324/500:  train Loss: 21.8192   val Loss: 37.9276   time: 129.20s   best: 25.3437
2023-11-07 21:14:52,302:INFO:  Epoch 325/500:  train Loss: 22.1748   val Loss: 26.3813   time: 127.94s   best: 25.3437
2023-11-07 21:17:00,076:INFO:  Epoch 326/500:  train Loss: 21.5441   val Loss: 26.1469   time: 127.75s   best: 25.3437
2023-11-07 21:19:08,039:INFO:  Epoch 327/500:  train Loss: 21.4731   val Loss: 25.8957   time: 127.96s   best: 25.3437
2023-11-07 21:21:15,838:INFO:  Epoch 328/500:  train Loss: 21.9198   val Loss: 36.1317   time: 127.80s   best: 25.3437
2023-11-07 21:23:23,879:INFO:  Epoch 329/500:  train Loss: 21.7943   val Loss: 25.8009   time: 128.02s   best: 25.3437
2023-11-07 21:25:31,734:INFO:  Epoch 330/500:  train Loss: 22.2681   val Loss: 26.5612   time: 127.84s   best: 25.3437
2023-11-07 21:27:39,794:INFO:  Epoch 331/500:  train Loss: 21.9578   val Loss: 25.8784   time: 128.05s   best: 25.3437
2023-11-07 21:29:48,344:INFO:  Epoch 332/500:  train Loss: 21.4329   val Loss: 26.3924   time: 128.54s   best: 25.3437
2023-11-07 21:31:56,591:INFO:  Epoch 333/500:  train Loss: 21.8445   val Loss: 26.9589   time: 128.24s   best: 25.3437
2023-11-07 21:34:04,061:INFO:  Epoch 334/500:  train Loss: 21.7168   val Loss: 25.5136   time: 127.46s   best: 25.3437
2023-11-07 21:36:11,666:INFO:  Epoch 335/500:  train Loss: 21.4741   val Loss: 26.0405   time: 127.59s   best: 25.3437
2023-11-07 21:38:19,613:INFO:  Epoch 336/500:  train Loss: 21.6566   val Loss: 26.9827   time: 127.95s   best: 25.3437
2023-11-07 21:40:27,209:INFO:  Epoch 337/500:  train Loss: 21.4724   val Loss: 25.6104   time: 127.58s   best: 25.3437
2023-11-07 21:42:34,781:INFO:  Epoch 338/500:  train Loss: 21.4099   val Loss: 26.5803   time: 127.56s   best: 25.3437
2023-11-07 21:44:42,710:INFO:  Epoch 339/500:  train Loss: 21.4076   val Loss: 26.1730   time: 127.92s   best: 25.3437
2023-11-07 21:46:50,227:INFO:  Epoch 340/500:  train Loss: 21.2912   val Loss: 25.7876   time: 127.52s   best: 25.3437
2023-11-07 21:48:59,406:INFO:  Epoch 341/500:  train Loss: 21.3326   val Loss: 25.8706   time: 129.17s   best: 25.3437
2023-11-07 21:51:07,288:INFO:  Epoch 342/500:  train Loss: 21.3964   val Loss: 25.4512   time: 127.88s   best: 25.3437
2023-11-07 21:53:15,248:INFO:  Epoch 343/500:  train Loss: 21.3522   val Loss: 25.4871   time: 127.94s   best: 25.3437
2023-11-07 21:55:22,972:INFO:  Epoch 344/500:  train Loss: 21.4370   val Loss: 25.7376   time: 127.71s   best: 25.3437
2023-11-07 21:57:30,830:INFO:  Epoch 345/500:  train Loss: 21.6435   val Loss: 26.2114   time: 127.85s   best: 25.3437
2023-11-07 21:59:38,783:INFO:  Epoch 346/500:  train Loss: 21.2245   val Loss: 25.8281   time: 127.94s   best: 25.3437
2023-11-07 22:01:46,591:INFO:  Epoch 347/500:  train Loss: 21.3456   val Loss: 25.7365   time: 127.81s   best: 25.3437
2023-11-07 22:03:54,323:INFO:  Epoch 348/500:  train Loss: 21.4175   val Loss: 25.7263   time: 127.72s   best: 25.3437
2023-11-07 22:06:02,668:INFO:  Epoch 349/500:  train Loss: 21.1184   val Loss: 25.8029   time: 128.33s   best: 25.3437
2023-11-07 22:08:11,442:INFO:  Epoch 350/500:  train Loss: 21.2859   val Loss: 25.5438   time: 128.77s   best: 25.3437
2023-11-07 22:10:19,866:INFO:  Epoch 351/500:  train Loss: 21.7341   val Loss: 25.7838   time: 128.41s   best: 25.3437
2023-11-07 22:12:27,475:INFO:  Epoch 352/500:  train Loss: 21.2055   val Loss: 25.7468   time: 127.60s   best: 25.3437
2023-11-07 22:14:35,527:INFO:  Epoch 353/500:  train Loss: 21.7490   val Loss: 29.1304   time: 128.05s   best: 25.3437
2023-11-07 22:16:42,946:INFO:  Epoch 354/500:  train Loss: 21.9300   val Loss: 25.5598   time: 127.42s   best: 25.3437
2023-11-07 22:18:50,773:INFO:  Epoch 355/500:  train Loss: 21.1280   val Loss: 25.7661   time: 127.83s   best: 25.3437
2023-11-07 22:20:59,439:INFO:  Epoch 356/500:  train Loss: 21.2894   val Loss: 25.8526   time: 128.67s   best: 25.3437
2023-11-07 22:23:07,953:INFO:  Epoch 357/500:  train Loss: 21.3119   val Loss: 25.6541   time: 128.51s   best: 25.3437
2023-11-07 22:25:16,370:INFO:  Epoch 358/500:  train Loss: 21.1656   val Loss: 26.0392   time: 128.41s   best: 25.3437
2023-11-07 22:27:24,097:INFO:  Epoch 359/500:  train Loss: 21.1801   val Loss: 25.6173   time: 127.71s   best: 25.3437
2023-11-07 22:29:32,561:INFO:  Epoch 360/500:  train Loss: 21.1916   val Loss: 26.4586   time: 128.45s   best: 25.3437
2023-11-07 22:31:40,943:INFO:  Epoch 361/500:  train Loss: 21.2944   val Loss: 26.0519   time: 128.37s   best: 25.3437
2023-11-07 22:33:48,693:INFO:  Epoch 362/500:  train Loss: 21.0819   val Loss: 25.3681   time: 127.74s   best: 25.3437
2023-11-07 22:35:56,435:INFO:  Epoch 363/500:  train Loss: 21.3613   val Loss: 26.5175   time: 127.73s   best: 25.3437
2023-11-07 22:38:03,849:INFO:  Epoch 364/500:  train Loss: 21.1786   val Loss: 25.6597   time: 127.40s   best: 25.3437
2023-11-07 22:40:11,238:INFO:  Epoch 365/500:  train Loss: 21.2916   val Loss: 25.6755   time: 127.38s   best: 25.3437
2023-11-07 22:42:18,418:INFO:  Epoch 366/500:  train Loss: 21.1222   val Loss: 25.9699   time: 127.18s   best: 25.3437
2023-11-07 22:44:25,666:INFO:  Epoch 367/500:  train Loss: 21.0459   val Loss: 26.8235   time: 127.24s   best: 25.3437
2023-11-07 22:46:32,832:INFO:  Epoch 368/500:  train Loss: 21.0031   val Loss: 25.4724   time: 127.15s   best: 25.3437
2023-11-07 22:48:40,727:INFO:  Epoch 369/500:  train Loss: 21.1619   val Loss: 25.8419   time: 127.88s   best: 25.3437
2023-11-07 22:50:49,400:INFO:  Epoch 370/500:  train Loss: 20.9077   val Loss: 25.4106   time: 128.66s   best: 25.3437
2023-11-07 22:52:57,397:INFO:  Epoch 371/500:  train Loss: 21.1826   val Loss: 25.6610   time: 127.99s   best: 25.3437
2023-11-07 22:55:05,892:INFO:  Epoch 372/500:  train Loss: 20.9866   val Loss: 25.4409   time: 128.47s   best: 25.3437
2023-11-07 22:57:13,555:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 22:57:13,581:INFO:  Epoch 373/500:  train Loss: 21.0233   val Loss: 25.1099   time: 127.63s   best: 25.1099
2023-11-07 22:59:21,183:INFO:  Epoch 374/500:  train Loss: 21.0362   val Loss: 25.4815   time: 127.59s   best: 25.1099
2023-11-07 23:01:28,846:INFO:  Epoch 375/500:  train Loss: 21.1123   val Loss: 25.3507   time: 127.64s   best: 25.1099
2023-11-07 23:03:37,109:INFO:  Epoch 376/500:  train Loss: 21.1477   val Loss: 25.5286   time: 128.25s   best: 25.1099
2023-11-07 23:05:45,549:INFO:  Epoch 377/500:  train Loss: 20.9208   val Loss: 25.7054   time: 128.44s   best: 25.1099
2023-11-07 23:07:53,049:INFO:  Epoch 378/500:  train Loss: 21.4420   val Loss: 25.2570   time: 127.49s   best: 25.1099
2023-11-07 23:10:00,568:INFO:  Epoch 379/500:  train Loss: 20.8965   val Loss: 26.0785   time: 127.51s   best: 25.1099
2023-11-07 23:12:07,864:INFO:  Epoch 380/500:  train Loss: 20.8766   val Loss: 25.6872   time: 127.30s   best: 25.1099
2023-11-07 23:14:15,190:INFO:  Epoch 381/500:  train Loss: 21.3677   val Loss: 26.2120   time: 127.30s   best: 25.1099
2023-11-07 23:16:22,461:INFO:  Epoch 382/500:  train Loss: 21.3029   val Loss: 25.3802   time: 127.26s   best: 25.1099
2023-11-07 23:18:30,459:INFO:  Epoch 383/500:  train Loss: 20.8749   val Loss: 25.2635   time: 128.00s   best: 25.1099
2023-11-07 23:20:37,663:INFO:  Epoch 384/500:  train Loss: 20.8409   val Loss: 25.4684   time: 127.20s   best: 25.1099
2023-11-07 23:22:46,281:INFO:  Epoch 385/500:  train Loss: 20.8690   val Loss: 25.3114   time: 128.61s   best: 25.1099
2023-11-07 23:24:54,674:INFO:  Epoch 386/500:  train Loss: 21.0596   val Loss: 26.6677   time: 128.38s   best: 25.1099
2023-11-07 23:27:02,262:INFO:  Epoch 387/500:  train Loss: 21.1565   val Loss: 25.4613   time: 127.58s   best: 25.1099
2023-11-07 23:29:09,903:INFO:  Epoch 388/500:  train Loss: 20.7036   val Loss: 25.9434   time: 127.63s   best: 25.1099
2023-11-07 23:31:17,310:INFO:  Epoch 389/500:  train Loss: 20.9109   val Loss: 25.5411   time: 127.39s   best: 25.1099
2023-11-07 23:33:24,560:INFO:  Epoch 390/500:  train Loss: 20.9800   val Loss: 25.3470   time: 127.24s   best: 25.1099
2023-11-07 23:35:31,835:INFO:  Epoch 391/500:  train Loss: 21.0632   val Loss: 25.4504   time: 127.27s   best: 25.1099
2023-11-07 23:37:39,087:INFO:  Epoch 392/500:  train Loss: 20.7714   val Loss: 25.5024   time: 127.24s   best: 25.1099
2023-11-07 23:39:46,683:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-07 23:39:46,716:INFO:  Epoch 393/500:  train Loss: 20.8840   val Loss: 24.8079   time: 127.58s   best: 24.8079
2023-11-07 23:41:54,216:INFO:  Epoch 394/500:  train Loss: 20.6552   val Loss: 25.1885   time: 127.48s   best: 24.8079
2023-11-07 23:44:01,687:INFO:  Epoch 395/500:  train Loss: 20.7811   val Loss: 25.1761   time: 127.46s   best: 24.8079
2023-11-07 23:46:09,318:INFO:  Epoch 396/500:  train Loss: 21.3405   val Loss: 26.0393   time: 127.63s   best: 24.8079
2023-11-07 23:48:16,680:INFO:  Epoch 397/500:  train Loss: 20.7093   val Loss: 25.1863   time: 127.34s   best: 24.8079
2023-11-07 23:50:23,847:INFO:  Epoch 398/500:  train Loss: 20.7479   val Loss: 25.2641   time: 127.17s   best: 24.8079
2023-11-07 23:52:31,138:INFO:  Epoch 399/500:  train Loss: 20.6915   val Loss: 25.1973   time: 127.29s   best: 24.8079
2023-11-07 23:54:38,902:INFO:  Epoch 400/500:  train Loss: 20.6939   val Loss: 25.7226   time: 127.75s   best: 24.8079
2023-11-07 23:56:46,647:INFO:  Epoch 401/500:  train Loss: 20.6934   val Loss: 25.9182   time: 127.74s   best: 24.8079
2023-11-07 23:58:55,139:INFO:  Epoch 402/500:  train Loss: 20.6086   val Loss: 25.3264   time: 128.48s   best: 24.8079
2023-11-08 00:01:03,184:INFO:  Epoch 403/500:  train Loss: 20.7477   val Loss: 25.1327   time: 128.04s   best: 24.8079
2023-11-08 00:03:10,944:INFO:  Epoch 404/500:  train Loss: 20.7050   val Loss: 25.4104   time: 127.75s   best: 24.8079
2023-11-08 00:05:19,154:INFO:  Epoch 405/500:  train Loss: 21.0006   val Loss: 26.6086   time: 128.20s   best: 24.8079
2023-11-08 00:07:26,900:INFO:  Epoch 406/500:  train Loss: 20.9911   val Loss: 25.4907   time: 127.75s   best: 24.8079
2023-11-08 00:09:35,083:INFO:  Epoch 407/500:  train Loss: 20.6554   val Loss: 25.5420   time: 128.16s   best: 24.8079
2023-11-08 00:11:42,712:INFO:  Epoch 408/500:  train Loss: 20.5554   val Loss: 27.5618   time: 127.62s   best: 24.8079
2023-11-08 00:13:50,465:INFO:  Epoch 409/500:  train Loss: 20.7147   val Loss: 26.2721   time: 127.74s   best: 24.8079
2023-11-08 00:15:58,479:INFO:  Epoch 410/500:  train Loss: 20.6462   val Loss: 24.9572   time: 128.01s   best: 24.8079
2023-11-08 00:18:05,818:INFO:  Epoch 411/500:  train Loss: 20.5701   val Loss: 25.3605   time: 127.34s   best: 24.8079
2023-11-08 00:20:12,910:INFO:  Epoch 412/500:  train Loss: 20.6498   val Loss: 25.4081   time: 127.09s   best: 24.8079
2023-11-08 00:22:20,114:INFO:  Epoch 413/500:  train Loss: 20.5194   val Loss: 25.1801   time: 127.20s   best: 24.8079
2023-11-08 00:24:27,862:INFO:  Epoch 414/500:  train Loss: 20.7544   val Loss: 25.0744   time: 127.74s   best: 24.8079
2023-11-08 00:26:35,199:INFO:  Epoch 415/500:  train Loss: 21.2208   val Loss: 25.3320   time: 127.34s   best: 24.8079
2023-11-08 00:28:42,419:INFO:  Epoch 416/500:  train Loss: 20.5457   val Loss: 25.1234   time: 127.21s   best: 24.8079
2023-11-08 00:30:49,730:INFO:  Epoch 417/500:  train Loss: 21.1859   val Loss: 25.2058   time: 127.30s   best: 24.8079
2023-11-08 00:32:57,518:INFO:  Epoch 418/500:  train Loss: 20.7632   val Loss: 26.5276   time: 127.78s   best: 24.8079
2023-11-08 00:35:05,642:INFO:  Epoch 419/500:  train Loss: 20.7378   val Loss: 25.3964   time: 128.11s   best: 24.8079
2023-11-08 00:37:13,674:INFO:  Epoch 420/500:  train Loss: 20.6540   val Loss: 26.0106   time: 128.01s   best: 24.8079
2023-11-08 00:39:21,652:INFO:  Epoch 421/500:  train Loss: 20.7994   val Loss: 25.3689   time: 127.97s   best: 24.8079
2023-11-08 00:41:29,240:INFO:  Epoch 422/500:  train Loss: 20.4761   val Loss: 25.5569   time: 127.58s   best: 24.8079
2023-11-08 00:43:37,718:INFO:  Epoch 423/500:  train Loss: 20.5594   val Loss: 25.4367   time: 128.47s   best: 24.8079
2023-11-08 00:45:45,986:INFO:  Epoch 424/500:  train Loss: 20.4663   val Loss: 25.2528   time: 128.24s   best: 24.8079
2023-11-08 00:47:54,508:INFO:  Epoch 425/500:  train Loss: 20.6590   val Loss: 25.5993   time: 128.51s   best: 24.8079
2023-11-08 00:50:02,067:INFO:  Epoch 426/500:  train Loss: 20.4600   val Loss: 25.6516   time: 127.55s   best: 24.8079
2023-11-08 00:52:09,855:INFO:  Epoch 427/500:  train Loss: 20.4793   val Loss: 27.5717   time: 127.79s   best: 24.8079
2023-11-08 00:54:18,043:INFO:  Epoch 428/500:  train Loss: 20.5543   val Loss: 25.6237   time: 128.16s   best: 24.8079
2023-11-08 00:56:25,255:INFO:  Epoch 429/500:  train Loss: 20.5829   val Loss: 28.1044   time: 127.21s   best: 24.8079
2023-11-08 00:58:32,857:INFO:  Epoch 430/500:  train Loss: 20.6937   val Loss: 25.5232   time: 127.58s   best: 24.8079
2023-11-08 01:00:40,769:INFO:  Epoch 431/500:  train Loss: 20.5295   val Loss: 25.7811   time: 127.89s   best: 24.8079
2023-11-08 01:02:48,994:INFO:  Epoch 432/500:  train Loss: 20.9068   val Loss: 25.4083   time: 128.21s   best: 24.8079
2023-11-08 01:04:57,427:INFO:  Epoch 433/500:  train Loss: 20.4899   val Loss: 27.6370   time: 128.42s   best: 24.8079
2023-11-08 01:07:05,013:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-08 01:07:05,043:INFO:  Epoch 434/500:  train Loss: 20.4482   val Loss: 24.7497   time: 127.56s   best: 24.7497
2023-11-08 01:09:12,327:INFO:  Epoch 435/500:  train Loss: 21.2598   val Loss: 25.0330   time: 127.27s   best: 24.7497
2023-11-08 01:11:19,436:INFO:  Epoch 436/500:  train Loss: 20.2770   val Loss: 25.2418   time: 127.10s   best: 24.7497
2023-11-08 01:13:26,649:INFO:  Epoch 437/500:  train Loss: 20.4799   val Loss: 25.4208   time: 127.21s   best: 24.7497
2023-11-08 01:15:33,987:INFO:  Epoch 438/500:  train Loss: 20.3638   val Loss: 25.0156   time: 127.31s   best: 24.7497
2023-11-08 01:17:42,001:INFO:  Epoch 439/500:  train Loss: 20.4887   val Loss: 25.2584   time: 128.00s   best: 24.7497
2023-11-08 01:19:49,364:INFO:  Epoch 440/500:  train Loss: 20.3949   val Loss: 25.0389   time: 127.34s   best: 24.7497
2023-11-08 01:21:56,697:INFO:  Epoch 441/500:  train Loss: 20.2827   val Loss: 25.2876   time: 127.33s   best: 24.7497
2023-11-08 01:24:04,925:INFO:  Epoch 442/500:  train Loss: 20.3941   val Loss: 25.0748   time: 128.22s   best: 24.7497
2023-11-08 01:26:12,964:INFO:  Epoch 443/500:  train Loss: 20.7057   val Loss: 25.2948   time: 128.04s   best: 24.7497
2023-11-08 01:28:20,154:INFO:  Epoch 444/500:  train Loss: 20.2528   val Loss: 25.9542   time: 127.18s   best: 24.7497
2023-11-08 01:30:27,619:INFO:  Epoch 445/500:  train Loss: 21.2796   val Loss: 28.2843   time: 127.45s   best: 24.7497
2023-11-08 01:32:34,627:INFO:  Epoch 446/500:  train Loss: 20.9176   val Loss: 25.0056   time: 127.00s   best: 24.7497
2023-11-08 01:34:41,745:INFO:  Epoch 447/500:  train Loss: 20.3594   val Loss: 30.9435   time: 127.12s   best: 24.7497
2023-11-08 01:36:49,039:INFO:  Epoch 448/500:  train Loss: 20.4147   val Loss: 25.0488   time: 127.29s   best: 24.7497
2023-11-08 01:38:56,507:INFO:  Epoch 449/500:  train Loss: 20.2338   val Loss: 25.0895   time: 127.46s   best: 24.7497
2023-11-08 01:41:04,954:INFO:  Epoch 450/500:  train Loss: 20.4891   val Loss: 25.2290   time: 128.45s   best: 24.7497
2023-11-08 01:43:12,794:INFO:  Epoch 451/500:  train Loss: 20.2356   val Loss: 25.1822   time: 127.83s   best: 24.7497
2023-11-08 01:45:20,052:INFO:  Epoch 452/500:  train Loss: 20.2579   val Loss: 25.1752   time: 127.25s   best: 24.7497
2023-11-08 01:47:27,234:INFO:  Epoch 453/500:  train Loss: 20.7068   val Loss: 25.1373   time: 127.16s   best: 24.7497
2023-11-08 01:49:34,549:INFO:  Epoch 454/500:  train Loss: 20.2187   val Loss: 25.0456   time: 127.30s   best: 24.7497
2023-11-08 01:51:42,585:INFO:  Epoch 455/500:  train Loss: 20.2313   val Loss: 24.9542   time: 128.04s   best: 24.7497
2023-11-08 01:53:49,686:INFO:  Epoch 456/500:  train Loss: 20.2664   val Loss: 25.8344   time: 127.10s   best: 24.7497
2023-11-08 01:55:57,418:INFO:  Epoch 457/500:  train Loss: 20.3096   val Loss: 26.6019   time: 127.73s   best: 24.7497
2023-11-08 01:58:05,477:INFO:  Epoch 458/500:  train Loss: 20.3334   val Loss: 25.8791   time: 128.05s   best: 24.7497
2023-11-08 02:00:12,954:INFO:  Epoch 459/500:  train Loss: 20.1576   val Loss: 25.4097   time: 127.48s   best: 24.7497
2023-11-08 02:02:20,207:INFO:  Epoch 460/500:  train Loss: 20.1719   val Loss: 25.7758   time: 127.25s   best: 24.7497
2023-11-08 02:04:27,266:INFO:  Epoch 461/500:  train Loss: 20.2011   val Loss: 25.2123   time: 127.05s   best: 24.7497
2023-11-08 02:06:34,621:INFO:  Epoch 462/500:  train Loss: 20.1937   val Loss: 25.5479   time: 127.34s   best: 24.7497
2023-11-08 02:08:41,942:INFO:  Epoch 463/500:  train Loss: 20.1572   val Loss: 25.0241   time: 127.31s   best: 24.7497
2023-11-08 02:10:49,268:INFO:  Epoch 464/500:  train Loss: 20.0912   val Loss: 25.2225   time: 127.31s   best: 24.7497
2023-11-08 02:12:57,944:INFO:  Epoch 465/500:  train Loss: 20.3950   val Loss: 25.4526   time: 128.68s   best: 24.7497
2023-11-08 02:15:05,508:INFO:  Epoch 466/500:  train Loss: 20.3600   val Loss: 25.9731   time: 127.56s   best: 24.7497
2023-11-08 02:17:12,884:INFO:  Epoch 467/500:  train Loss: 20.1740   val Loss: 24.9423   time: 127.36s   best: 24.7497
2023-11-08 02:19:20,279:INFO:  Epoch 468/500:  train Loss: 20.3022   val Loss: 25.1261   time: 127.37s   best: 24.7497
2023-11-08 02:21:27,557:INFO:  Epoch 469/500:  train Loss: 20.1224   val Loss: 24.7553   time: 127.27s   best: 24.7497
2023-11-08 02:23:34,896:INFO:  Epoch 470/500:  train Loss: 20.0383   val Loss: 25.2757   time: 127.33s   best: 24.7497
2023-11-08 02:25:42,300:INFO:  Epoch 471/500:  train Loss: 20.6941   val Loss: 25.6194   time: 127.38s   best: 24.7497
2023-11-08 02:27:49,642:INFO:  Epoch 472/500:  train Loss: 20.0171   val Loss: 25.2976   time: 127.33s   best: 24.7497
2023-11-08 02:29:57,047:INFO:  Epoch 473/500:  train Loss: 20.0155   val Loss: 25.7110   time: 127.39s   best: 24.7497
2023-11-08 02:32:04,338:INFO:  Epoch 474/500:  train Loss: 20.3367   val Loss: 24.7929   time: 127.29s   best: 24.7497
2023-11-08 02:34:11,515:INFO:  Epoch 475/500:  train Loss: 20.0668   val Loss: 25.1983   time: 127.17s   best: 24.7497
2023-11-08 02:36:18,693:INFO:  Epoch 476/500:  train Loss: 20.0400   val Loss: 24.9410   time: 127.18s   best: 24.7497
2023-11-08 02:38:26,146:INFO:  Epoch 477/500:  train Loss: 20.0178   val Loss: 25.0166   time: 127.43s   best: 24.7497
2023-11-08 02:40:33,769:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_36f6.pt
2023-11-08 02:40:33,801:INFO:  Epoch 478/500:  train Loss: 20.1574   val Loss: 24.5831   time: 127.60s   best: 24.5831
2023-11-08 02:42:41,387:INFO:  Epoch 479/500:  train Loss: 19.9515   val Loss: 24.8205   time: 127.58s   best: 24.5831
2023-11-08 02:44:48,901:INFO:  Epoch 480/500:  train Loss: 20.3704   val Loss: 25.2798   time: 127.51s   best: 24.5831
2023-11-08 02:46:56,469:INFO:  Epoch 481/500:  train Loss: 20.1755   val Loss: 24.9129   time: 127.56s   best: 24.5831
2023-11-08 02:49:04,131:INFO:  Epoch 482/500:  train Loss: 20.0016   val Loss: 31.5297   time: 127.66s   best: 24.5831
2023-11-08 02:51:11,877:INFO:  Epoch 483/500:  train Loss: 20.1856   val Loss: 24.9506   time: 127.73s   best: 24.5831
2023-11-08 02:53:19,205:INFO:  Epoch 484/500:  train Loss: 20.0387   val Loss: 25.5093   time: 127.31s   best: 24.5831
2023-11-08 02:55:26,491:INFO:  Epoch 485/500:  train Loss: 20.6745   val Loss: 25.6547   time: 127.26s   best: 24.5831
2023-11-08 02:57:33,488:INFO:  Epoch 486/500:  train Loss: 19.9780   val Loss: 24.8364   time: 127.00s   best: 24.5831
2023-11-08 02:59:40,715:INFO:  Epoch 487/500:  train Loss: 20.2121   val Loss: 25.4251   time: 127.22s   best: 24.5831
2023-11-08 03:01:48,035:INFO:  Epoch 488/500:  train Loss: 20.0972   val Loss: 25.2960   time: 127.32s   best: 24.5831
2023-11-08 03:03:55,347:INFO:  Epoch 489/500:  train Loss: 20.2045   val Loss: 24.8890   time: 127.31s   best: 24.5831
2023-11-08 03:06:02,625:INFO:  Epoch 490/500:  train Loss: 20.0580   val Loss: 26.5834   time: 127.28s   best: 24.5831
2023-11-08 03:08:09,988:INFO:  Epoch 491/500:  train Loss: 20.3580   val Loss: 24.9598   time: 127.34s   best: 24.5831
2023-11-08 03:10:17,274:INFO:  Epoch 492/500:  train Loss: 19.8443   val Loss: 24.9894   time: 127.28s   best: 24.5831
2023-11-08 03:12:24,806:INFO:  Epoch 493/500:  train Loss: 19.9081   val Loss: 25.7074   time: 127.53s   best: 24.5831
2023-11-08 03:14:32,237:INFO:  Epoch 494/500:  train Loss: 19.8965   val Loss: 25.1893   time: 127.42s   best: 24.5831
2023-11-08 03:16:39,720:INFO:  Epoch 495/500:  train Loss: 19.8777   val Loss: 27.4428   time: 127.47s   best: 24.5831
2023-11-08 03:18:46,942:INFO:  Epoch 496/500:  train Loss: 20.0587   val Loss: 25.6514   time: 127.21s   best: 24.5831
2023-11-08 03:20:54,220:INFO:  Epoch 497/500:  train Loss: 19.8629   val Loss: 25.1314   time: 127.27s   best: 24.5831
2023-11-08 03:23:01,698:INFO:  Epoch 498/500:  train Loss: 20.7228   val Loss: 28.0735   time: 127.47s   best: 24.5831
2023-11-08 03:25:10,010:INFO:  Epoch 499/500:  train Loss: 21.6343   val Loss: 24.9646   time: 128.29s   best: 24.5831
2023-11-08 03:27:17,603:INFO:  Epoch 500/500:  train Loss: 19.8704   val Loss: 25.1573   time: 127.58s   best: 24.5831
2023-11-08 03:27:17,615:INFO:  -----> Training complete in 1072m 22s   best validation loss: 24.5831
 
2023-11-08 12:50:47,193:INFO:  Starting experiment lstm autoencoder with 0.2 dataset (0.1 dropout)
2023-11-08 12:50:47,212:INFO:  Defining the model
2023-11-08 12:50:47,306:INFO:  Reading the dataset
2023-11-08 13:23:08,169:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:23:08,198:INFO:  Epoch 1/500:  train Loss: 87.5094   val Loss: 84.4853   time: 131.98s   best: 84.4853
2023-11-08 13:25:20,428:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:25:20,449:INFO:  Epoch 2/500:  train Loss: 79.6794   val Loss: 74.6737   time: 132.23s   best: 74.6737
2023-11-08 13:27:34,221:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:27:34,241:INFO:  Epoch 3/500:  train Loss: 73.4473   val Loss: 71.0342   time: 133.77s   best: 71.0342
2023-11-08 13:29:44,657:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:29:44,678:INFO:  Epoch 4/500:  train Loss: 70.3925   val Loss: 69.7231   time: 130.41s   best: 69.7231
2023-11-08 13:31:57,118:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:31:57,140:INFO:  Epoch 5/500:  train Loss: 68.4734   val Loss: 67.1835   time: 132.44s   best: 67.1835
2023-11-08 13:34:09,618:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:34:09,639:INFO:  Epoch 6/500:  train Loss: 66.8954   val Loss: 66.8678   time: 132.47s   best: 66.8678
2023-11-08 13:36:38,616:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:36:38,636:INFO:  Epoch 7/500:  train Loss: 65.4653   val Loss: 64.4745   time: 148.97s   best: 64.4745
2023-11-08 13:38:49,930:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:38:50,004:INFO:  Epoch 8/500:  train Loss: 64.2466   val Loss: 63.3494   time: 131.29s   best: 63.3494
2023-11-08 13:41:11,314:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:41:11,343:INFO:  Epoch 9/500:  train Loss: 63.2183   val Loss: 63.3139   time: 141.31s   best: 63.3139
2023-11-08 13:43:52,216:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:43:52,254:INFO:  Epoch 10/500:  train Loss: 61.9845   val Loss: 61.0828   time: 160.86s   best: 61.0828
2023-11-08 13:46:25,314:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:46:25,335:INFO:  Epoch 11/500:  train Loss: 61.2765   val Loss: 60.3185   time: 153.03s   best: 60.3185
2023-11-08 13:48:58,661:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:48:58,682:INFO:  Epoch 12/500:  train Loss: 60.2762   val Loss: 60.0454   time: 153.30s   best: 60.0454
2023-11-08 13:51:10,946:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:51:10,973:INFO:  Epoch 13/500:  train Loss: 59.4842   val Loss: 58.5231   time: 132.26s   best: 58.5231
2023-11-08 13:53:28,009:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:53:28,030:INFO:  Epoch 14/500:  train Loss: 58.3370   val Loss: 58.4669   time: 137.03s   best: 58.4669
2023-11-08 13:55:39,478:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:55:39,498:INFO:  Epoch 15/500:  train Loss: 57.1688   val Loss: 55.9196   time: 131.44s   best: 55.9196
2023-11-08 13:57:52,061:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 13:57:52,082:INFO:  Epoch 16/500:  train Loss: 56.0267   val Loss: 55.1474   time: 132.56s   best: 55.1474
2023-11-08 14:00:04,167:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:00:04,188:INFO:  Epoch 17/500:  train Loss: 55.0115   val Loss: 53.9867   time: 132.08s   best: 53.9867
2023-11-08 14:02:17,081:INFO:  Epoch 18/500:  train Loss: 53.7289   val Loss: 54.1594   time: 132.89s   best: 53.9867
2023-11-08 14:04:29,991:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:04:30,012:INFO:  Epoch 19/500:  train Loss: 52.4971   val Loss: 52.0567   time: 132.90s   best: 52.0567
2023-11-08 14:06:42,161:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:06:42,182:INFO:  Epoch 20/500:  train Loss: 51.3733   val Loss: 50.5266   time: 132.14s   best: 50.5266
2023-11-08 14:08:54,195:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:08:54,227:INFO:  Epoch 21/500:  train Loss: 50.2879   val Loss: 50.4402   time: 132.01s   best: 50.4402
2023-11-08 14:11:05,718:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:11:05,739:INFO:  Epoch 22/500:  train Loss: 49.3583   val Loss: 48.8168   time: 131.49s   best: 48.8168
2023-11-08 14:13:18,619:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:13:18,640:INFO:  Epoch 23/500:  train Loss: 48.3802   val Loss: 48.1704   time: 132.87s   best: 48.1704
2023-11-08 14:15:29,723:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:15:29,744:INFO:  Epoch 24/500:  train Loss: 47.7576   val Loss: 47.4033   time: 131.08s   best: 47.4033
2023-11-08 14:17:41,999:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:17:42,019:INFO:  Epoch 25/500:  train Loss: 46.8761   val Loss: 47.0577   time: 132.25s   best: 47.0577
2023-11-08 14:19:52,086:INFO:  Epoch 26/500:  train Loss: 46.1644   val Loss: 47.0673   time: 130.07s   best: 47.0577
2023-11-08 14:22:02,376:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:22:02,396:INFO:  Epoch 27/500:  train Loss: 45.4117   val Loss: 45.3861   time: 130.29s   best: 45.3861
2023-11-08 14:24:14,937:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:24:15,085:INFO:  Epoch 28/500:  train Loss: 44.8601   val Loss: 44.4765   time: 132.45s   best: 44.4765
2023-11-08 14:26:26,060:INFO:  Epoch 29/500:  train Loss: 43.8770   val Loss: 44.5681   time: 130.97s   best: 44.4765
2023-11-08 14:28:38,985:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:28:39,005:INFO:  Epoch 30/500:  train Loss: 43.6182   val Loss: 43.6160   time: 132.92s   best: 43.6160
2023-11-08 14:30:49,483:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:30:49,503:INFO:  Epoch 31/500:  train Loss: 43.1103   val Loss: 43.1821   time: 130.47s   best: 43.1821
2023-11-08 14:32:59,686:INFO:  Epoch 32/500:  train Loss: 42.3062   val Loss: 44.2121   time: 130.18s   best: 43.1821
2023-11-08 14:35:11,455:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:35:11,475:INFO:  Epoch 33/500:  train Loss: 41.7778   val Loss: 42.6509   time: 131.76s   best: 42.6509
2023-11-08 14:37:23,939:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:37:23,959:INFO:  Epoch 34/500:  train Loss: 41.2706   val Loss: 42.3290   time: 132.46s   best: 42.3290
2023-11-08 14:39:36,392:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:39:36,411:INFO:  Epoch 35/500:  train Loss: 40.8699   val Loss: 41.6386   time: 132.43s   best: 41.6386
2023-11-08 14:41:48,862:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:41:48,883:INFO:  Epoch 36/500:  train Loss: 40.3565   val Loss: 41.5252   time: 132.45s   best: 41.5252
2023-11-08 14:44:00,398:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:44:00,419:INFO:  Epoch 37/500:  train Loss: 39.9735   val Loss: 40.7881   time: 131.51s   best: 40.7881
2023-11-08 14:46:13,121:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:46:13,141:INFO:  Epoch 38/500:  train Loss: 39.6382   val Loss: 39.8369   time: 132.70s   best: 39.8369
2023-11-08 14:48:24,730:INFO:  Epoch 39/500:  train Loss: 39.1211   val Loss: 40.0063   time: 131.59s   best: 39.8369
2023-11-08 14:50:35,763:INFO:  Epoch 40/500:  train Loss: 38.7613   val Loss: 40.0586   time: 131.03s   best: 39.8369
2023-11-08 14:52:47,181:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:52:47,211:INFO:  Epoch 41/500:  train Loss: 38.3743   val Loss: 39.2640   time: 131.40s   best: 39.2640
2023-11-08 14:55:58,307:INFO:  Epoch 42/500:  train Loss: 38.0463   val Loss: 39.9912   time: 191.10s   best: 39.2640
2023-11-08 14:58:11,538:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 14:58:11,558:INFO:  Epoch 43/500:  train Loss: 37.6188   val Loss: 39.1162   time: 133.22s   best: 39.1162
2023-11-08 15:00:24,003:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:00:24,023:INFO:  Epoch 44/500:  train Loss: 37.5923   val Loss: 38.3711   time: 132.44s   best: 38.3711
2023-11-08 15:02:53,662:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:02:53,682:INFO:  Epoch 45/500:  train Loss: 37.2469   val Loss: 37.9033   time: 149.63s   best: 37.9033
2023-11-08 15:05:39,267:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:05:39,321:INFO:  Epoch 46/500:  train Loss: 36.9844   val Loss: 37.7334   time: 165.57s   best: 37.7334
2023-11-08 15:08:56,415:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:08:56,453:INFO:  Epoch 47/500:  train Loss: 36.4964   val Loss: 37.3952   time: 197.08s   best: 37.3952
2023-11-08 15:12:15,129:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:12:15,167:INFO:  Epoch 48/500:  train Loss: 36.3872   val Loss: 36.6710   time: 198.67s   best: 36.6710
2023-11-08 15:14:27,940:INFO:  Epoch 49/500:  train Loss: 36.2152   val Loss: 37.3182   time: 132.77s   best: 36.6710
2023-11-08 15:17:19,919:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:17:20,394:INFO:  Epoch 50/500:  train Loss: 35.7267   val Loss: 36.0612   time: 171.97s   best: 36.0612
2023-11-08 15:19:30,344:INFO:  Epoch 51/500:  train Loss: 35.9136   val Loss: 36.3958   time: 129.95s   best: 36.0612
2023-11-08 15:21:42,728:INFO:  Epoch 52/500:  train Loss: 35.2349   val Loss: 36.1688   time: 132.38s   best: 36.0612
2023-11-08 15:23:55,208:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:23:55,228:INFO:  Epoch 53/500:  train Loss: 34.9683   val Loss: 35.5086   time: 132.48s   best: 35.5086
2023-11-08 15:26:07,565:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:26:07,585:INFO:  Epoch 54/500:  train Loss: 34.8064   val Loss: 35.3241   time: 132.32s   best: 35.3241
2023-11-08 15:28:19,668:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:28:19,688:INFO:  Epoch 55/500:  train Loss: 34.7056   val Loss: 34.8208   time: 132.08s   best: 34.8208
2023-11-08 15:30:29,926:INFO:  Epoch 56/500:  train Loss: 34.5770   val Loss: 35.7708   time: 130.24s   best: 34.8208
2023-11-08 15:32:43,300:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:32:43,320:INFO:  Epoch 57/500:  train Loss: 34.2565   val Loss: 34.6712   time: 133.37s   best: 34.6712
2023-11-08 15:34:58,041:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:34:58,061:INFO:  Epoch 58/500:  train Loss: 33.8997   val Loss: 34.5008   time: 134.72s   best: 34.5008
2023-11-08 15:37:23,847:INFO:  Epoch 59/500:  train Loss: 33.8006   val Loss: 34.8416   time: 145.78s   best: 34.5008
2023-11-08 15:40:22,485:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:40:22,522:INFO:  Epoch 60/500:  train Loss: 33.6623   val Loss: 33.8414   time: 178.63s   best: 33.8414
2023-11-08 15:43:09,417:INFO:  Epoch 61/500:  train Loss: 33.4141   val Loss: 34.0397   time: 166.89s   best: 33.8414
2023-11-08 15:45:45,427:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:45:45,448:INFO:  Epoch 62/500:  train Loss: 33.1065   val Loss: 33.6742   time: 156.01s   best: 33.6742
2023-11-08 15:48:24,073:INFO:  Epoch 63/500:  train Loss: 33.3296   val Loss: 34.0823   time: 158.61s   best: 33.6742
2023-11-08 15:50:36,402:INFO:  Epoch 64/500:  train Loss: 32.9366   val Loss: 33.7854   time: 132.32s   best: 33.6742
2023-11-08 15:53:38,383:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 15:53:38,421:INFO:  Epoch 65/500:  train Loss: 32.6599   val Loss: 33.1984   time: 181.97s   best: 33.1984
2023-11-08 15:56:31,866:INFO:  Epoch 66/500:  train Loss: 32.6946   val Loss: 33.9910   time: 173.44s   best: 33.1984
2023-11-08 15:58:41,956:INFO:  Epoch 67/500:  train Loss: 32.3864   val Loss: 33.2038   time: 130.08s   best: 33.1984
2023-11-08 16:00:53,559:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 16:00:53,579:INFO:  Epoch 68/500:  train Loss: 32.1431   val Loss: 33.0522   time: 131.60s   best: 33.0522
2023-11-08 16:03:05,151:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 16:03:05,211:INFO:  Epoch 69/500:  train Loss: 32.0596   val Loss: 32.8625   time: 131.57s   best: 32.8625
2023-11-08 16:05:17,792:INFO:  Epoch 70/500:  train Loss: 32.0567   val Loss: 32.9594   time: 132.58s   best: 32.8625
2023-11-08 16:07:30,011:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 16:07:30,032:INFO:  Epoch 71/500:  train Loss: 31.8496   val Loss: 32.6096   time: 132.21s   best: 32.6096
2023-11-08 16:09:41,805:INFO:  Epoch 72/500:  train Loss: 31.6503   val Loss: 33.0331   time: 131.77s   best: 32.6096
2023-11-08 16:11:54,605:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 16:11:54,686:INFO:  Epoch 73/500:  train Loss: 31.4877   val Loss: 32.5438   time: 132.80s   best: 32.5438
2023-11-08 16:14:05,673:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 16:14:05,694:INFO:  Epoch 74/500:  train Loss: 31.3336   val Loss: 32.0195   time: 130.98s   best: 32.0195
2023-11-08 16:17:12,909:INFO:  Epoch 75/500:  train Loss: 31.1825   val Loss: 32.2920   time: 187.21s   best: 32.0195
2023-11-08 16:19:45,060:INFO:  Epoch 76/500:  train Loss: 31.1511   val Loss: 32.7051   time: 152.15s   best: 32.0195
2023-11-08 16:21:56,528:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 16:21:56,562:INFO:  Epoch 77/500:  train Loss: 30.8810   val Loss: 31.6949   time: 131.46s   best: 31.6949
2023-11-08 16:24:06,093:INFO:  Epoch 78/500:  train Loss: 31.0609   val Loss: 31.8614   time: 129.53s   best: 31.6949
2023-11-08 16:26:31,075:INFO:  Epoch 79/500:  train Loss: 30.7042   val Loss: 32.0496   time: 144.98s   best: 31.6949
2023-11-08 16:28:55,185:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 16:28:55,206:INFO:  Epoch 80/500:  train Loss: 30.5502   val Loss: 31.4933   time: 144.11s   best: 31.4933
2023-11-08 16:31:05,239:INFO:  Epoch 81/500:  train Loss: 30.7990   val Loss: 31.7650   time: 130.03s   best: 31.4933
2023-11-08 16:33:15,188:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 16:33:15,208:INFO:  Epoch 82/500:  train Loss: 30.3550   val Loss: 31.2512   time: 129.94s   best: 31.2512
2023-11-08 16:35:30,625:INFO:  Epoch 83/500:  train Loss: 30.1712   val Loss: 31.7035   time: 135.42s   best: 31.2512
2023-11-08 16:38:42,712:INFO:  Epoch 84/500:  train Loss: 30.1911   val Loss: 31.6024   time: 192.09s   best: 31.2512
2023-11-08 16:41:13,017:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 16:41:13,039:INFO:  Epoch 85/500:  train Loss: 30.0027   val Loss: 30.8783   time: 150.28s   best: 30.8783
2023-11-08 16:44:24,282:INFO:  Epoch 86/500:  train Loss: 29.9147   val Loss: 30.9654   time: 191.24s   best: 30.8783
2023-11-08 16:47:10,491:INFO:  Epoch 87/500:  train Loss: 29.7584   val Loss: 31.1220   time: 166.21s   best: 30.8783
2023-11-08 16:49:20,344:INFO:  Epoch 88/500:  train Loss: 30.0034   val Loss: 31.2222   time: 129.84s   best: 30.8783
2023-11-08 16:51:31,555:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 16:51:31,576:INFO:  Epoch 89/500:  train Loss: 29.5490   val Loss: 30.4944   time: 131.21s   best: 30.4944
2023-11-08 16:53:55,549:INFO:  Epoch 90/500:  train Loss: 29.5187   val Loss: 30.6185   time: 143.97s   best: 30.4944
2023-11-08 16:57:10,991:INFO:  Epoch 91/500:  train Loss: 29.6757   val Loss: 31.5515   time: 195.44s   best: 30.4944
2023-11-08 16:59:22,264:INFO:  Epoch 92/500:  train Loss: 29.2480   val Loss: 30.9154   time: 131.27s   best: 30.4944
2023-11-08 17:01:34,212:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 17:01:34,233:INFO:  Epoch 93/500:  train Loss: 29.2649   val Loss: 30.3727   time: 131.94s   best: 30.3727
2023-11-08 17:03:45,719:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 17:03:45,741:INFO:  Epoch 94/500:  train Loss: 29.1439   val Loss: 30.3669   time: 131.46s   best: 30.3669
2023-11-08 17:05:58,498:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 17:05:58,519:INFO:  Epoch 95/500:  train Loss: 29.4370   val Loss: 30.0295   time: 132.75s   best: 30.0295
2023-11-08 17:08:09,381:INFO:  Epoch 96/500:  train Loss: 29.0212   val Loss: 30.5253   time: 130.86s   best: 30.0295
2023-11-08 17:10:21,468:INFO:  Epoch 97/500:  train Loss: 28.9797   val Loss: 31.2765   time: 132.09s   best: 30.0295
2023-11-08 17:12:34,696:INFO:  Epoch 98/500:  train Loss: 28.8679   val Loss: 30.1654   time: 133.23s   best: 30.0295
2023-11-08 17:14:45,943:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 17:14:45,980:INFO:  Epoch 99/500:  train Loss: 28.7415   val Loss: 29.9098   time: 131.24s   best: 29.9098
2023-11-08 17:16:56,930:INFO:  Epoch 100/500:  train Loss: 28.6243   val Loss: 30.0938   time: 130.95s   best: 29.9098
2023-11-08 17:19:34,512:INFO:  Epoch 101/500:  train Loss: 28.9084   val Loss: 30.3219   time: 157.58s   best: 29.9098
2023-11-08 17:22:06,053:INFO:  Epoch 102/500:  train Loss: 28.5656   val Loss: 30.3416   time: 151.54s   best: 29.9098
2023-11-08 17:24:18,380:INFO:  Epoch 103/500:  train Loss: 28.7620   val Loss: 34.8955   time: 132.33s   best: 29.9098
2023-11-08 17:26:29,446:INFO:  Epoch 104/500:  train Loss: 29.0025   val Loss: 30.1494   time: 131.07s   best: 29.9098
2023-11-08 17:28:41,852:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 17:28:41,873:INFO:  Epoch 105/500:  train Loss: 28.2505   val Loss: 29.5101   time: 132.38s   best: 29.5101
2023-11-08 17:31:00,268:INFO:  Epoch 106/500:  train Loss: 28.3052   val Loss: 29.5656   time: 138.39s   best: 29.5101
2023-11-08 17:33:15,253:INFO:  Epoch 107/500:  train Loss: 28.6282   val Loss: 34.7535   time: 134.98s   best: 29.5101
2023-11-08 17:35:27,257:INFO:  Epoch 108/500:  train Loss: 28.4854   val Loss: 29.7678   time: 132.00s   best: 29.5101
2023-11-08 17:37:37,436:INFO:  Epoch 109/500:  train Loss: 28.1453   val Loss: 29.7491   time: 130.18s   best: 29.5101
2023-11-08 17:39:47,289:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 17:39:47,309:INFO:  Epoch 110/500:  train Loss: 27.9168   val Loss: 29.1271   time: 129.85s   best: 29.1271
2023-11-08 17:42:07,173:INFO:  Epoch 111/500:  train Loss: 28.1056   val Loss: 29.1748   time: 139.86s   best: 29.1271
2023-11-08 17:44:45,398:INFO:  Epoch 112/500:  train Loss: 28.0989   val Loss: 29.4056   time: 158.22s   best: 29.1271
2023-11-08 17:47:25,261:INFO:  Epoch 113/500:  train Loss: 27.8668   val Loss: 29.2074   time: 159.86s   best: 29.1271
2023-11-08 17:49:37,577:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 17:49:37,628:INFO:  Epoch 114/500:  train Loss: 27.9303   val Loss: 29.0524   time: 132.31s   best: 29.0524
2023-11-08 17:51:48,686:INFO:  Epoch 115/500:  train Loss: 27.6937   val Loss: 32.2501   time: 131.06s   best: 29.0524
2023-11-08 17:54:00,294:INFO:  Epoch 116/500:  train Loss: 27.5672   val Loss: 29.8265   time: 131.61s   best: 29.0524
2023-11-08 17:56:32,397:INFO:  Epoch 117/500:  train Loss: 27.5935   val Loss: 29.2999   time: 152.10s   best: 29.0524
2023-11-08 17:58:43,763:INFO:  Epoch 118/500:  train Loss: 27.6532   val Loss: 29.5602   time: 131.35s   best: 29.0524
2023-11-08 18:00:56,264:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 18:00:56,286:INFO:  Epoch 119/500:  train Loss: 27.4611   val Loss: 29.0184   time: 132.50s   best: 29.0184
2023-11-08 18:03:08,301:INFO:  Epoch 120/500:  train Loss: 27.3152   val Loss: 31.4982   time: 132.01s   best: 29.0184
2023-11-08 18:05:31,793:INFO:  Epoch 121/500:  train Loss: 27.3765   val Loss: 29.1504   time: 143.48s   best: 29.0184
2023-11-08 18:07:44,615:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 18:07:44,636:INFO:  Epoch 122/500:  train Loss: 27.1161   val Loss: 29.0051   time: 132.81s   best: 29.0051
2023-11-08 18:09:59,823:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 18:09:59,844:INFO:  Epoch 123/500:  train Loss: 27.3492   val Loss: 28.8553   time: 135.18s   best: 28.8553
2023-11-08 18:12:13,842:INFO:  Epoch 124/500:  train Loss: 27.1462   val Loss: 29.0466   time: 134.00s   best: 28.8553
2023-11-08 18:14:25,706:INFO:  Epoch 125/500:  train Loss: 27.0250   val Loss: 29.1460   time: 131.86s   best: 28.8553
2023-11-08 18:16:35,495:INFO:  Epoch 126/500:  train Loss: 27.0086   val Loss: 30.2943   time: 129.79s   best: 28.8553
2023-11-08 18:18:46,487:INFO:  Epoch 127/500:  train Loss: 26.8526   val Loss: 29.2134   time: 130.99s   best: 28.8553
2023-11-08 18:20:58,300:INFO:  Epoch 128/500:  train Loss: 26.9647   val Loss: 29.3120   time: 131.81s   best: 28.8553
2023-11-08 18:23:21,474:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 18:23:21,513:INFO:  Epoch 129/500:  train Loss: 26.7198   val Loss: 28.7207   time: 143.16s   best: 28.7207
2023-11-08 18:25:33,069:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 18:25:33,090:INFO:  Epoch 130/500:  train Loss: 26.7671   val Loss: 28.6377   time: 131.55s   best: 28.6377
2023-11-08 18:27:44,219:INFO:  Epoch 131/500:  train Loss: 26.6577   val Loss: 28.8737   time: 131.12s   best: 28.6377
2023-11-08 18:30:02,069:INFO:  Epoch 132/500:  train Loss: 27.0294   val Loss: 28.9261   time: 137.85s   best: 28.6377
2023-11-08 18:33:15,799:INFO:  Epoch 133/500:  train Loss: 27.0265   val Loss: 28.8959   time: 193.73s   best: 28.6377
2023-11-08 18:35:35,300:INFO:  Epoch 134/500:  train Loss: 26.9927   val Loss: 30.3284   time: 139.49s   best: 28.6377
2023-11-08 18:37:46,890:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 18:37:46,911:INFO:  Epoch 135/500:  train Loss: 26.8286   val Loss: 28.1318   time: 131.59s   best: 28.1318
2023-11-08 18:40:03,337:INFO:  Epoch 136/500:  train Loss: 26.4101   val Loss: 28.7059   time: 136.43s   best: 28.1318
2023-11-08 18:42:45,395:INFO:  Epoch 137/500:  train Loss: 26.3515   val Loss: 28.3061   time: 162.06s   best: 28.1318
2023-11-08 18:44:55,713:INFO:  Epoch 138/500:  train Loss: 26.3945   val Loss: 34.2080   time: 130.32s   best: 28.1318
2023-11-08 18:48:05,463:INFO:  Epoch 139/500:  train Loss: 26.2737   val Loss: 28.4066   time: 189.75s   best: 28.1318
2023-11-08 18:50:17,604:INFO:  Epoch 140/500:  train Loss: 26.2288   val Loss: 28.3190   time: 132.14s   best: 28.1318
2023-11-08 18:52:28,182:INFO:  Epoch 141/500:  train Loss: 26.3197   val Loss: 28.6021   time: 130.58s   best: 28.1318
2023-11-08 18:55:19,762:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 18:55:19,812:INFO:  Epoch 142/500:  train Loss: 26.0644   val Loss: 27.6592   time: 171.57s   best: 27.6592
2023-11-08 18:57:31,681:INFO:  Epoch 143/500:  train Loss: 26.0058   val Loss: 27.9849   time: 131.87s   best: 27.6592
2023-11-08 18:59:45,235:INFO:  Epoch 144/500:  train Loss: 26.1451   val Loss: 27.9962   time: 133.55s   best: 27.6592
2023-11-08 19:02:04,979:INFO:  Epoch 145/500:  train Loss: 26.3165   val Loss: 28.4621   time: 139.74s   best: 27.6592
2023-11-08 19:05:07,231:INFO:  Epoch 146/500:  train Loss: 25.9858   val Loss: 28.1054   time: 182.25s   best: 27.6592
2023-11-08 19:07:57,386:INFO:  Epoch 147/500:  train Loss: 25.9753   val Loss: 28.1935   time: 170.15s   best: 27.6592
2023-11-08 19:11:16,275:INFO:  Epoch 148/500:  train Loss: 25.8766   val Loss: 28.3465   time: 198.89s   best: 27.6592
2023-11-08 19:13:28,545:INFO:  Epoch 149/500:  train Loss: 25.9599   val Loss: 37.9718   time: 132.27s   best: 27.6592
2023-11-08 19:15:41,982:INFO:  Epoch 150/500:  train Loss: 26.4490   val Loss: 28.1708   time: 133.44s   best: 27.6592
2023-11-08 19:17:54,070:INFO:  Epoch 151/500:  train Loss: 25.9705   val Loss: 28.8046   time: 132.09s   best: 27.6592
2023-11-08 19:20:06,648:INFO:  Epoch 152/500:  train Loss: 25.7469   val Loss: 28.0780   time: 132.58s   best: 27.6592
2023-11-08 19:22:16,836:INFO:  Epoch 153/500:  train Loss: 26.2941   val Loss: 27.9880   time: 130.19s   best: 27.6592
2023-11-08 19:24:48,292:INFO:  Epoch 154/500:  train Loss: 25.9389   val Loss: 28.9483   time: 151.46s   best: 27.6592
2023-11-08 19:27:01,441:INFO:  Epoch 155/500:  train Loss: 25.6067   val Loss: 27.8022   time: 133.15s   best: 27.6592
2023-11-08 19:29:12,116:INFO:  Epoch 156/500:  train Loss: 25.6687   val Loss: 28.0053   time: 130.67s   best: 27.6592
2023-11-08 19:31:24,690:INFO:  Epoch 157/500:  train Loss: 25.4843   val Loss: 29.5937   time: 132.57s   best: 27.6592
2023-11-08 19:33:36,031:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 19:33:36,051:INFO:  Epoch 158/500:  train Loss: 25.5787   val Loss: 27.5470   time: 131.34s   best: 27.5470
2023-11-08 19:35:49,421:INFO:  Epoch 159/500:  train Loss: 25.3639   val Loss: 29.0537   time: 133.37s   best: 27.5470
2023-11-08 19:38:01,015:INFO:  Epoch 160/500:  train Loss: 25.7142   val Loss: 27.6469   time: 131.59s   best: 27.5470
2023-11-08 19:40:11,956:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 19:40:11,976:INFO:  Epoch 161/500:  train Loss: 25.2670   val Loss: 27.4440   time: 130.93s   best: 27.4440
2023-11-08 19:42:23,946:INFO:  Epoch 162/500:  train Loss: 25.2168   val Loss: 28.1155   time: 131.97s   best: 27.4440
2023-11-08 19:44:33,861:INFO:  Epoch 163/500:  train Loss: 25.1145   val Loss: 27.9934   time: 129.91s   best: 27.4440
2023-11-08 19:46:54,331:INFO:  Epoch 164/500:  train Loss: 25.0983   val Loss: 27.8631   time: 140.47s   best: 27.4440
2023-11-08 19:49:06,691:INFO:  Epoch 165/500:  train Loss: 25.4062   val Loss: 27.8443   time: 132.36s   best: 27.4440
2023-11-08 19:51:16,986:INFO:  Epoch 166/500:  train Loss: 25.0141   val Loss: 27.9521   time: 130.29s   best: 27.4440
2023-11-08 19:53:26,738:INFO:  Epoch 167/500:  train Loss: 25.0826   val Loss: 27.8397   time: 129.75s   best: 27.4440
2023-11-08 19:55:37,956:INFO:  Epoch 168/500:  train Loss: 26.1183   val Loss: 27.6266   time: 131.22s   best: 27.4440
2023-11-08 19:57:49,301:INFO:  Epoch 169/500:  train Loss: 24.8684   val Loss: 27.5972   time: 131.34s   best: 27.4440
2023-11-08 20:00:01,167:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 20:00:01,187:INFO:  Epoch 170/500:  train Loss: 24.8894   val Loss: 27.1244   time: 131.86s   best: 27.1244
2023-11-08 20:02:13,394:INFO:  Epoch 171/500:  train Loss: 24.9549   val Loss: 27.8689   time: 132.21s   best: 27.1244
2023-11-08 20:04:24,221:INFO:  Epoch 172/500:  train Loss: 24.8534   val Loss: 27.9744   time: 130.83s   best: 27.1244
2023-11-08 20:06:48,358:INFO:  Epoch 173/500:  train Loss: 24.7248   val Loss: 27.7338   time: 144.14s   best: 27.1244
2023-11-08 20:09:18,909:INFO:  Epoch 174/500:  train Loss: 25.2541   val Loss: 34.6961   time: 150.55s   best: 27.1244
2023-11-08 20:12:26,409:INFO:  Epoch 175/500:  train Loss: 25.1768   val Loss: 27.3662   time: 187.50s   best: 27.1244
2023-11-08 20:14:41,195:INFO:  Epoch 176/500:  train Loss: 24.7237   val Loss: 28.3883   time: 134.78s   best: 27.1244
2023-11-08 20:17:15,556:INFO:  Epoch 177/500:  train Loss: 24.7737   val Loss: 27.6432   time: 154.36s   best: 27.1244
2023-11-08 20:20:09,850:INFO:  Epoch 178/500:  train Loss: 24.8162   val Loss: 27.8989   time: 174.28s   best: 27.1244
2023-11-08 20:23:28,038:INFO:  Epoch 179/500:  train Loss: 24.4662   val Loss: 27.6030   time: 198.19s   best: 27.1244
2023-11-08 20:25:37,776:INFO:  Epoch 180/500:  train Loss: 24.7858   val Loss: 28.4764   time: 129.74s   best: 27.1244
2023-11-08 20:27:47,461:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 20:27:47,489:INFO:  Epoch 181/500:  train Loss: 24.4666   val Loss: 27.0859   time: 129.68s   best: 27.0859
2023-11-08 20:29:56,847:INFO:  Epoch 182/500:  train Loss: 24.7842   val Loss: 27.1994   time: 129.36s   best: 27.0859
2023-11-08 20:32:06,796:INFO:  Epoch 183/500:  train Loss: 24.4277   val Loss: 28.0792   time: 129.95s   best: 27.0859
2023-11-08 20:34:16,557:INFO:  Epoch 184/500:  train Loss: 24.6314   val Loss: 28.3605   time: 129.76s   best: 27.0859
2023-11-08 20:36:26,286:INFO:  Epoch 185/500:  train Loss: 24.3372   val Loss: 27.4496   time: 129.73s   best: 27.0859
2023-11-08 20:38:35,765:INFO:  Epoch 186/500:  train Loss: 24.6282   val Loss: 28.3470   time: 129.48s   best: 27.0859
2023-11-08 20:40:47,547:INFO:  Epoch 187/500:  train Loss: 24.3236   val Loss: 28.1244   time: 131.78s   best: 27.0859
2023-11-08 20:42:58,041:INFO:  Epoch 188/500:  train Loss: 24.3903   val Loss: 28.0643   time: 130.49s   best: 27.0859
2023-11-08 20:45:09,280:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 20:45:09,308:INFO:  Epoch 189/500:  train Loss: 24.3520   val Loss: 26.8659   time: 131.23s   best: 26.8659
2023-11-08 20:48:12,555:INFO:  Epoch 190/500:  train Loss: 24.5704   val Loss: 27.6096   time: 183.25s   best: 26.8659
2023-11-08 20:50:22,504:INFO:  Epoch 191/500:  train Loss: 24.1890   val Loss: 27.0205   time: 129.95s   best: 26.8659
2023-11-08 20:52:33,785:INFO:  Epoch 192/500:  train Loss: 24.4387   val Loss: 27.3310   time: 131.28s   best: 26.8659
2023-11-08 20:54:44,404:INFO:  Epoch 193/500:  train Loss: 24.8880   val Loss: 28.6389   time: 130.62s   best: 26.8659
2023-11-08 20:57:05,892:INFO:  Epoch 194/500:  train Loss: 24.3692   val Loss: 27.0028   time: 141.49s   best: 26.8659
2023-11-08 21:00:21,226:INFO:  Epoch 195/500:  train Loss: 24.0401   val Loss: 27.2765   time: 195.33s   best: 26.8659
2023-11-08 21:03:38,820:INFO:  Epoch 196/500:  train Loss: 24.4794   val Loss: 26.9989   time: 197.59s   best: 26.8659
2023-11-08 21:05:50,486:INFO:  Epoch 197/500:  train Loss: 23.9455   val Loss: 26.9012   time: 131.66s   best: 26.8659
2023-11-08 21:08:00,722:INFO:  Epoch 198/500:  train Loss: 23.9421   val Loss: 27.1985   time: 130.24s   best: 26.8659
2023-11-08 21:10:25,249:INFO:  Epoch 199/500:  train Loss: 24.2377   val Loss: 27.2712   time: 144.53s   best: 26.8659
2023-11-08 21:12:35,840:INFO:  Epoch 200/500:  train Loss: 23.8499   val Loss: 27.2996   time: 130.59s   best: 26.8659
2023-11-08 21:15:29,117:INFO:  Epoch 201/500:  train Loss: 23.9089   val Loss: 27.1165   time: 173.28s   best: 26.8659
2023-11-08 21:17:41,032:INFO:  Epoch 202/500:  train Loss: 23.9306   val Loss: 27.3956   time: 131.91s   best: 26.8659
2023-11-08 21:19:59,979:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 21:19:59,998:INFO:  Epoch 203/500:  train Loss: 24.1429   val Loss: 26.3776   time: 138.94s   best: 26.3776
2023-11-08 21:22:23,532:INFO:  Epoch 204/500:  train Loss: 23.7471   val Loss: 26.4781   time: 143.53s   best: 26.3776
2023-11-08 21:24:51,603:INFO:  Epoch 205/500:  train Loss: 24.9009   val Loss: 29.6433   time: 148.07s   best: 26.3776
2023-11-08 21:27:01,256:INFO:  Epoch 206/500:  train Loss: 24.2448   val Loss: 27.1248   time: 129.65s   best: 26.3776
2023-11-08 21:29:24,914:INFO:  Epoch 207/500:  train Loss: 24.0189   val Loss: 27.3616   time: 143.66s   best: 26.3776
2023-11-08 21:31:36,379:INFO:  Epoch 208/500:  train Loss: 23.7189   val Loss: 26.6686   time: 131.45s   best: 26.3776
2023-11-08 21:33:49,188:INFO:  Epoch 209/500:  train Loss: 23.8057   val Loss: 26.8455   time: 132.81s   best: 26.3776
2023-11-08 21:35:59,922:INFO:  Epoch 210/500:  train Loss: 23.6234   val Loss: 26.9427   time: 130.73s   best: 26.3776
2023-11-08 21:38:09,745:INFO:  Epoch 211/500:  train Loss: 23.6175   val Loss: 26.6623   time: 129.82s   best: 26.3776
2023-11-08 21:40:19,319:INFO:  Epoch 212/500:  train Loss: 23.5429   val Loss: 26.5961   time: 129.57s   best: 26.3776
2023-11-08 21:42:37,220:INFO:  Epoch 213/500:  train Loss: 23.9000   val Loss: 26.7503   time: 137.90s   best: 26.3776
2023-11-08 21:45:16,429:INFO:  Epoch 214/500:  train Loss: 24.1514   val Loss: 33.5754   time: 159.21s   best: 26.3776
2023-11-08 21:47:28,131:INFO:  Epoch 215/500:  train Loss: 23.8489   val Loss: 26.5491   time: 131.70s   best: 26.3776
2023-11-08 21:49:37,953:INFO:  Epoch 216/500:  train Loss: 23.4756   val Loss: 26.7273   time: 129.82s   best: 26.3776
2023-11-08 21:51:50,848:INFO:  Epoch 217/500:  train Loss: 23.9887   val Loss: 27.3291   time: 132.89s   best: 26.3776
2023-11-08 21:54:04,488:INFO:  Epoch 218/500:  train Loss: 24.0865   val Loss: 26.7283   time: 133.63s   best: 26.3776
2023-11-08 21:56:16,369:INFO:  Epoch 219/500:  train Loss: 23.3784   val Loss: 26.4410   time: 131.88s   best: 26.3776
2023-11-08 21:58:27,359:INFO:  Epoch 220/500:  train Loss: 23.3841   val Loss: 26.6694   time: 130.99s   best: 26.3776
2023-11-08 22:00:42,471:INFO:  Epoch 221/500:  train Loss: 23.7112   val Loss: 26.8882   time: 135.11s   best: 26.3776
2023-11-08 22:02:52,640:INFO:  Epoch 222/500:  train Loss: 23.2986   val Loss: 26.5096   time: 130.17s   best: 26.3776
2023-11-08 22:05:13,285:INFO:  Epoch 223/500:  train Loss: 23.2640   val Loss: 28.3586   time: 140.64s   best: 26.3776
2023-11-08 22:07:23,257:INFO:  Epoch 224/500:  train Loss: 23.5037   val Loss: 27.1754   time: 129.97s   best: 26.3776
2023-11-08 22:09:33,865:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 22:09:33,913:INFO:  Epoch 225/500:  train Loss: 23.3307   val Loss: 26.3217   time: 130.59s   best: 26.3217
2023-11-08 22:12:45,279:INFO:  Epoch 226/500:  train Loss: 23.3271   val Loss: 26.5630   time: 191.36s   best: 26.3217
2023-11-08 22:15:28,035:INFO:  Epoch 227/500:  train Loss: 23.3300   val Loss: 26.8391   time: 162.76s   best: 26.3217
2023-11-08 22:17:40,543:INFO:  Epoch 228/500:  train Loss: 23.3895   val Loss: 26.6927   time: 132.51s   best: 26.3217
2023-11-08 22:19:53,541:INFO:  Epoch 229/500:  train Loss: 23.1904   val Loss: 27.2312   time: 133.00s   best: 26.3217
2023-11-08 22:22:02,801:INFO:  Epoch 230/500:  train Loss: 23.3264   val Loss: 26.7843   time: 129.26s   best: 26.3217
2023-11-08 22:24:12,230:INFO:  Epoch 231/500:  train Loss: 23.3640   val Loss: 26.9601   time: 129.43s   best: 26.3217
2023-11-08 22:26:22,663:INFO:  Epoch 232/500:  train Loss: 24.3411   val Loss: 26.6316   time: 130.43s   best: 26.3217
2023-11-08 22:28:34,279:INFO:  Epoch 233/500:  train Loss: 23.1503   val Loss: 26.6930   time: 131.62s   best: 26.3217
2023-11-08 22:30:44,404:INFO:  Epoch 234/500:  train Loss: 23.0205   val Loss: 26.4327   time: 130.12s   best: 26.3217
2023-11-08 22:33:51,926:INFO:  Epoch 235/500:  train Loss: 23.1407   val Loss: 26.4613   time: 187.52s   best: 26.3217
2023-11-08 22:36:09,907:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 22:36:09,927:INFO:  Epoch 236/500:  train Loss: 23.1755   val Loss: 26.2900   time: 137.95s   best: 26.2900
2023-11-08 22:38:19,676:INFO:  Epoch 237/500:  train Loss: 23.5718   val Loss: 27.1871   time: 129.75s   best: 26.2900
2023-11-08 22:40:28,811:INFO:  Epoch 238/500:  train Loss: 23.1459   val Loss: 26.6431   time: 129.14s   best: 26.2900
2023-11-08 22:42:38,939:INFO:  Epoch 239/500:  train Loss: 23.1469   val Loss: 26.4495   time: 130.13s   best: 26.2900
2023-11-08 22:44:53,691:INFO:  Epoch 240/500:  train Loss: 22.9819   val Loss: 26.6620   time: 134.75s   best: 26.2900
2023-11-08 22:47:18,752:INFO:  Epoch 241/500:  train Loss: 23.0976   val Loss: 26.9354   time: 145.06s   best: 26.2900
2023-11-08 22:49:42,020:INFO:  Epoch 242/500:  train Loss: 22.9094   val Loss: 26.4205   time: 143.27s   best: 26.2900
2023-11-08 22:51:52,534:INFO:  Epoch 243/500:  train Loss: 23.1786   val Loss: 26.3751   time: 130.51s   best: 26.2900
2023-11-08 22:54:04,802:INFO:  Epoch 244/500:  train Loss: 22.9523   val Loss: 26.5550   time: 132.27s   best: 26.2900
2023-11-08 22:56:14,437:INFO:  Epoch 245/500:  train Loss: 23.8085   val Loss: 26.8649   time: 129.63s   best: 26.2900
2023-11-08 22:58:23,704:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 22:58:23,725:INFO:  Epoch 246/500:  train Loss: 22.9475   val Loss: 26.0159   time: 129.26s   best: 26.0159
2023-11-08 23:00:33,320:INFO:  Epoch 247/500:  train Loss: 22.8333   val Loss: 27.0532   time: 129.59s   best: 26.0159
2023-11-08 23:02:43,753:INFO:  Epoch 248/500:  train Loss: 23.0238   val Loss: 26.1092   time: 130.43s   best: 26.0159
2023-11-08 23:04:55,682:INFO:  Epoch 249/500:  train Loss: 22.7925   val Loss: 26.1295   time: 131.93s   best: 26.0159
2023-11-08 23:07:05,993:INFO:  Epoch 250/500:  train Loss: 22.8070   val Loss: 26.6327   time: 130.31s   best: 26.0159
2023-11-08 23:09:16,544:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 23:09:16,565:INFO:  Epoch 251/500:  train Loss: 22.7714   val Loss: 25.8127   time: 130.55s   best: 25.8127
2023-11-08 23:11:26,050:INFO:  Epoch 252/500:  train Loss: 23.2255   val Loss: 26.5719   time: 129.48s   best: 25.8127
2023-11-08 23:13:35,793:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 23:13:35,836:INFO:  Epoch 253/500:  train Loss: 22.6818   val Loss: 25.7273   time: 129.74s   best: 25.7273
2023-11-08 23:15:46,828:INFO:  Epoch 254/500:  train Loss: 22.7193   val Loss: 26.0684   time: 130.99s   best: 25.7273
2023-11-08 23:17:58,126:INFO:  Epoch 255/500:  train Loss: 22.5795   val Loss: 26.1481   time: 131.30s   best: 25.7273
2023-11-08 23:20:07,613:INFO:  Epoch 256/500:  train Loss: 23.1638   val Loss: 26.8191   time: 129.49s   best: 25.7273
2023-11-08 23:22:16,952:INFO:  Epoch 257/500:  train Loss: 22.8921   val Loss: 26.1544   time: 129.34s   best: 25.7273
2023-11-08 23:24:26,542:INFO:  Epoch 258/500:  train Loss: 22.6338   val Loss: 26.0461   time: 129.59s   best: 25.7273
2023-11-08 23:26:36,587:INFO:  Epoch 259/500:  train Loss: 22.5893   val Loss: 26.7294   time: 130.04s   best: 25.7273
2023-11-08 23:28:46,938:INFO:  Epoch 260/500:  train Loss: 22.7075   val Loss: 26.1794   time: 130.34s   best: 25.7273
2023-11-08 23:30:58,304:INFO:  Epoch 261/500:  train Loss: 22.9555   val Loss: 26.5711   time: 131.36s   best: 25.7273
2023-11-08 23:33:58,768:INFO:  Epoch 262/500:  train Loss: 22.5499   val Loss: 26.0288   time: 180.45s   best: 25.7273
2023-11-08 23:36:47,229:INFO:  Epoch 263/500:  train Loss: 22.8634   val Loss: 26.1236   time: 168.46s   best: 25.7273
2023-11-08 23:40:01,460:INFO:  Epoch 264/500:  train Loss: 22.4580   val Loss: 26.9588   time: 194.20s   best: 25.7273
2023-11-08 23:42:15,434:INFO:  Epoch 265/500:  train Loss: 22.9072   val Loss: 26.1956   time: 133.97s   best: 25.7273
2023-11-08 23:44:25,614:INFO:  Epoch 266/500:  train Loss: 22.3791   val Loss: 26.0096   time: 130.18s   best: 25.7273
2023-11-08 23:46:37,906:INFO:  Epoch 267/500:  train Loss: 22.7047   val Loss: 26.4481   time: 132.29s   best: 25.7273
2023-11-08 23:48:49,759:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-08 23:48:49,787:INFO:  Epoch 268/500:  train Loss: 22.4153   val Loss: 25.5286   time: 131.85s   best: 25.5286
2023-11-08 23:51:00,892:INFO:  Epoch 269/500:  train Loss: 22.5556   val Loss: 27.1612   time: 131.10s   best: 25.5286
2023-11-08 23:54:15,596:INFO:  Epoch 270/500:  train Loss: 22.9217   val Loss: 28.4184   time: 194.70s   best: 25.5286
2023-11-08 23:57:31,820:INFO:  Epoch 271/500:  train Loss: 22.6127   val Loss: 26.4251   time: 196.22s   best: 25.5286
2023-11-09 00:00:49,715:INFO:  Epoch 272/500:  train Loss: 22.3912   val Loss: 26.0995   time: 197.89s   best: 25.5286
2023-11-09 00:03:00,800:INFO:  Epoch 273/500:  train Loss: 23.1133   val Loss: 26.2831   time: 131.08s   best: 25.5286
2023-11-09 00:05:11,605:INFO:  Epoch 274/500:  train Loss: 22.3050   val Loss: 26.0721   time: 130.80s   best: 25.5286
2023-11-09 00:07:22,150:INFO:  Epoch 275/500:  train Loss: 22.4229   val Loss: 25.8218   time: 130.54s   best: 25.5286
2023-11-09 00:09:32,593:INFO:  Epoch 276/500:  train Loss: 22.3114   val Loss: 26.1949   time: 130.43s   best: 25.5286
2023-11-09 00:11:43,824:INFO:  Epoch 277/500:  train Loss: 22.4395   val Loss: 26.7882   time: 131.23s   best: 25.5286
2023-11-09 00:13:53,933:INFO:  Epoch 278/500:  train Loss: 22.3968   val Loss: 25.7520   time: 130.10s   best: 25.5286
2023-11-09 00:16:05,830:INFO:  Epoch 279/500:  train Loss: 22.3253   val Loss: 25.9168   time: 131.90s   best: 25.5286
2023-11-09 00:18:15,957:INFO:  Epoch 280/500:  train Loss: 22.3653   val Loss: 25.8195   time: 130.13s   best: 25.5286
2023-11-09 00:20:27,867:INFO:  Epoch 281/500:  train Loss: 22.1887   val Loss: 26.0329   time: 131.91s   best: 25.5286
2023-11-09 00:22:39,790:INFO:  Epoch 282/500:  train Loss: 22.2452   val Loss: 26.2424   time: 131.91s   best: 25.5286
2023-11-09 00:24:50,409:INFO:  Epoch 283/500:  train Loss: 22.2465   val Loss: 25.8517   time: 130.62s   best: 25.5286
2023-11-09 00:27:01,520:INFO:  Epoch 284/500:  train Loss: 22.2668   val Loss: 25.6857   time: 131.11s   best: 25.5286
2023-11-09 00:29:12,967:INFO:  Epoch 285/500:  train Loss: 22.3831   val Loss: 35.7492   time: 131.45s   best: 25.5286
2023-11-09 00:32:23,020:INFO:  Epoch 286/500:  train Loss: 23.2082   val Loss: 26.0205   time: 190.05s   best: 25.5286
2023-11-09 00:34:47,035:INFO:  Epoch 287/500:  train Loss: 22.1163   val Loss: 25.9841   time: 144.01s   best: 25.5286
2023-11-09 00:36:58,693:INFO:  Epoch 288/500:  train Loss: 22.3016   val Loss: 27.2266   time: 131.66s   best: 25.5286
2023-11-09 00:39:08,519:INFO:  Epoch 289/500:  train Loss: 22.0540   val Loss: 26.1196   time: 129.83s   best: 25.5286
2023-11-09 00:41:19,851:INFO:  Epoch 290/500:  train Loss: 22.2479   val Loss: 25.9714   time: 131.33s   best: 25.5286
2023-11-09 00:43:29,862:INFO:  Epoch 291/500:  train Loss: 22.1157   val Loss: 25.8645   time: 130.01s   best: 25.5286
2023-11-09 00:45:40,956:INFO:  Epoch 292/500:  train Loss: 22.1536   val Loss: 25.9377   time: 131.09s   best: 25.5286
2023-11-09 00:47:51,787:INFO:  Epoch 293/500:  train Loss: 22.9878   val Loss: 26.0703   time: 130.82s   best: 25.5286
2023-11-09 00:50:02,094:INFO:  Epoch 294/500:  train Loss: 22.2638   val Loss: 25.5448   time: 130.31s   best: 25.5286
2023-11-09 00:52:12,767:INFO:  Epoch 295/500:  train Loss: 22.0898   val Loss: 25.7615   time: 130.67s   best: 25.5286
2023-11-09 00:54:24,587:INFO:  Epoch 296/500:  train Loss: 22.1656   val Loss: 25.7620   time: 131.82s   best: 25.5286
2023-11-09 00:56:35,233:INFO:  Epoch 297/500:  train Loss: 21.9313   val Loss: 25.6829   time: 130.65s   best: 25.5286
2023-11-09 00:58:46,753:INFO:  Epoch 298/500:  train Loss: 21.9817   val Loss: 26.0029   time: 131.52s   best: 25.5286
2023-11-09 01:00:58,390:INFO:  Epoch 299/500:  train Loss: 22.0795   val Loss: 26.4242   time: 131.64s   best: 25.5286
2023-11-09 01:03:10,403:INFO:  Epoch 300/500:  train Loss: 21.9734   val Loss: 25.9215   time: 132.01s   best: 25.5286
2023-11-09 01:05:21,671:INFO:  Epoch 301/500:  train Loss: 21.8599   val Loss: 25.6871   time: 131.27s   best: 25.5286
2023-11-09 01:07:31,630:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-09 01:07:31,649:INFO:  Epoch 302/500:  train Loss: 22.1024   val Loss: 25.5005   time: 129.95s   best: 25.5005
2023-11-09 01:09:42,277:INFO:  Epoch 303/500:  train Loss: 21.8717   val Loss: 25.6725   time: 130.63s   best: 25.5005
2023-11-09 01:12:02,500:INFO:  Epoch 304/500:  train Loss: 21.9304   val Loss: 25.5147   time: 140.22s   best: 25.5005
2023-11-09 01:14:13,903:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-09 01:14:13,924:INFO:  Epoch 305/500:  train Loss: 22.2528   val Loss: 25.3653   time: 131.40s   best: 25.3653
2023-11-09 01:16:25,030:INFO:  Epoch 306/500:  train Loss: 21.9121   val Loss: 26.0045   time: 131.10s   best: 25.3653
2023-11-09 01:18:34,679:INFO:  Epoch 307/500:  train Loss: 21.8084   val Loss: 25.7634   time: 129.65s   best: 25.3653
2023-11-09 01:20:44,084:INFO:  Epoch 308/500:  train Loss: 21.8925   val Loss: 32.8891   time: 129.40s   best: 25.3653
2023-11-09 01:22:53,856:INFO:  Epoch 309/500:  train Loss: 22.0216   val Loss: 25.7453   time: 129.77s   best: 25.3653
2023-11-09 01:25:04,290:INFO:  Epoch 310/500:  train Loss: 21.8373   val Loss: 27.5795   time: 130.43s   best: 25.3653
2023-11-09 01:27:15,877:INFO:  Epoch 311/500:  train Loss: 21.7263   val Loss: 25.4973   time: 131.59s   best: 25.3653
2023-11-09 01:29:27,485:INFO:  Epoch 312/500:  train Loss: 21.7618   val Loss: 26.1506   time: 131.58s   best: 25.3653
2023-11-09 01:31:39,396:INFO:  Epoch 313/500:  train Loss: 21.8892   val Loss: 27.0366   time: 131.89s   best: 25.3653
2023-11-09 01:33:51,072:INFO:  Epoch 314/500:  train Loss: 21.6674   val Loss: 25.5170   time: 131.67s   best: 25.3653
2023-11-09 01:36:03,009:INFO:  Epoch 315/500:  train Loss: 21.6337   val Loss: 25.4226   time: 131.94s   best: 25.3653
2023-11-09 01:38:15,874:INFO:  Epoch 316/500:  train Loss: 21.6607   val Loss: 25.5829   time: 132.86s   best: 25.3653
2023-11-09 01:40:27,489:INFO:  Epoch 317/500:  train Loss: 22.0891   val Loss: 25.6203   time: 131.61s   best: 25.3653
2023-11-09 01:42:38,843:INFO:  Epoch 318/500:  train Loss: 22.0187   val Loss: 25.9472   time: 131.35s   best: 25.3653
2023-11-09 01:44:49,625:INFO:  Epoch 319/500:  train Loss: 21.7635   val Loss: 25.6149   time: 130.78s   best: 25.3653
2023-11-09 01:47:47,813:INFO:  Epoch 320/500:  train Loss: 21.6479   val Loss: 25.9465   time: 178.16s   best: 25.3653
2023-11-09 01:50:00,326:INFO:  Epoch 321/500:  train Loss: 21.6458   val Loss: 25.3759   time: 132.51s   best: 25.3653
2023-11-09 01:52:12,385:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-09 01:52:12,405:INFO:  Epoch 322/500:  train Loss: 21.6759   val Loss: 25.3604   time: 132.05s   best: 25.3604
2023-11-09 01:54:24,586:INFO:  Epoch 323/500:  train Loss: 21.5232   val Loss: 26.0916   time: 132.18s   best: 25.3604
2023-11-09 01:56:36,616:INFO:  Epoch 324/500:  train Loss: 21.5728   val Loss: 25.6551   time: 132.03s   best: 25.3604
2023-11-09 01:58:48,767:INFO:  Epoch 325/500:  train Loss: 21.7231   val Loss: 25.6965   time: 132.15s   best: 25.3604
2023-11-09 02:01:00,915:INFO:  Epoch 326/500:  train Loss: 21.7656   val Loss: 25.8405   time: 132.15s   best: 25.3604
2023-11-09 02:03:11,493:INFO:  Epoch 327/500:  train Loss: 21.5466   val Loss: 25.4316   time: 130.58s   best: 25.3604
2023-11-09 02:05:21,903:INFO:  Epoch 328/500:  train Loss: 21.7105   val Loss: 26.6500   time: 130.41s   best: 25.3604
2023-11-09 02:07:33,326:INFO:  Epoch 329/500:  train Loss: 21.5896   val Loss: 25.5606   time: 131.42s   best: 25.3604
2023-11-09 02:09:44,591:INFO:  Epoch 330/500:  train Loss: 21.4494   val Loss: 25.9506   time: 131.26s   best: 25.3604
2023-11-09 02:12:06,392:INFO:  Epoch 331/500:  train Loss: 21.4678   val Loss: 25.4583   time: 141.80s   best: 25.3604
2023-11-09 02:14:27,501:INFO:  Epoch 332/500:  train Loss: 21.7014   val Loss: 25.7783   time: 141.11s   best: 25.3604
2023-11-09 02:16:54,736:INFO:  Epoch 333/500:  train Loss: 21.5399   val Loss: 25.8289   time: 147.23s   best: 25.3604
2023-11-09 02:19:04,164:INFO:  Epoch 334/500:  train Loss: 21.4339   val Loss: 25.7634   time: 129.43s   best: 25.3604
2023-11-09 02:21:16,091:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-09 02:21:16,112:INFO:  Epoch 335/500:  train Loss: 21.6261   val Loss: 25.3301   time: 131.92s   best: 25.3301
2023-11-09 02:24:28,124:INFO:  Epoch 336/500:  train Loss: 21.3304   val Loss: 25.9683   time: 192.01s   best: 25.3301
2023-11-09 02:27:08,233:INFO:  Epoch 337/500:  train Loss: 21.5618   val Loss: 25.6683   time: 160.11s   best: 25.3301
2023-11-09 02:29:37,261:INFO:  Epoch 338/500:  train Loss: 21.3604   val Loss: 25.5822   time: 149.03s   best: 25.3301
2023-11-09 02:32:46,403:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-09 02:32:46,442:INFO:  Epoch 339/500:  train Loss: 21.3662   val Loss: 25.2163   time: 189.14s   best: 25.2163
2023-11-09 02:35:10,828:INFO:  Epoch 340/500:  train Loss: 22.7411   val Loss: 26.0975   time: 144.36s   best: 25.2163
2023-11-09 02:37:21,887:INFO:  Epoch 341/500:  train Loss: 21.8859   val Loss: 25.9743   time: 131.04s   best: 25.2163
2023-11-09 02:39:32,537:INFO:  Epoch 342/500:  train Loss: 21.5028   val Loss: 25.5842   time: 130.65s   best: 25.2163
2023-11-09 02:42:32,628:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-09 02:42:32,665:INFO:  Epoch 343/500:  train Loss: 21.3190   val Loss: 25.1013   time: 180.08s   best: 25.1013
2023-11-09 02:45:49,092:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.2 dataset (0.1 dropout)_9310.pt
2023-11-09 02:45:49,127:INFO:  Epoch 344/500:  train Loss: 21.5467   val Loss: 24.6532   time: 196.41s   best: 24.6532
2023-11-09 02:49:02,431:INFO:  Epoch 345/500:  train Loss: 22.0614   val Loss: 25.2655   time: 193.30s   best: 24.6532
2023-11-09 02:51:13,155:INFO:  Epoch 346/500:  train Loss: 21.4543   val Loss: 25.6768   time: 130.72s   best: 24.6532
2023-11-09 02:53:35,840:INFO:  Epoch 347/500:  train Loss: 21.4823   val Loss: 25.3363   time: 142.68s   best: 24.6532
2023-11-09 02:55:45,687:INFO:  Epoch 348/500:  train Loss: 21.3176   val Loss: 27.6039   time: 129.83s   best: 24.6532
2023-11-09 02:58:35,889:INFO:  Epoch 349/500:  train Loss: 21.3892   val Loss: 25.7681   time: 170.20s   best: 24.6532
2023-11-09 03:00:46,038:INFO:  Epoch 350/500:  train Loss: 21.3333   val Loss: 25.7221   time: 130.15s   best: 24.6532
2023-11-09 03:02:56,817:INFO:  Epoch 351/500:  train Loss: 21.3875   val Loss: 25.6036   time: 130.78s   best: 24.6532
2023-11-09 03:05:07,593:INFO:  Epoch 352/500:  train Loss: 21.1446   val Loss: 25.9665   time: 130.77s   best: 24.6532
2023-11-09 03:07:31,587:INFO:  Epoch 353/500:  train Loss: 21.6664   val Loss: 27.1641   time: 143.99s   best: 24.6532
2023-11-09 03:09:42,060:INFO:  Epoch 354/500:  train Loss: 21.5791   val Loss: 29.8560   time: 130.47s   best: 24.6532
2023-11-09 03:12:05,773:INFO:  Epoch 355/500:  train Loss: 21.4445   val Loss: 26.0727   time: 143.71s   best: 24.6532
2023-11-09 03:14:24,394:INFO:  Epoch 356/500:  train Loss: 21.3545   val Loss: 27.2025   time: 138.62s   best: 24.6532
2023-11-09 03:16:35,050:INFO:  Epoch 357/500:  train Loss: 21.3368   val Loss: 25.8182   time: 130.65s   best: 24.6532
2023-11-09 03:19:44,578:INFO:  Epoch 358/500:  train Loss: 21.2144   val Loss: 25.5491   time: 189.53s   best: 24.6532
2023-11-09 03:22:59,506:INFO:  Epoch 359/500:  train Loss: 21.2957   val Loss: 25.4160   time: 194.93s   best: 24.6532
2023-11-09 03:25:08,316:INFO:  Epoch 360/500:  train Loss: 21.8732   val Loss: 26.2283   time: 128.81s   best: 24.6532
2023-11-09 03:27:17,213:INFO:  Epoch 361/500:  train Loss: 21.3644   val Loss: 25.3308   time: 128.90s   best: 24.6532
2023-11-09 03:29:53,281:INFO:  Epoch 362/500:  train Loss: 21.0573   val Loss: 25.7843   time: 156.06s   best: 24.6532
2023-11-09 03:32:04,105:INFO:  Epoch 363/500:  train Loss: 21.2474   val Loss: 25.0455   time: 130.82s   best: 24.6532
2023-11-09 03:34:13,337:INFO:  Epoch 364/500:  train Loss: 21.0283   val Loss: 25.2892   time: 129.23s   best: 24.6532
2023-11-09 03:36:22,455:INFO:  Epoch 365/500:  train Loss: 21.0990   val Loss: 25.4310   time: 129.12s   best: 24.6532
2023-11-09 03:38:30,618:INFO:  Epoch 366/500:  train Loss: 21.2796   val Loss: 25.8274   time: 128.16s   best: 24.6532
2023-11-09 03:40:41,326:INFO:  Epoch 367/500:  train Loss: 21.3276   val Loss: 25.4809   time: 130.71s   best: 24.6532
2023-11-09 03:42:51,582:INFO:  Epoch 368/500:  train Loss: 21.1326   val Loss: 25.4539   time: 130.25s   best: 24.6532
2023-11-09 03:45:03,114:INFO:  Epoch 369/500:  train Loss: 21.0107   val Loss: 25.0884   time: 131.53s   best: 24.6532
2023-11-09 03:47:13,183:INFO:  Epoch 370/500:  train Loss: 21.1285   val Loss: 25.8047   time: 130.07s   best: 24.6532
2023-11-09 03:49:22,621:INFO:  Epoch 371/500:  train Loss: 21.0778   val Loss: 25.3346   time: 129.44s   best: 24.6532
2023-11-09 03:51:30,765:INFO:  Epoch 372/500:  train Loss: 21.2959   val Loss: 25.5247   time: 128.14s   best: 24.6532
2023-11-09 03:53:39,108:INFO:  Epoch 373/500:  train Loss: 21.1323   val Loss: 25.6708   time: 128.34s   best: 24.6532
2023-11-09 03:56:40,741:INFO:  Epoch 374/500:  train Loss: 21.0966   val Loss: 25.8438   time: 181.63s   best: 24.6532
2023-11-09 03:59:56,815:INFO:  Epoch 375/500:  train Loss: 20.9898   val Loss: 25.7965   time: 196.07s   best: 24.6532
2023-11-09 04:03:07,918:INFO:  Epoch 376/500:  train Loss: 21.0340   val Loss: 25.1437   time: 191.10s   best: 24.6532
2023-11-09 04:05:18,504:INFO:  Epoch 377/500:  train Loss: 21.3476   val Loss: 25.1553   time: 130.58s   best: 24.6532
2023-11-09 04:07:29,139:INFO:  Epoch 378/500:  train Loss: 20.9120   val Loss: 25.7089   time: 130.63s   best: 24.6532
2023-11-09 04:10:33,500:INFO:  Epoch 379/500:  train Loss: 21.2433   val Loss: 25.7818   time: 184.36s   best: 24.6532
2023-11-09 04:13:49,246:INFO:  Epoch 380/500:  train Loss: 21.1590   val Loss: 25.5789   time: 195.74s   best: 24.6532
2023-11-09 04:17:05,045:INFO:  Epoch 381/500:  train Loss: 21.0085   val Loss: 25.4263   time: 195.80s   best: 24.6532
2023-11-09 04:20:21,195:INFO:  Epoch 382/500:  train Loss: 21.1010   val Loss: 24.9294   time: 196.15s   best: 24.6532
2023-11-09 04:22:39,906:INFO:  Epoch 383/500:  train Loss: 21.0523   val Loss: 26.6101   time: 138.71s   best: 24.6532
2023-11-09 04:24:49,900:INFO:  Epoch 384/500:  train Loss: 22.6551   val Loss: 25.9844   time: 129.99s   best: 24.6532
2023-11-09 04:27:15,975:INFO:  Epoch 385/500:  train Loss: 21.1040   val Loss: 25.2842   time: 146.07s   best: 24.6532
2023-11-09 04:30:31,735:INFO:  Epoch 386/500:  train Loss: 21.1523   val Loss: 25.1376   time: 195.76s   best: 24.6532
2023-11-09 04:33:48,077:INFO:  Epoch 387/500:  train Loss: 20.9067   val Loss: 25.9762   time: 196.34s   best: 24.6532
2023-11-09 04:37:04,069:INFO:  Epoch 388/500:  train Loss: 20.9075   val Loss: 25.1738   time: 195.99s   best: 24.6532
2023-11-09 04:40:20,863:INFO:  Epoch 389/500:  train Loss: 20.8278   val Loss: 25.4239   time: 196.79s   best: 24.6532
2023-11-09 04:43:05,130:INFO:  Epoch 390/500:  train Loss: 20.9111   val Loss: 25.6279   time: 164.27s   best: 24.6532
2023-11-09 04:45:14,721:INFO:  Epoch 391/500:  train Loss: 20.9836   val Loss: 25.1464   time: 129.57s   best: 24.6532
2023-11-09 04:47:23,211:INFO:  Epoch 392/500:  train Loss: 20.8891   val Loss: 25.6041   time: 128.49s   best: 24.6532
2023-11-09 04:49:32,702:INFO:  Epoch 393/500:  train Loss: 20.8777   val Loss: 25.2177   time: 129.48s   best: 24.6532
2023-11-09 04:51:42,820:INFO:  Epoch 394/500:  train Loss: 20.7824   val Loss: 25.2193   time: 130.12s   best: 24.6532
2023-11-09 04:53:51,704:INFO:  Epoch 395/500:  train Loss: 20.8230   val Loss: 25.0948   time: 128.88s   best: 24.6532
2023-11-09 04:56:01,439:INFO:  Epoch 396/500:  train Loss: 21.0079   val Loss: 25.5137   time: 129.73s   best: 24.6532
2023-11-09 04:58:30,609:INFO:  Epoch 397/500:  train Loss: 20.9431   val Loss: 25.5973   time: 149.17s   best: 24.6532
2023-11-09 05:00:40,700:INFO:  Epoch 398/500:  train Loss: 20.9804   val Loss: 26.2078   time: 130.09s   best: 24.6532
2023-11-09 05:02:51,038:INFO:  Epoch 399/500:  train Loss: 21.5119   val Loss: 26.3340   time: 130.34s   best: 24.6532
2023-11-09 05:05:01,441:INFO:  Epoch 400/500:  train Loss: 21.6121   val Loss: 25.6360   time: 130.40s   best: 24.6532
2023-11-09 05:07:26,871:INFO:  Epoch 401/500:  train Loss: 20.9087   val Loss: 25.1995   time: 145.43s   best: 24.6532
2023-11-09 05:09:36,484:INFO:  Epoch 402/500:  train Loss: 21.5879   val Loss: 25.4782   time: 129.61s   best: 24.6532
2023-11-09 05:11:46,638:INFO:  Epoch 403/500:  train Loss: 20.7426   val Loss: 25.4365   time: 130.15s   best: 24.6532
2023-11-09 05:13:55,785:INFO:  Epoch 404/500:  train Loss: 20.7569   val Loss: 25.5344   time: 129.15s   best: 24.6532
2023-11-09 05:17:09,277:INFO:  Epoch 405/500:  train Loss: 20.8548   val Loss: 25.2578   time: 193.49s   best: 24.6532
2023-11-09 05:19:19,949:INFO:  Epoch 406/500:  train Loss: 20.6721   val Loss: 25.3546   time: 130.67s   best: 24.6532
2023-11-09 05:21:30,373:INFO:  Epoch 407/500:  train Loss: 20.7187   val Loss: 25.2780   time: 130.42s   best: 24.6532
2023-11-09 05:23:46,319:INFO:  Epoch 408/500:  train Loss: 21.0459   val Loss: 25.1575   time: 135.94s   best: 24.6532
2023-11-09 05:26:05,591:INFO:  Epoch 409/500:  train Loss: 20.6971   val Loss: 25.9158   time: 139.27s   best: 24.6532
2023-11-09 05:28:41,693:INFO:  Epoch 410/500:  train Loss: 21.0339   val Loss: 25.4151   time: 156.10s   best: 24.6532
2023-11-09 05:30:51,586:INFO:  Epoch 411/500:  train Loss: 21.6213   val Loss: 25.4661   time: 129.89s   best: 24.6532
2023-11-09 05:32:59,606:INFO:  Epoch 412/500:  train Loss: 20.8674   val Loss: 25.1365   time: 128.02s   best: 24.6532
2023-11-09 05:36:02,065:INFO:  Epoch 413/500:  train Loss: 21.2372   val Loss: 34.4616   time: 182.46s   best: 24.6532
2023-11-09 05:39:17,540:INFO:  Epoch 414/500:  train Loss: 21.0585   val Loss: 25.1623   time: 195.47s   best: 24.6532
2023-11-09 05:41:26,678:INFO:  Epoch 415/500:  train Loss: 20.8024   val Loss: 25.4069   time: 129.14s   best: 24.6532
2023-11-09 05:43:51,870:INFO:  Epoch 416/500:  train Loss: 20.6743   val Loss: 25.2586   time: 145.19s   best: 24.6532
2023-11-09 05:46:01,531:INFO:  Epoch 417/500:  train Loss: 20.8226   val Loss: 30.6313   time: 129.66s   best: 24.6532
2023-11-09 05:48:10,766:INFO:  Epoch 418/500:  train Loss: 21.0098   val Loss: 25.5819   time: 129.23s   best: 24.6532
2023-11-09 05:50:21,300:INFO:  Epoch 419/500:  train Loss: 20.9169   val Loss: 25.1461   time: 130.53s   best: 24.6532
2023-11-09 05:52:31,730:INFO:  Epoch 420/500:  train Loss: 20.7973   val Loss: 25.2859   time: 130.43s   best: 24.6532
2023-11-09 05:54:42,223:INFO:  Epoch 421/500:  train Loss: 20.7325   val Loss: 26.4515   time: 130.49s   best: 24.6532
2023-11-09 05:56:50,238:INFO:  Epoch 422/500:  train Loss: 20.5704   val Loss: 25.2400   time: 128.01s   best: 24.6532
2023-11-09 05:58:58,893:INFO:  Epoch 423/500:  train Loss: 20.5902   val Loss: 25.6685   time: 128.65s   best: 24.6532
2023-11-09 06:01:08,644:INFO:  Epoch 424/500:  train Loss: 20.6204   val Loss: 25.1457   time: 129.75s   best: 24.6532
2023-11-09 06:03:18,184:INFO:  Epoch 425/500:  train Loss: 20.8516   val Loss: 25.6535   time: 129.54s   best: 24.6532
2023-11-09 06:05:28,145:INFO:  Epoch 426/500:  train Loss: 21.0060   val Loss: 25.1911   time: 129.96s   best: 24.6532
2023-11-09 06:07:36,676:INFO:  Epoch 427/500:  train Loss: 20.4820   val Loss: 25.0161   time: 128.53s   best: 24.6532
2023-11-09 06:09:44,674:INFO:  Epoch 428/500:  train Loss: 20.5571   val Loss: 24.9326   time: 128.00s   best: 24.6532
2023-11-09 06:11:54,785:INFO:  Epoch 429/500:  train Loss: 20.9902   val Loss: 25.5905   time: 130.11s   best: 24.6532
2023-11-09 06:14:05,095:INFO:  Epoch 430/500:  train Loss: 20.6654   val Loss: 25.5738   time: 130.31s   best: 24.6532
2023-11-09 06:16:15,032:INFO:  Epoch 431/500:  train Loss: 20.4981   val Loss: 25.2292   time: 129.94s   best: 24.6532
2023-11-09 06:18:39,999:INFO:  Epoch 432/500:  train Loss: 20.4720   val Loss: 25.1901   time: 144.97s   best: 24.6532
2023-11-09 06:20:50,790:INFO:  Epoch 433/500:  train Loss: 20.5516   val Loss: 25.2766   time: 130.79s   best: 24.6532
2023-11-09 06:22:59,641:INFO:  Epoch 434/500:  train Loss: 20.6256   val Loss: 25.3803   time: 128.85s   best: 24.6532
2023-11-09 06:25:09,324:INFO:  Epoch 435/500:  train Loss: 20.5300   val Loss: 25.4513   time: 129.68s   best: 24.6532
2023-11-09 06:27:19,857:INFO:  Epoch 436/500:  train Loss: 20.5013   val Loss: 25.5447   time: 130.53s   best: 24.6532
2023-11-09 06:29:29,668:INFO:  Epoch 437/500:  train Loss: 21.0104   val Loss: 25.2119   time: 129.81s   best: 24.6532
2023-11-09 06:31:38,663:INFO:  Epoch 438/500:  train Loss: 20.5054   val Loss: 25.3061   time: 128.99s   best: 24.6532
2023-11-09 06:33:50,223:INFO:  Epoch 439/500:  train Loss: 20.6531   val Loss: 25.8830   time: 131.55s   best: 24.6532
2023-11-09 06:35:59,858:INFO:  Epoch 440/500:  train Loss: 20.5468   val Loss: 25.1431   time: 129.63s   best: 24.6532
2023-11-09 06:38:10,196:INFO:  Epoch 441/500:  train Loss: 20.4749   val Loss: 24.8969   time: 130.34s   best: 24.6532
2023-11-09 06:40:19,620:INFO:  Epoch 442/500:  train Loss: 20.4710   val Loss: 25.2018   time: 129.42s   best: 24.6532
2023-11-09 06:42:29,070:INFO:  Epoch 443/500:  train Loss: 20.4496   val Loss: 25.5751   time: 129.43s   best: 24.6532
2023-11-09 06:44:37,985:INFO:  Epoch 444/500:  train Loss: 20.4124   val Loss: 25.2528   time: 128.91s   best: 24.6532
2023-11-09 06:46:47,291:INFO:  Epoch 445/500:  train Loss: 20.5123   val Loss: 25.2392   time: 129.31s   best: 24.6532
2023-11-09 06:48:57,112:INFO:  Epoch 446/500:  train Loss: 20.6788   val Loss: 25.2524   time: 129.82s   best: 24.6532
2023-11-09 06:51:06,617:INFO:  Epoch 447/500:  train Loss: 20.3151   val Loss: 24.8289   time: 129.50s   best: 24.6532
2023-11-09 06:53:16,806:INFO:  Epoch 448/500:  train Loss: 20.5746   val Loss: 25.4591   time: 130.19s   best: 24.6532
2023-11-09 06:55:25,795:INFO:  Epoch 449/500:  train Loss: 20.6333   val Loss: 26.3024   time: 128.99s   best: 24.6532
2023-11-09 06:57:35,716:INFO:  Epoch 450/500:  train Loss: 20.6835   val Loss: 25.4269   time: 129.92s   best: 24.6532
2023-11-09 06:59:46,497:INFO:  Epoch 451/500:  train Loss: 20.5932   val Loss: 25.4173   time: 130.78s   best: 24.6532
2023-11-09 07:01:54,386:INFO:  Epoch 452/500:  train Loss: 20.4704   val Loss: 25.2230   time: 127.88s   best: 24.6532
2023-11-09 07:04:02,899:INFO:  Epoch 453/500:  train Loss: 20.3221   val Loss: 25.4590   time: 128.51s   best: 24.6532
2023-11-09 07:06:12,170:INFO:  Epoch 454/500:  train Loss: 20.4902   val Loss: 25.1292   time: 129.27s   best: 24.6532
2023-11-09 07:08:21,462:INFO:  Epoch 455/500:  train Loss: 22.4263   val Loss: 26.6544   time: 129.29s   best: 24.6532
2023-11-09 07:10:29,427:INFO:  Epoch 456/500:  train Loss: 20.7113   val Loss: 25.1110   time: 127.96s   best: 24.6532
2023-11-09 07:12:38,646:INFO:  Epoch 457/500:  train Loss: 20.4358   val Loss: 25.4788   time: 129.20s   best: 24.6532
2023-11-09 07:14:49,128:INFO:  Epoch 458/500:  train Loss: 20.3564   val Loss: 25.3372   time: 130.48s   best: 24.6532
2023-11-09 07:16:58,034:INFO:  Epoch 459/500:  train Loss: 20.5329   val Loss: 25.6954   time: 128.90s   best: 24.6532
2023-11-09 07:19:08,282:INFO:  Epoch 460/500:  train Loss: 20.2899   val Loss: 25.0735   time: 130.25s   best: 24.6532
2023-11-09 07:21:18,673:INFO:  Epoch 461/500:  train Loss: 20.3444   val Loss: 25.3132   time: 130.37s   best: 24.6532
2023-11-09 07:23:30,492:INFO:  Epoch 462/500:  train Loss: 20.3295   val Loss: 25.0024   time: 131.82s   best: 24.6532
2023-11-09 07:25:41,554:INFO:  Epoch 463/500:  train Loss: 20.3062   val Loss: 25.4052   time: 131.06s   best: 24.6532
2023-11-09 07:27:51,861:INFO:  Epoch 464/500:  train Loss: 20.2327   val Loss: 25.0794   time: 130.31s   best: 24.6532
2023-11-09 07:30:46,344:INFO:  Epoch 465/500:  train Loss: 20.2445   val Loss: 25.4385   time: 174.48s   best: 24.6532
2023-11-09 07:34:02,379:INFO:  Epoch 466/500:  train Loss: 20.6321   val Loss: 25.0980   time: 196.03s   best: 24.6532
2023-11-09 07:36:21,805:INFO:  Epoch 467/500:  train Loss: 20.2734   val Loss: 25.2657   time: 139.43s   best: 24.6532
2023-11-09 07:38:31,346:INFO:  Epoch 468/500:  train Loss: 20.2691   val Loss: 25.2695   time: 129.54s   best: 24.6532
2023-11-09 07:40:41,635:INFO:  Epoch 469/500:  train Loss: 20.4126   val Loss: 25.3116   time: 130.29s   best: 24.6532
2023-11-09 07:43:56,229:INFO:  Epoch 470/500:  train Loss: 20.2670   val Loss: 26.8146   time: 194.59s   best: 24.6532
2023-11-09 07:46:08,629:INFO:  Epoch 471/500:  train Loss: 20.6033   val Loss: 26.8065   time: 132.40s   best: 24.6532
2023-11-09 07:48:18,066:INFO:  Epoch 472/500:  train Loss: 20.3188   val Loss: 25.3335   time: 129.44s   best: 24.6532
2023-11-09 07:50:28,740:INFO:  Epoch 473/500:  train Loss: 20.1838   val Loss: 25.8750   time: 130.67s   best: 24.6532
2023-11-09 07:52:38,899:INFO:  Epoch 474/500:  train Loss: 20.1804   val Loss: 26.4776   time: 130.16s   best: 24.6532
2023-11-09 07:55:03,785:INFO:  Epoch 475/500:  train Loss: 20.7743   val Loss: 26.2311   time: 144.89s   best: 24.6532
2023-11-09 07:57:13,079:INFO:  Epoch 476/500:  train Loss: 20.9994   val Loss: 26.5629   time: 129.29s   best: 24.6532
2023-11-09 07:59:23,157:INFO:  Epoch 477/500:  train Loss: 20.4218   val Loss: 25.1659   time: 130.08s   best: 24.6532
2023-11-09 08:02:28,997:INFO:  Epoch 478/500:  train Loss: 20.3167   val Loss: 25.0959   time: 185.84s   best: 24.6532
2023-11-09 08:04:39,115:INFO:  Epoch 479/500:  train Loss: 20.3586   val Loss: 25.6934   time: 130.12s   best: 24.6532
2023-11-09 08:07:47,428:INFO:  Epoch 480/500:  train Loss: 20.1708   val Loss: 25.3092   time: 188.31s   best: 24.6532
2023-11-09 08:09:57,502:INFO:  Epoch 481/500:  train Loss: 20.2077   val Loss: 24.9054   time: 130.07s   best: 24.6532
2023-11-09 08:12:07,459:INFO:  Epoch 482/500:  train Loss: 20.2634   val Loss: 25.5284   time: 129.96s   best: 24.6532
2023-11-09 08:14:18,864:INFO:  Epoch 483/500:  train Loss: 20.2306   val Loss: 25.4016   time: 131.40s   best: 24.6532
2023-11-09 08:16:29,056:INFO:  Epoch 484/500:  train Loss: 20.1231   val Loss: 24.9690   time: 130.19s   best: 24.6532
2023-11-09 08:18:38,443:INFO:  Epoch 485/500:  train Loss: 21.1830   val Loss: 26.2281   time: 129.39s   best: 24.6532
2023-11-09 08:21:20,970:INFO:  Epoch 486/500:  train Loss: 20.3579   val Loss: 25.3188   time: 162.53s   best: 24.6532
2023-11-09 08:23:33,621:INFO:  Epoch 487/500:  train Loss: 20.0603   val Loss: 25.3605   time: 132.65s   best: 24.6532
2023-11-09 08:25:43,183:INFO:  Epoch 488/500:  train Loss: 20.3707   val Loss: 25.2421   time: 129.56s   best: 24.6532
2023-11-09 08:28:55,992:INFO:  Epoch 489/500:  train Loss: 20.3283   val Loss: 25.8044   time: 192.81s   best: 24.6532
2023-11-09 08:31:06,561:INFO:  Epoch 490/500:  train Loss: 20.2367   val Loss: 25.0900   time: 130.56s   best: 24.6532
2023-11-09 08:33:16,773:INFO:  Epoch 491/500:  train Loss: 20.1347   val Loss: 26.6394   time: 130.21s   best: 24.6532
2023-11-09 08:35:27,314:INFO:  Epoch 492/500:  train Loss: 20.1978   val Loss: 25.7072   time: 130.54s   best: 24.6532
2023-11-09 08:37:37,906:INFO:  Epoch 493/500:  train Loss: 20.1562   val Loss: 25.3103   time: 130.59s   best: 24.6532
2023-11-09 08:39:54,629:INFO:  Epoch 494/500:  train Loss: 20.4111   val Loss: 25.4105   time: 136.72s   best: 24.6532
2023-11-09 08:42:05,229:INFO:  Epoch 495/500:  train Loss: 20.2642   val Loss: 24.7268   time: 130.60s   best: 24.6532
2023-11-09 08:44:14,285:INFO:  Epoch 496/500:  train Loss: 20.1345   val Loss: 28.2819   time: 129.05s   best: 24.6532
2023-11-09 08:46:23,394:INFO:  Epoch 497/500:  train Loss: 20.2747   val Loss: 25.1506   time: 129.11s   best: 24.6532
2023-11-09 08:48:31,500:INFO:  Epoch 498/500:  train Loss: 20.1884   val Loss: 25.5319   time: 128.11s   best: 24.6532
2023-11-09 08:50:41,143:INFO:  Epoch 499/500:  train Loss: 20.2726   val Loss: 25.1274   time: 129.64s   best: 24.6532
2023-11-09 08:52:49,847:INFO:  Epoch 500/500:  train Loss: 21.1334   val Loss: 26.5118   time: 128.70s   best: 24.6532
2023-11-09 08:52:49,847:INFO:  -----> Training complete in 1171m 54s   best validation loss: 24.6532
 
2023-11-09 13:27:34,115:INFO:  Starting experiment lstm autoencoder with 0.6 dataset (0.1 dropout)
2023-11-09 13:27:34,156:INFO:  Defining the model
2023-11-09 13:27:34,261:INFO:  Reading the dataset
2023-11-09 13:54:12,674:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 13:54:12,699:INFO:  Epoch 1/500:  train Loss: 78.5351   val Loss: 74.4967   time: 307.39s   best: 74.4967
2023-11-09 13:59:16,498:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 13:59:16,522:INFO:  Epoch 2/500:  train Loss: 67.9995   val Loss: 66.6005   time: 303.79s   best: 66.6005
2023-11-09 14:04:08,561:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:04:08,581:INFO:  Epoch 3/500:  train Loss: 63.8623   val Loss: 62.0044   time: 292.03s   best: 62.0044
2023-11-09 14:09:00,249:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:09:00,269:INFO:  Epoch 4/500:  train Loss: 60.8332   val Loss: 59.4888   time: 291.66s   best: 59.4888
2023-11-09 14:13:53,839:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:13:53,859:INFO:  Epoch 5/500:  train Loss: 57.9652   val Loss: 56.1365   time: 293.57s   best: 56.1365
2023-11-09 14:18:50,706:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:18:50,726:INFO:  Epoch 6/500:  train Loss: 54.8479   val Loss: 52.6269   time: 296.84s   best: 52.6269
2023-11-09 14:23:45,936:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:23:46,012:INFO:  Epoch 7/500:  train Loss: 51.6243   val Loss: 49.4543   time: 295.21s   best: 49.4543
2023-11-09 14:28:43,455:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:28:43,483:INFO:  Epoch 8/500:  train Loss: 48.5180   val Loss: 47.1525   time: 297.44s   best: 47.1525
2023-11-09 14:33:40,306:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:33:40,326:INFO:  Epoch 9/500:  train Loss: 45.9838   val Loss: 45.5594   time: 296.82s   best: 45.5594
2023-11-09 14:39:51,889:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:39:52,015:INFO:  Epoch 10/500:  train Loss: 43.7550   val Loss: 43.6579   time: 371.55s   best: 43.6579
2023-11-09 14:44:43,846:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:44:43,865:INFO:  Epoch 11/500:  train Loss: 42.0451   val Loss: 41.8515   time: 291.82s   best: 41.8515
2023-11-09 14:49:37,512:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:49:37,694:INFO:  Epoch 12/500:  train Loss: 40.3268   val Loss: 40.5308   time: 293.64s   best: 40.5308
2023-11-09 14:54:30,316:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:54:30,348:INFO:  Epoch 13/500:  train Loss: 38.9616   val Loss: 39.1205   time: 292.61s   best: 39.1205
2023-11-09 14:59:29,177:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 14:59:29,197:INFO:  Epoch 14/500:  train Loss: 37.9226   val Loss: 38.2908   time: 298.82s   best: 38.2908
2023-11-09 15:05:17,768:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 15:05:17,806:INFO:  Epoch 15/500:  train Loss: 36.7168   val Loss: 36.9645   time: 348.56s   best: 36.9645
2023-11-09 15:11:17,601:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 15:11:17,620:INFO:  Epoch 16/500:  train Loss: 35.9776   val Loss: 36.7944   time: 359.79s   best: 36.7944
2023-11-09 15:16:58,372:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 15:16:58,392:INFO:  Epoch 17/500:  train Loss: 35.0808   val Loss: 35.0887   time: 340.75s   best: 35.0887
2023-11-09 15:21:57,285:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 15:21:57,305:INFO:  Epoch 18/500:  train Loss: 34.2893   val Loss: 33.9881   time: 298.89s   best: 33.9881
2023-11-09 15:26:58,769:INFO:  Epoch 19/500:  train Loss: 33.6405   val Loss: 34.5340   time: 301.46s   best: 33.9881
2023-11-09 15:32:05,254:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 15:32:05,273:INFO:  Epoch 20/500:  train Loss: 32.9965   val Loss: 33.2343   time: 306.48s   best: 33.2343
2023-11-09 15:37:01,037:INFO:  Epoch 21/500:  train Loss: 32.4720   val Loss: 33.4217   time: 295.76s   best: 33.2343
2023-11-09 15:41:53,381:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 15:41:53,408:INFO:  Epoch 22/500:  train Loss: 32.0817   val Loss: 32.4936   time: 292.32s   best: 32.4936
2023-11-09 15:46:49,443:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 15:46:49,488:INFO:  Epoch 23/500:  train Loss: 31.5436   val Loss: 32.1369   time: 296.03s   best: 32.1369
2023-11-09 15:52:53,017:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 15:52:53,092:INFO:  Epoch 24/500:  train Loss: 31.2483   val Loss: 31.4123   time: 363.52s   best: 31.4123
2023-11-09 15:59:36,467:INFO:  Epoch 25/500:  train Loss: 30.7781   val Loss: 32.5952   time: 403.37s   best: 31.4123
2023-11-09 16:04:27,890:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 16:04:27,964:INFO:  Epoch 26/500:  train Loss: 30.4151   val Loss: 30.8895   time: 291.37s   best: 30.8895
2023-11-09 16:09:22,060:INFO:  Epoch 27/500:  train Loss: 30.0635   val Loss: 31.4516   time: 294.09s   best: 30.8895
2023-11-09 16:14:20,111:INFO:  Epoch 28/500:  train Loss: 29.7528   val Loss: 31.2333   time: 298.05s   best: 30.8895
2023-11-09 16:19:57,922:INFO:  Epoch 29/500:  train Loss: 29.3502   val Loss: 32.7798   time: 337.80s   best: 30.8895
2023-11-09 16:24:52,417:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 16:24:52,461:INFO:  Epoch 30/500:  train Loss: 29.2375   val Loss: 29.7434   time: 294.48s   best: 29.7434
2023-11-09 16:29:41,907:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 16:29:41,928:INFO:  Epoch 31/500:  train Loss: 28.9038   val Loss: 29.5858   time: 289.44s   best: 29.5858
2023-11-09 16:34:34,150:INFO:  Epoch 32/500:  train Loss: 28.6118   val Loss: 30.1837   time: 292.22s   best: 29.5858
2023-11-09 16:39:22,930:INFO:  Epoch 33/500:  train Loss: 28.4274   val Loss: 29.7105   time: 288.78s   best: 29.5858
2023-11-09 16:44:15,048:INFO:  Epoch 34/500:  train Loss: 28.2405   val Loss: 30.1846   time: 292.12s   best: 29.5858
2023-11-09 16:49:04,426:INFO:  Epoch 35/500:  train Loss: 27.9693   val Loss: 29.6837   time: 289.38s   best: 29.5858
2023-11-09 16:53:56,191:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 16:53:56,222:INFO:  Epoch 36/500:  train Loss: 27.8106   val Loss: 28.7489   time: 291.76s   best: 28.7489
2023-11-09 16:58:45,797:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 16:58:45,817:INFO:  Epoch 37/500:  train Loss: 27.5735   val Loss: 28.2406   time: 289.57s   best: 28.2406
2023-11-09 17:03:38,325:INFO:  Epoch 38/500:  train Loss: 27.3333   val Loss: 30.9642   time: 292.51s   best: 28.2406
2023-11-09 17:08:29,253:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 17:08:29,278:INFO:  Epoch 39/500:  train Loss: 27.3158   val Loss: 28.1117   time: 290.92s   best: 28.1117
2023-11-09 17:14:57,880:INFO:  Epoch 40/500:  train Loss: 26.9900   val Loss: 29.0435   time: 388.60s   best: 28.1117
2023-11-09 17:19:49,526:INFO:  Epoch 41/500:  train Loss: 26.8924   val Loss: 28.6308   time: 291.65s   best: 28.1117
2023-11-09 17:24:40,228:INFO:  Epoch 42/500:  train Loss: 26.6101   val Loss: 28.1758   time: 290.70s   best: 28.1117
2023-11-09 17:29:33,160:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 17:29:33,181:INFO:  Epoch 43/500:  train Loss: 26.4172   val Loss: 27.9783   time: 292.93s   best: 27.9783
2023-11-09 17:34:25,450:INFO:  Epoch 44/500:  train Loss: 26.4038   val Loss: 28.0283   time: 292.27s   best: 27.9783
2023-11-09 17:39:30,139:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 17:39:30,162:INFO:  Epoch 45/500:  train Loss: 26.2058   val Loss: 27.5135   time: 304.68s   best: 27.5135
2023-11-09 17:44:18,772:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 17:44:18,833:INFO:  Epoch 46/500:  train Loss: 26.1355   val Loss: 27.2533   time: 288.61s   best: 27.2533
2023-11-09 17:49:08,428:INFO:  Epoch 47/500:  train Loss: 25.8652   val Loss: 27.3450   time: 289.59s   best: 27.2533
2023-11-09 17:53:59,959:INFO:  Epoch 48/500:  train Loss: 25.8231   val Loss: 28.1741   time: 291.53s   best: 27.2533
2023-11-09 17:58:49,674:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 17:58:49,695:INFO:  Epoch 49/500:  train Loss: 25.7092   val Loss: 26.9453   time: 289.71s   best: 26.9453
2023-11-09 18:03:40,515:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 18:03:40,544:INFO:  Epoch 50/500:  train Loss: 25.5604   val Loss: 26.9137   time: 290.82s   best: 26.9137
2023-11-09 18:08:30,318:INFO:  Epoch 51/500:  train Loss: 25.4902   val Loss: 29.0562   time: 289.77s   best: 26.9137
2023-11-09 18:13:19,539:INFO:  Epoch 52/500:  train Loss: 25.3336   val Loss: 27.6940   time: 289.22s   best: 26.9137
2023-11-09 18:18:12,697:INFO:  Epoch 53/500:  train Loss: 25.1163   val Loss: 26.9184   time: 293.16s   best: 26.9137
2023-11-09 18:23:05,533:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 18:23:05,553:INFO:  Epoch 54/500:  train Loss: 24.9823   val Loss: 26.8395   time: 292.81s   best: 26.8395
2023-11-09 18:27:55,072:INFO:  Epoch 55/500:  train Loss: 25.0550   val Loss: 28.1004   time: 289.52s   best: 26.8395
2023-11-09 18:32:47,560:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 18:32:47,580:INFO:  Epoch 56/500:  train Loss: 24.8012   val Loss: 26.5485   time: 292.48s   best: 26.5485
2023-11-09 18:37:40,069:INFO:  Epoch 57/500:  train Loss: 24.6873   val Loss: 26.7257   time: 292.49s   best: 26.5485
2023-11-09 18:42:29,495:INFO:  Epoch 58/500:  train Loss: 24.6697   val Loss: 26.5634   time: 289.41s   best: 26.5485
2023-11-09 18:47:19,557:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 18:47:19,578:INFO:  Epoch 59/500:  train Loss: 24.5357   val Loss: 26.5132   time: 290.06s   best: 26.5132
2023-11-09 18:52:08,794:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 18:52:08,827:INFO:  Epoch 60/500:  train Loss: 24.5982   val Loss: 26.4556   time: 289.21s   best: 26.4556
2023-11-09 18:56:59,138:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 18:56:59,159:INFO:  Epoch 61/500:  train Loss: 24.3942   val Loss: 26.0733   time: 290.31s   best: 26.0733
2023-11-09 19:01:49,025:INFO:  Epoch 62/500:  train Loss: 24.5601   val Loss: 26.7306   time: 289.87s   best: 26.0733
2023-11-09 19:06:38,291:INFO:  Epoch 63/500:  train Loss: 24.3321   val Loss: 27.8930   time: 289.26s   best: 26.0733
2023-11-09 19:11:28,279:INFO:  Epoch 64/500:  train Loss: 24.1147   val Loss: 26.2926   time: 289.99s   best: 26.0733
2023-11-09 19:16:17,423:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 19:16:17,443:INFO:  Epoch 65/500:  train Loss: 24.0975   val Loss: 25.9313   time: 289.14s   best: 25.9313
2023-11-09 19:21:09,764:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 19:21:09,784:INFO:  Epoch 66/500:  train Loss: 24.0428   val Loss: 25.8956   time: 292.32s   best: 25.8956
2023-11-09 19:25:59,128:INFO:  Epoch 67/500:  train Loss: 23.9134   val Loss: 34.8056   time: 289.34s   best: 25.8956
2023-11-09 19:30:48,365:INFO:  Epoch 68/500:  train Loss: 23.8354   val Loss: 25.9112   time: 289.24s   best: 25.8956
2023-11-09 19:35:37,444:INFO:  Epoch 69/500:  train Loss: 23.7433   val Loss: 26.1377   time: 289.08s   best: 25.8956
2023-11-09 19:40:26,626:INFO:  Epoch 70/500:  train Loss: 23.6664   val Loss: 28.2338   time: 289.18s   best: 25.8956
2023-11-09 19:45:18,032:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 19:45:18,053:INFO:  Epoch 71/500:  train Loss: 23.5544   val Loss: 25.8360   time: 291.40s   best: 25.8360
2023-11-09 19:50:09,060:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 19:50:09,080:INFO:  Epoch 72/500:  train Loss: 23.5698   val Loss: 25.5912   time: 291.00s   best: 25.5912
2023-11-09 19:55:01,415:INFO:  Epoch 73/500:  train Loss: 23.5088   val Loss: 26.0163   time: 292.33s   best: 25.5912
2023-11-09 19:59:50,067:INFO:  Epoch 74/500:  train Loss: 23.3843   val Loss: 27.3256   time: 288.65s   best: 25.5912
2023-11-09 20:04:39,519:INFO:  Epoch 75/500:  train Loss: 23.5588   val Loss: 25.7810   time: 289.45s   best: 25.5912
2023-11-09 20:09:29,729:INFO:  Epoch 76/500:  train Loss: 23.2904   val Loss: 25.6415   time: 290.21s   best: 25.5912
2023-11-09 20:14:22,691:INFO:  Epoch 77/500:  train Loss: 23.1914   val Loss: 25.7352   time: 292.96s   best: 25.5912
2023-11-09 20:19:11,277:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 20:19:11,298:INFO:  Epoch 78/500:  train Loss: 23.1699   val Loss: 25.3207   time: 288.58s   best: 25.3207
2023-11-09 20:24:04,092:INFO:  Epoch 79/500:  train Loss: 23.1051   val Loss: 25.4835   time: 292.79s   best: 25.3207
2023-11-09 20:28:55,548:INFO:  Epoch 80/500:  train Loss: 23.0820   val Loss: 25.4458   time: 291.46s   best: 25.3207
2023-11-09 20:33:45,919:INFO:  Epoch 81/500:  train Loss: 23.1633   val Loss: 25.7049   time: 290.37s   best: 25.3207
2023-11-09 20:38:35,369:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 20:38:35,390:INFO:  Epoch 82/500:  train Loss: 22.8960   val Loss: 25.3093   time: 289.45s   best: 25.3093
2023-11-09 20:43:25,125:INFO:  Epoch 83/500:  train Loss: 22.8577   val Loss: 26.1485   time: 289.73s   best: 25.3093
2023-11-09 20:48:15,851:INFO:  Epoch 84/500:  train Loss: 22.8246   val Loss: 25.4675   time: 290.73s   best: 25.3093
2023-11-09 20:53:05,623:INFO:  Epoch 85/500:  train Loss: 22.8746   val Loss: 27.9587   time: 289.77s   best: 25.3093
2023-11-09 20:57:54,656:INFO:  Epoch 86/500:  train Loss: 22.6424   val Loss: 25.3576   time: 289.03s   best: 25.3093
2023-11-09 21:02:45,811:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 21:02:45,830:INFO:  Epoch 87/500:  train Loss: 22.7788   val Loss: 25.2504   time: 291.15s   best: 25.2504
2023-11-09 21:07:35,704:INFO:  Epoch 88/500:  train Loss: 22.5387   val Loss: 26.2993   time: 289.87s   best: 25.2504
2023-11-09 21:12:26,275:INFO:  Epoch 89/500:  train Loss: 22.5673   val Loss: 25.9535   time: 290.57s   best: 25.2504
2023-11-09 21:17:17,311:INFO:  Epoch 90/500:  train Loss: 22.4604   val Loss: 25.8336   time: 291.04s   best: 25.2504
2023-11-09 21:22:27,295:INFO:  Epoch 91/500:  train Loss: 22.4283   val Loss: 25.6172   time: 309.98s   best: 25.2504
2023-11-09 21:27:16,121:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 21:27:16,147:INFO:  Epoch 92/500:  train Loss: 22.3129   val Loss: 25.2396   time: 288.82s   best: 25.2396
2023-11-09 21:32:05,277:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 21:32:05,297:INFO:  Epoch 93/500:  train Loss: 22.4443   val Loss: 25.0542   time: 289.13s   best: 25.0542
2023-11-09 21:36:54,362:INFO:  Epoch 94/500:  train Loss: 22.2905   val Loss: 27.4854   time: 289.06s   best: 25.0542
2023-11-09 21:41:45,624:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 21:41:45,644:INFO:  Epoch 95/500:  train Loss: 22.2856   val Loss: 24.9033   time: 291.26s   best: 24.9033
2023-11-09 21:46:35,269:INFO:  Epoch 96/500:  train Loss: 22.3876   val Loss: 24.9339   time: 289.62s   best: 24.9033
2023-11-09 21:51:27,940:INFO:  Epoch 97/500:  train Loss: 22.1275   val Loss: 25.1842   time: 292.67s   best: 24.9033
2023-11-09 21:56:17,664:INFO:  Epoch 98/500:  train Loss: 22.2604   val Loss: 26.8112   time: 289.71s   best: 24.9033
2023-11-09 22:01:11,969:INFO:  Epoch 99/500:  train Loss: 22.2386   val Loss: 25.5336   time: 294.30s   best: 24.9033
2023-11-09 22:06:01,112:INFO:  Epoch 100/500:  train Loss: 22.1577   val Loss: 25.0712   time: 289.14s   best: 24.9033
2023-11-09 22:11:21,873:INFO:  Epoch 101/500:  train Loss: 22.0539   val Loss: 24.9294   time: 320.76s   best: 24.9033
2023-11-09 22:16:13,868:INFO:  Epoch 102/500:  train Loss: 21.9248   val Loss: 25.2195   time: 291.99s   best: 24.9033
2023-11-09 22:21:05,332:INFO:  Epoch 103/500:  train Loss: 21.8447   val Loss: 25.2166   time: 291.46s   best: 24.9033
2023-11-09 22:25:54,153:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 22:25:54,173:INFO:  Epoch 104/500:  train Loss: 23.0265   val Loss: 24.8448   time: 288.82s   best: 24.8448
2023-11-09 22:30:43,080:INFO:  Epoch 105/500:  train Loss: 22.0045   val Loss: 25.0355   time: 288.91s   best: 24.8448
2023-11-09 22:35:31,700:INFO:  Epoch 106/500:  train Loss: 22.1512   val Loss: 25.2459   time: 288.62s   best: 24.8448
2023-11-09 22:40:20,808:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 22:40:20,828:INFO:  Epoch 107/500:  train Loss: 21.7661   val Loss: 24.6861   time: 289.10s   best: 24.6861
2023-11-09 22:45:10,118:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 22:45:10,139:INFO:  Epoch 108/500:  train Loss: 21.8994   val Loss: 24.5294   time: 289.29s   best: 24.5294
2023-11-09 22:50:00,677:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 22:50:00,697:INFO:  Epoch 109/500:  train Loss: 21.7209   val Loss: 24.4519   time: 290.53s   best: 24.4519
2023-11-09 22:54:50,041:INFO:  Epoch 110/500:  train Loss: 21.6983   val Loss: 25.2132   time: 289.34s   best: 24.4519
2023-11-09 22:59:40,287:INFO:  Epoch 111/500:  train Loss: 21.6945   val Loss: 32.2241   time: 290.25s   best: 24.4519
2023-11-09 23:04:28,954:INFO:  Epoch 112/500:  train Loss: 21.5657   val Loss: 26.1207   time: 288.67s   best: 24.4519
2023-11-09 23:09:18,290:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 23:09:18,310:INFO:  Epoch 113/500:  train Loss: 21.5573   val Loss: 24.4062   time: 289.33s   best: 24.4062
2023-11-09 23:14:07,010:INFO:  Epoch 114/500:  train Loss: 21.6032   val Loss: 24.7570   time: 288.70s   best: 24.4062
2023-11-09 23:18:56,703:INFO:  Epoch 115/500:  train Loss: 21.5297   val Loss: 25.0909   time: 289.69s   best: 24.4062
2023-11-09 23:23:45,184:INFO:  Epoch 116/500:  train Loss: 21.4640   val Loss: 24.8360   time: 288.48s   best: 24.4062
2023-11-09 23:28:34,551:INFO:  Epoch 117/500:  train Loss: 21.4767   val Loss: 24.6598   time: 289.37s   best: 24.4062
2023-11-09 23:34:06,470:INFO:  Epoch 118/500:  train Loss: 21.4840   val Loss: 24.5747   time: 331.92s   best: 24.4062
2023-11-09 23:39:30,618:INFO:  Epoch 119/500:  train Loss: 21.2873   val Loss: 24.4826   time: 324.12s   best: 24.4062
2023-11-09 23:46:02,787:INFO:  Epoch 120/500:  train Loss: 21.3509   val Loss: 24.4792   time: 392.17s   best: 24.4062
2023-11-09 23:52:11,302:INFO:  Epoch 121/500:  train Loss: 21.3283   val Loss: 24.5623   time: 368.51s   best: 24.4062
2023-11-09 23:58:46,940:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-09 23:58:46,975:INFO:  Epoch 122/500:  train Loss: 21.3211   val Loss: 24.3713   time: 395.63s   best: 24.3713
2023-11-10 00:03:54,261:INFO:  Epoch 123/500:  train Loss: 21.2516   val Loss: 26.7751   time: 307.28s   best: 24.3713
2023-11-10 00:10:25,753:INFO:  Epoch 124/500:  train Loss: 21.2807   val Loss: 24.4281   time: 391.49s   best: 24.3713
2023-11-10 00:17:09,446:INFO:  Epoch 125/500:  train Loss: 21.5428   val Loss: 24.4151   time: 403.69s   best: 24.3713
2023-11-10 00:22:50,139:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 00:22:50,159:INFO:  Epoch 126/500:  train Loss: 21.2129   val Loss: 24.2071   time: 340.69s   best: 24.2071
2023-11-10 00:27:47,091:INFO:  Epoch 127/500:  train Loss: 21.0390   val Loss: 25.2065   time: 296.93s   best: 24.2071
2023-11-10 00:33:35,301:INFO:  Epoch 128/500:  train Loss: 21.2673   val Loss: 24.5285   time: 348.21s   best: 24.2071
2023-11-10 00:38:28,409:INFO:  Epoch 129/500:  train Loss: 21.1037   val Loss: 24.6262   time: 293.11s   best: 24.2071
2023-11-10 00:44:24,296:INFO:  Epoch 130/500:  train Loss: 21.1899   val Loss: 24.7052   time: 355.88s   best: 24.2071
2023-11-10 00:50:54,876:INFO:  Epoch 131/500:  train Loss: 21.0365   val Loss: 24.3265   time: 390.58s   best: 24.2071
2023-11-10 00:56:18,104:INFO:  Epoch 132/500:  train Loss: 21.0624   val Loss: 24.3569   time: 323.22s   best: 24.2071
2023-11-10 01:01:07,130:INFO:  Epoch 133/500:  train Loss: 20.9743   val Loss: 24.4294   time: 289.03s   best: 24.2071
2023-11-10 01:05:56,148:INFO:  Epoch 134/500:  train Loss: 20.9486   val Loss: 24.8057   time: 289.02s   best: 24.2071
2023-11-10 01:10:47,224:INFO:  Epoch 135/500:  train Loss: 21.4312   val Loss: 28.0179   time: 291.07s   best: 24.2071
2023-11-10 01:15:36,830:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 01:15:36,850:INFO:  Epoch 136/500:  train Loss: 21.7349   val Loss: 24.1655   time: 289.59s   best: 24.1655
2023-11-10 01:20:26,560:INFO:  Epoch 137/500:  train Loss: 21.0861   val Loss: 24.3001   time: 289.71s   best: 24.1655
2023-11-10 01:25:18,086:INFO:  Epoch 138/500:  train Loss: 21.3717   val Loss: 24.4675   time: 291.52s   best: 24.1655
2023-11-10 01:30:10,222:INFO:  Epoch 139/500:  train Loss: 20.7966   val Loss: 24.5763   time: 292.14s   best: 24.1655
2023-11-10 01:34:59,740:INFO:  Epoch 140/500:  train Loss: 20.7708   val Loss: 25.0887   time: 289.52s   best: 24.1655
2023-11-10 01:39:51,224:INFO:  Epoch 141/500:  train Loss: 20.7934   val Loss: 24.3968   time: 291.48s   best: 24.1655
2023-11-10 01:44:43,589:INFO:  Epoch 142/500:  train Loss: 20.6702   val Loss: 24.5015   time: 292.36s   best: 24.1655
2023-11-10 01:49:33,476:INFO:  Epoch 143/500:  train Loss: 21.2296   val Loss: 24.1962   time: 289.89s   best: 24.1655
2023-11-10 01:54:25,978:INFO:  Epoch 144/500:  train Loss: 20.7377   val Loss: 24.9907   time: 292.50s   best: 24.1655
2023-11-10 01:59:15,002:INFO:  Epoch 145/500:  train Loss: 20.7077   val Loss: 24.4866   time: 289.02s   best: 24.1655
2023-11-10 02:04:05,445:INFO:  Epoch 146/500:  train Loss: 20.6527   val Loss: 24.2058   time: 290.44s   best: 24.1655
2023-11-10 02:08:54,773:INFO:  Epoch 147/500:  train Loss: 20.5899   val Loss: 24.4046   time: 289.33s   best: 24.1655
2023-11-10 02:13:44,195:INFO:  Epoch 148/500:  train Loss: 20.7120   val Loss: 24.5785   time: 289.42s   best: 24.1655
2023-11-10 02:18:35,842:INFO:  Epoch 149/500:  train Loss: 20.5959   val Loss: 24.6090   time: 291.65s   best: 24.1655
2023-11-10 02:23:25,189:INFO:  Epoch 150/500:  train Loss: 20.7236   val Loss: 27.4829   time: 289.35s   best: 24.1655
2023-11-10 02:28:14,495:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 02:28:14,513:INFO:  Epoch 151/500:  train Loss: 20.6001   val Loss: 24.0913   time: 289.30s   best: 24.0913
2023-11-10 02:33:04,422:INFO:  Epoch 152/500:  train Loss: 20.4678   val Loss: 26.3783   time: 289.91s   best: 24.0913
2023-11-10 02:37:56,315:INFO:  Epoch 153/500:  train Loss: 20.5036   val Loss: 25.1901   time: 291.89s   best: 24.0913
2023-11-10 02:42:45,756:INFO:  Epoch 154/500:  train Loss: 20.5453   val Loss: 25.0216   time: 289.44s   best: 24.0913
2023-11-10 02:47:35,468:INFO:  Epoch 155/500:  train Loss: 20.4704   val Loss: 24.4238   time: 289.71s   best: 24.0913
2023-11-10 02:52:24,225:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 02:52:24,245:INFO:  Epoch 156/500:  train Loss: 20.4077   val Loss: 23.8897   time: 288.75s   best: 23.8897
2023-11-10 02:57:16,625:INFO:  Epoch 157/500:  train Loss: 20.6061   val Loss: 24.1794   time: 292.38s   best: 23.8897
2023-11-10 03:02:05,600:INFO:  Epoch 158/500:  train Loss: 20.4411   val Loss: 25.3378   time: 288.97s   best: 23.8897
2023-11-10 03:06:58,078:INFO:  Epoch 159/500:  train Loss: 20.4243   val Loss: 25.5191   time: 292.48s   best: 23.8897
2023-11-10 03:12:47,648:INFO:  Epoch 160/500:  train Loss: 20.4726   val Loss: 24.2546   time: 349.57s   best: 23.8897
2023-11-10 03:18:03,232:INFO:  Epoch 161/500:  train Loss: 20.8444   val Loss: 24.1250   time: 315.58s   best: 23.8897
2023-11-10 03:22:51,988:INFO:  Epoch 162/500:  train Loss: 20.7337   val Loss: 24.2150   time: 288.76s   best: 23.8897
2023-11-10 03:27:44,684:INFO:  Epoch 163/500:  train Loss: 20.3218   val Loss: 24.0055   time: 292.69s   best: 23.8897
2023-11-10 03:32:34,107:INFO:  Epoch 164/500:  train Loss: 20.4331   val Loss: 24.1254   time: 289.42s   best: 23.8897
2023-11-10 03:37:25,621:INFO:  Epoch 165/500:  train Loss: 20.4448   val Loss: 24.2284   time: 291.51s   best: 23.8897
2023-11-10 03:42:14,944:INFO:  Epoch 166/500:  train Loss: 20.3275   val Loss: 24.1351   time: 289.32s   best: 23.8897
2023-11-10 03:47:06,186:INFO:  Epoch 167/500:  train Loss: 20.7400   val Loss: 24.3333   time: 291.24s   best: 23.8897
2023-11-10 03:51:55,877:INFO:  Epoch 168/500:  train Loss: 20.3035   val Loss: 27.2890   time: 289.69s   best: 23.8897
2023-11-10 03:56:45,449:INFO:  Epoch 169/500:  train Loss: 20.4048   val Loss: 23.9419   time: 289.57s   best: 23.8897
2023-11-10 04:01:35,030:INFO:  Epoch 170/500:  train Loss: 20.1595   val Loss: 24.1206   time: 289.58s   best: 23.8897
2023-11-10 04:06:24,395:INFO:  Epoch 171/500:  train Loss: 20.1328   val Loss: 24.5464   time: 289.36s   best: 23.8897
2023-11-10 04:11:13,263:INFO:  Epoch 172/500:  train Loss: 20.2110   val Loss: 24.2584   time: 288.87s   best: 23.8897
2023-11-10 04:16:02,249:INFO:  Epoch 173/500:  train Loss: 20.1400   val Loss: 24.4310   time: 288.98s   best: 23.8897
2023-11-10 04:20:53,787:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 04:20:53,805:INFO:  Epoch 174/500:  train Loss: 20.1568   val Loss: 23.8603   time: 291.53s   best: 23.8603
2023-11-10 04:25:43,360:INFO:  Epoch 175/500:  train Loss: 20.1684   val Loss: 25.5932   time: 289.55s   best: 23.8603
2023-11-10 04:30:35,397:INFO:  Epoch 176/500:  train Loss: 20.1444   val Loss: 24.2950   time: 292.04s   best: 23.8603
2023-11-10 04:35:24,521:INFO:  Epoch 177/500:  train Loss: 20.0796   val Loss: 24.0167   time: 289.11s   best: 23.8603
2023-11-10 04:40:13,401:INFO:  Epoch 178/500:  train Loss: 20.1610   val Loss: 24.3768   time: 288.88s   best: 23.8603
2023-11-10 04:45:02,832:INFO:  Epoch 179/500:  train Loss: 20.2899   val Loss: 24.2035   time: 289.43s   best: 23.8603
2023-11-10 04:49:53,853:INFO:  Epoch 180/500:  train Loss: 20.6184   val Loss: 24.4425   time: 291.02s   best: 23.8603
2023-11-10 04:54:45,613:INFO:  Epoch 181/500:  train Loss: 20.1963   val Loss: 24.2520   time: 291.76s   best: 23.8603
2023-11-10 04:59:34,018:INFO:  Epoch 182/500:  train Loss: 19.9632   val Loss: 24.1215   time: 288.40s   best: 23.8603
2023-11-10 05:04:25,317:INFO:  Epoch 183/500:  train Loss: 19.9890   val Loss: 24.1793   time: 291.30s   best: 23.8603
2023-11-10 05:09:16,630:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 05:09:16,650:INFO:  Epoch 184/500:  train Loss: 19.9269   val Loss: 23.7872   time: 291.31s   best: 23.7872
2023-11-10 05:14:08,922:INFO:  Epoch 185/500:  train Loss: 20.0779   val Loss: 24.6512   time: 292.27s   best: 23.7872
2023-11-10 05:18:58,420:INFO:  Epoch 186/500:  train Loss: 19.8429   val Loss: 24.1036   time: 289.50s   best: 23.7872
2023-11-10 05:23:48,339:INFO:  Epoch 187/500:  train Loss: 19.9438   val Loss: 24.3376   time: 289.92s   best: 23.7872
2023-11-10 05:28:36,881:INFO:  Epoch 188/500:  train Loss: 20.0398   val Loss: 24.0163   time: 288.54s   best: 23.7872
2023-11-10 05:33:28,062:INFO:  Epoch 189/500:  train Loss: 20.0909   val Loss: 23.9728   time: 291.18s   best: 23.7872
2023-11-10 05:38:16,821:INFO:  Epoch 190/500:  train Loss: 20.3321   val Loss: 24.4745   time: 288.76s   best: 23.7872
2023-11-10 05:43:06,839:INFO:  Epoch 191/500:  train Loss: 19.8416   val Loss: 24.2285   time: 290.02s   best: 23.7872
2023-11-10 05:48:41,196:INFO:  Epoch 192/500:  train Loss: 19.8192   val Loss: 23.9557   time: 334.36s   best: 23.7872
2023-11-10 05:53:32,446:INFO:  Epoch 193/500:  train Loss: 20.1211   val Loss: 23.9018   time: 291.22s   best: 23.7872
2023-11-10 05:59:04,295:INFO:  Epoch 194/500:  train Loss: 19.8583   val Loss: 24.0160   time: 331.84s   best: 23.7872
2023-11-10 06:04:41,111:INFO:  Epoch 195/500:  train Loss: 19.7539   val Loss: 24.5842   time: 336.81s   best: 23.7872
2023-11-10 06:11:00,890:INFO:  Epoch 196/500:  train Loss: 20.1525   val Loss: 24.2419   time: 379.77s   best: 23.7872
2023-11-10 06:16:00,073:INFO:  Epoch 197/500:  train Loss: 19.8210   val Loss: 24.0171   time: 299.17s   best: 23.7872
2023-11-10 06:20:49,057:INFO:  Epoch 198/500:  train Loss: 20.0143   val Loss: 24.2837   time: 288.97s   best: 23.7872
2023-11-10 06:25:38,264:INFO:  Epoch 199/500:  train Loss: 19.7406   val Loss: 23.7944   time: 289.21s   best: 23.7872
2023-11-10 06:30:27,815:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 06:30:27,834:INFO:  Epoch 200/500:  train Loss: 19.7062   val Loss: 23.6645   time: 289.55s   best: 23.6645
2023-11-10 06:35:16,903:INFO:  Epoch 201/500:  train Loss: 19.8881   val Loss: 23.7960   time: 289.07s   best: 23.6645
2023-11-10 06:40:06,081:INFO:  Epoch 202/500:  train Loss: 19.7080   val Loss: 24.0077   time: 289.18s   best: 23.6645
2023-11-10 06:44:55,125:INFO:  Epoch 203/500:  train Loss: 20.1001   val Loss: 23.7288   time: 289.04s   best: 23.6645
2023-11-10 06:49:44,237:INFO:  Epoch 204/500:  train Loss: 19.8881   val Loss: 23.7366   time: 289.11s   best: 23.6645
2023-11-10 06:54:35,481:INFO:  Epoch 205/500:  train Loss: 19.6979   val Loss: 24.9733   time: 291.24s   best: 23.6645
2023-11-10 06:59:25,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 06:59:25,687:INFO:  Epoch 206/500:  train Loss: 19.6256   val Loss: 23.6078   time: 290.18s   best: 23.6078
2023-11-10 07:04:14,988:INFO:  Epoch 207/500:  train Loss: 19.7181   val Loss: 23.7231   time: 289.30s   best: 23.6078
2023-11-10 07:09:04,983:INFO:  Epoch 208/500:  train Loss: 19.5540   val Loss: 24.3686   time: 289.99s   best: 23.6078
2023-11-10 07:13:55,125:INFO:  Epoch 209/500:  train Loss: 20.4607   val Loss: 24.0469   time: 290.14s   best: 23.6078
2023-11-10 07:18:43,899:INFO:  Epoch 210/500:  train Loss: 19.6583   val Loss: 23.6555   time: 288.77s   best: 23.6078
2023-11-10 07:23:34,772:INFO:  Epoch 211/500:  train Loss: 19.8324   val Loss: 25.2428   time: 290.87s   best: 23.6078
2023-11-10 07:29:25,961:INFO:  Epoch 212/500:  train Loss: 19.5791   val Loss: 25.6665   time: 351.17s   best: 23.6078
2023-11-10 07:34:15,539:INFO:  Epoch 213/500:  train Loss: 19.4744   val Loss: 23.8082   time: 289.58s   best: 23.6078
2023-11-10 07:40:55,869:INFO:  Epoch 214/500:  train Loss: 19.5367   val Loss: 26.3194   time: 400.33s   best: 23.6078
2023-11-10 07:46:43,349:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 07:46:43,368:INFO:  Epoch 215/500:  train Loss: 19.5565   val Loss: 23.5572   time: 347.48s   best: 23.5572
2023-11-10 07:51:37,811:INFO:  Epoch 216/500:  train Loss: 19.8775   val Loss: 23.5688   time: 294.44s   best: 23.5572
2023-11-10 07:56:28,330:INFO:  Epoch 217/500:  train Loss: 19.5500   val Loss: 23.6165   time: 290.52s   best: 23.5572
2023-11-10 08:01:16,824:INFO:  Epoch 218/500:  train Loss: 19.6081   val Loss: 23.8383   time: 288.49s   best: 23.5572
2023-11-10 08:06:05,765:INFO:  Epoch 219/500:  train Loss: 19.7439   val Loss: 24.6307   time: 288.94s   best: 23.5572
2023-11-10 08:10:55,553:INFO:  Epoch 220/500:  train Loss: 19.4874   val Loss: 24.1798   time: 289.79s   best: 23.5572
2023-11-10 08:15:45,416:INFO:  Epoch 221/500:  train Loss: 19.5532   val Loss: 23.8758   time: 289.86s   best: 23.5572
2023-11-10 08:20:33,793:INFO:  Epoch 222/500:  train Loss: 19.7434   val Loss: 23.8476   time: 288.38s   best: 23.5572
2023-11-10 08:25:24,287:INFO:  Epoch 223/500:  train Loss: 19.6189   val Loss: 24.3545   time: 290.49s   best: 23.5572
2023-11-10 08:30:16,298:INFO:  Epoch 224/500:  train Loss: 19.6654   val Loss: 23.6380   time: 292.01s   best: 23.5572
2023-11-10 08:35:07,538:INFO:  Epoch 225/500:  train Loss: 19.5597   val Loss: 23.7678   time: 291.24s   best: 23.5572
2023-11-10 08:39:59,105:INFO:  Epoch 226/500:  train Loss: 19.4155   val Loss: 24.2118   time: 291.57s   best: 23.5572
2023-11-10 08:45:22,414:INFO:  Epoch 227/500:  train Loss: 19.3956   val Loss: 23.9300   time: 323.31s   best: 23.5572
2023-11-10 08:50:12,470:INFO:  Epoch 228/500:  train Loss: 19.4419   val Loss: 23.6228   time: 290.05s   best: 23.5572
2023-11-10 08:55:14,932:INFO:  Epoch 229/500:  train Loss: 19.4989   val Loss: 24.0663   time: 302.45s   best: 23.5572
2023-11-10 09:00:06,461:INFO:  Epoch 230/500:  train Loss: 19.4559   val Loss: 23.7744   time: 291.53s   best: 23.5572
2023-11-10 09:04:56,743:INFO:  Epoch 231/500:  train Loss: 19.3275   val Loss: 23.8116   time: 290.28s   best: 23.5572
2023-11-10 09:09:59,426:INFO:  Epoch 232/500:  train Loss: 19.7643   val Loss: 23.6866   time: 302.68s   best: 23.5572
2023-11-10 09:14:57,843:INFO:  Epoch 233/500:  train Loss: 19.4061   val Loss: 23.7550   time: 298.42s   best: 23.5572
2023-11-10 09:20:18,440:INFO:  Epoch 234/500:  train Loss: 19.3946   val Loss: 23.9789   time: 320.60s   best: 23.5572
2023-11-10 09:25:22,281:INFO:  Epoch 235/500:  train Loss: 20.0397   val Loss: 24.1206   time: 303.84s   best: 23.5572
2023-11-10 09:30:15,897:INFO:  Epoch 236/500:  train Loss: 19.5144   val Loss: 24.2800   time: 293.61s   best: 23.5572
2023-11-10 09:35:08,234:INFO:  Epoch 237/500:  train Loss: 19.4005   val Loss: 24.1990   time: 292.34s   best: 23.5572
2023-11-10 09:39:57,872:INFO:  Epoch 238/500:  train Loss: 19.3392   val Loss: 23.6128   time: 289.64s   best: 23.5572
2023-11-10 09:44:48,432:INFO:  Epoch 239/500:  train Loss: 19.2332   val Loss: 24.0496   time: 290.56s   best: 23.5572
2023-11-10 09:49:40,760:INFO:  Epoch 240/500:  train Loss: 19.6667   val Loss: 23.6892   time: 292.33s   best: 23.5572
2023-11-10 09:54:31,866:INFO:  Epoch 241/500:  train Loss: 19.2966   val Loss: 23.6424   time: 291.11s   best: 23.5572
2023-11-10 09:59:21,663:INFO:  Epoch 242/500:  train Loss: 19.2069   val Loss: 25.0781   time: 289.79s   best: 23.5572
2023-11-10 10:04:13,172:INFO:  Epoch 243/500:  train Loss: 19.4288   val Loss: 25.6766   time: 291.51s   best: 23.5572
2023-11-10 10:09:05,325:INFO:  Epoch 244/500:  train Loss: 19.5593   val Loss: 23.7127   time: 292.15s   best: 23.5572
2023-11-10 10:13:58,780:INFO:  Epoch 245/500:  train Loss: 19.3608   val Loss: 23.7866   time: 293.45s   best: 23.5572
2023-11-10 10:18:51,328:INFO:  Epoch 246/500:  train Loss: 19.4271   val Loss: 24.3064   time: 292.55s   best: 23.5572
2023-11-10 10:23:45,340:INFO:  Epoch 247/500:  train Loss: 19.3976   val Loss: 23.8009   time: 294.01s   best: 23.5572
2023-11-10 10:28:43,377:INFO:  Epoch 248/500:  train Loss: 19.1614   val Loss: 23.5931   time: 298.04s   best: 23.5572
2023-11-10 10:33:42,548:INFO:  Epoch 249/500:  train Loss: 19.3075   val Loss: 24.1030   time: 299.17s   best: 23.5572
2023-11-10 10:38:32,367:INFO:  Epoch 250/500:  train Loss: 19.1640   val Loss: 25.5274   time: 289.82s   best: 23.5572
2023-11-10 10:43:22,529:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 10:43:22,972:INFO:  Epoch 251/500:  train Loss: 19.6654   val Loss: 23.4082   time: 290.16s   best: 23.4082
2023-11-10 10:48:14,151:INFO:  Epoch 252/500:  train Loss: 19.3946   val Loss: 23.6764   time: 291.18s   best: 23.4082
2023-11-10 10:53:04,437:INFO:  Epoch 253/500:  train Loss: 19.4889   val Loss: 24.3355   time: 290.29s   best: 23.4082
2023-11-10 10:58:07,943:INFO:  Epoch 254/500:  train Loss: 19.2639   val Loss: 23.7231   time: 303.50s   best: 23.4082
2023-11-10 11:02:56,919:INFO:  Epoch 255/500:  train Loss: 19.1034   val Loss: 23.8082   time: 288.97s   best: 23.4082
2023-11-10 11:07:45,456:INFO:  Epoch 256/500:  train Loss: 19.1710   val Loss: 23.8914   time: 288.54s   best: 23.4082
2023-11-10 11:12:33,960:INFO:  Epoch 257/500:  train Loss: 19.2374   val Loss: 23.4220   time: 288.50s   best: 23.4082
2023-11-10 11:17:24,735:INFO:  Epoch 258/500:  train Loss: 19.0583   val Loss: 24.1769   time: 290.77s   best: 23.4082
2023-11-10 11:22:16,325:INFO:  Epoch 259/500:  train Loss: 19.1312   val Loss: 31.9784   time: 291.59s   best: 23.4082
2023-11-10 11:27:06,639:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 11:27:06,658:INFO:  Epoch 260/500:  train Loss: 19.2928   val Loss: 23.0632   time: 290.30s   best: 23.0632
2023-11-10 11:31:56,157:INFO:  Epoch 261/500:  train Loss: 19.0777   val Loss: 23.6589   time: 289.50s   best: 23.0632
2023-11-10 11:36:45,774:INFO:  Epoch 262/500:  train Loss: 19.0975   val Loss: 23.7303   time: 289.62s   best: 23.0632
2023-11-10 11:41:38,174:INFO:  Epoch 263/500:  train Loss: 19.0809   val Loss: 24.2557   time: 292.40s   best: 23.0632
2023-11-10 11:46:26,459:INFO:  Epoch 264/500:  train Loss: 19.1895   val Loss: 24.0011   time: 288.28s   best: 23.0632
2023-11-10 11:51:16,603:INFO:  Epoch 265/500:  train Loss: 19.1269   val Loss: 23.4200   time: 290.14s   best: 23.0632
2023-11-10 11:56:08,733:INFO:  Epoch 266/500:  train Loss: 18.9882   val Loss: 23.4864   time: 292.11s   best: 23.0632
2023-11-10 12:00:57,939:INFO:  Epoch 267/500:  train Loss: 19.1088   val Loss: 23.2586   time: 289.21s   best: 23.0632
2023-11-10 12:05:49,178:INFO:  Epoch 268/500:  train Loss: 19.1642   val Loss: 23.3764   time: 291.24s   best: 23.0632
2023-11-10 12:10:38,788:INFO:  Epoch 269/500:  train Loss: 19.0203   val Loss: 23.6120   time: 289.60s   best: 23.0632
2023-11-10 12:15:30,708:INFO:  Epoch 270/500:  train Loss: 18.9646   val Loss: 23.5550   time: 291.92s   best: 23.0632
2023-11-10 12:20:20,101:INFO:  Epoch 271/500:  train Loss: 19.1420   val Loss: 23.9091   time: 289.39s   best: 23.0632
2023-11-10 12:25:12,134:INFO:  Epoch 272/500:  train Loss: 19.0148   val Loss: 23.4654   time: 292.03s   best: 23.0632
2023-11-10 12:30:01,189:INFO:  Epoch 273/500:  train Loss: 19.2019   val Loss: 23.5144   time: 289.05s   best: 23.0632
2023-11-10 12:34:49,993:INFO:  Epoch 274/500:  train Loss: 19.2205   val Loss: 23.8960   time: 288.80s   best: 23.0632
2023-11-10 12:39:41,395:INFO:  Epoch 275/500:  train Loss: 19.0179   val Loss: 23.5729   time: 291.40s   best: 23.0632
2023-11-10 12:44:29,562:INFO:  Epoch 276/500:  train Loss: 18.9659   val Loss: 23.3052   time: 288.17s   best: 23.0632
2023-11-10 12:49:19,369:INFO:  Epoch 277/500:  train Loss: 18.9128   val Loss: 23.2786   time: 289.81s   best: 23.0632
2023-11-10 12:54:10,437:INFO:  Epoch 278/500:  train Loss: 18.9903   val Loss: 24.0492   time: 291.07s   best: 23.0632
2023-11-10 12:58:59,936:INFO:  Epoch 279/500:  train Loss: 19.1197   val Loss: 23.4488   time: 289.50s   best: 23.0632
2023-11-10 13:03:49,096:INFO:  Epoch 280/500:  train Loss: 18.8993   val Loss: 23.9430   time: 289.16s   best: 23.0632
2023-11-10 13:08:38,727:INFO:  Epoch 281/500:  train Loss: 19.2255   val Loss: 23.9798   time: 289.63s   best: 23.0632
2023-11-10 13:13:54,494:INFO:  Epoch 282/500:  train Loss: 19.2937   val Loss: 23.5600   time: 315.76s   best: 23.0632
2023-11-10 13:18:45,631:INFO:  Epoch 283/500:  train Loss: 18.8604   val Loss: 23.5050   time: 291.14s   best: 23.0632
2023-11-10 13:23:34,342:INFO:  Epoch 284/500:  train Loss: 19.0192   val Loss: 25.5458   time: 288.71s   best: 23.0632
2023-11-10 13:28:23,375:INFO:  Epoch 285/500:  train Loss: 19.0325   val Loss: 23.7857   time: 289.03s   best: 23.0632
2023-11-10 13:33:11,529:INFO:  Epoch 286/500:  train Loss: 19.5366   val Loss: 24.5918   time: 288.15s   best: 23.0632
2023-11-10 13:38:02,194:INFO:  Epoch 287/500:  train Loss: 20.1962   val Loss: 23.4988   time: 290.66s   best: 23.0632
2023-11-10 13:42:50,129:INFO:  Epoch 288/500:  train Loss: 18.9495   val Loss: 25.2796   time: 287.93s   best: 23.0632
2023-11-10 13:47:41,045:INFO:  Epoch 289/500:  train Loss: 18.9571   val Loss: 24.1340   time: 290.91s   best: 23.0632
2023-11-10 13:52:32,604:INFO:  Epoch 290/500:  train Loss: 18.8502   val Loss: 23.9789   time: 291.56s   best: 23.0632
2023-11-10 13:57:21,744:INFO:  Epoch 291/500:  train Loss: 19.0564   val Loss: 25.3544   time: 289.14s   best: 23.0632
2023-11-10 14:02:10,465:INFO:  Epoch 292/500:  train Loss: 19.5525   val Loss: 23.5178   time: 288.72s   best: 23.0632
2023-11-10 14:06:59,420:INFO:  Epoch 293/500:  train Loss: 19.1570   val Loss: 23.9470   time: 288.95s   best: 23.0632
2023-11-10 14:11:49,755:INFO:  Epoch 294/500:  train Loss: 18.8169   val Loss: 23.1535   time: 290.33s   best: 23.0632
2023-11-10 14:16:37,789:INFO:  Epoch 295/500:  train Loss: 19.0371   val Loss: 23.4143   time: 288.03s   best: 23.0632
2023-11-10 14:21:27,414:INFO:  Epoch 296/500:  train Loss: 19.0316   val Loss: 24.6209   time: 289.61s   best: 23.0632
2023-11-10 14:26:19,349:INFO:  Epoch 297/500:  train Loss: 18.7337   val Loss: 25.4124   time: 291.93s   best: 23.0632
2023-11-10 14:31:07,697:INFO:  Epoch 298/500:  train Loss: 18.9866   val Loss: 24.2289   time: 288.35s   best: 23.0632
2023-11-10 14:35:58,815:INFO:  Epoch 299/500:  train Loss: 18.9229   val Loss: 24.4870   time: 291.12s   best: 23.0632
2023-11-10 14:40:49,226:INFO:  Epoch 300/500:  train Loss: 19.3667   val Loss: 23.6280   time: 290.41s   best: 23.0632
2023-11-10 14:45:48,281:INFO:  Epoch 301/500:  train Loss: 18.7672   val Loss: 25.4502   time: 299.05s   best: 23.0632
2023-11-10 14:50:41,421:INFO:  Epoch 302/500:  train Loss: 18.8776   val Loss: 23.4584   time: 293.14s   best: 23.0632
2023-11-10 14:55:31,007:INFO:  Epoch 303/500:  train Loss: 19.0636   val Loss: 24.0387   time: 289.59s   best: 23.0632
2023-11-10 15:00:19,678:INFO:  Epoch 304/500:  train Loss: 18.8042   val Loss: 23.9797   time: 288.67s   best: 23.0632
2023-11-10 15:05:11,684:INFO:  Epoch 305/500:  train Loss: 18.9834   val Loss: 23.3136   time: 292.01s   best: 23.0632
2023-11-10 15:10:01,833:INFO:  Epoch 306/500:  train Loss: 18.8458   val Loss: 23.5528   time: 290.15s   best: 23.0632
2023-11-10 15:14:51,997:INFO:  Epoch 307/500:  train Loss: 18.7122   val Loss: 23.4495   time: 290.16s   best: 23.0632
2023-11-10 15:19:41,608:INFO:  Epoch 308/500:  train Loss: 18.7211   val Loss: 23.5085   time: 289.61s   best: 23.0632
2023-11-10 15:24:34,005:INFO:  Epoch 309/500:  train Loss: 18.6200   val Loss: 24.7410   time: 292.38s   best: 23.0632
2023-11-10 15:29:23,988:INFO:  Epoch 310/500:  train Loss: 18.6911   val Loss: 23.5481   time: 289.98s   best: 23.0632
2023-11-10 15:34:16,038:INFO:  Epoch 311/500:  train Loss: 18.7894   val Loss: 23.6705   time: 292.05s   best: 23.0632
2023-11-10 15:39:06,088:INFO:  Epoch 312/500:  train Loss: 19.0272   val Loss: 24.4192   time: 290.05s   best: 23.0632
2023-11-10 15:43:57,234:INFO:  Epoch 313/500:  train Loss: 18.8036   val Loss: 23.9852   time: 291.15s   best: 23.0632
2023-11-10 15:48:46,021:INFO:  Epoch 314/500:  train Loss: 18.8129   val Loss: 24.9747   time: 288.79s   best: 23.0632
2023-11-10 15:53:36,102:INFO:  Epoch 315/500:  train Loss: 18.9897   val Loss: 25.2590   time: 290.08s   best: 23.0632
2023-11-10 15:58:25,434:INFO:  Epoch 316/500:  train Loss: 18.8477   val Loss: 23.5478   time: 289.33s   best: 23.0632
2023-11-10 16:03:14,809:INFO:  Epoch 317/500:  train Loss: 18.8033   val Loss: 23.8829   time: 289.37s   best: 23.0632
2023-11-10 16:08:07,850:INFO:  Epoch 318/500:  train Loss: 18.9904   val Loss: 23.8914   time: 293.04s   best: 23.0632
2023-11-10 16:13:00,745:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 16:13:00,778:INFO:  Epoch 319/500:  train Loss: 18.8921   val Loss: 23.0530   time: 292.89s   best: 23.0530
2023-11-10 16:17:50,491:INFO:  Epoch 320/500:  train Loss: 18.5564   val Loss: 23.7427   time: 289.71s   best: 23.0530
2023-11-10 16:22:41,089:INFO:  Epoch 321/500:  train Loss: 19.0475   val Loss: 23.5705   time: 290.60s   best: 23.0530
2023-11-10 16:27:33,472:INFO:  Epoch 322/500:  train Loss: 18.8106   val Loss: 23.8146   time: 292.36s   best: 23.0530
2023-11-10 16:32:23,459:INFO:  Epoch 323/500:  train Loss: 18.6600   val Loss: 23.8605   time: 289.99s   best: 23.0530
2023-11-10 16:37:11,929:INFO:  Epoch 324/500:  train Loss: 18.5901   val Loss: 24.8668   time: 288.47s   best: 23.0530
2023-11-10 16:42:00,750:INFO:  Epoch 325/500:  train Loss: 18.6183   val Loss: 23.5470   time: 288.82s   best: 23.0530
2023-11-10 16:46:51,793:INFO:  Epoch 326/500:  train Loss: 18.8748   val Loss: 23.7129   time: 291.04s   best: 23.0530
2023-11-10 16:51:43,264:INFO:  Epoch 327/500:  train Loss: 18.7806   val Loss: 23.2337   time: 291.47s   best: 23.0530
2023-11-10 16:56:31,754:INFO:  Epoch 328/500:  train Loss: 18.8523   val Loss: 23.6436   time: 288.49s   best: 23.0530
2023-11-10 17:01:21,282:INFO:  Epoch 329/500:  train Loss: 18.7485   val Loss: 24.0781   time: 289.53s   best: 23.0530
2023-11-10 17:06:10,190:INFO:  Epoch 330/500:  train Loss: 18.5336   val Loss: 24.0789   time: 288.91s   best: 23.0530
2023-11-10 17:11:01,545:INFO:  Epoch 331/500:  train Loss: 18.8189   val Loss: 23.6802   time: 291.35s   best: 23.0530
2023-11-10 17:15:53,098:INFO:  Epoch 332/500:  train Loss: 18.8617   val Loss: 23.9036   time: 291.55s   best: 23.0530
2023-11-10 17:20:44,567:INFO:  Epoch 333/500:  train Loss: 18.5557   val Loss: 28.7096   time: 291.47s   best: 23.0530
2023-11-10 17:25:33,320:INFO:  Epoch 334/500:  train Loss: 18.7017   val Loss: 24.2270   time: 288.75s   best: 23.0530
2023-11-10 17:30:21,956:INFO:  Epoch 335/500:  train Loss: 18.5285   val Loss: 23.4110   time: 288.63s   best: 23.0530
2023-11-10 17:35:10,556:INFO:  Epoch 336/500:  train Loss: 18.6987   val Loss: 23.4005   time: 288.57s   best: 23.0530
2023-11-10 17:40:00,195:INFO:  Epoch 337/500:  train Loss: 18.6399   val Loss: 23.5744   time: 289.64s   best: 23.0530
2023-11-10 17:44:48,833:INFO:  Epoch 338/500:  train Loss: 18.6291   val Loss: 23.6635   time: 288.64s   best: 23.0530
2023-11-10 17:49:44,258:INFO:  Epoch 339/500:  train Loss: 18.5419   val Loss: 23.5811   time: 295.42s   best: 23.0530
2023-11-10 17:54:34,446:INFO:  Epoch 340/500:  train Loss: 18.6012   val Loss: 25.4651   time: 290.19s   best: 23.0530
2023-11-10 17:59:23,391:INFO:  Epoch 341/500:  train Loss: 18.5624   val Loss: 23.8461   time: 288.94s   best: 23.0530
2023-11-10 18:04:11,775:INFO:  Epoch 342/500:  train Loss: 18.5066   val Loss: 24.0503   time: 288.38s   best: 23.0530
2023-11-10 18:09:16,697:INFO:  Epoch 343/500:  train Loss: 18.6367   val Loss: 23.8577   time: 304.92s   best: 23.0530
2023-11-10 18:14:05,618:INFO:  Epoch 344/500:  train Loss: 18.5140   val Loss: 23.9126   time: 288.92s   best: 23.0530
2023-11-10 18:18:58,146:INFO:  Epoch 345/500:  train Loss: 18.4549   val Loss: 23.3157   time: 292.53s   best: 23.0530
2023-11-10 18:23:46,737:INFO:  Epoch 346/500:  train Loss: 18.7779   val Loss: 23.5064   time: 288.59s   best: 23.0530
2023-11-10 18:28:35,390:INFO:  Epoch 347/500:  train Loss: 18.6522   val Loss: 23.4270   time: 288.65s   best: 23.0530
2023-11-10 18:33:32,231:INFO:  Epoch 348/500:  train Loss: 18.5233   val Loss: 27.8242   time: 296.84s   best: 23.0530
2023-11-10 18:38:41,672:INFO:  Epoch 349/500:  train Loss: 18.9968   val Loss: 23.7895   time: 309.43s   best: 23.0530
2023-11-10 18:43:37,139:INFO:  Epoch 350/500:  train Loss: 18.5695   val Loss: 24.3494   time: 295.47s   best: 23.0530
2023-11-10 18:48:27,464:INFO:  Epoch 351/500:  train Loss: 18.5079   val Loss: 23.6943   time: 290.32s   best: 23.0530
2023-11-10 18:53:17,882:INFO:  Epoch 352/500:  train Loss: 18.4259   val Loss: 23.4989   time: 290.42s   best: 23.0530
2023-11-10 18:58:17,507:INFO:  Epoch 353/500:  train Loss: 18.3898   val Loss: 24.0487   time: 299.62s   best: 23.0530
2023-11-10 19:03:11,096:INFO:  Epoch 354/500:  train Loss: 18.7624   val Loss: 23.6167   time: 293.59s   best: 23.0530
2023-11-10 19:07:59,501:INFO:  Epoch 355/500:  train Loss: 18.5063   val Loss: 23.6935   time: 288.40s   best: 23.0530
2023-11-10 19:12:57,835:INFO:  Epoch 356/500:  train Loss: 18.6035   val Loss: 23.2538   time: 298.33s   best: 23.0530
2023-11-10 19:17:49,003:INFO:  Epoch 357/500:  train Loss: 18.4165   val Loss: 23.8196   time: 291.17s   best: 23.0530
2023-11-10 19:22:39,214:INFO:  Epoch 358/500:  train Loss: 18.5141   val Loss: 23.5301   time: 290.21s   best: 23.0530
2023-11-10 19:27:29,572:INFO:  Epoch 359/500:  train Loss: 18.5630   val Loss: 24.4570   time: 290.36s   best: 23.0530
2023-11-10 19:32:20,293:INFO:  Epoch 360/500:  train Loss: 18.4502   val Loss: 23.5934   time: 290.72s   best: 23.0530
2023-11-10 19:37:08,748:INFO:  Epoch 361/500:  train Loss: 18.4342   val Loss: 23.5385   time: 288.45s   best: 23.0530
2023-11-10 19:41:57,894:INFO:  Epoch 362/500:  train Loss: 18.4204   val Loss: 24.7308   time: 289.15s   best: 23.0530
2023-11-10 19:42:54,727:INFO:  Starting experiment lstm autoencoder with 0.7 dataset (0.1 dropout)
2023-11-10 19:42:54,735:INFO:  Defining the model
2023-11-10 19:42:54,796:INFO:  Reading the dataset
2023-11-10 19:46:49,474:INFO:  Epoch 363/500:  train Loss: 18.3614   val Loss: 23.6686   time: 291.57s   best: 23.0530
2023-11-10 19:51:40,434:INFO:  Epoch 364/500:  train Loss: 18.3002   val Loss: 23.5769   time: 290.94s   best: 23.0530
2023-11-10 19:57:57,648:INFO:  Epoch 365/500:  train Loss: 18.6823   val Loss: 23.1644   time: 377.21s   best: 23.0530
2023-11-10 20:03:10,481:INFO:  Epoch 366/500:  train Loss: 18.5687   val Loss: 23.8870   time: 312.83s   best: 23.0530
2023-11-10 20:08:01,434:INFO:  Epoch 367/500:  train Loss: 18.6575   val Loss: 23.6879   time: 290.95s   best: 23.0530
2023-11-10 20:09:24,220:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 20:09:24,244:INFO:  Epoch 1/500:  train Loss: 78.0088   val Loss: 70.5894   time: 324.08s   best: 70.5894
2023-11-10 20:12:53,500:INFO:  Epoch 368/500:  train Loss: 18.3820   val Loss: 23.6680   time: 292.07s   best: 23.0530
2023-11-10 20:14:44,924:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 20:14:44,975:INFO:  Epoch 2/500:  train Loss: 66.9338   val Loss: 63.7762   time: 320.67s   best: 63.7762
2023-11-10 20:17:45,989:INFO:  Epoch 369/500:  train Loss: 18.3529   val Loss: 23.2061   time: 292.48s   best: 23.0530
2023-11-10 20:20:09,115:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 20:20:09,139:INFO:  Epoch 3/500:  train Loss: 60.9739   val Loss: 58.7404   time: 324.11s   best: 58.7404
2023-11-10 20:22:36,064:INFO:  Epoch 370/500:  train Loss: 18.3950   val Loss: 23.9743   time: 290.07s   best: 23.0530
2023-11-10 20:25:30,222:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 20:25:30,247:INFO:  Epoch 4/500:  train Loss: 56.6915   val Loss: 55.3930   time: 321.08s   best: 55.3930
2023-11-10 20:27:26,410:INFO:  Epoch 371/500:  train Loss: 18.5501   val Loss: 23.4446   time: 290.33s   best: 23.0530
2023-11-10 20:30:54,485:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 20:30:54,511:INFO:  Epoch 5/500:  train Loss: 52.8027   val Loss: 50.7161   time: 324.23s   best: 50.7161
2023-11-10 20:32:16,433:INFO:  Epoch 372/500:  train Loss: 18.3838   val Loss: 24.4573   time: 290.01s   best: 23.0530
2023-11-10 20:36:17,848:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 20:36:17,876:INFO:  Epoch 6/500:  train Loss: 49.2846   val Loss: 48.6336   time: 323.33s   best: 48.6336
2023-11-10 20:37:08,984:INFO:  Epoch 373/500:  train Loss: 18.3307   val Loss: 23.8563   time: 292.55s   best: 23.0530
2023-11-10 20:41:42,186:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 20:41:42,216:INFO:  Epoch 7/500:  train Loss: 46.2648   val Loss: 45.3479   time: 324.29s   best: 45.3479
2023-11-10 20:42:00,559:INFO:  Epoch 374/500:  train Loss: 18.3412   val Loss: 23.6063   time: 291.56s   best: 23.0530
2023-11-10 20:46:50,865:INFO:  Epoch 375/500:  train Loss: 19.0470   val Loss: 23.6588   time: 290.30s   best: 23.0530
2023-11-10 20:47:06,354:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 20:47:06,371:INFO:  Epoch 8/500:  train Loss: 43.7810   val Loss: 43.3844   time: 324.13s   best: 43.3844
2023-11-10 20:51:42,226:INFO:  Epoch 376/500:  train Loss: 18.2903   val Loss: 23.3469   time: 291.35s   best: 23.0530
2023-11-10 20:52:28,024:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 20:52:28,047:INFO:  Epoch 9/500:  train Loss: 41.7108   val Loss: 42.1068   time: 321.65s   best: 42.1068
2023-11-10 20:56:32,574:INFO:  Epoch 377/500:  train Loss: 18.3513   val Loss: 24.1448   time: 290.34s   best: 23.0530
2023-11-10 20:57:52,194:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 20:57:52,219:INFO:  Epoch 10/500:  train Loss: 40.2388   val Loss: 40.3397   time: 324.13s   best: 40.3397
2023-11-10 21:01:22,699:INFO:  Epoch 378/500:  train Loss: 18.4680   val Loss: 23.9889   time: 290.11s   best: 23.0530
2023-11-10 21:03:15,408:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 21:03:15,440:INFO:  Epoch 11/500:  train Loss: 38.7793   val Loss: 39.9342   time: 323.18s   best: 39.9342
2023-11-10 21:06:15,156:INFO:  Epoch 379/500:  train Loss: 18.3091   val Loss: 23.5514   time: 292.44s   best: 23.0530
2023-11-10 21:08:36,758:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 21:08:36,791:INFO:  Epoch 12/500:  train Loss: 37.5762   val Loss: 38.5222   time: 321.30s   best: 38.5222
2023-11-10 21:11:04,034:INFO:  Epoch 380/500:  train Loss: 18.4948   val Loss: 23.7945   time: 288.86s   best: 23.0530
2023-11-10 21:13:58,215:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 21:13:58,240:INFO:  Epoch 13/500:  train Loss: 36.5033   val Loss: 37.5269   time: 321.41s   best: 37.5269
2023-11-10 21:15:53,780:INFO:  Epoch 381/500:  train Loss: 18.3625   val Loss: 24.5664   time: 289.74s   best: 23.0530
2023-11-10 21:19:23,440:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 21:19:23,471:INFO:  Epoch 14/500:  train Loss: 35.6111   val Loss: 36.9752   time: 325.19s   best: 36.9752
2023-11-10 21:20:43,206:INFO:  Epoch 382/500:  train Loss: 18.4455   val Loss: 23.3908   time: 289.41s   best: 23.0530
2023-11-10 21:24:47,028:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 21:24:47,053:INFO:  Epoch 15/500:  train Loss: 34.7131   val Loss: 35.2606   time: 323.54s   best: 35.2606
2023-11-10 21:25:33,718:INFO:  Epoch 383/500:  train Loss: 18.3378   val Loss: 23.9310   time: 290.50s   best: 23.0530
2023-11-10 21:30:11,969:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 21:30:12,001:INFO:  Epoch 16/500:  train Loss: 33.9546   val Loss: 34.0263   time: 324.91s   best: 34.0263
2023-11-10 21:30:26,251:INFO:  Epoch 384/500:  train Loss: 18.3859   val Loss: 23.6142   time: 292.53s   best: 23.0530
2023-11-10 21:35:15,164:INFO:  Epoch 385/500:  train Loss: 18.5158   val Loss: 24.2628   time: 288.91s   best: 23.0530
2023-11-10 21:35:34,005:INFO:  Epoch 17/500:  train Loss: 33.3817   val Loss: 35.7496   time: 322.00s   best: 34.0263
2023-11-10 21:40:06,178:INFO:  Epoch 386/500:  train Loss: 18.3267   val Loss: 23.7664   time: 291.01s   best: 23.0530
2023-11-10 21:40:55,412:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 21:40:55,448:INFO:  Epoch 18/500:  train Loss: 32.7937   val Loss: 33.9526   time: 321.38s   best: 33.9526
2023-11-10 21:44:58,387:INFO:  Epoch 387/500:  train Loss: 18.2335   val Loss: 23.3860   time: 292.20s   best: 23.0530
2023-11-10 21:46:16,415:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 21:46:16,443:INFO:  Epoch 19/500:  train Loss: 32.2136   val Loss: 33.5520   time: 320.95s   best: 33.5520
2023-11-10 21:49:49,077:INFO:  Epoch 388/500:  train Loss: 18.6054   val Loss: 24.1945   time: 290.66s   best: 23.0530
2023-11-10 21:51:37,721:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 21:51:37,755:INFO:  Epoch 20/500:  train Loss: 31.8015   val Loss: 32.4331   time: 321.27s   best: 32.4331
2023-11-10 21:54:42,447:INFO:  Epoch 389/500:  train Loss: 18.3954   val Loss: 23.1616   time: 293.36s   best: 23.0530
2023-11-10 21:57:02,631:INFO:  Epoch 21/500:  train Loss: 31.2262   val Loss: 32.6826   time: 324.86s   best: 32.4331
2023-11-10 21:59:31,379:INFO:  Epoch 390/500:  train Loss: 18.2094   val Loss: 23.3496   time: 288.91s   best: 23.0530
2023-11-10 22:02:25,404:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 22:02:25,434:INFO:  Epoch 22/500:  train Loss: 30.8104   val Loss: 32.2029   time: 322.76s   best: 32.2029
2023-11-10 22:04:23,391:INFO:  Epoch 391/500:  train Loss: 18.2564   val Loss: 23.7492   time: 291.99s   best: 23.0530
2023-11-10 22:07:46,086:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 22:07:46,118:INFO:  Epoch 23/500:  train Loss: 30.3553   val Loss: 31.1202   time: 320.64s   best: 31.1202
2023-11-10 22:09:15,527:INFO:  Epoch 392/500:  train Loss: 18.5557   val Loss: 23.1652   time: 292.12s   best: 23.0530
2023-11-10 22:13:07,882:INFO:  Epoch 24/500:  train Loss: 30.0083   val Loss: 32.6073   time: 321.75s   best: 31.1202
2023-11-10 22:14:04,456:INFO:  Epoch 393/500:  train Loss: 18.2553   val Loss: 23.6092   time: 288.91s   best: 23.0530
2023-11-10 22:18:32,356:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 22:18:32,380:INFO:  Epoch 25/500:  train Loss: 29.5867   val Loss: 30.4349   time: 324.44s   best: 30.4349
2023-11-10 22:18:56,588:INFO:  Epoch 394/500:  train Loss: 18.2934   val Loss: 23.1272   time: 292.11s   best: 23.0530
2023-11-10 22:23:45,287:INFO:  Epoch 395/500:  train Loss: 18.3169   val Loss: 23.5413   time: 288.70s   best: 23.0530
2023-11-10 22:23:56,160:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 22:23:56,178:INFO:  Epoch 26/500:  train Loss: 29.2670   val Loss: 29.9733   time: 323.76s   best: 29.9733
2023-11-10 22:28:37,607:INFO:  Epoch 396/500:  train Loss: 18.2271   val Loss: 23.4652   time: 292.32s   best: 23.0530
2023-11-10 22:29:21,127:INFO:  Epoch 27/500:  train Loss: 29.0776   val Loss: 30.0712   time: 324.95s   best: 29.9733
2023-11-10 22:33:26,240:INFO:  Epoch 397/500:  train Loss: 18.8118   val Loss: 23.9140   time: 288.63s   best: 23.0530
2023-11-10 22:34:46,871:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 22:34:46,898:INFO:  Epoch 28/500:  train Loss: 28.8005   val Loss: 29.6880   time: 325.71s   best: 29.6880
2023-11-10 22:38:17,927:INFO:  Epoch 398/500:  train Loss: 18.3787   val Loss: 23.5084   time: 291.67s   best: 23.0530
2023-11-10 22:40:08,710:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 22:40:08,731:INFO:  Epoch 29/500:  train Loss: 28.5038   val Loss: 29.5891   time: 321.80s   best: 29.5891
2023-11-10 22:43:07,592:INFO:  Epoch 399/500:  train Loss: 18.2555   val Loss: 23.1562   time: 289.65s   best: 23.0530
2023-11-10 22:45:32,897:INFO:  Epoch 30/500:  train Loss: 28.2215   val Loss: 37.6628   time: 324.17s   best: 29.5891
2023-11-10 22:47:59,175:INFO:  Epoch 400/500:  train Loss: 18.3909   val Loss: 23.7132   time: 291.57s   best: 23.0530
2023-11-10 22:50:55,970:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 22:50:55,994:INFO:  Epoch 31/500:  train Loss: 27.9384   val Loss: 29.0139   time: 323.05s   best: 29.0139
2023-11-10 22:52:51,296:INFO:  Epoch 401/500:  train Loss: 18.3009   val Loss: 23.2333   time: 292.11s   best: 23.0530
2023-11-10 22:56:17,418:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 22:56:17,451:INFO:  Epoch 32/500:  train Loss: 27.7306   val Loss: 28.7750   time: 321.42s   best: 28.7750
2023-11-10 22:57:41,555:INFO:  Epoch 402/500:  train Loss: 18.3378   val Loss: 23.6689   time: 290.24s   best: 23.0530
2023-11-10 23:01:38,938:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 23:01:38,974:INFO:  Epoch 33/500:  train Loss: 27.5654   val Loss: 28.5657   time: 321.45s   best: 28.5657
2023-11-10 23:02:33,033:INFO:  Epoch 403/500:  train Loss: 18.5557   val Loss: 24.1055   time: 291.46s   best: 23.0530
2023-11-10 23:06:59,687:INFO:  Epoch 34/500:  train Loss: 27.7020   val Loss: 28.5962   time: 320.69s   best: 28.5657
2023-11-10 23:07:24,605:INFO:  Epoch 404/500:  train Loss: 18.4302   val Loss: 23.5698   time: 291.56s   best: 23.0530
2023-11-10 23:12:15,315:INFO:  Epoch 405/500:  train Loss: 18.4012   val Loss: 23.3400   time: 290.71s   best: 23.0530
2023-11-10 23:12:20,009:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 23:12:20,027:INFO:  Epoch 35/500:  train Loss: 27.0732   val Loss: 28.4061   time: 320.31s   best: 28.4061
2023-11-10 23:17:06,593:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-10 23:17:06,617:INFO:  Epoch 406/500:  train Loss: 18.1831   val Loss: 22.9217   time: 291.27s   best: 22.9217
2023-11-10 23:17:41,482:INFO:  Epoch 36/500:  train Loss: 26.9356   val Loss: 29.1865   time: 321.45s   best: 28.4061
2023-11-10 23:21:56,094:INFO:  Epoch 407/500:  train Loss: 18.7854   val Loss: 23.8531   time: 289.48s   best: 22.9217
2023-11-10 23:23:05,946:INFO:  Epoch 37/500:  train Loss: 26.7069   val Loss: 28.7147   time: 324.44s   best: 28.4061
2023-11-10 23:26:45,868:INFO:  Epoch 408/500:  train Loss: 18.2005   val Loss: 23.6037   time: 289.76s   best: 22.9217
2023-11-10 23:28:30,704:INFO:  Epoch 38/500:  train Loss: 26.4635   val Loss: 28.4783   time: 324.74s   best: 28.4061
2023-11-10 23:31:37,127:INFO:  Epoch 409/500:  train Loss: 18.2462   val Loss: 23.1546   time: 291.25s   best: 22.9217
2023-11-10 23:33:54,440:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 23:33:54,465:INFO:  Epoch 39/500:  train Loss: 26.3281   val Loss: 28.1347   time: 323.72s   best: 28.1347
2023-11-10 23:36:26,222:INFO:  Epoch 410/500:  train Loss: 18.4072   val Loss: 26.6555   time: 289.08s   best: 22.9217
2023-11-10 23:39:17,989:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 23:39:18,014:INFO:  Epoch 40/500:  train Loss: 26.3200   val Loss: 27.7810   time: 323.52s   best: 27.7810
2023-11-10 23:41:33,751:INFO:  Epoch 411/500:  train Loss: 18.1245   val Loss: 23.3971   time: 307.52s   best: 22.9217
2023-11-10 23:44:38,548:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 23:44:38,571:INFO:  Epoch 41/500:  train Loss: 26.0491   val Loss: 27.7632   time: 320.52s   best: 27.7632
2023-11-10 23:46:29,627:INFO:  Epoch 412/500:  train Loss: 18.3530   val Loss: 23.6456   time: 295.85s   best: 22.9217
2023-11-10 23:49:59,589:INFO:  Epoch 42/500:  train Loss: 25.7525   val Loss: 28.5516   time: 321.01s   best: 27.7632
2023-11-10 23:51:20,634:INFO:  Epoch 413/500:  train Loss: 18.3126   val Loss: 24.2675   time: 290.99s   best: 22.9217
2023-11-10 23:55:21,506:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-10 23:55:21,535:INFO:  Epoch 43/500:  train Loss: 25.6244   val Loss: 27.2757   time: 321.87s   best: 27.2757
2023-11-10 23:56:09,142:INFO:  Epoch 414/500:  train Loss: 18.2501   val Loss: 23.3905   time: 288.49s   best: 22.9217
2023-11-11 00:00:45,620:INFO:  Epoch 44/500:  train Loss: 25.4642   val Loss: 27.6841   time: 324.07s   best: 27.2757
2023-11-11 00:01:21,864:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-11 00:01:21,901:INFO:  Epoch 415/500:  train Loss: 18.1914   val Loss: 22.8942   time: 312.70s   best: 22.8942
2023-11-11 00:06:09,973:INFO:  Epoch 45/500:  train Loss: 25.3487   val Loss: 27.7266   time: 324.33s   best: 27.2757
2023-11-11 00:06:24,843:INFO:  Epoch 416/500:  train Loss: 18.2024   val Loss: 24.0366   time: 302.94s   best: 22.8942
2023-11-11 00:11:13,608:INFO:  Epoch 417/500:  train Loss: 18.3966   val Loss: 23.3239   time: 288.76s   best: 22.8942
2023-11-11 00:11:33,038:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 00:11:33,079:INFO:  Epoch 46/500:  train Loss: 25.5564   val Loss: 26.7981   time: 323.04s   best: 26.7981
2023-11-11 00:16:04,155:INFO:  Epoch 418/500:  train Loss: 18.2364   val Loss: 23.7654   time: 290.55s   best: 22.8942
2023-11-11 00:16:56,641:INFO:  Epoch 47/500:  train Loss: 25.0971   val Loss: 27.4382   time: 323.55s   best: 26.7981
2023-11-11 00:20:54,127:INFO:  Epoch 419/500:  train Loss: 18.7112   val Loss: 23.6665   time: 289.96s   best: 22.8942
2023-11-11 00:22:17,726:INFO:  Epoch 48/500:  train Loss: 25.0963   val Loss: 28.3079   time: 321.07s   best: 26.7981
2023-11-11 00:25:42,390:INFO:  Epoch 420/500:  train Loss: 18.1837   val Loss: 25.3323   time: 288.26s   best: 22.8942
2023-11-11 00:27:40,533:INFO:  Epoch 49/500:  train Loss: 25.1845   val Loss: 27.0526   time: 322.79s   best: 26.7981
2023-11-11 00:30:31,813:INFO:  Epoch 421/500:  train Loss: 18.2632   val Loss: 24.3315   time: 289.40s   best: 22.8942
2023-11-11 00:33:01,334:INFO:  Epoch 50/500:  train Loss: 24.7755   val Loss: 27.4000   time: 320.78s   best: 26.7981
2023-11-11 00:35:22,221:INFO:  Epoch 422/500:  train Loss: 18.0934   val Loss: 26.3281   time: 290.40s   best: 22.8942
2023-11-11 00:38:21,945:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 00:38:21,973:INFO:  Epoch 51/500:  train Loss: 24.6693   val Loss: 26.6622   time: 320.58s   best: 26.6622
2023-11-11 00:40:14,380:INFO:  Epoch 423/500:  train Loss: 18.1342   val Loss: 23.5612   time: 292.15s   best: 22.8942
2023-11-11 00:43:43,684:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 00:43:43,706:INFO:  Epoch 52/500:  train Loss: 24.6184   val Loss: 26.4587   time: 321.71s   best: 26.4587
2023-11-11 00:45:06,377:INFO:  Epoch 424/500:  train Loss: 18.1287   val Loss: 23.8780   time: 291.99s   best: 22.8942
2023-11-11 00:49:04,830:INFO:  Epoch 53/500:  train Loss: 24.5114   val Loss: 27.7948   time: 321.11s   best: 26.4587
2023-11-11 00:50:05,460:INFO:  Epoch 425/500:  train Loss: 18.1341   val Loss: 25.7422   time: 299.08s   best: 22.8942
2023-11-11 00:54:25,313:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 00:54:25,354:INFO:  Epoch 54/500:  train Loss: 24.3690   val Loss: 26.3212   time: 320.46s   best: 26.3212
2023-11-11 00:54:55,365:INFO:  Epoch 426/500:  train Loss: 18.1299   val Loss: 23.1799   time: 289.89s   best: 22.8942
2023-11-11 00:59:44,393:INFO:  Epoch 427/500:  train Loss: 17.9851   val Loss: 23.5313   time: 289.02s   best: 22.8942
2023-11-11 00:59:46,935:INFO:  Epoch 55/500:  train Loss: 24.4374   val Loss: 26.7754   time: 321.57s   best: 26.3212
2023-11-11 01:04:32,895:INFO:  Epoch 428/500:  train Loss: 18.0694   val Loss: 23.2952   time: 288.50s   best: 22.8942
2023-11-11 01:05:09,864:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 01:05:09,894:INFO:  Epoch 56/500:  train Loss: 24.3674   val Loss: 26.2205   time: 322.87s   best: 26.2205
2023-11-11 01:09:23,632:INFO:  Epoch 429/500:  train Loss: 18.2413   val Loss: 24.2820   time: 290.73s   best: 22.8942
2023-11-11 01:10:32,461:INFO:  Epoch 57/500:  train Loss: 24.2696   val Loss: 26.2212   time: 322.56s   best: 26.2205
2023-11-11 01:14:15,295:INFO:  Epoch 430/500:  train Loss: 18.1588   val Loss: 23.9577   time: 291.65s   best: 22.8942
2023-11-11 01:15:56,456:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 01:15:56,872:INFO:  Epoch 58/500:  train Loss: 24.0697   val Loss: 26.0331   time: 323.98s   best: 26.0331
2023-11-11 01:19:04,033:INFO:  Epoch 431/500:  train Loss: 18.0825   val Loss: 23.6935   time: 288.73s   best: 22.8942
2023-11-11 01:21:18,896:INFO:  Epoch 59/500:  train Loss: 23.8634   val Loss: 26.5995   time: 322.02s   best: 26.0331
2023-11-11 01:23:52,598:INFO:  Epoch 432/500:  train Loss: 18.1573   val Loss: 23.8982   time: 288.55s   best: 22.8942
2023-11-11 01:26:43,327:INFO:  Epoch 60/500:  train Loss: 23.8003   val Loss: 26.3346   time: 324.42s   best: 26.0331
2023-11-11 01:29:47,025:INFO:  Epoch 433/500:  train Loss: 18.1336   val Loss: 23.1774   time: 354.41s   best: 22.8942
2023-11-11 01:32:08,682:INFO:  Epoch 61/500:  train Loss: 23.7308   val Loss: 26.9094   time: 325.35s   best: 26.0331
2023-11-11 01:34:35,386:INFO:  Epoch 434/500:  train Loss: 18.1716   val Loss: 23.6150   time: 288.35s   best: 22.8942
2023-11-11 01:37:30,269:INFO:  Epoch 62/500:  train Loss: 23.8282   val Loss: 26.3749   time: 321.56s   best: 26.0331
2023-11-11 01:39:24,349:INFO:  Epoch 435/500:  train Loss: 18.1282   val Loss: 23.2174   time: 288.96s   best: 22.8942
2023-11-11 01:42:54,630:INFO:  Epoch 63/500:  train Loss: 23.4999   val Loss: 26.9697   time: 324.33s   best: 26.0331
2023-11-11 01:44:14,551:INFO:  Epoch 436/500:  train Loss: 18.1453   val Loss: 23.5314   time: 290.19s   best: 22.8942
2023-11-11 01:48:19,352:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 01:48:19,378:INFO:  Epoch 64/500:  train Loss: 23.5952   val Loss: 25.7885   time: 324.71s   best: 25.7885
2023-11-11 01:49:03,277:INFO:  Epoch 437/500:  train Loss: 18.0584   val Loss: 23.1509   time: 288.72s   best: 22.8942
2023-11-11 01:53:39,933:INFO:  Epoch 65/500:  train Loss: 23.5324   val Loss: 26.7975   time: 320.54s   best: 25.7885
2023-11-11 01:53:55,489:INFO:  Epoch 438/500:  train Loss: 18.3809   val Loss: 24.3475   time: 292.20s   best: 22.8942
2023-11-11 01:58:44,586:INFO:  Epoch 439/500:  train Loss: 18.5519   val Loss: 24.1961   time: 289.10s   best: 22.8942
2023-11-11 01:59:01,217:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 01:59:01,234:INFO:  Epoch 66/500:  train Loss: 23.3721   val Loss: 25.7115   time: 321.24s   best: 25.7115
2023-11-11 02:03:35,482:INFO:  Epoch 440/500:  train Loss: 18.3984   val Loss: 23.7811   time: 290.90s   best: 22.8942
2023-11-11 02:04:23,281:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 02:04:23,317:INFO:  Epoch 67/500:  train Loss: 23.3637   val Loss: 25.6064   time: 322.04s   best: 25.6064
2023-11-11 02:08:24,208:INFO:  Epoch 441/500:  train Loss: 18.4654   val Loss: 23.5057   time: 288.72s   best: 22.8942
2023-11-11 02:09:45,934:INFO:  Epoch 68/500:  train Loss: 23.2400   val Loss: 26.5268   time: 322.61s   best: 25.6064
2023-11-11 02:13:12,787:INFO:  Epoch 442/500:  train Loss: 18.1059   val Loss: 24.4962   time: 288.56s   best: 22.8942
2023-11-11 02:15:11,580:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 02:15:11,604:INFO:  Epoch 69/500:  train Loss: 23.1930   val Loss: 25.5238   time: 325.61s   best: 25.5238
2023-11-11 02:18:01,951:INFO:  Epoch 443/500:  train Loss: 18.3423   val Loss: 23.8034   time: 289.15s   best: 22.8942
2023-11-11 02:20:37,996:INFO:  Epoch 70/500:  train Loss: 23.1281   val Loss: 25.5646   time: 326.39s   best: 25.5238
2023-11-11 02:22:52,463:INFO:  Epoch 444/500:  train Loss: 18.2752   val Loss: 24.6227   time: 290.51s   best: 22.8942
2023-11-11 02:26:01,357:INFO:  Epoch 71/500:  train Loss: 23.2795   val Loss: 26.4344   time: 323.34s   best: 25.5238
2023-11-11 02:27:41,585:INFO:  Epoch 445/500:  train Loss: 18.0301   val Loss: 30.0422   time: 289.10s   best: 22.8942
2023-11-11 02:31:25,209:INFO:  Epoch 72/500:  train Loss: 22.9079   val Loss: 25.5770   time: 323.82s   best: 25.5238
2023-11-11 02:32:30,383:INFO:  Epoch 446/500:  train Loss: 18.3227   val Loss: 23.8730   time: 288.78s   best: 22.8942
2023-11-11 02:36:50,166:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 02:36:50,196:INFO:  Epoch 73/500:  train Loss: 22.8691   val Loss: 25.3912   time: 324.93s   best: 25.3912
2023-11-11 02:37:19,868:INFO:  Epoch 447/500:  train Loss: 18.3131   val Loss: 23.1132   time: 289.48s   best: 22.8942
2023-11-11 02:42:11,899:INFO:  Epoch 448/500:  train Loss: 18.0729   val Loss: 23.4311   time: 292.03s   best: 22.8942
2023-11-11 02:42:12,063:INFO:  Epoch 74/500:  train Loss: 22.8169   val Loss: 34.7816   time: 321.84s   best: 25.3912
2023-11-11 02:47:02,189:INFO:  Epoch 449/500:  train Loss: 17.9968   val Loss: 23.5537   time: 290.29s   best: 22.8942
2023-11-11 02:47:34,008:INFO:  Epoch 75/500:  train Loss: 23.1299   val Loss: 26.2850   time: 321.91s   best: 25.3912
2023-11-11 02:51:51,111:INFO:  Epoch 450/500:  train Loss: 18.7177   val Loss: 24.5123   time: 288.91s   best: 22.8942
2023-11-11 02:52:58,836:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 02:52:58,866:INFO:  Epoch 76/500:  train Loss: 22.7463   val Loss: 25.2967   time: 324.82s   best: 25.2967
2023-11-11 02:56:40,196:INFO:  Epoch 451/500:  train Loss: 18.1621   val Loss: 23.5398   time: 289.07s   best: 22.8942
2023-11-11 02:58:20,055:INFO:  Epoch 77/500:  train Loss: 22.7485   val Loss: 26.4793   time: 321.19s   best: 25.2967
2023-11-11 03:01:28,047:INFO:  Epoch 452/500:  train Loss: 18.2065   val Loss: 23.1017   time: 287.84s   best: 22.8942
2023-11-11 03:03:42,297:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 03:03:42,330:INFO:  Epoch 78/500:  train Loss: 22.6970   val Loss: 25.2444   time: 322.22s   best: 25.2444
2023-11-11 03:06:18,361:INFO:  Epoch 453/500:  train Loss: 18.0645   val Loss: 23.4871   time: 290.31s   best: 22.8942
2023-11-11 03:09:04,269:INFO:  Epoch 79/500:  train Loss: 22.4836   val Loss: 25.5499   time: 321.92s   best: 25.2444
2023-11-11 03:11:08,912:INFO:  Epoch 454/500:  train Loss: 17.9832   val Loss: 23.5109   time: 290.54s   best: 22.8942
2023-11-11 03:14:29,110:INFO:  Epoch 80/500:  train Loss: 22.3672   val Loss: 25.3903   time: 324.82s   best: 25.2444
2023-11-11 03:16:00,520:INFO:  Epoch 455/500:  train Loss: 17.9709   val Loss: 26.5584   time: 291.60s   best: 22.8942
2023-11-11 03:19:50,569:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 03:19:50,592:INFO:  Epoch 81/500:  train Loss: 22.4262   val Loss: 25.0627   time: 321.43s   best: 25.0627
2023-11-11 03:20:50,230:INFO:  Epoch 456/500:  train Loss: 18.3732   val Loss: 23.3896   time: 289.70s   best: 22.8942
2023-11-11 03:25:12,214:INFO:  Epoch 82/500:  train Loss: 22.3155   val Loss: 25.2218   time: 321.61s   best: 25.0627
2023-11-11 03:25:40,442:INFO:  Epoch 457/500:  train Loss: 17.9453   val Loss: 24.1251   time: 290.21s   best: 22.8942
2023-11-11 03:30:29,447:INFO:  Epoch 458/500:  train Loss: 18.0059   val Loss: 23.5169   time: 289.00s   best: 22.8942
2023-11-11 03:30:35,474:INFO:  Epoch 83/500:  train Loss: 22.3001   val Loss: 25.4007   time: 323.23s   best: 25.0627
2023-11-11 03:35:21,153:INFO:  Epoch 459/500:  train Loss: 17.9708   val Loss: 24.2777   time: 291.71s   best: 22.8942
2023-11-11 03:35:56,658:INFO:  Epoch 84/500:  train Loss: 22.3664   val Loss: 25.4976   time: 321.17s   best: 25.0627
2023-11-11 03:40:12,570:INFO:  Epoch 460/500:  train Loss: 18.0923   val Loss: 23.6335   time: 291.40s   best: 22.8942
2023-11-11 03:41:21,122:INFO:  Epoch 85/500:  train Loss: 22.2997   val Loss: 25.2141   time: 324.42s   best: 25.0627
2023-11-11 03:45:01,236:INFO:  Epoch 461/500:  train Loss: 18.0616   val Loss: 25.0803   time: 288.65s   best: 22.8942
2023-11-11 03:46:42,942:INFO:  Epoch 86/500:  train Loss: 22.1060   val Loss: 25.0859   time: 321.80s   best: 25.0627
2023-11-11 03:49:53,407:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-11 03:49:53,440:INFO:  Epoch 462/500:  train Loss: 17.9669   val Loss: 22.8799   time: 292.15s   best: 22.8799
2023-11-11 03:52:03,637:INFO:  Epoch 87/500:  train Loss: 22.3718   val Loss: 25.4911   time: 320.67s   best: 25.0627
2023-11-11 03:54:42,816:INFO:  Epoch 463/500:  train Loss: 18.0601   val Loss: 23.2958   time: 289.38s   best: 22.8799
2023-11-11 03:57:26,246:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 03:57:26,274:INFO:  Epoch 88/500:  train Loss: 22.2411   val Loss: 24.8401   time: 322.58s   best: 24.8401
2023-11-11 03:59:35,099:INFO:  Epoch 464/500:  train Loss: 18.1575   val Loss: 23.2215   time: 292.27s   best: 22.8799
2023-11-11 04:02:47,901:INFO:  Epoch 89/500:  train Loss: 22.0008   val Loss: 25.0585   time: 321.61s   best: 24.8401
2023-11-11 04:04:26,152:INFO:  Epoch 465/500:  train Loss: 17.9953   val Loss: 23.4264   time: 291.03s   best: 22.8799
2023-11-11 04:08:10,464:INFO:  Epoch 90/500:  train Loss: 21.9807   val Loss: 25.0712   time: 322.53s   best: 24.8401
2023-11-11 04:09:15,695:INFO:  Epoch 466/500:  train Loss: 17.9794   val Loss: 23.2716   time: 289.53s   best: 22.8799
2023-11-11 04:13:32,217:INFO:  Epoch 91/500:  train Loss: 21.9985   val Loss: 25.0919   time: 321.74s   best: 24.8401
2023-11-11 04:14:05,892:INFO:  Epoch 467/500:  train Loss: 18.0627   val Loss: 23.6225   time: 290.19s   best: 22.8799
2023-11-11 04:18:54,946:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 04:18:54,977:INFO:  Epoch 92/500:  train Loss: 22.0006   val Loss: 24.6273   time: 322.70s   best: 24.6273
2023-11-11 04:18:57,412:INFO:  Epoch 468/500:  train Loss: 17.8626   val Loss: 25.8863   time: 291.52s   best: 22.8799
2023-11-11 04:23:48,677:INFO:  Epoch 469/500:  train Loss: 18.1948   val Loss: 24.0789   time: 291.24s   best: 22.8799
2023-11-11 04:24:16,227:INFO:  Epoch 93/500:  train Loss: 22.1423   val Loss: 25.5242   time: 321.24s   best: 24.6273
2023-11-11 04:28:37,382:INFO:  Epoch 470/500:  train Loss: 18.1979   val Loss: 24.0501   time: 288.70s   best: 22.8799
2023-11-11 04:29:38,907:INFO:  Epoch 94/500:  train Loss: 21.8507   val Loss: 24.8863   time: 322.64s   best: 24.6273
2023-11-11 04:33:27,454:INFO:  Epoch 471/500:  train Loss: 18.0186   val Loss: 23.5270   time: 290.06s   best: 22.8799
2023-11-11 04:35:03,249:INFO:  Epoch 95/500:  train Loss: 22.0087   val Loss: 24.7742   time: 324.33s   best: 24.6273
2023-11-11 04:38:18,301:INFO:  Epoch 472/500:  train Loss: 18.0013   val Loss: 24.0241   time: 290.83s   best: 22.8799
2023-11-11 04:40:28,319:INFO:  Epoch 96/500:  train Loss: 21.6927   val Loss: 28.8224   time: 325.04s   best: 24.6273
2023-11-11 04:43:07,398:INFO:  Epoch 473/500:  train Loss: 17.9999   val Loss: 22.9029   time: 289.09s   best: 22.8799
2023-11-11 04:45:52,518:INFO:  Epoch 97/500:  train Loss: 21.6023   val Loss: 24.7644   time: 324.17s   best: 24.6273
2023-11-11 04:47:56,539:INFO:  Epoch 474/500:  train Loss: 17.9607   val Loss: 23.5731   time: 289.13s   best: 22.8799
2023-11-11 04:51:15,687:INFO:  Epoch 98/500:  train Loss: 21.5316   val Loss: 25.1376   time: 323.14s   best: 24.6273
2023-11-11 04:52:49,993:INFO:  Epoch 475/500:  train Loss: 18.0272   val Loss: 23.4876   time: 293.44s   best: 22.8799
2023-11-11 04:56:39,438:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 04:56:39,473:INFO:  Epoch 99/500:  train Loss: 21.6070   val Loss: 24.6013   time: 323.73s   best: 24.6013
2023-11-11 04:57:41,233:INFO:  Epoch 476/500:  train Loss: 17.9049   val Loss: 23.4551   time: 291.23s   best: 22.8799
2023-11-11 05:02:03,549:INFO:  Epoch 100/500:  train Loss: 21.6408   val Loss: 25.7988   time: 324.06s   best: 24.6013
2023-11-11 05:02:30,579:INFO:  Epoch 477/500:  train Loss: 17.8756   val Loss: 23.4299   time: 289.32s   best: 22.8799
2023-11-11 05:07:19,137:INFO:  Epoch 478/500:  train Loss: 18.0028   val Loss: 23.4975   time: 288.56s   best: 22.8799
2023-11-11 05:07:24,802:INFO:  Epoch 101/500:  train Loss: 21.5713   val Loss: 25.1419   time: 321.22s   best: 24.6013
2023-11-11 05:12:10,986:INFO:  Epoch 479/500:  train Loss: 17.7712   val Loss: 23.3100   time: 291.85s   best: 22.8799
2023-11-11 05:12:46,936:INFO:  Epoch 102/500:  train Loss: 21.4085   val Loss: 24.8989   time: 322.11s   best: 24.6013
2023-11-11 05:17:03,095:INFO:  Epoch 480/500:  train Loss: 17.9259   val Loss: 23.5416   time: 292.09s   best: 22.8799
2023-11-11 05:18:08,348:INFO:  Epoch 103/500:  train Loss: 21.5319   val Loss: 24.8730   time: 321.40s   best: 24.6013
2023-11-11 05:21:51,773:INFO:  Epoch 481/500:  train Loss: 17.8472   val Loss: 23.8656   time: 288.67s   best: 22.8799
2023-11-11 05:23:29,917:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 05:23:29,954:INFO:  Epoch 104/500:  train Loss: 21.3730   val Loss: 24.2468   time: 321.51s   best: 24.2468
2023-11-11 05:26:42,125:INFO:  Epoch 482/500:  train Loss: 17.9122   val Loss: 22.9463   time: 290.34s   best: 22.8799
2023-11-11 05:28:54,209:INFO:  Epoch 105/500:  train Loss: 21.3934   val Loss: 24.6937   time: 324.25s   best: 24.2468
2023-11-11 05:31:31,854:INFO:  Epoch 483/500:  train Loss: 18.2366   val Loss: 23.0562   time: 289.72s   best: 22.8799
2023-11-11 05:34:15,419:INFO:  Epoch 106/500:  train Loss: 21.3682   val Loss: 24.6306   time: 321.20s   best: 24.2468
2023-11-11 05:36:23,548:INFO:  Epoch 484/500:  train Loss: 17.9646   val Loss: 23.7161   time: 291.68s   best: 22.8799
2023-11-11 05:39:36,549:INFO:  Epoch 107/500:  train Loss: 21.2632   val Loss: 25.0813   time: 321.11s   best: 24.2468
2023-11-11 05:41:16,022:INFO:  Epoch 485/500:  train Loss: 17.8595   val Loss: 23.2326   time: 292.46s   best: 22.8799
2023-11-11 05:44:57,990:INFO:  Epoch 108/500:  train Loss: 21.5305   val Loss: 32.1546   time: 321.39s   best: 24.2468
2023-11-11 05:46:08,052:INFO:  Epoch 486/500:  train Loss: 18.4578   val Loss: 23.5685   time: 292.03s   best: 22.8799
2023-11-11 05:50:22,258:INFO:  Epoch 109/500:  train Loss: 21.1572   val Loss: 25.0523   time: 324.25s   best: 24.2468
2023-11-11 05:50:59,709:INFO:  Epoch 487/500:  train Loss: 17.9207   val Loss: 23.3430   time: 291.65s   best: 22.8799
2023-11-11 05:55:43,323:INFO:  Epoch 110/500:  train Loss: 21.1247   val Loss: 25.3281   time: 321.04s   best: 24.2468
2023-11-11 05:55:49,751:INFO:  Epoch 488/500:  train Loss: 17.8884   val Loss: 23.3032   time: 290.04s   best: 22.8799
2023-11-11 06:00:38,845:INFO:  Epoch 489/500:  train Loss: 18.0313   val Loss: 23.2260   time: 289.09s   best: 22.8799
2023-11-11 06:01:03,857:INFO:  Epoch 111/500:  train Loss: 21.1906   val Loss: 24.6984   time: 320.50s   best: 24.2468
2023-11-11 06:05:28,784:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.6 dataset (0.1 dropout)_ad19.pt
2023-11-11 06:05:28,806:INFO:  Epoch 490/500:  train Loss: 17.9691   val Loss: 22.8070   time: 289.93s   best: 22.8070
2023-11-11 06:06:28,457:INFO:  Epoch 112/500:  train Loss: 21.1841   val Loss: 24.7169   time: 324.59s   best: 24.2468
2023-11-11 06:10:20,347:INFO:  Epoch 491/500:  train Loss: 18.0364   val Loss: 23.4344   time: 291.54s   best: 22.8070
2023-11-11 06:11:49,546:INFO:  Epoch 113/500:  train Loss: 21.1519   val Loss: 24.4219   time: 321.06s   best: 24.2468
2023-11-11 06:15:09,114:INFO:  Epoch 492/500:  train Loss: 17.9992   val Loss: 23.1782   time: 288.76s   best: 22.8070
2023-11-11 06:17:13,276:INFO:  Epoch 114/500:  train Loss: 21.0592   val Loss: 24.3439   time: 323.71s   best: 24.2468
2023-11-11 06:19:59,397:INFO:  Epoch 493/500:  train Loss: 17.8094   val Loss: 24.0986   time: 290.28s   best: 22.8070
2023-11-11 06:22:34,910:INFO:  Epoch 115/500:  train Loss: 21.0493   val Loss: 24.3605   time: 321.61s   best: 24.2468
2023-11-11 06:24:47,314:INFO:  Epoch 494/500:  train Loss: 18.2020   val Loss: 24.2337   time: 287.90s   best: 22.8070
2023-11-11 06:27:59,872:INFO:  Epoch 116/500:  train Loss: 20.9394   val Loss: 26.7897   time: 324.94s   best: 24.2468
2023-11-11 06:29:35,960:INFO:  Epoch 495/500:  train Loss: 17.8681   val Loss: 24.3568   time: 288.63s   best: 22.8070
2023-11-11 06:33:23,699:INFO:  Epoch 117/500:  train Loss: 21.4836   val Loss: 24.8090   time: 323.80s   best: 24.2468
2023-11-11 06:34:25,664:INFO:  Epoch 496/500:  train Loss: 17.7711   val Loss: 23.2286   time: 289.69s   best: 22.8070
2023-11-11 06:38:49,382:INFO:  Epoch 118/500:  train Loss: 21.3151   val Loss: 26.2234   time: 325.66s   best: 24.2468
2023-11-11 06:39:20,805:INFO:  Epoch 497/500:  train Loss: 17.7733   val Loss: 23.3337   time: 295.13s   best: 22.8070
2023-11-11 06:44:09,626:INFO:  Epoch 498/500:  train Loss: 17.7840   val Loss: 23.7359   time: 288.82s   best: 22.8070
2023-11-11 06:44:10,525:INFO:  Epoch 119/500:  train Loss: 21.5637   val Loss: 24.8953   time: 321.10s   best: 24.2468
2023-11-11 06:49:02,035:INFO:  Epoch 499/500:  train Loss: 18.1178   val Loss: 23.1030   time: 292.41s   best: 22.8070
2023-11-11 06:49:35,956:INFO:  Epoch 120/500:  train Loss: 20.9033   val Loss: 24.6851   time: 325.36s   best: 24.2468
2023-11-11 06:53:52,796:INFO:  Epoch 500/500:  train Loss: 17.8418   val Loss: 23.4325   time: 290.75s   best: 22.8070
2023-11-11 06:53:52,807:INFO:  -----> Training complete in 2464m 48s   best validation loss: 22.8070
 
2023-11-11 06:55:01,511:INFO:  Epoch 121/500:  train Loss: 21.2012   val Loss: 24.5787   time: 325.53s   best: 24.2468
2023-11-11 07:00:24,839:INFO:  Epoch 122/500:  train Loss: 20.9529   val Loss: 24.9789   time: 323.31s   best: 24.2468
2023-11-11 07:05:45,617:INFO:  Epoch 123/500:  train Loss: 20.8961   val Loss: 25.1133   time: 320.76s   best: 24.2468
2023-11-11 07:11:07,226:INFO:  Epoch 124/500:  train Loss: 20.6909   val Loss: 24.5791   time: 321.58s   best: 24.2468
2023-11-11 07:16:28,711:INFO:  Epoch 125/500:  train Loss: 20.6825   val Loss: 24.9188   time: 321.47s   best: 24.2468
2023-11-11 07:21:52,764:INFO:  Epoch 126/500:  train Loss: 20.8514   val Loss: 24.9590   time: 324.03s   best: 24.2468
2023-11-11 07:27:15,682:INFO:  Epoch 127/500:  train Loss: 20.6529   val Loss: 24.6676   time: 322.90s   best: 24.2468
2023-11-11 07:32:39,415:INFO:  Epoch 128/500:  train Loss: 20.6462   val Loss: 24.3396   time: 323.73s   best: 24.2468
2023-11-11 07:38:00,443:INFO:  Epoch 129/500:  train Loss: 20.6534   val Loss: 24.5100   time: 321.01s   best: 24.2468
2023-11-11 07:43:22,609:INFO:  Epoch 130/500:  train Loss: 20.5377   val Loss: 24.8919   time: 322.16s   best: 24.2468
2023-11-11 07:48:44,687:INFO:  Epoch 131/500:  train Loss: 20.5795   val Loss: 24.3749   time: 322.02s   best: 24.2468
2023-11-11 07:54:07,577:INFO:  Epoch 132/500:  train Loss: 20.4908   val Loss: 25.1169   time: 322.88s   best: 24.2468
2023-11-11 07:59:29,707:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 07:59:29,777:INFO:  Epoch 133/500:  train Loss: 20.6322   val Loss: 24.0355   time: 322.10s   best: 24.0355
2023-11-11 08:04:54,771:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 08:04:54,792:INFO:  Epoch 134/500:  train Loss: 20.4887   val Loss: 23.8429   time: 324.97s   best: 23.8429
2023-11-11 08:10:16,506:INFO:  Epoch 135/500:  train Loss: 20.5215   val Loss: 28.0336   time: 321.71s   best: 23.8429
2023-11-11 08:15:39,291:INFO:  Epoch 136/500:  train Loss: 20.4131   val Loss: 24.6254   time: 322.75s   best: 23.8429
2023-11-11 08:21:01,352:INFO:  Epoch 137/500:  train Loss: 20.3315   val Loss: 28.1989   time: 322.04s   best: 23.8429
2023-11-11 08:26:23,517:INFO:  Epoch 138/500:  train Loss: 20.4786   val Loss: 24.4695   time: 322.14s   best: 23.8429
2023-11-11 08:31:45,272:INFO:  Epoch 139/500:  train Loss: 20.5326   val Loss: 24.4580   time: 321.73s   best: 23.8429
2023-11-11 08:37:09,325:INFO:  Epoch 140/500:  train Loss: 20.2577   val Loss: 24.4184   time: 324.05s   best: 23.8429
2023-11-11 08:42:33,249:INFO:  Epoch 141/500:  train Loss: 20.2912   val Loss: 35.8304   time: 323.90s   best: 23.8429
2023-11-11 08:47:57,379:INFO:  Epoch 142/500:  train Loss: 20.4569   val Loss: 24.8409   time: 324.11s   best: 23.8429
2023-11-11 08:53:20,427:INFO:  Epoch 143/500:  train Loss: 20.2849   val Loss: 24.4507   time: 323.03s   best: 23.8429
2023-11-11 08:58:41,598:INFO:  Epoch 144/500:  train Loss: 20.1632   val Loss: 23.8591   time: 321.15s   best: 23.8429
2023-11-11 09:04:04,622:INFO:  Epoch 145/500:  train Loss: 20.3152   val Loss: 24.1449   time: 323.01s   best: 23.8429
2023-11-11 09:09:27,210:INFO:  Epoch 146/500:  train Loss: 20.2979   val Loss: 24.2474   time: 322.57s   best: 23.8429
2023-11-11 09:14:51,257:INFO:  Epoch 147/500:  train Loss: 20.1654   val Loss: 24.9731   time: 324.02s   best: 23.8429
2023-11-11 09:20:16,055:INFO:  Epoch 148/500:  train Loss: 20.6370   val Loss: 24.3846   time: 324.77s   best: 23.8429
2023-11-11 09:25:38,345:INFO:  Epoch 149/500:  train Loss: 20.0917   val Loss: 24.2915   time: 322.27s   best: 23.8429
2023-11-11 09:31:00,390:INFO:  Epoch 150/500:  train Loss: 20.1553   val Loss: 24.6967   time: 322.02s   best: 23.8429
2023-11-11 09:36:25,554:INFO:  Epoch 151/500:  train Loss: 20.0146   val Loss: 23.8639   time: 325.12s   best: 23.8429
2023-11-11 09:41:51,165:INFO:  Epoch 152/500:  train Loss: 20.1371   val Loss: 27.8970   time: 325.57s   best: 23.8429
2023-11-11 09:47:16,010:INFO:  Epoch 153/500:  train Loss: 20.1328   val Loss: 24.2717   time: 324.81s   best: 23.8429
2023-11-11 09:52:37,745:INFO:  Epoch 154/500:  train Loss: 20.0286   val Loss: 23.9046   time: 321.71s   best: 23.8429
2023-11-11 09:58:01,446:INFO:  Epoch 155/500:  train Loss: 20.0010   val Loss: 23.9164   time: 323.67s   best: 23.8429
2023-11-11 10:03:26,583:INFO:  Epoch 156/500:  train Loss: 20.2570   val Loss: 24.3791   time: 325.10s   best: 23.8429
2023-11-11 10:08:50,198:INFO:  Epoch 157/500:  train Loss: 20.5819   val Loss: 24.5597   time: 323.59s   best: 23.8429
2023-11-11 10:14:11,551:INFO:  Epoch 158/500:  train Loss: 19.9407   val Loss: 24.6529   time: 321.33s   best: 23.8429
2023-11-11 10:19:32,826:INFO:  Epoch 159/500:  train Loss: 19.8869   val Loss: 24.1626   time: 321.25s   best: 23.8429
2023-11-11 10:24:55,288:INFO:  Epoch 160/500:  train Loss: 20.2051   val Loss: 24.1548   time: 322.43s   best: 23.8429
2023-11-11 10:30:19,996:INFO:  Epoch 161/500:  train Loss: 19.8729   val Loss: 24.3666   time: 324.69s   best: 23.8429
2023-11-11 10:35:45,002:INFO:  Epoch 162/500:  train Loss: 19.9294   val Loss: 24.1591   time: 324.97s   best: 23.8429
2023-11-11 10:41:09,157:INFO:  Epoch 163/500:  train Loss: 20.1609   val Loss: 28.0089   time: 324.13s   best: 23.8429
2023-11-11 10:46:33,291:INFO:  Epoch 164/500:  train Loss: 19.9446   val Loss: 27.9850   time: 324.11s   best: 23.8429
2023-11-11 10:51:57,070:INFO:  Epoch 165/500:  train Loss: 19.9421   val Loss: 24.4225   time: 323.72s   best: 23.8429
2023-11-11 10:57:19,932:INFO:  Epoch 166/500:  train Loss: 19.8558   val Loss: 23.9652   time: 322.79s   best: 23.8429
2023-11-11 11:02:44,832:INFO:  Epoch 167/500:  train Loss: 19.9554   val Loss: 24.9431   time: 324.87s   best: 23.8429
2023-11-11 11:08:09,319:INFO:  Epoch 168/500:  train Loss: 19.9926   val Loss: 24.0344   time: 324.46s   best: 23.8429
2023-11-11 11:13:33,517:INFO:  Epoch 169/500:  train Loss: 19.7572   val Loss: 26.1132   time: 324.17s   best: 23.8429
2023-11-11 11:18:55,475:INFO:  Epoch 170/500:  train Loss: 19.9841   val Loss: 25.0086   time: 321.93s   best: 23.8429
2023-11-11 11:24:19,668:INFO:  Epoch 171/500:  train Loss: 20.2269   val Loss: 24.0930   time: 324.17s   best: 23.8429
2023-11-11 11:29:43,884:INFO:  Epoch 172/500:  train Loss: 19.7935   val Loss: 23.9448   time: 324.19s   best: 23.8429
2023-11-11 11:35:08,027:INFO:  Epoch 173/500:  train Loss: 19.7196   val Loss: 24.2902   time: 324.13s   best: 23.8429
2023-11-11 11:40:31,359:INFO:  Epoch 174/500:  train Loss: 20.0836   val Loss: 24.1905   time: 323.31s   best: 23.8429
2023-11-11 11:45:56,710:INFO:  Epoch 175/500:  train Loss: 19.7384   val Loss: 24.3333   time: 325.32s   best: 23.8429
2023-11-11 11:51:22,082:INFO:  Epoch 176/500:  train Loss: 19.6509   val Loss: 23.9726   time: 325.34s   best: 23.8429
2023-11-11 11:56:43,193:INFO:  Epoch 177/500:  train Loss: 19.7812   val Loss: 23.9978   time: 321.08s   best: 23.8429
2023-11-11 12:02:07,459:INFO:  Epoch 178/500:  train Loss: 19.6319   val Loss: 24.2137   time: 324.15s   best: 23.8429
2023-11-11 12:07:29,812:INFO:  Epoch 179/500:  train Loss: 19.6984   val Loss: 23.9682   time: 322.33s   best: 23.8429
2023-11-11 12:12:55,527:INFO:  Epoch 180/500:  train Loss: 19.7258   val Loss: 24.2567   time: 325.69s   best: 23.8429
2023-11-11 12:18:19,870:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 12:18:19,920:INFO:  Epoch 181/500:  train Loss: 19.5209   val Loss: 23.7162   time: 324.33s   best: 23.7162
2023-11-11 12:23:42,684:INFO:  Epoch 182/500:  train Loss: 19.6307   val Loss: 24.4017   time: 322.75s   best: 23.7162
2023-11-11 12:29:08,356:INFO:  Epoch 183/500:  train Loss: 19.7209   val Loss: 24.5603   time: 325.65s   best: 23.7162
2023-11-11 12:34:32,367:INFO:  Epoch 184/500:  train Loss: 19.5060   val Loss: 24.7750   time: 323.99s   best: 23.7162
2023-11-11 12:39:56,486:INFO:  Epoch 185/500:  train Loss: 19.4846   val Loss: 24.8702   time: 324.02s   best: 23.7162
2023-11-11 12:45:21,586:INFO:  Epoch 186/500:  train Loss: 20.0208   val Loss: 24.8566   time: 325.08s   best: 23.7162
2023-11-11 12:50:44,287:INFO:  Epoch 187/500:  train Loss: 19.6521   val Loss: 27.3885   time: 322.68s   best: 23.7162
2023-11-11 12:56:08,740:INFO:  Epoch 188/500:  train Loss: 19.5802   val Loss: 24.0340   time: 324.43s   best: 23.7162
2023-11-11 13:01:33,133:INFO:  Epoch 189/500:  train Loss: 19.6324   val Loss: 24.6593   time: 324.37s   best: 23.7162
2023-11-11 13:06:55,180:INFO:  Epoch 190/500:  train Loss: 19.4570   val Loss: 26.2212   time: 322.02s   best: 23.7162
2023-11-11 13:12:19,091:INFO:  Epoch 191/500:  train Loss: 19.5168   val Loss: 24.4162   time: 323.89s   best: 23.7162
2023-11-11 13:17:42,059:INFO:  Epoch 192/500:  train Loss: 19.4108   val Loss: 23.7671   time: 322.93s   best: 23.7162
2023-11-11 13:23:06,149:INFO:  Epoch 193/500:  train Loss: 19.4795   val Loss: 23.9958   time: 324.09s   best: 23.7162
2023-11-11 13:28:30,777:INFO:  Epoch 194/500:  train Loss: 19.4499   val Loss: 24.0585   time: 324.58s   best: 23.7162
2023-11-11 13:33:52,440:INFO:  Epoch 195/500:  train Loss: 19.5090   val Loss: 24.1734   time: 321.63s   best: 23.7162
2023-11-11 13:37:49,854:INFO:  Starting experiment lstm autoencoder with 0.8 dataset (0.1 dropout)
2023-11-11 13:37:49,865:INFO:  Defining the model
2023-11-11 13:37:49,920:INFO:  Reading the dataset
2023-11-11 13:39:15,859:INFO:  Epoch 196/500:  train Loss: 19.3575   val Loss: 24.1139   time: 323.41s   best: 23.7162
2023-11-11 13:44:37,486:INFO:  Epoch 197/500:  train Loss: 19.3697   val Loss: 23.8368   time: 321.58s   best: 23.7162
2023-11-11 13:50:02,226:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 13:50:02,256:INFO:  Epoch 198/500:  train Loss: 19.4401   val Loss: 23.6998   time: 324.71s   best: 23.6998
2023-11-11 13:55:25,266:INFO:  Epoch 199/500:  train Loss: 19.4341   val Loss: 23.9200   time: 323.00s   best: 23.6998
2023-11-11 14:00:47,691:INFO:  Epoch 200/500:  train Loss: 19.4076   val Loss: 23.7429   time: 322.40s   best: 23.6998
2023-11-11 14:05:17,083:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 14:05:17,116:INFO:  Epoch 1/500:  train Loss: 76.1521   val Loss: 68.2180   time: 355.86s   best: 68.2180
2023-11-11 14:06:11,109:INFO:  Epoch 201/500:  train Loss: 19.2962   val Loss: 24.2069   time: 323.41s   best: 23.6998
2023-11-11 14:11:11,124:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 14:11:11,155:INFO:  Epoch 2/500:  train Loss: 64.7196   val Loss: 61.7778   time: 353.99s   best: 61.7778
2023-11-11 14:11:34,049:INFO:  Epoch 202/500:  train Loss: 19.3212   val Loss: 24.0665   time: 322.91s   best: 23.6998
2023-11-11 14:16:55,581:INFO:  Epoch 203/500:  train Loss: 19.6038   val Loss: 23.9302   time: 321.50s   best: 23.6998
2023-11-11 14:17:08,999:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 14:17:09,021:INFO:  Epoch 3/500:  train Loss: 59.2078   val Loss: 56.3336   time: 357.82s   best: 56.3336
2023-11-11 14:22:17,260:INFO:  Epoch 204/500:  train Loss: 19.3180   val Loss: 23.8271   time: 321.66s   best: 23.6998
2023-11-11 14:23:03,346:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 14:23:03,365:INFO:  Epoch 4/500:  train Loss: 54.5031   val Loss: 52.0048   time: 354.31s   best: 52.0048
2023-11-11 14:27:39,607:INFO:  Epoch 205/500:  train Loss: 19.2013   val Loss: 23.9527   time: 322.32s   best: 23.6998
2023-11-11 14:28:57,727:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 14:28:57,746:INFO:  Epoch 5/500:  train Loss: 50.0569   val Loss: 48.5778   time: 354.36s   best: 48.5778
2023-11-11 14:33:03,867:INFO:  Epoch 206/500:  train Loss: 19.2453   val Loss: 26.1267   time: 324.23s   best: 23.6998
2023-11-11 14:34:55,176:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 14:34:55,210:INFO:  Epoch 6/500:  train Loss: 46.7039   val Loss: 45.6166   time: 357.43s   best: 45.6166
2023-11-11 14:38:25,914:INFO:  Epoch 207/500:  train Loss: 19.3152   val Loss: 30.1611   time: 322.01s   best: 23.6998
2023-11-11 14:40:50,467:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 14:40:50,499:INFO:  Epoch 7/500:  train Loss: 43.8672   val Loss: 44.3525   time: 355.24s   best: 44.3525
2023-11-11 14:43:49,061:INFO:  Epoch 208/500:  train Loss: 19.3733   val Loss: 23.9504   time: 323.12s   best: 23.6998
2023-11-11 14:46:49,163:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 14:46:49,196:INFO:  Epoch 8/500:  train Loss: 41.9047   val Loss: 41.9430   time: 358.65s   best: 41.9430
2023-11-11 14:49:12,396:INFO:  Epoch 209/500:  train Loss: 19.2836   val Loss: 23.8968   time: 323.30s   best: 23.6998
2023-11-11 14:52:46,003:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 14:52:46,026:INFO:  Epoch 9/500:  train Loss: 39.7858   val Loss: 39.9800   time: 356.79s   best: 39.9800
2023-11-11 14:54:38,010:INFO:  Epoch 210/500:  train Loss: 19.7829   val Loss: 24.6864   time: 325.58s   best: 23.6998
2023-11-11 14:58:40,892:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 14:58:40,923:INFO:  Epoch 10/500:  train Loss: 38.2984   val Loss: 38.0835   time: 354.85s   best: 38.0835
2023-11-11 15:00:00,013:INFO:  Epoch 211/500:  train Loss: 19.4217   val Loss: 26.0757   time: 321.96s   best: 23.6998
2023-11-11 15:04:39,089:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 15:04:39,120:INFO:  Epoch 11/500:  train Loss: 36.9844   val Loss: 37.2007   time: 358.15s   best: 37.2007
2023-11-11 15:05:23,162:INFO:  Epoch 212/500:  train Loss: 19.4164   val Loss: 24.0414   time: 323.14s   best: 23.6998
2023-11-11 15:10:35,904:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 15:10:35,925:INFO:  Epoch 12/500:  train Loss: 35.7343   val Loss: 36.3542   time: 356.75s   best: 36.3542
2023-11-11 15:10:43,894:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 15:10:43,913:INFO:  Epoch 213/500:  train Loss: 19.2059   val Loss: 23.6745   time: 320.73s   best: 23.6745
2023-11-11 15:16:08,626:INFO:  Epoch 214/500:  train Loss: 19.2594   val Loss: 23.7908   time: 324.70s   best: 23.6745
2023-11-11 15:16:33,183:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 15:16:33,202:INFO:  Epoch 13/500:  train Loss: 34.7009   val Loss: 35.0277   time: 357.24s   best: 35.0277
2023-11-11 15:21:31,517:INFO:  Epoch 215/500:  train Loss: 19.2669   val Loss: 25.3362   time: 322.87s   best: 23.6745
2023-11-11 15:22:29,511:INFO:  Epoch 14/500:  train Loss: 33.8133   val Loss: 35.3963   time: 356.30s   best: 35.0277
2023-11-11 15:26:55,723:INFO:  Epoch 216/500:  train Loss: 19.4329   val Loss: 27.2958   time: 324.20s   best: 23.6745
2023-11-11 15:28:24,526:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 15:28:24,558:INFO:  Epoch 15/500:  train Loss: 33.1173   val Loss: 33.8604   time: 355.00s   best: 33.8604
2023-11-11 15:32:16,801:INFO:  Epoch 217/500:  train Loss: 19.3291   val Loss: 24.1392   time: 321.05s   best: 23.6745
2023-11-11 15:34:22,166:INFO:  Epoch 16/500:  train Loss: 32.4763   val Loss: 35.7725   time: 357.61s   best: 33.8604
2023-11-11 15:37:38,695:INFO:  Epoch 218/500:  train Loss: 19.2180   val Loss: 23.8837   time: 321.87s   best: 23.6745
2023-11-11 15:40:18,780:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 15:40:18,810:INFO:  Epoch 17/500:  train Loss: 31.8257   val Loss: 32.3895   time: 356.59s   best: 32.3895
2023-11-11 15:43:03,534:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 15:43:03,570:INFO:  Epoch 219/500:  train Loss: 19.2114   val Loss: 23.5106   time: 324.81s   best: 23.5106
2023-11-11 15:46:17,027:INFO:  Epoch 18/500:  train Loss: 31.3404   val Loss: 33.8865   time: 358.22s   best: 32.3895
2023-11-11 15:48:29,148:INFO:  Epoch 220/500:  train Loss: 19.2145   val Loss: 27.6428   time: 325.56s   best: 23.5106
2023-11-11 15:52:12,005:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 15:52:12,028:INFO:  Epoch 19/500:  train Loss: 30.8299   val Loss: 31.5479   time: 354.94s   best: 31.5479
2023-11-11 15:53:53,062:INFO:  Epoch 221/500:  train Loss: 19.0282   val Loss: 24.5440   time: 323.89s   best: 23.5106
2023-11-11 15:58:10,382:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 15:58:10,417:INFO:  Epoch 20/500:  train Loss: 30.4308   val Loss: 31.1370   time: 358.35s   best: 31.1370
2023-11-11 15:59:14,663:INFO:  Epoch 222/500:  train Loss: 19.2845   val Loss: 23.6697   time: 321.57s   best: 23.5106
2023-11-11 16:04:05,934:INFO:  Epoch 21/500:  train Loss: 29.8841   val Loss: 31.3973   time: 355.51s   best: 31.1370
2023-11-11 16:04:35,441:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 16:04:35,472:INFO:  Epoch 223/500:  train Loss: 19.1876   val Loss: 23.4725   time: 320.75s   best: 23.4725
2023-11-11 16:09:57,436:INFO:  Epoch 224/500:  train Loss: 19.1060   val Loss: 23.4961   time: 321.96s   best: 23.4725
2023-11-11 16:10:02,160:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 16:10:02,187:INFO:  Epoch 22/500:  train Loss: 29.4820   val Loss: 30.6957   time: 356.20s   best: 30.6957
2023-11-11 16:15:18,450:INFO:  Epoch 225/500:  train Loss: 19.1448   val Loss: 24.0621   time: 320.97s   best: 23.4725
2023-11-11 16:15:57,410:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 16:15:57,442:INFO:  Epoch 23/500:  train Loss: 29.1516   val Loss: 30.4219   time: 355.21s   best: 30.4219
2023-11-11 16:20:39,748:INFO:  Epoch 226/500:  train Loss: 19.0911   val Loss: 24.0782   time: 321.27s   best: 23.4725
2023-11-11 16:21:53,604:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 16:21:53,711:INFO:  Epoch 24/500:  train Loss: 28.7220   val Loss: 30.1750   time: 356.16s   best: 30.1750
2023-11-11 16:26:01,545:INFO:  Epoch 227/500:  train Loss: 19.5779   val Loss: 25.4621   time: 321.78s   best: 23.4725
2023-11-11 16:27:51,105:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 16:27:51,136:INFO:  Epoch 25/500:  train Loss: 28.6966   val Loss: 29.5862   time: 357.37s   best: 29.5862
2023-11-11 16:31:25,166:INFO:  Epoch 228/500:  train Loss: 19.0453   val Loss: 25.7483   time: 323.59s   best: 23.4725
2023-11-11 16:33:49,109:INFO:  Epoch 26/500:  train Loss: 28.2483   val Loss: 30.1984   time: 357.96s   best: 29.5862
2023-11-11 16:36:49,139:INFO:  Epoch 229/500:  train Loss: 18.9725   val Loss: 24.1896   time: 323.94s   best: 23.4725
2023-11-11 16:39:46,554:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 16:39:46,587:INFO:  Epoch 27/500:  train Loss: 27.8958   val Loss: 29.4737   time: 357.39s   best: 29.4737
2023-11-11 16:42:13,155:INFO:  Epoch 230/500:  train Loss: 18.9578   val Loss: 23.6087   time: 323.99s   best: 23.4725
2023-11-11 16:45:43,305:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 16:45:43,693:INFO:  Epoch 28/500:  train Loss: 27.7070   val Loss: 28.8445   time: 356.70s   best: 28.8445
2023-11-11 16:47:37,916:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-11 16:47:37,951:INFO:  Epoch 231/500:  train Loss: 19.1333   val Loss: 23.2703   time: 324.74s   best: 23.2703
2023-11-11 16:51:42,558:INFO:  Epoch 29/500:  train Loss: 27.4773   val Loss: 29.5076   time: 358.86s   best: 28.8445
2023-11-11 16:52:59,572:INFO:  Epoch 232/500:  train Loss: 19.0040   val Loss: 24.0860   time: 321.62s   best: 23.2703
2023-11-11 16:57:37,503:INFO:  Epoch 30/500:  train Loss: 27.6952   val Loss: 29.9456   time: 354.92s   best: 28.8445
2023-11-11 16:58:23,751:INFO:  Epoch 233/500:  train Loss: 19.0586   val Loss: 23.9159   time: 324.14s   best: 23.2703
2023-11-11 17:03:33,208:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 17:03:33,250:INFO:  Epoch 31/500:  train Loss: 27.0250   val Loss: 28.5378   time: 355.66s   best: 28.5378
2023-11-11 17:03:47,689:INFO:  Epoch 234/500:  train Loss: 19.0335   val Loss: 24.7736   time: 323.91s   best: 23.2703
2023-11-11 17:09:09,297:INFO:  Epoch 235/500:  train Loss: 19.0876   val Loss: 23.8176   time: 321.59s   best: 23.2703
2023-11-11 17:09:29,591:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 17:09:29,610:INFO:  Epoch 32/500:  train Loss: 26.7423   val Loss: 28.3919   time: 356.34s   best: 28.3919
2023-11-11 17:14:32,752:INFO:  Epoch 236/500:  train Loss: 19.2300   val Loss: 24.1979   time: 323.45s   best: 23.2703
2023-11-11 17:15:28,871:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 17:15:28,912:INFO:  Epoch 33/500:  train Loss: 26.4888   val Loss: 28.3404   time: 359.23s   best: 28.3404
2023-11-11 17:19:56,375:INFO:  Epoch 237/500:  train Loss: 19.1217   val Loss: 23.4517   time: 323.54s   best: 23.2703
2023-11-11 17:21:25,170:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 17:21:25,199:INFO:  Epoch 34/500:  train Loss: 26.3079   val Loss: 27.8866   time: 356.25s   best: 27.8866
2023-11-11 17:25:20,783:INFO:  Epoch 238/500:  train Loss: 19.0199   val Loss: 23.5306   time: 324.37s   best: 23.2703
2023-11-11 17:27:22,466:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 17:27:22,496:INFO:  Epoch 35/500:  train Loss: 26.1384   val Loss: 27.5880   time: 357.25s   best: 27.5880
2023-11-11 17:30:45,842:INFO:  Epoch 239/500:  train Loss: 18.9625   val Loss: 23.9505   time: 325.04s   best: 23.2703
2023-11-11 17:33:21,443:INFO:  Epoch 36/500:  train Loss: 25.9743   val Loss: 28.8884   time: 358.92s   best: 27.5880
2023-11-11 17:36:09,388:INFO:  Epoch 240/500:  train Loss: 18.9959   val Loss: 23.9530   time: 323.53s   best: 23.2703
2023-11-11 17:39:16,348:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 17:39:16,390:INFO:  Epoch 37/500:  train Loss: 25.6942   val Loss: 27.4106   time: 354.89s   best: 27.4106
2023-11-11 17:41:31,002:INFO:  Epoch 241/500:  train Loss: 18.8805   val Loss: 23.9112   time: 321.58s   best: 23.2703
2023-11-11 17:45:14,116:INFO:  Epoch 38/500:  train Loss: 25.5339   val Loss: 27.7056   time: 357.71s   best: 27.4106
2023-11-11 17:46:53,927:INFO:  Epoch 242/500:  train Loss: 18.9512   val Loss: 23.6165   time: 322.90s   best: 23.2703
2023-11-11 17:51:10,674:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 17:51:10,708:INFO:  Epoch 39/500:  train Loss: 25.5415   val Loss: 26.7731   time: 356.54s   best: 26.7731
2023-11-11 17:52:15,616:INFO:  Epoch 243/500:  train Loss: 18.9046   val Loss: 23.6403   time: 321.66s   best: 23.2703
2023-11-11 17:57:08,699:INFO:  Epoch 40/500:  train Loss: 25.2477   val Loss: 27.2545   time: 357.97s   best: 26.7731
2023-11-11 17:57:38,317:INFO:  Epoch 244/500:  train Loss: 19.8561   val Loss: 23.8270   time: 322.69s   best: 23.2703
2023-11-11 18:03:00,054:INFO:  Epoch 245/500:  train Loss: 19.0873   val Loss: 23.4085   time: 321.71s   best: 23.2703
2023-11-11 18:03:07,100:INFO:  Epoch 41/500:  train Loss: 25.0822   val Loss: 27.1877   time: 358.37s   best: 26.7731
2023-11-11 18:08:23,161:INFO:  Epoch 246/500:  train Loss: 18.8967   val Loss: 23.6998   time: 323.09s   best: 23.2703
2023-11-11 18:09:04,966:INFO:  Epoch 42/500:  train Loss: 25.0888   val Loss: 27.2677   time: 357.86s   best: 26.7731
2023-11-11 18:13:45,746:INFO:  Epoch 247/500:  train Loss: 19.0342   val Loss: 24.2165   time: 322.56s   best: 23.2703
2023-11-11 18:15:00,011:INFO:  Epoch 43/500:  train Loss: 24.9268   val Loss: 27.2554   time: 355.04s   best: 26.7731
2023-11-11 18:19:08,046:INFO:  Epoch 248/500:  train Loss: 18.9229   val Loss: 23.7370   time: 322.29s   best: 23.2703
2023-11-11 18:20:59,039:INFO:  Epoch 44/500:  train Loss: 24.7221   val Loss: 26.9934   time: 358.99s   best: 26.7731
2023-11-11 18:24:31,724:INFO:  Epoch 249/500:  train Loss: 19.0176   val Loss: 23.4859   time: 323.67s   best: 23.2703
2023-11-11 18:26:54,002:INFO:  Epoch 45/500:  train Loss: 24.6887   val Loss: 28.0317   time: 354.94s   best: 26.7731
2023-11-11 18:29:53,144:INFO:  Epoch 250/500:  train Loss: 19.1553   val Loss: 23.5995   time: 321.40s   best: 23.2703
2023-11-11 18:32:50,797:INFO:  Epoch 46/500:  train Loss: 24.5231   val Loss: 28.0287   time: 356.75s   best: 26.7731
2023-11-11 18:35:18,709:INFO:  Epoch 251/500:  train Loss: 18.7179   val Loss: 23.7172   time: 325.55s   best: 23.2703
2023-11-11 18:38:49,346:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 18:38:49,375:INFO:  Epoch 47/500:  train Loss: 24.4604   val Loss: 26.4221   time: 358.52s   best: 26.4221
2023-11-11 18:40:44,785:INFO:  Epoch 252/500:  train Loss: 19.1553   val Loss: 23.5202   time: 326.06s   best: 23.2703
2023-11-11 18:44:45,034:INFO:  Epoch 48/500:  train Loss: 24.3082   val Loss: 26.6476   time: 355.65s   best: 26.4221
2023-11-11 18:46:06,529:INFO:  Epoch 253/500:  train Loss: 18.8967   val Loss: 23.5584   time: 321.71s   best: 23.2703
2023-11-11 18:50:40,233:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 18:50:40,269:INFO:  Epoch 49/500:  train Loss: 24.2154   val Loss: 26.2209   time: 355.17s   best: 26.2209
2023-11-11 18:51:30,479:INFO:  Epoch 254/500:  train Loss: 18.7517   val Loss: 23.8295   time: 323.94s   best: 23.2703
2023-11-11 18:56:36,123:INFO:  Epoch 50/500:  train Loss: 24.0088   val Loss: 26.5711   time: 355.85s   best: 26.2209
2023-11-11 18:56:51,712:INFO:  Epoch 255/500:  train Loss: 18.8066   val Loss: 23.7602   time: 321.21s   best: 23.2703
2023-11-11 19:02:14,129:INFO:  Epoch 256/500:  train Loss: 18.7265   val Loss: 23.9215   time: 322.35s   best: 23.2703
2023-11-11 19:02:34,162:INFO:  Epoch 51/500:  train Loss: 24.2513   val Loss: 26.4749   time: 358.01s   best: 26.2209
2023-11-11 19:07:37,848:INFO:  Epoch 257/500:  train Loss: 18.8369   val Loss: 24.3457   time: 323.70s   best: 23.2703
2023-11-11 19:08:30,333:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 19:08:30,368:INFO:  Epoch 52/500:  train Loss: 23.9602   val Loss: 26.1559   time: 356.13s   best: 26.1559
2023-11-11 19:13:00,501:INFO:  Epoch 258/500:  train Loss: 18.7545   val Loss: 23.5442   time: 322.64s   best: 23.2703
2023-11-11 19:14:26,009:INFO:  Epoch 53/500:  train Loss: 23.7822   val Loss: 27.1009   time: 355.62s   best: 26.1559
2023-11-11 19:18:22,121:INFO:  Epoch 259/500:  train Loss: 18.6815   val Loss: 23.7332   time: 321.60s   best: 23.2703
2023-11-11 19:20:21,661:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 19:20:21,688:INFO:  Epoch 54/500:  train Loss: 23.5511   val Loss: 25.9889   time: 355.62s   best: 25.9889
2023-11-11 19:23:47,081:INFO:  Epoch 260/500:  train Loss: 18.8409   val Loss: 23.3211   time: 324.92s   best: 23.2703
2023-11-11 19:26:14,958:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 19:26:14,986:INFO:  Epoch 55/500:  train Loss: 23.5217   val Loss: 25.5299   time: 353.27s   best: 25.5299
2023-11-11 19:29:11,953:INFO:  Epoch 261/500:  train Loss: 18.8651   val Loss: 23.9097   time: 324.83s   best: 23.2703
2023-11-11 19:32:13,136:INFO:  Epoch 56/500:  train Loss: 23.3689   val Loss: 26.0448   time: 358.14s   best: 25.5299
2023-11-11 19:34:36,364:INFO:  Epoch 262/500:  train Loss: 18.7614   val Loss: 23.6337   time: 324.39s   best: 23.2703
2023-11-11 19:38:07,062:INFO:  Epoch 57/500:  train Loss: 23.3020   val Loss: 25.6384   time: 353.90s   best: 25.5299
2023-11-11 19:39:57,365:INFO:  Epoch 263/500:  train Loss: 18.6827   val Loss: 23.7131   time: 320.98s   best: 23.2703
2023-11-11 19:44:01,922:INFO:  Epoch 58/500:  train Loss: 23.4518   val Loss: 25.6500   time: 354.83s   best: 25.5299
2023-11-11 19:45:18,997:INFO:  Epoch 264/500:  train Loss: 18.7811   val Loss: 23.8168   time: 321.62s   best: 23.2703
2023-11-11 19:49:56,310:INFO:  Epoch 59/500:  train Loss: 23.1150   val Loss: 25.7010   time: 354.37s   best: 25.5299
2023-11-11 19:50:40,556:INFO:  Epoch 265/500:  train Loss: 18.6281   val Loss: 23.5189   time: 321.55s   best: 23.2703
2023-11-11 19:55:53,412:INFO:  Epoch 60/500:  train Loss: 23.1340   val Loss: 25.8195   time: 357.08s   best: 25.5299
2023-11-11 19:56:05,065:INFO:  Epoch 266/500:  train Loss: 18.7968   val Loss: 23.6610   time: 324.51s   best: 23.2703
2023-11-11 20:01:26,632:INFO:  Epoch 267/500:  train Loss: 18.7229   val Loss: 23.8919   time: 321.55s   best: 23.2703
2023-11-11 20:01:49,784:INFO:  Epoch 61/500:  train Loss: 23.0094   val Loss: 25.9148   time: 356.36s   best: 25.5299
2023-11-11 20:06:50,762:INFO:  Epoch 268/500:  train Loss: 18.7042   val Loss: 23.7780   time: 324.10s   best: 23.2703
2023-11-11 20:07:48,453:INFO:  Epoch 62/500:  train Loss: 22.9764   val Loss: 25.5712   time: 358.64s   best: 25.5299
2023-11-11 20:12:16,665:INFO:  Epoch 269/500:  train Loss: 18.6992   val Loss: 27.6916   time: 325.87s   best: 23.2703
2023-11-11 20:13:44,529:INFO:  Epoch 63/500:  train Loss: 22.8118   val Loss: 26.1959   time: 356.06s   best: 25.5299
2023-11-11 20:17:38,913:INFO:  Epoch 270/500:  train Loss: 18.5880   val Loss: 24.0184   time: 322.21s   best: 23.2703
2023-11-11 20:20:36,336:INFO:  Epoch 64/500:  train Loss: 22.7408   val Loss: 25.6213   time: 411.77s   best: 25.5299
2023-11-11 20:23:01,957:INFO:  Epoch 271/500:  train Loss: 18.6866   val Loss: 23.8445   time: 323.00s   best: 23.2703
2023-11-11 20:26:31,397:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 20:26:31,424:INFO:  Epoch 65/500:  train Loss: 22.6540   val Loss: 25.2239   time: 355.03s   best: 25.2239
2023-11-11 20:28:23,475:INFO:  Epoch 272/500:  train Loss: 19.5069   val Loss: 23.8202   time: 321.49s   best: 23.2703
2023-11-11 20:32:25,565:INFO:  Epoch 66/500:  train Loss: 22.7270   val Loss: 25.9634   time: 354.13s   best: 25.2239
2023-11-11 20:33:47,071:INFO:  Epoch 273/500:  train Loss: 18.9719   val Loss: 23.5485   time: 323.58s   best: 23.2703
2023-11-11 20:38:45,881:INFO:  Epoch 67/500:  train Loss: 22.5384   val Loss: 26.5727   time: 380.29s   best: 25.2239
2023-11-11 20:39:12,191:INFO:  Epoch 274/500:  train Loss: 18.8222   val Loss: 23.7003   time: 325.10s   best: 23.2703
2023-11-11 20:44:38,176:INFO:  Epoch 275/500:  train Loss: 18.6608   val Loss: 23.7151   time: 325.96s   best: 23.2703
2023-11-11 20:49:08,779:INFO:  Epoch 68/500:  train Loss: 22.4948   val Loss: 25.7459   time: 622.88s   best: 25.2239
2023-11-11 20:50:03,283:INFO:  Epoch 276/500:  train Loss: 18.6840   val Loss: 23.4329   time: 325.07s   best: 23.2703
2023-11-11 20:55:26,871:INFO:  Epoch 277/500:  train Loss: 18.5723   val Loss: 24.0558   time: 323.59s   best: 23.2703
2023-11-11 20:58:07,834:INFO:  Epoch 69/500:  train Loss: 22.4614   val Loss: 25.2965   time: 539.04s   best: 25.2239
2023-11-11 21:00:51,561:INFO:  Epoch 278/500:  train Loss: 18.5464   val Loss: 23.5001   time: 324.68s   best: 23.2703
2023-11-11 21:04:05,661:INFO:  Epoch 70/500:  train Loss: 22.4138   val Loss: 25.8092   time: 357.79s   best: 25.2239
2023-11-11 21:06:16,616:INFO:  Epoch 279/500:  train Loss: 18.6959   val Loss: 23.6651   time: 325.02s   best: 23.2703
2023-11-11 21:09:59,963:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 21:10:00,004:INFO:  Epoch 71/500:  train Loss: 22.4147   val Loss: 24.9082   time: 354.27s   best: 24.9082
2023-11-11 21:11:40,332:INFO:  Epoch 280/500:  train Loss: 19.7840   val Loss: 24.4464   time: 323.69s   best: 23.2703
2023-11-11 21:15:55,693:INFO:  Epoch 72/500:  train Loss: 22.3087   val Loss: 25.3004   time: 355.69s   best: 24.9082
2023-11-11 21:17:03,887:INFO:  Epoch 281/500:  train Loss: 19.0420   val Loss: 23.7572   time: 323.54s   best: 23.2703
2023-11-11 21:22:25,206:INFO:  Epoch 282/500:  train Loss: 18.9913   val Loss: 24.2417   time: 321.29s   best: 23.2703
2023-11-11 21:24:22,895:INFO:  Epoch 73/500:  train Loss: 22.1936   val Loss: 25.0593   time: 507.13s   best: 24.9082
2023-11-11 21:27:47,456:INFO:  Epoch 283/500:  train Loss: 18.8186   val Loss: 23.9845   time: 322.24s   best: 23.2703
2023-11-11 21:30:51,693:INFO:  Epoch 74/500:  train Loss: 22.1104   val Loss: 25.2895   time: 388.79s   best: 24.9082
2023-11-11 21:33:11,219:INFO:  Epoch 284/500:  train Loss: 18.9970   val Loss: 24.1278   time: 323.72s   best: 23.2703
2023-11-11 21:36:46,585:INFO:  Epoch 75/500:  train Loss: 22.4796   val Loss: 24.9520   time: 354.85s   best: 24.9082
2023-11-11 21:38:35,625:INFO:  Epoch 285/500:  train Loss: 18.6967   val Loss: 24.3935   time: 324.39s   best: 23.2703
2023-11-11 21:42:44,110:INFO:  Epoch 76/500:  train Loss: 22.1197   val Loss: 25.2998   time: 357.49s   best: 24.9082
2023-11-11 21:44:01,071:INFO:  Epoch 286/500:  train Loss: 18.8465   val Loss: 24.0864   time: 325.41s   best: 23.2703
2023-11-11 21:48:41,049:INFO:  Epoch 77/500:  train Loss: 22.0254   val Loss: 25.1371   time: 356.91s   best: 24.9082
2023-11-11 21:49:22,396:INFO:  Epoch 287/500:  train Loss: 18.7120   val Loss: 25.1757   time: 321.30s   best: 23.2703
2023-11-11 21:54:38,525:INFO:  Epoch 78/500:  train Loss: 21.9159   val Loss: 24.9810   time: 357.46s   best: 24.9082
2023-11-11 21:54:45,033:INFO:  Epoch 288/500:  train Loss: 18.7643   val Loss: 23.8404   time: 322.61s   best: 23.2703
2023-11-11 22:00:06,979:INFO:  Epoch 289/500:  train Loss: 18.7347   val Loss: 24.0843   time: 321.93s   best: 23.2703
2023-11-11 22:00:33,260:INFO:  Epoch 79/500:  train Loss: 22.0269   val Loss: 25.9463   time: 354.70s   best: 24.9082
2023-11-11 22:05:28,790:INFO:  Epoch 290/500:  train Loss: 18.5634   val Loss: 27.3178   time: 321.80s   best: 23.2703
2023-11-11 22:06:30,966:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 22:06:30,985:INFO:  Epoch 80/500:  train Loss: 21.8182   val Loss: 24.8719   time: 357.65s   best: 24.8719
2023-11-11 22:10:54,019:INFO:  Epoch 291/500:  train Loss: 18.5698   val Loss: 24.0078   time: 325.20s   best: 23.2703
2023-11-11 22:12:26,005:INFO:  Epoch 81/500:  train Loss: 21.7901   val Loss: 26.8672   time: 355.00s   best: 24.8719
2023-11-11 22:16:15,622:INFO:  Epoch 292/500:  train Loss: 18.6231   val Loss: 23.8908   time: 321.58s   best: 23.2703
2023-11-11 22:18:23,169:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 22:18:23,202:INFO:  Epoch 82/500:  train Loss: 21.7520   val Loss: 24.4325   time: 357.14s   best: 24.4325
2023-11-11 22:21:39,740:INFO:  Epoch 293/500:  train Loss: 18.8329   val Loss: 23.5308   time: 324.09s   best: 23.2703
2023-11-11 22:24:18,463:INFO:  Epoch 83/500:  train Loss: 21.6363   val Loss: 25.2859   time: 355.26s   best: 24.4325
2023-11-11 22:27:03,626:INFO:  Epoch 294/500:  train Loss: 18.5914   val Loss: 23.6619   time: 323.86s   best: 23.2703
2023-11-11 22:30:16,104:INFO:  Epoch 84/500:  train Loss: 21.6106   val Loss: 24.8032   time: 357.61s   best: 24.4325
2023-11-11 22:32:25,315:INFO:  Epoch 295/500:  train Loss: 18.8281   val Loss: 24.0808   time: 321.65s   best: 23.2703
2023-11-11 22:36:12,092:INFO:  Epoch 85/500:  train Loss: 21.5649   val Loss: 25.1680   time: 355.96s   best: 24.4325
2023-11-11 22:37:46,704:INFO:  Epoch 296/500:  train Loss: 18.5189   val Loss: 23.5018   time: 321.38s   best: 23.2703
2023-11-11 22:42:06,956:INFO:  Epoch 86/500:  train Loss: 21.4899   val Loss: 24.5766   time: 354.85s   best: 24.4325
2023-11-11 22:43:07,797:INFO:  Epoch 297/500:  train Loss: 18.5768   val Loss: 23.6851   time: 321.06s   best: 23.2703
2023-11-11 22:48:01,993:INFO:  Epoch 87/500:  train Loss: 21.4553   val Loss: 24.9507   time: 355.03s   best: 24.4325
2023-11-11 22:48:29,699:INFO:  Epoch 298/500:  train Loss: 18.6139   val Loss: 24.0437   time: 321.84s   best: 23.2703
2023-11-11 22:53:50,662:INFO:  Epoch 299/500:  train Loss: 18.6435   val Loss: 24.0406   time: 320.94s   best: 23.2703
2023-11-11 22:53:57,461:INFO:  Epoch 88/500:  train Loss: 21.6141   val Loss: 24.4977   time: 355.44s   best: 24.4325
2023-11-11 22:59:14,180:INFO:  Epoch 300/500:  train Loss: 18.5324   val Loss: 23.5551   time: 323.50s   best: 23.2703
2023-11-11 22:59:53,345:INFO:  Epoch 89/500:  train Loss: 21.3644   val Loss: 24.5052   time: 355.87s   best: 24.4325
2023-11-11 23:04:36,238:INFO:  Epoch 301/500:  train Loss: 18.5412   val Loss: 23.6752   time: 322.04s   best: 23.2703
2023-11-11 23:05:51,593:INFO:  Epoch 90/500:  train Loss: 21.2957   val Loss: 25.1901   time: 358.23s   best: 24.4325
2023-11-11 23:10:01,549:INFO:  Epoch 302/500:  train Loss: 18.5513   val Loss: 24.0871   time: 325.27s   best: 23.2703
2023-11-11 23:11:46,523:INFO:  Epoch 91/500:  train Loss: 21.3376   val Loss: 25.3780   time: 354.93s   best: 24.4325
2023-11-11 23:15:23,523:INFO:  Epoch 303/500:  train Loss: 19.0613   val Loss: 23.9793   time: 321.95s   best: 23.2703
2023-11-11 23:17:41,500:INFO:  Epoch 92/500:  train Loss: 21.3342   val Loss: 24.5913   time: 354.95s   best: 24.4325
2023-11-11 23:20:47,601:INFO:  Epoch 304/500:  train Loss: 18.6007   val Loss: 23.6328   time: 324.05s   best: 23.2703
2023-11-11 23:25:13,293:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 23:25:13,336:INFO:  Epoch 93/500:  train Loss: 21.2952   val Loss: 24.3123   time: 451.76s   best: 24.3123
2023-11-11 23:26:09,698:INFO:  Epoch 305/500:  train Loss: 18.3967   val Loss: 24.0728   time: 322.09s   best: 23.2703
2023-11-11 23:31:07,098:INFO:  Epoch 94/500:  train Loss: 21.1529   val Loss: 24.3287   time: 353.74s   best: 24.3123
2023-11-11 23:31:32,807:INFO:  Epoch 306/500:  train Loss: 18.5915   val Loss: 23.5660   time: 323.07s   best: 23.2703
2023-11-11 23:36:55,760:INFO:  Epoch 307/500:  train Loss: 18.4784   val Loss: 23.7068   time: 322.93s   best: 23.2703
2023-11-11 23:37:01,741:INFO:  Epoch 95/500:  train Loss: 21.1685   val Loss: 24.3895   time: 354.62s   best: 24.3123
2023-11-11 23:42:17,045:INFO:  Epoch 308/500:  train Loss: 18.4032   val Loss: 23.8968   time: 321.26s   best: 23.2703
2023-11-11 23:43:12,514:INFO:  Epoch 96/500:  train Loss: 21.1789   val Loss: 24.7105   time: 370.76s   best: 24.3123
2023-11-11 23:47:38,482:INFO:  Epoch 309/500:  train Loss: 18.4785   val Loss: 23.4514   time: 321.40s   best: 23.2703
2023-11-11 23:49:47,594:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-11 23:49:47,671:INFO:  Epoch 97/500:  train Loss: 21.1438   val Loss: 24.2723   time: 395.07s   best: 24.2723
2023-11-11 23:52:59,949:INFO:  Epoch 310/500:  train Loss: 18.3820   val Loss: 24.0413   time: 321.44s   best: 23.2703
2023-11-11 23:55:46,405:INFO:  Epoch 98/500:  train Loss: 21.0525   val Loss: 24.4852   time: 358.73s   best: 24.2723
2023-11-11 23:58:24,919:INFO:  Epoch 311/500:  train Loss: 18.4254   val Loss: 23.4038   time: 324.93s   best: 23.2703
2023-11-12 00:01:40,326:INFO:  Epoch 99/500:  train Loss: 20.9975   val Loss: 24.4450   time: 353.90s   best: 24.2723
2023-11-12 00:03:47,687:INFO:  Epoch 312/500:  train Loss: 18.8563   val Loss: 23.5065   time: 322.74s   best: 23.2703
2023-11-12 00:07:36,828:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 00:07:36,859:INFO:  Epoch 100/500:  train Loss: 21.2462   val Loss: 24.1821   time: 356.47s   best: 24.1821
2023-11-12 00:09:12,829:INFO:  Epoch 313/500:  train Loss: 18.4196   val Loss: 24.4369   time: 325.12s   best: 23.2703
2023-11-12 00:13:31,335:INFO:  Epoch 101/500:  train Loss: 20.8743   val Loss: 24.3421   time: 354.45s   best: 24.1821
2023-11-12 00:14:35,181:INFO:  Epoch 314/500:  train Loss: 18.4691   val Loss: 23.2935   time: 322.32s   best: 23.2703
2023-11-12 00:19:29,623:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 00:19:29,652:INFO:  Epoch 102/500:  train Loss: 20.9339   val Loss: 24.1793   time: 358.24s   best: 24.1793
2023-11-12 00:19:59,170:INFO:  Epoch 315/500:  train Loss: 18.4652   val Loss: 23.4285   time: 323.95s   best: 23.2703
2023-11-12 00:25:21,136:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-12 00:25:21,161:INFO:  Epoch 316/500:  train Loss: 18.2883   val Loss: 23.2498   time: 321.94s   best: 23.2498
2023-11-12 00:25:31,760:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 00:25:31,809:INFO:  Epoch 103/500:  train Loss: 20.8759   val Loss: 23.9325   time: 362.09s   best: 23.9325
2023-11-12 00:30:45,906:INFO:  Epoch 317/500:  train Loss: 18.8002   val Loss: 23.3151   time: 324.73s   best: 23.2498
2023-11-12 00:31:28,897:INFO:  Epoch 104/500:  train Loss: 20.8417   val Loss: 24.3480   time: 357.08s   best: 23.9325
2023-11-12 00:36:09,638:INFO:  Epoch 318/500:  train Loss: 18.7501   val Loss: 23.6552   time: 323.70s   best: 23.2498
2023-11-12 00:37:23,560:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 00:37:23,584:INFO:  Epoch 105/500:  train Loss: 20.8824   val Loss: 23.8319   time: 354.64s   best: 23.8319
2023-11-12 00:41:32,561:INFO:  Epoch 319/500:  train Loss: 18.4285   val Loss: 23.8039   time: 322.89s   best: 23.2498
2023-11-12 00:43:18,477:INFO:  Epoch 106/500:  train Loss: 20.8840   val Loss: 24.9641   time: 354.89s   best: 23.8319
2023-11-12 00:46:57,118:INFO:  Epoch 320/500:  train Loss: 18.5867   val Loss: 23.6948   time: 324.50s   best: 23.2498
2023-11-12 00:49:15,338:INFO:  Epoch 107/500:  train Loss: 20.7042   val Loss: 28.4632   time: 356.84s   best: 23.8319
2023-11-12 00:52:19,106:INFO:  Epoch 321/500:  train Loss: 18.3281   val Loss: 23.9022   time: 321.98s   best: 23.2498
2023-11-12 00:55:18,629:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 00:55:18,674:INFO:  Epoch 108/500:  train Loss: 20.9106   val Loss: 23.8094   time: 363.27s   best: 23.8094
2023-11-12 00:57:43,935:INFO:  Epoch 322/500:  train Loss: 18.3177   val Loss: 23.8733   time: 324.77s   best: 23.2498
2023-11-12 01:01:17,179:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 01:01:17,208:INFO:  Epoch 109/500:  train Loss: 20.6221   val Loss: 23.7261   time: 358.50s   best: 23.7261
2023-11-12 01:03:07,430:INFO:  Epoch 323/500:  train Loss: 18.4219   val Loss: 23.4355   time: 323.48s   best: 23.2498
2023-11-12 01:07:16,085:INFO:  Epoch 110/500:  train Loss: 20.6288   val Loss: 24.6255   time: 358.85s   best: 23.7261
2023-11-12 01:08:29,682:INFO:  Epoch 324/500:  train Loss: 18.2447   val Loss: 23.5951   time: 322.24s   best: 23.2498
2023-11-12 01:13:14,623:INFO:  Epoch 111/500:  train Loss: 21.0710   val Loss: 23.9015   time: 358.51s   best: 23.7261
2023-11-12 01:13:53,857:INFO:  Epoch 325/500:  train Loss: 18.2937   val Loss: 26.1028   time: 324.16s   best: 23.2498
2023-11-12 01:19:12,733:INFO:  Epoch 112/500:  train Loss: 20.6094   val Loss: 24.1199   time: 358.08s   best: 23.7261
2023-11-12 01:19:16,280:INFO:  Epoch 326/500:  train Loss: 18.5980   val Loss: 24.9327   time: 322.39s   best: 23.2498
2023-11-12 01:24:38,864:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-12 01:24:38,885:INFO:  Epoch 327/500:  train Loss: 18.4010   val Loss: 23.2136   time: 322.52s   best: 23.2136
2023-11-12 01:25:09,056:INFO:  Epoch 113/500:  train Loss: 20.5176   val Loss: 23.9475   time: 356.31s   best: 23.7261
2023-11-12 01:30:03,251:INFO:  Epoch 328/500:  train Loss: 18.4490   val Loss: 24.2472   time: 324.35s   best: 23.2136
2023-11-12 01:31:03,383:INFO:  Epoch 114/500:  train Loss: 20.6199   val Loss: 23.7352   time: 354.32s   best: 23.7261
2023-11-12 01:35:24,915:INFO:  Epoch 329/500:  train Loss: 18.3285   val Loss: 23.3190   time: 321.64s   best: 23.2136
2023-11-12 01:37:37,482:INFO:  Epoch 115/500:  train Loss: 20.5468   val Loss: 23.8165   time: 394.07s   best: 23.7261
2023-11-12 01:40:49,218:INFO:  Epoch 330/500:  train Loss: 18.2602   val Loss: 23.5953   time: 324.29s   best: 23.2136
2023-11-12 01:43:57,447:INFO:  Epoch 116/500:  train Loss: 20.4454   val Loss: 23.7864   time: 379.94s   best: 23.7261
2023-11-12 01:46:14,532:INFO:  Epoch 331/500:  train Loss: 18.2278   val Loss: 23.8105   time: 325.28s   best: 23.2136
2023-11-12 01:49:55,879:INFO:  Epoch 117/500:  train Loss: 20.4823   val Loss: 24.2630   time: 358.42s   best: 23.7261
2023-11-12 01:51:35,792:INFO:  Epoch 332/500:  train Loss: 19.0542   val Loss: 32.2628   time: 321.24s   best: 23.2136
2023-11-12 01:55:52,409:INFO:  Epoch 118/500:  train Loss: 20.4187   val Loss: 24.0144   time: 356.50s   best: 23.7261
2023-11-12 01:56:57,657:INFO:  Epoch 333/500:  train Loss: 18.2557   val Loss: 23.8045   time: 321.86s   best: 23.2136
2023-11-12 02:01:51,818:INFO:  Epoch 119/500:  train Loss: 20.4477   val Loss: 23.8773   time: 359.39s   best: 23.7261
2023-11-12 02:02:19,488:INFO:  Epoch 334/500:  train Loss: 18.4948   val Loss: 23.6918   time: 321.79s   best: 23.2136
2023-11-12 02:07:43,574:INFO:  Epoch 335/500:  train Loss: 18.1532   val Loss: 23.6211   time: 324.06s   best: 23.2136
2023-11-12 02:07:49,693:INFO:  Epoch 120/500:  train Loss: 20.5313   val Loss: 24.0339   time: 357.84s   best: 23.7261
2023-11-12 02:13:08,347:INFO:  Epoch 336/500:  train Loss: 18.2417   val Loss: 24.0860   time: 324.77s   best: 23.2136
2023-11-12 02:13:47,309:INFO:  Epoch 121/500:  train Loss: 20.3202   val Loss: 23.8813   time: 357.59s   best: 23.7261
2023-11-12 02:18:30,029:INFO:  Epoch 337/500:  train Loss: 18.2538   val Loss: 24.0378   time: 321.67s   best: 23.2136
2023-11-12 02:19:42,002:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 02:19:42,052:INFO:  Epoch 122/500:  train Loss: 20.3954   val Loss: 23.5535   time: 354.67s   best: 23.5535
2023-11-12 02:23:56,240:INFO:  Epoch 338/500:  train Loss: 18.2161   val Loss: 23.7111   time: 326.20s   best: 23.2136
2023-11-12 02:25:37,048:INFO:  Epoch 123/500:  train Loss: 20.2749   val Loss: 24.1298   time: 354.98s   best: 23.5535
2023-11-12 02:29:20,639:INFO:  Epoch 339/500:  train Loss: 18.7585   val Loss: 24.3058   time: 324.35s   best: 23.2136
2023-11-12 02:31:34,851:INFO:  Epoch 124/500:  train Loss: 20.1726   val Loss: 24.2009   time: 357.78s   best: 23.5535
2023-11-12 02:34:43,300:INFO:  Epoch 340/500:  train Loss: 18.3045   val Loss: 24.2594   time: 322.64s   best: 23.2136
2023-11-12 02:37:29,279:INFO:  Epoch 125/500:  train Loss: 20.1975   val Loss: 23.7266   time: 354.39s   best: 23.5535
2023-11-12 02:40:07,950:INFO:  Epoch 341/500:  train Loss: 18.2611   val Loss: 23.5957   time: 324.63s   best: 23.2136
2023-11-12 02:43:23,202:INFO:  Epoch 126/500:  train Loss: 20.4363   val Loss: 24.5224   time: 353.89s   best: 23.5535
2023-11-12 02:45:33,763:INFO:  Epoch 342/500:  train Loss: 18.3328   val Loss: 23.5032   time: 325.80s   best: 23.2136
2023-11-12 02:50:16,990:INFO:  Epoch 127/500:  train Loss: 20.1740   val Loss: 24.5786   time: 413.76s   best: 23.5535
2023-11-12 02:50:59,303:INFO:  Epoch 343/500:  train Loss: 18.3221   val Loss: 23.6525   time: 325.52s   best: 23.2136
2023-11-12 02:56:22,391:INFO:  Epoch 344/500:  train Loss: 18.3463   val Loss: 23.4479   time: 323.08s   best: 23.2136
2023-11-12 02:57:32,669:INFO:  Epoch 128/500:  train Loss: 20.3967   val Loss: 23.8115   time: 435.66s   best: 23.5535
2023-11-12 03:01:46,372:INFO:  Epoch 345/500:  train Loss: 18.2579   val Loss: 23.8545   time: 323.94s   best: 23.2136
2023-11-12 03:03:28,901:INFO:  Epoch 129/500:  train Loss: 20.1945   val Loss: 23.5767   time: 356.21s   best: 23.5535
2023-11-12 03:07:07,856:INFO:  Epoch 346/500:  train Loss: 18.1670   val Loss: 23.9122   time: 321.45s   best: 23.2136
2023-11-12 03:09:23,791:INFO:  Epoch 130/500:  train Loss: 20.1331   val Loss: 23.7296   time: 354.87s   best: 23.5535
2023-11-12 03:12:29,314:INFO:  Epoch 347/500:  train Loss: 18.4082   val Loss: 23.4674   time: 321.44s   best: 23.2136
2023-11-12 03:15:18,660:INFO:  Epoch 131/500:  train Loss: 20.3332   val Loss: 24.3926   time: 354.85s   best: 23.5535
2023-11-12 03:17:52,666:INFO:  Epoch 348/500:  train Loss: 18.2454   val Loss: 23.5583   time: 323.30s   best: 23.2136
2023-11-12 03:21:14,155:INFO:  Epoch 132/500:  train Loss: 20.0813   val Loss: 23.9271   time: 355.47s   best: 23.5535
2023-11-12 03:23:14,686:INFO:  Epoch 349/500:  train Loss: 18.2843   val Loss: 23.6547   time: 322.00s   best: 23.2136
2023-11-12 03:27:12,264:INFO:  Epoch 133/500:  train Loss: 20.0004   val Loss: 24.3990   time: 358.09s   best: 23.5535
2023-11-12 03:28:39,161:INFO:  Epoch 350/500:  train Loss: 18.1813   val Loss: 23.7713   time: 324.43s   best: 23.2136
2023-11-12 03:33:19,447:INFO:  Epoch 134/500:  train Loss: 20.0534   val Loss: 25.4253   time: 367.16s   best: 23.5535
2023-11-12 03:34:01,894:INFO:  Epoch 351/500:  train Loss: 18.3416   val Loss: 23.8819   time: 322.72s   best: 23.2136
2023-11-12 03:39:18,709:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 03:39:18,741:INFO:  Epoch 135/500:  train Loss: 20.0093   val Loss: 23.4903   time: 359.23s   best: 23.4903
2023-11-12 03:39:25,170:INFO:  Epoch 352/500:  train Loss: 18.3346   val Loss: 23.6481   time: 323.27s   best: 23.2136
2023-11-12 03:44:48,505:INFO:  Epoch 353/500:  train Loss: 18.1379   val Loss: 23.4427   time: 323.27s   best: 23.2136
2023-11-12 03:45:14,571:INFO:  Epoch 136/500:  train Loss: 19.9863   val Loss: 24.2579   time: 355.83s   best: 23.4903
2023-11-12 03:50:10,663:INFO:  Epoch 354/500:  train Loss: 18.1740   val Loss: 23.9046   time: 322.14s   best: 23.2136
2023-11-12 03:51:09,459:INFO:  Epoch 137/500:  train Loss: 19.9066   val Loss: 23.5506   time: 354.86s   best: 23.4903
2023-11-12 03:55:31,709:INFO:  Epoch 355/500:  train Loss: 18.1380   val Loss: 23.7407   time: 321.03s   best: 23.2136
2023-11-12 03:57:07,407:INFO:  Epoch 138/500:  train Loss: 19.9318   val Loss: 25.5727   time: 357.91s   best: 23.4903
2023-11-12 04:00:52,702:INFO:  Epoch 356/500:  train Loss: 18.1550   val Loss: 23.4484   time: 320.95s   best: 23.2136
2023-11-12 04:03:05,270:INFO:  Epoch 139/500:  train Loss: 19.9351   val Loss: 24.1900   time: 357.84s   best: 23.4903
2023-11-12 04:06:14,444:INFO:  Epoch 357/500:  train Loss: 18.4312   val Loss: 23.8234   time: 321.71s   best: 23.2136
2023-11-12 04:09:01,996:INFO:  Epoch 140/500:  train Loss: 20.0615   val Loss: 24.2065   time: 356.71s   best: 23.4903
2023-11-12 04:11:39,195:INFO:  Epoch 358/500:  train Loss: 18.1362   val Loss: 23.2876   time: 324.72s   best: 23.2136
2023-11-12 04:14:58,732:INFO:  Epoch 141/500:  train Loss: 19.9493   val Loss: 23.5187   time: 356.73s   best: 23.4903
2023-11-12 04:17:01,312:INFO:  Epoch 359/500:  train Loss: 18.1040   val Loss: 23.4786   time: 322.11s   best: 23.2136
2023-11-12 04:20:56,713:INFO:  Epoch 142/500:  train Loss: 19.8899   val Loss: 23.7863   time: 357.94s   best: 23.4903
2023-11-12 04:22:26,144:INFO:  Epoch 360/500:  train Loss: 18.2054   val Loss: 23.8280   time: 324.82s   best: 23.2136
2023-11-12 04:26:54,694:INFO:  Epoch 143/500:  train Loss: 19.8626   val Loss: 24.3862   time: 357.94s   best: 23.4903
2023-11-12 04:27:49,250:INFO:  Epoch 361/500:  train Loss: 18.1497   val Loss: 23.5698   time: 323.10s   best: 23.2136
2023-11-12 04:32:52,782:INFO:  Epoch 144/500:  train Loss: 20.0337   val Loss: 23.7119   time: 358.06s   best: 23.4903
2023-11-12 04:33:10,769:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-12 04:33:10,787:INFO:  Epoch 362/500:  train Loss: 18.1400   val Loss: 23.0248   time: 321.49s   best: 23.0248
2023-11-12 04:38:32,303:INFO:  Epoch 363/500:  train Loss: 18.0365   val Loss: 23.3673   time: 321.50s   best: 23.0248
2023-11-12 04:38:47,185:INFO:  Epoch 145/500:  train Loss: 19.8405   val Loss: 24.0933   time: 354.37s   best: 23.4903
2023-11-12 04:43:55,351:INFO:  Epoch 364/500:  train Loss: 18.1768   val Loss: 23.3616   time: 323.02s   best: 23.0248
2023-11-12 04:44:42,455:INFO:  Epoch 146/500:  train Loss: 20.0929   val Loss: 24.2602   time: 355.23s   best: 23.4903
2023-11-12 04:49:19,657:INFO:  Epoch 365/500:  train Loss: 18.3010   val Loss: 23.4959   time: 324.27s   best: 23.0248
2023-11-12 04:50:38,990:INFO:  Epoch 147/500:  train Loss: 19.6925   val Loss: 24.0179   time: 356.52s   best: 23.4903
2023-11-12 04:54:41,800:INFO:  Epoch 366/500:  train Loss: 18.0509   val Loss: 23.3670   time: 322.12s   best: 23.0248
2023-11-12 04:56:38,036:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 04:56:38,061:INFO:  Epoch 148/500:  train Loss: 19.7666   val Loss: 23.4391   time: 359.02s   best: 23.4391
2023-11-12 05:00:05,924:INFO:  Epoch 367/500:  train Loss: 18.0692   val Loss: 23.7610   time: 324.10s   best: 23.0248
2023-11-12 05:02:35,078:INFO:  Epoch 149/500:  train Loss: 20.0997   val Loss: 23.4613   time: 357.01s   best: 23.4391
2023-11-12 05:05:27,985:INFO:  Epoch 368/500:  train Loss: 18.0754   val Loss: 23.0369   time: 322.03s   best: 23.0248
2023-11-12 05:08:32,445:INFO:  Epoch 150/500:  train Loss: 19.6711   val Loss: 24.6354   time: 357.34s   best: 23.4391
2023-11-12 05:10:49,430:INFO:  Epoch 369/500:  train Loss: 18.0373   val Loss: 24.5558   time: 321.41s   best: 23.0248
2023-11-12 05:14:29,469:INFO:  Epoch 151/500:  train Loss: 19.8205   val Loss: 24.0946   time: 357.01s   best: 23.4391
2023-11-12 05:16:12,642:INFO:  Epoch 370/500:  train Loss: 18.2586   val Loss: 23.6065   time: 323.18s   best: 23.0248
2023-11-12 05:20:24,304:INFO:  Epoch 152/500:  train Loss: 19.7611   val Loss: 23.8901   time: 354.81s   best: 23.4391
2023-11-12 05:21:34,236:INFO:  Epoch 371/500:  train Loss: 18.3692   val Loss: 23.8596   time: 321.56s   best: 23.0248
2023-11-12 05:26:20,045:INFO:  Epoch 153/500:  train Loss: 19.5684   val Loss: 24.3859   time: 355.71s   best: 23.4391
2023-11-12 05:26:59,707:INFO:  Epoch 372/500:  train Loss: 18.1461   val Loss: 23.2852   time: 325.44s   best: 23.0248
2023-11-12 05:32:18,885:INFO:  Epoch 154/500:  train Loss: 19.8253   val Loss: 23.6221   time: 358.81s   best: 23.4391
2023-11-12 05:32:21,167:INFO:  Epoch 373/500:  train Loss: 18.1216   val Loss: 23.8560   time: 321.43s   best: 23.0248
2023-11-12 05:37:46,050:INFO:  Epoch 374/500:  train Loss: 17.9610   val Loss: 23.3559   time: 324.86s   best: 23.0248
2023-11-12 05:38:16,282:INFO:  Epoch 155/500:  train Loss: 19.6025   val Loss: 23.9949   time: 357.35s   best: 23.4391
2023-11-12 05:43:09,864:INFO:  Epoch 375/500:  train Loss: 18.0254   val Loss: 23.3119   time: 323.80s   best: 23.0248
2023-11-12 05:44:15,531:INFO:  Epoch 156/500:  train Loss: 19.6515   val Loss: 23.4447   time: 359.22s   best: 23.4391
2023-11-12 05:48:33,454:INFO:  Epoch 376/500:  train Loss: 18.0267   val Loss: 23.2853   time: 323.58s   best: 23.0248
2023-11-12 05:50:10,153:INFO:  Epoch 157/500:  train Loss: 19.4885   val Loss: 23.5785   time: 354.60s   best: 23.4391
2023-11-12 05:53:54,944:INFO:  Epoch 377/500:  train Loss: 18.1697   val Loss: 23.9851   time: 321.47s   best: 23.0248
2023-11-12 05:56:08,262:INFO:  Epoch 158/500:  train Loss: 19.6168   val Loss: 23.8431   time: 358.09s   best: 23.4391
2023-11-12 05:59:18,628:INFO:  Epoch 378/500:  train Loss: 18.0417   val Loss: 23.2410   time: 323.67s   best: 23.0248
2023-11-12 06:02:03,361:INFO:  Epoch 159/500:  train Loss: 19.6660   val Loss: 23.7665   time: 355.07s   best: 23.4391
2023-11-12 06:04:40,404:INFO:  Epoch 379/500:  train Loss: 17.9405   val Loss: 26.1284   time: 321.75s   best: 23.0248
2023-11-12 06:07:58,346:INFO:  Epoch 160/500:  train Loss: 19.6312   val Loss: 26.1083   time: 354.96s   best: 23.4391
2023-11-12 06:10:04,318:INFO:  Epoch 380/500:  train Loss: 18.1189   val Loss: 23.5070   time: 323.88s   best: 23.0248
2023-11-12 06:13:53,668:INFO:  Epoch 161/500:  train Loss: 19.5614   val Loss: 23.7651   time: 355.30s   best: 23.4391
2023-11-12 06:15:26,283:INFO:  Epoch 381/500:  train Loss: 18.0117   val Loss: 23.1376   time: 321.94s   best: 23.0248
2023-11-12 06:19:52,323:INFO:  Epoch 162/500:  train Loss: 19.5578   val Loss: 24.8048   time: 358.63s   best: 23.4391
2023-11-12 06:20:51,918:INFO:  Epoch 382/500:  train Loss: 17.9812   val Loss: 25.3115   time: 325.60s   best: 23.0248
2023-11-12 06:25:47,223:INFO:  Epoch 163/500:  train Loss: 19.4866   val Loss: 23.5112   time: 354.87s   best: 23.4391
2023-11-12 06:26:13,665:INFO:  Epoch 383/500:  train Loss: 17.9914   val Loss: 23.4160   time: 321.74s   best: 23.0248
2023-11-12 06:31:35,514:INFO:  Epoch 384/500:  train Loss: 17.9943   val Loss: 23.1123   time: 321.82s   best: 23.0248
2023-11-12 06:31:42,309:INFO:  Epoch 164/500:  train Loss: 19.4129   val Loss: 23.8054   time: 355.06s   best: 23.4391
2023-11-12 06:36:57,715:INFO:  Epoch 385/500:  train Loss: 18.1595   val Loss: 23.5029   time: 322.17s   best: 23.0248
2023-11-12 06:37:37,017:INFO:  Epoch 165/500:  train Loss: 19.5259   val Loss: 23.9402   time: 354.68s   best: 23.4391
2023-11-12 06:42:21,502:INFO:  Epoch 386/500:  train Loss: 18.0474   val Loss: 23.3226   time: 323.78s   best: 23.0248
2023-11-12 06:43:31,181:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 06:43:31,229:INFO:  Epoch 166/500:  train Loss: 19.4083   val Loss: 23.3131   time: 354.14s   best: 23.3131
2023-11-12 06:47:42,517:INFO:  Epoch 387/500:  train Loss: 17.9787   val Loss: 23.5949   time: 321.00s   best: 23.0248
2023-11-12 06:49:28,162:INFO:  Epoch 167/500:  train Loss: 19.5408   val Loss: 23.6939   time: 356.93s   best: 23.3131
2023-11-12 06:53:04,071:INFO:  Epoch 388/500:  train Loss: 18.0475   val Loss: 23.7123   time: 321.54s   best: 23.0248
2023-11-12 06:55:24,595:INFO:  Epoch 168/500:  train Loss: 19.2957   val Loss: 23.6772   time: 356.41s   best: 23.3131
2023-11-12 06:58:27,747:INFO:  Epoch 389/500:  train Loss: 18.0691   val Loss: 29.7495   time: 323.66s   best: 23.0248
2023-11-12 07:01:20,341:INFO:  Epoch 169/500:  train Loss: 19.3928   val Loss: 32.5547   time: 355.72s   best: 23.3131
2023-11-12 07:03:52,787:INFO:  Epoch 390/500:  train Loss: 18.0918   val Loss: 23.6683   time: 325.02s   best: 23.0248
2023-11-12 07:07:18,391:INFO:  Epoch 170/500:  train Loss: 19.6263   val Loss: 23.8878   time: 358.02s   best: 23.3131
2023-11-12 07:09:15,454:INFO:  Epoch 391/500:  train Loss: 17.8640   val Loss: 24.3194   time: 322.66s   best: 23.0248
2023-11-12 07:13:14,253:INFO:  Epoch 171/500:  train Loss: 19.3405   val Loss: 23.7093   time: 355.83s   best: 23.3131
2023-11-12 07:14:36,645:INFO:  Epoch 392/500:  train Loss: 17.9738   val Loss: 24.7572   time: 321.18s   best: 23.0248
2023-11-12 07:19:09,021:INFO:  Epoch 172/500:  train Loss: 19.3055   val Loss: 25.5164   time: 354.74s   best: 23.3131
2023-11-12 07:19:57,861:INFO:  Epoch 393/500:  train Loss: 18.0209   val Loss: 23.4534   time: 321.19s   best: 23.0248
2023-11-12 07:25:04,734:INFO:  Epoch 173/500:  train Loss: 19.3844   val Loss: 24.5595   time: 355.69s   best: 23.3131
2023-11-12 07:25:21,999:INFO:  Epoch 394/500:  train Loss: 18.0288   val Loss: 23.3703   time: 324.11s   best: 23.0248
2023-11-12 07:30:43,460:INFO:  Epoch 395/500:  train Loss: 17.9403   val Loss: 23.7288   time: 321.45s   best: 23.0248
2023-11-12 07:31:03,024:INFO:  Epoch 174/500:  train Loss: 19.3650   val Loss: 23.8574   time: 358.27s   best: 23.3131
2023-11-12 07:36:05,742:INFO:  Epoch 396/500:  train Loss: 17.9516   val Loss: 23.0938   time: 322.26s   best: 23.0248
2023-11-12 07:36:58,476:INFO:  Epoch 175/500:  train Loss: 19.3783   val Loss: 24.6574   time: 355.43s   best: 23.3131
2023-11-12 07:41:27,606:INFO:  Epoch 397/500:  train Loss: 18.8195   val Loss: 23.6795   time: 321.85s   best: 23.0248
2023-11-12 07:42:53,739:INFO:  Epoch 176/500:  train Loss: 19.2439   val Loss: 23.8600   time: 355.19s   best: 23.3131
2023-11-12 07:46:51,990:INFO:  Epoch 398/500:  train Loss: 18.4659   val Loss: 23.8517   time: 324.37s   best: 23.0248
2023-11-12 07:48:48,672:INFO:  Epoch 177/500:  train Loss: 19.5665   val Loss: 23.7499   time: 354.91s   best: 23.3131
2023-11-12 07:52:14,537:INFO:  Epoch 399/500:  train Loss: 17.9572   val Loss: 23.5584   time: 322.53s   best: 23.0248
2023-11-12 07:54:43,650:INFO:  Epoch 178/500:  train Loss: 19.3760   val Loss: 23.6670   time: 354.97s   best: 23.3131
2023-11-12 07:57:37,257:INFO:  Epoch 400/500:  train Loss: 17.9273   val Loss: 23.4324   time: 322.71s   best: 23.0248
2023-11-12 08:00:38,173:INFO:  Epoch 179/500:  train Loss: 19.2338   val Loss: 23.7383   time: 354.51s   best: 23.3131
2023-11-12 08:03:01,028:INFO:  Epoch 401/500:  train Loss: 17.8809   val Loss: 23.4886   time: 323.74s   best: 23.0248
2023-11-12 08:07:26,529:INFO:  Epoch 180/500:  train Loss: 19.1526   val Loss: 25.8782   time: 408.28s   best: 23.3131
2023-11-12 08:08:22,714:INFO:  Epoch 402/500:  train Loss: 18.1409   val Loss: 23.4347   time: 321.66s   best: 23.0248
2023-11-12 08:13:43,731:INFO:  Epoch 403/500:  train Loss: 17.8490   val Loss: 23.0953   time: 321.00s   best: 23.0248
2023-11-12 08:13:48,476:INFO:  Epoch 181/500:  train Loss: 19.1957   val Loss: 26.9120   time: 381.94s   best: 23.3131
2023-11-12 08:19:05,201:INFO:  Epoch 404/500:  train Loss: 18.2519   val Loss: 24.7005   time: 321.45s   best: 23.0248
2023-11-12 08:19:46,741:INFO:  Epoch 182/500:  train Loss: 19.1466   val Loss: 23.7418   time: 358.24s   best: 23.3131
2023-11-12 08:24:26,534:INFO:  Epoch 405/500:  train Loss: 17.9367   val Loss: 23.8756   time: 321.30s   best: 23.0248
2023-11-12 08:25:43,783:INFO:  Epoch 183/500:  train Loss: 19.2073   val Loss: 23.6338   time: 357.03s   best: 23.3131
2023-11-12 08:29:48,430:INFO:  Epoch 406/500:  train Loss: 18.0002   val Loss: 23.7435   time: 321.87s   best: 23.0248
2023-11-12 08:31:39,120:INFO:  Epoch 184/500:  train Loss: 19.1708   val Loss: 23.6083   time: 355.30s   best: 23.3131
2023-11-12 08:35:11,844:INFO:  Epoch 407/500:  train Loss: 17.8849   val Loss: 25.1672   time: 323.39s   best: 23.0248
2023-11-12 08:37:36,727:INFO:  Epoch 185/500:  train Loss: 19.2147   val Loss: 23.6896   time: 357.58s   best: 23.3131
2023-11-12 08:40:36,914:INFO:  Epoch 408/500:  train Loss: 17.9486   val Loss: 23.2141   time: 325.06s   best: 23.0248
2023-11-12 08:43:34,367:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 08:43:34,448:INFO:  Epoch 186/500:  train Loss: 19.1126   val Loss: 23.2811   time: 357.61s   best: 23.2811
2023-11-12 08:45:59,665:INFO:  Epoch 409/500:  train Loss: 17.7911   val Loss: 23.5178   time: 322.73s   best: 23.0248
2023-11-12 08:49:32,388:INFO:  Epoch 187/500:  train Loss: 19.0610   val Loss: 24.1096   time: 357.94s   best: 23.2811
2023-11-12 08:51:25,298:INFO:  Epoch 410/500:  train Loss: 17.9203   val Loss: 24.2772   time: 325.60s   best: 23.0248
2023-11-12 08:55:31,254:INFO:  Epoch 188/500:  train Loss: 19.3022   val Loss: 23.5209   time: 358.84s   best: 23.2811
2023-11-12 08:56:46,541:INFO:  Epoch 411/500:  train Loss: 17.9251   val Loss: 23.1761   time: 321.21s   best: 23.0248
2023-11-12 09:01:26,259:INFO:  Epoch 189/500:  train Loss: 19.2288   val Loss: 32.4125   time: 354.98s   best: 23.2811
2023-11-12 09:02:09,748:INFO:  Epoch 412/500:  train Loss: 18.2866   val Loss: 24.2387   time: 323.18s   best: 23.0248
2023-11-12 09:07:20,759:INFO:  Epoch 190/500:  train Loss: 19.2032   val Loss: 23.5811   time: 354.47s   best: 23.2811
2023-11-12 09:07:32,515:INFO:  Epoch 413/500:  train Loss: 17.9988   val Loss: 23.4419   time: 322.75s   best: 23.0248
2023-11-12 09:12:54,863:INFO:  Epoch 414/500:  train Loss: 18.0567   val Loss: 23.9245   time: 322.33s   best: 23.0248
2023-11-12 09:13:25,572:INFO:  Epoch 191/500:  train Loss: 19.0485   val Loss: 23.7790   time: 364.79s   best: 23.2811
2023-11-12 09:18:15,775:INFO:  Epoch 415/500:  train Loss: 17.8807   val Loss: 23.7927   time: 320.90s   best: 23.0248
2023-11-12 09:19:20,058:INFO:  Epoch 192/500:  train Loss: 19.0019   val Loss: 23.6883   time: 354.45s   best: 23.2811
2023-11-12 09:23:41,190:INFO:  Epoch 416/500:  train Loss: 18.0289   val Loss: 23.9416   time: 325.39s   best: 23.0248
2023-11-12 09:25:17,727:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 09:25:17,754:INFO:  Epoch 193/500:  train Loss: 19.0557   val Loss: 23.0717   time: 357.63s   best: 23.0717
2023-11-12 09:29:03,109:INFO:  Epoch 417/500:  train Loss: 17.8007   val Loss: 23.7706   time: 321.90s   best: 23.0248
2023-11-12 09:31:57,269:INFO:  Epoch 194/500:  train Loss: 19.1491   val Loss: 23.9034   time: 399.50s   best: 23.0717
2023-11-12 09:34:25,191:INFO:  Epoch 418/500:  train Loss: 17.9129   val Loss: 24.2085   time: 322.06s   best: 23.0248
2023-11-12 09:37:54,143:INFO:  Epoch 195/500:  train Loss: 19.2246   val Loss: 23.3344   time: 356.84s   best: 23.0717
2023-11-12 09:39:49,548:INFO:  Epoch 419/500:  train Loss: 18.1127   val Loss: 23.5630   time: 324.32s   best: 23.0248
2023-11-12 09:43:49,610:INFO:  Epoch 196/500:  train Loss: 18.9230   val Loss: 23.7470   time: 355.44s   best: 23.0717
2023-11-12 09:45:14,895:INFO:  Epoch 420/500:  train Loss: 18.1548   val Loss: 24.4097   time: 325.31s   best: 23.0248
2023-11-12 09:49:46,743:INFO:  Epoch 197/500:  train Loss: 18.9881   val Loss: 23.4527   time: 357.12s   best: 23.0717
2023-11-12 09:50:36,649:INFO:  Epoch 421/500:  train Loss: 17.8340   val Loss: 25.2781   time: 321.75s   best: 23.0248
2023-11-12 09:55:42,553:INFO:  Epoch 198/500:  train Loss: 18.9924   val Loss: 23.7715   time: 355.80s   best: 23.0717
2023-11-12 09:55:58,979:INFO:  Epoch 422/500:  train Loss: 17.8186   val Loss: 23.6017   time: 322.30s   best: 23.0248
2023-11-12 10:01:24,412:INFO:  Epoch 423/500:  train Loss: 18.3047   val Loss: 23.4260   time: 325.41s   best: 23.0248
2023-11-12 10:01:36,423:INFO:  Epoch 199/500:  train Loss: 19.1073   val Loss: 23.3084   time: 353.84s   best: 23.0717
2023-11-12 10:06:48,515:INFO:  Epoch 424/500:  train Loss: 18.0335   val Loss: 23.2160   time: 324.09s   best: 23.0248
2023-11-12 10:07:34,604:INFO:  Epoch 200/500:  train Loss: 18.9264   val Loss: 23.8326   time: 358.16s   best: 23.0717
2023-11-12 10:12:09,668:INFO:  Epoch 425/500:  train Loss: 17.7790   val Loss: 23.2852   time: 321.12s   best: 23.0248
2023-11-12 10:13:30,566:INFO:  Epoch 201/500:  train Loss: 18.9301   val Loss: 25.7736   time: 355.93s   best: 23.0717
2023-11-12 10:17:31,687:INFO:  Epoch 426/500:  train Loss: 17.9219   val Loss: 23.2458   time: 321.99s   best: 23.0248
2023-11-12 10:19:27,914:INFO:  Epoch 202/500:  train Loss: 19.0641   val Loss: 23.7164   time: 357.32s   best: 23.0717
2023-11-12 10:22:55,893:INFO:  Epoch 427/500:  train Loss: 17.9854   val Loss: 23.1294   time: 324.16s   best: 23.0248
2023-11-12 10:25:22,980:INFO:  Epoch 203/500:  train Loss: 18.8743   val Loss: 24.1299   time: 355.05s   best: 23.0717
2023-11-12 10:28:17,505:INFO:  Epoch 428/500:  train Loss: 17.8099   val Loss: 24.4122   time: 321.58s   best: 23.0248
2023-11-12 10:31:17,957:INFO:  Epoch 204/500:  train Loss: 18.8089   val Loss: 23.3778   time: 354.95s   best: 23.0717
2023-11-12 10:33:41,320:INFO:  Epoch 429/500:  train Loss: 17.7434   val Loss: 23.5105   time: 323.80s   best: 23.0248
2023-11-12 10:37:15,714:INFO:  Epoch 205/500:  train Loss: 18.9498   val Loss: 23.6312   time: 357.73s   best: 23.0717
2023-11-12 10:39:03,146:INFO:  Epoch 430/500:  train Loss: 18.3132   val Loss: 24.6258   time: 321.79s   best: 23.0248
2023-11-12 10:43:10,178:INFO:  Epoch 206/500:  train Loss: 18.8751   val Loss: 23.9618   time: 354.43s   best: 23.0717
2023-11-12 10:44:24,212:INFO:  Epoch 431/500:  train Loss: 17.7414   val Loss: 23.4458   time: 321.04s   best: 23.0248
2023-11-12 10:49:05,782:INFO:  Epoch 207/500:  train Loss: 18.7550   val Loss: 23.5655   time: 355.58s   best: 23.0717
2023-11-12 10:49:48,389:INFO:  Epoch 432/500:  train Loss: 18.0236   val Loss: 23.5957   time: 324.17s   best: 23.0248
2023-11-12 10:55:04,014:INFO:  Epoch 208/500:  train Loss: 18.7434   val Loss: 24.2454   time: 358.22s   best: 23.0717
2023-11-12 10:55:12,291:INFO:  Epoch 433/500:  train Loss: 17.7947   val Loss: 23.7221   time: 323.88s   best: 23.0248
2023-11-12 11:00:34,748:INFO:  Epoch 434/500:  train Loss: 17.7866   val Loss: 26.5882   time: 322.42s   best: 23.0248
2023-11-12 11:01:00,645:INFO:  Epoch 209/500:  train Loss: 18.8196   val Loss: 29.3522   time: 356.61s   best: 23.0717
2023-11-12 11:05:55,701:INFO:  Epoch 435/500:  train Loss: 17.6995   val Loss: 23.5401   time: 320.93s   best: 23.0248
2023-11-12 11:09:22,825:INFO:  Epoch 210/500:  train Loss: 19.2099   val Loss: 23.6830   time: 502.16s   best: 23.0717
2023-11-12 11:11:17,547:INFO:  Epoch 436/500:  train Loss: 17.7820   val Loss: 24.2206   time: 321.81s   best: 23.0248
2023-11-12 11:15:15,862:INFO:  Epoch 211/500:  train Loss: 18.8290   val Loss: 23.4827   time: 353.01s   best: 23.0717
2023-11-12 11:16:39,250:INFO:  Epoch 437/500:  train Loss: 17.7594   val Loss: 23.7703   time: 321.69s   best: 23.0248
2023-11-12 11:21:12,558:INFO:  Epoch 212/500:  train Loss: 18.8514   val Loss: 23.6363   time: 356.67s   best: 23.0717
2023-11-12 11:22:02,814:INFO:  Epoch 438/500:  train Loss: 18.1132   val Loss: 23.6639   time: 323.53s   best: 23.0248
2023-11-12 11:27:07,468:INFO:  Epoch 213/500:  train Loss: 18.7225   val Loss: 23.2345   time: 354.90s   best: 23.0717
2023-11-12 11:27:24,939:INFO:  Epoch 439/500:  train Loss: 18.1252   val Loss: 23.4045   time: 322.10s   best: 23.0248
2023-11-12 11:32:49,311:INFO:  Epoch 440/500:  train Loss: 17.8852   val Loss: 23.3887   time: 324.35s   best: 23.0248
2023-11-12 11:33:16,090:INFO:  Epoch 214/500:  train Loss: 19.2801   val Loss: 24.0404   time: 368.61s   best: 23.0717
2023-11-12 11:38:10,960:INFO:  Epoch 441/500:  train Loss: 18.0563   val Loss: 23.1473   time: 321.64s   best: 23.0248
2023-11-12 11:39:11,928:INFO:  Epoch 215/500:  train Loss: 18.7620   val Loss: 24.0824   time: 355.81s   best: 23.0717
2023-11-12 11:43:36,473:INFO:  Epoch 442/500:  train Loss: 17.8707   val Loss: 24.6863   time: 325.48s   best: 23.0248
2023-11-12 11:45:07,872:INFO:  Epoch 216/500:  train Loss: 18.7400   val Loss: 23.6469   time: 355.92s   best: 23.0717
2023-11-12 11:49:02,067:INFO:  Epoch 443/500:  train Loss: 17.7857   val Loss: 23.3496   time: 325.58s   best: 23.0248
2023-11-12 11:51:06,682:INFO:  Epoch 217/500:  train Loss: 18.7195   val Loss: 23.5977   time: 358.79s   best: 23.0717
2023-11-12 11:54:24,356:INFO:  Epoch 444/500:  train Loss: 17.7319   val Loss: 23.1091   time: 322.27s   best: 23.0248
2023-11-12 11:57:02,715:INFO:  Epoch 218/500:  train Loss: 18.8752   val Loss: 23.2386   time: 356.00s   best: 23.0717
2023-11-12 11:59:46,581:INFO:  Epoch 445/500:  train Loss: 17.7657   val Loss: 23.8739   time: 322.21s   best: 23.0248
2023-11-12 12:02:58,749:INFO:  Epoch 219/500:  train Loss: 18.6873   val Loss: 23.8706   time: 356.01s   best: 23.0717
2023-11-12 12:05:11,645:INFO:  Epoch 446/500:  train Loss: 17.7145   val Loss: 24.0497   time: 325.05s   best: 23.0248
2023-11-12 12:08:53,573:INFO:  Epoch 220/500:  train Loss: 18.7850   val Loss: 23.3808   time: 354.80s   best: 23.0717
2023-11-12 12:10:35,688:INFO:  Epoch 447/500:  train Loss: 17.7355   val Loss: 23.9858   time: 324.02s   best: 23.0248
2023-11-12 12:14:48,136:INFO:  Epoch 221/500:  train Loss: 18.8548   val Loss: 23.5973   time: 354.56s   best: 23.0717
2023-11-12 12:16:00,051:INFO:  Epoch 448/500:  train Loss: 18.3017   val Loss: 23.9471   time: 324.35s   best: 23.0248
2023-11-12 12:20:43,164:INFO:  Epoch 222/500:  train Loss: 18.7170   val Loss: 23.3708   time: 355.00s   best: 23.0717
2023-11-12 12:21:24,140:INFO:  Epoch 449/500:  train Loss: 17.6980   val Loss: 23.3825   time: 324.05s   best: 23.0248
2023-11-12 12:26:37,480:INFO:  Epoch 223/500:  train Loss: 18.8147   val Loss: 23.7292   time: 354.29s   best: 23.0717
2023-11-12 12:26:45,345:INFO:  Epoch 450/500:  train Loss: 17.8620   val Loss: 23.7748   time: 321.19s   best: 23.0248
2023-11-12 12:32:07,616:INFO:  Epoch 451/500:  train Loss: 17.8094   val Loss: 23.5394   time: 322.16s   best: 23.0248
2023-11-12 12:32:31,006:INFO:  Epoch 224/500:  train Loss: 18.6688   val Loss: 24.4820   time: 353.52s   best: 23.0717
2023-11-12 12:37:28,274:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-12 12:37:28,312:INFO:  Epoch 452/500:  train Loss: 17.6731   val Loss: 22.9368   time: 320.64s   best: 22.9368
2023-11-12 12:38:27,389:INFO:  Epoch 225/500:  train Loss: 18.7557   val Loss: 24.1574   time: 356.35s   best: 23.0717
2023-11-12 12:42:49,479:INFO:  Epoch 453/500:  train Loss: 17.8358   val Loss: 23.5782   time: 321.16s   best: 22.9368
2023-11-12 12:44:22,001:INFO:  Epoch 226/500:  train Loss: 18.7163   val Loss: 23.5162   time: 354.61s   best: 23.0717
2023-11-12 12:48:12,157:INFO:  Epoch 454/500:  train Loss: 17.6901   val Loss: 23.8625   time: 322.64s   best: 22.9368
2023-11-12 12:50:17,957:INFO:  Epoch 227/500:  train Loss: 18.6608   val Loss: 24.0680   time: 355.94s   best: 23.0717
2023-11-12 12:53:35,240:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.7 dataset (0.1 dropout)_96a5.pt
2023-11-12 12:53:35,269:INFO:  Epoch 455/500:  train Loss: 17.7481   val Loss: 22.8664   time: 323.06s   best: 22.8664
2023-11-12 12:56:14,749:INFO:  Epoch 228/500:  train Loss: 18.6897   val Loss: 33.5931   time: 356.77s   best: 23.0717
2023-11-12 12:59:00,512:INFO:  Epoch 456/500:  train Loss: 17.6987   val Loss: 23.7511   time: 325.23s   best: 22.8664
2023-11-12 13:02:09,485:INFO:  Epoch 229/500:  train Loss: 19.4307   val Loss: 23.3045   time: 354.71s   best: 23.0717
2023-11-12 13:04:25,065:INFO:  Epoch 457/500:  train Loss: 17.8481   val Loss: 23.0379   time: 324.52s   best: 22.8664
2023-11-12 13:08:04,097:INFO:  Epoch 230/500:  train Loss: 18.6332   val Loss: 23.2947   time: 354.59s   best: 23.0717
2023-11-12 13:09:49,175:INFO:  Epoch 458/500:  train Loss: 17.6472   val Loss: 23.5197   time: 324.07s   best: 22.8664
2023-11-12 13:13:58,365:INFO:  Epoch 231/500:  train Loss: 18.6677   val Loss: 23.3688   time: 354.26s   best: 23.0717
2023-11-12 13:15:11,210:INFO:  Epoch 459/500:  train Loss: 17.7220   val Loss: 25.7304   time: 322.01s   best: 22.8664
2023-11-12 13:19:53,533:INFO:  Epoch 232/500:  train Loss: 18.6537   val Loss: 23.5128   time: 355.15s   best: 23.0717
2023-11-12 13:20:33,409:INFO:  Epoch 460/500:  train Loss: 18.1702   val Loss: 23.9198   time: 322.17s   best: 22.8664
2023-11-12 13:25:51,608:INFO:  Epoch 233/500:  train Loss: 18.5859   val Loss: 23.6312   time: 358.05s   best: 23.0717
2023-11-12 13:25:55,278:INFO:  Epoch 461/500:  train Loss: 17.6927   val Loss: 23.9811   time: 321.85s   best: 22.8664
2023-11-12 13:31:19,607:INFO:  Epoch 462/500:  train Loss: 18.1694   val Loss: 23.0484   time: 324.31s   best: 22.8664
2023-11-12 13:31:47,135:INFO:  Epoch 234/500:  train Loss: 18.8393   val Loss: 23.6163   time: 355.51s   best: 23.0717
2023-11-12 13:36:42,116:INFO:  Epoch 463/500:  train Loss: 17.6356   val Loss: 23.6458   time: 322.49s   best: 22.8664
2023-11-12 13:37:40,624:INFO:  Epoch 235/500:  train Loss: 18.6744   val Loss: 23.2549   time: 353.47s   best: 23.0717
2023-11-12 13:42:07,736:INFO:  Epoch 464/500:  train Loss: 18.0100   val Loss: 23.5191   time: 325.61s   best: 22.8664
2023-11-12 13:43:38,457:INFO:  Epoch 236/500:  train Loss: 18.7925   val Loss: 23.9814   time: 357.81s   best: 23.0717
2023-11-12 13:47:33,055:INFO:  Epoch 465/500:  train Loss: 17.7449   val Loss: 23.7571   time: 325.31s   best: 22.8664
2023-11-12 13:49:32,109:INFO:  Epoch 237/500:  train Loss: 18.7911   val Loss: 23.6600   time: 353.64s   best: 23.0717
2023-11-12 13:52:57,414:INFO:  Epoch 466/500:  train Loss: 17.9896   val Loss: 23.3866   time: 324.35s   best: 22.8664
2023-11-12 13:55:26,513:INFO:  Epoch 238/500:  train Loss: 18.6283   val Loss: 23.3375   time: 354.38s   best: 23.0717
2023-11-12 13:58:19,649:INFO:  Epoch 467/500:  train Loss: 17.8510   val Loss: 23.3557   time: 322.22s   best: 22.8664
2023-11-12 14:01:23,813:INFO:  Epoch 239/500:  train Loss: 18.5820   val Loss: 23.1192   time: 357.28s   best: 23.0717
2023-11-12 14:03:44,095:INFO:  Epoch 468/500:  train Loss: 17.8383   val Loss: 23.5380   time: 324.44s   best: 22.8664
2023-11-12 14:07:20,347:INFO:  Epoch 240/500:  train Loss: 18.5254   val Loss: 23.5612   time: 356.50s   best: 23.0717
2023-11-12 14:09:05,616:INFO:  Epoch 469/500:  train Loss: 17.6761   val Loss: 23.5867   time: 321.49s   best: 22.8664
2023-11-12 14:13:15,179:INFO:  Epoch 241/500:  train Loss: 18.5448   val Loss: 23.5657   time: 354.81s   best: 23.0717
2023-11-12 14:14:30,414:INFO:  Epoch 470/500:  train Loss: 17.7628   val Loss: 23.6044   time: 324.75s   best: 22.8664
2023-11-12 14:19:12,217:INFO:  Epoch 242/500:  train Loss: 18.4711   val Loss: 23.2423   time: 357.01s   best: 23.0717
2023-11-12 14:19:52,536:INFO:  Epoch 471/500:  train Loss: 17.8708   val Loss: 23.3793   time: 322.10s   best: 22.8664
2023-11-12 14:25:07,002:INFO:  Epoch 243/500:  train Loss: 18.6445   val Loss: 24.4870   time: 354.76s   best: 23.0717
2023-11-12 14:25:17,829:INFO:  Epoch 472/500:  train Loss: 17.6744   val Loss: 23.3440   time: 325.27s   best: 22.8664
2023-11-12 14:30:39,882:INFO:  Epoch 473/500:  train Loss: 17.6495   val Loss: 23.1951   time: 322.05s   best: 22.8664
2023-11-12 14:31:02,949:INFO:  Epoch 244/500:  train Loss: 18.6408   val Loss: 23.7635   time: 355.89s   best: 23.0717
2023-11-12 14:36:02,622:INFO:  Epoch 474/500:  train Loss: 18.1458   val Loss: 23.5605   time: 322.72s   best: 22.8664
2023-11-12 14:36:57,221:INFO:  Epoch 245/500:  train Loss: 18.7728   val Loss: 23.3071   time: 354.25s   best: 23.0717
2023-11-12 14:41:28,147:INFO:  Epoch 475/500:  train Loss: 17.8035   val Loss: 23.7623   time: 325.50s   best: 22.8664
2023-11-12 14:42:52,462:INFO:  Epoch 246/500:  train Loss: 18.7184   val Loss: 25.2878   time: 355.23s   best: 23.0717
2023-11-12 14:46:52,689:INFO:  Epoch 476/500:  train Loss: 17.7699   val Loss: 23.6009   time: 324.52s   best: 22.8664
2023-11-12 14:48:47,577:INFO:  Epoch 247/500:  train Loss: 18.5737   val Loss: 23.6513   time: 355.10s   best: 23.0717
2023-11-12 14:52:17,233:INFO:  Epoch 477/500:  train Loss: 17.6337   val Loss: 23.0129   time: 324.52s   best: 22.8664
2023-11-12 14:54:41,645:INFO:  Epoch 248/500:  train Loss: 18.4048   val Loss: 24.1336   time: 354.04s   best: 23.0717
2023-11-12 14:57:42,462:INFO:  Epoch 478/500:  train Loss: 17.6490   val Loss: 23.6717   time: 325.20s   best: 22.8664
2023-11-12 15:00:35,892:INFO:  Epoch 249/500:  train Loss: 18.4620   val Loss: 23.1988   time: 354.22s   best: 23.0717
2023-11-12 15:03:06,698:INFO:  Epoch 479/500:  train Loss: 17.8209   val Loss: 23.7711   time: 324.20s   best: 22.8664
2023-11-12 15:06:30,097:INFO:  Epoch 250/500:  train Loss: 18.5295   val Loss: 23.6027   time: 354.18s   best: 23.0717
2023-11-12 15:08:28,362:INFO:  Epoch 480/500:  train Loss: 17.5846   val Loss: 23.4395   time: 321.65s   best: 22.8664
2023-11-12 15:12:25,455:INFO:  Epoch 251/500:  train Loss: 18.4461   val Loss: 23.2510   time: 355.33s   best: 23.0717
2023-11-12 15:13:53,392:INFO:  Epoch 481/500:  train Loss: 17.9490   val Loss: 23.0085   time: 325.01s   best: 22.8664
2023-11-12 15:18:24,541:INFO:  Epoch 252/500:  train Loss: 18.4515   val Loss: 23.2598   time: 359.07s   best: 23.0717
2023-11-12 15:19:18,032:INFO:  Epoch 482/500:  train Loss: 17.6327   val Loss: 23.4954   time: 324.63s   best: 22.8664
2023-11-12 15:24:18,896:INFO:  Epoch 253/500:  train Loss: 18.3680   val Loss: 23.3515   time: 354.33s   best: 23.0717
2023-11-12 15:24:42,395:INFO:  Epoch 483/500:  train Loss: 17.7138   val Loss: 23.5581   time: 324.34s   best: 22.8664
2023-11-12 15:30:04,106:INFO:  Epoch 484/500:  train Loss: 17.6750   val Loss: 29.7611   time: 321.70s   best: 22.8664
2023-11-12 15:30:17,127:INFO:  Epoch 254/500:  train Loss: 18.4136   val Loss: 23.3827   time: 358.20s   best: 23.0717
2023-11-12 15:35:26,248:INFO:  Epoch 485/500:  train Loss: 17.8174   val Loss: 23.0824   time: 322.12s   best: 22.8664
2023-11-12 15:36:14,778:INFO:  Epoch 255/500:  train Loss: 19.3218   val Loss: 23.4656   time: 357.65s   best: 23.0717
2023-11-12 15:40:48,493:INFO:  Epoch 486/500:  train Loss: 17.5967   val Loss: 23.5945   time: 322.22s   best: 22.8664
2023-11-12 15:42:12,570:INFO:  Epoch 256/500:  train Loss: 18.4847   val Loss: 26.3201   time: 357.77s   best: 23.0717
2023-11-12 15:46:09,943:INFO:  Epoch 487/500:  train Loss: 17.6997   val Loss: 25.2055   time: 321.44s   best: 22.8664
2023-11-12 15:50:19,094:INFO:  Epoch 257/500:  train Loss: 18.4761   val Loss: 24.1604   time: 486.50s   best: 23.0717
2023-11-12 15:51:35,234:INFO:  Epoch 488/500:  train Loss: 17.7447   val Loss: 23.2800   time: 325.27s   best: 22.8664
2023-11-12 15:56:18,642:INFO:  Epoch 258/500:  train Loss: 18.3445   val Loss: 23.5054   time: 359.54s   best: 23.0717
2023-11-12 15:56:58,799:INFO:  Epoch 489/500:  train Loss: 17.6205   val Loss: 23.4808   time: 323.55s   best: 22.8664
2023-11-12 16:02:13,543:INFO:  Epoch 259/500:  train Loss: 18.3348   val Loss: 23.3715   time: 354.88s   best: 23.0717
2023-11-12 16:02:21,188:INFO:  Epoch 490/500:  train Loss: 17.5448   val Loss: 22.9558   time: 322.38s   best: 22.8664
2023-11-12 16:07:43,582:INFO:  Epoch 491/500:  train Loss: 17.9019   val Loss: 23.2139   time: 322.39s   best: 22.8664
2023-11-12 16:08:11,432:INFO:  Epoch 260/500:  train Loss: 18.3322   val Loss: 23.1567   time: 357.86s   best: 23.0717
2023-11-12 16:13:06,682:INFO:  Epoch 492/500:  train Loss: 17.5080   val Loss: 23.3421   time: 323.08s   best: 22.8664
2023-11-12 16:14:09,784:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 16:14:09,828:INFO:  Epoch 261/500:  train Loss: 18.4362   val Loss: 22.8933   time: 358.33s   best: 22.8933
2023-11-12 16:18:31,025:INFO:  Epoch 493/500:  train Loss: 17.6098   val Loss: 23.6410   time: 324.31s   best: 22.8664
2023-11-12 16:20:09,321:INFO:  Epoch 262/500:  train Loss: 18.3335   val Loss: 24.6337   time: 359.48s   best: 22.8933
2023-11-12 16:23:54,947:INFO:  Epoch 494/500:  train Loss: 17.9199   val Loss: 23.6750   time: 323.88s   best: 22.8664
2023-11-12 16:26:08,317:INFO:  Epoch 263/500:  train Loss: 18.3749   val Loss: 24.9995   time: 358.98s   best: 22.8933
2023-11-12 16:29:17,719:INFO:  Epoch 495/500:  train Loss: 17.6666   val Loss: 23.0287   time: 322.75s   best: 22.8664
2023-11-12 16:32:03,516:INFO:  Epoch 264/500:  train Loss: 18.3718   val Loss: 23.2393   time: 355.16s   best: 22.8933
2023-11-12 16:34:43,530:INFO:  Epoch 496/500:  train Loss: 18.2771   val Loss: 23.3657   time: 325.79s   best: 22.8664
2023-11-12 16:37:58,005:INFO:  Epoch 265/500:  train Loss: 18.5474   val Loss: 24.0301   time: 354.46s   best: 22.8933
2023-11-12 16:40:06,859:INFO:  Epoch 497/500:  train Loss: 17.7521   val Loss: 23.8580   time: 323.30s   best: 22.8664
2023-11-12 16:43:54,999:INFO:  Epoch 266/500:  train Loss: 18.3335   val Loss: 23.9854   time: 356.94s   best: 22.8933
2023-11-12 16:45:32,113:INFO:  Epoch 498/500:  train Loss: 17.6946   val Loss: 22.8986   time: 325.23s   best: 22.8664
2023-11-12 16:49:51,437:INFO:  Epoch 267/500:  train Loss: 18.2668   val Loss: 23.6993   time: 356.42s   best: 22.8933
2023-11-12 16:50:54,049:INFO:  Epoch 499/500:  train Loss: 18.0406   val Loss: 23.4872   time: 321.92s   best: 22.8664
2023-11-12 16:55:47,003:INFO:  Epoch 268/500:  train Loss: 18.4454   val Loss: 31.0625   time: 355.56s   best: 22.8933
2023-11-12 16:56:17,960:INFO:  Epoch 500/500:  train Loss: 17.8111   val Loss: 24.7825   time: 323.87s   best: 22.8664
2023-11-12 16:56:17,997:INFO:  -----> Training complete in 2692m 18s   best validation loss: 22.8664
 
2023-11-12 17:01:44,221:INFO:  Epoch 269/500:  train Loss: 18.5742   val Loss: 23.6254   time: 357.17s   best: 22.8933
2023-11-12 17:07:41,883:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 17:07:41,914:INFO:  Epoch 270/500:  train Loss: 18.3267   val Loss: 22.7587   time: 357.65s   best: 22.7587
2023-11-12 17:13:38,672:INFO:  Epoch 271/500:  train Loss: 18.3746   val Loss: 23.4303   time: 356.76s   best: 22.7587
2023-11-12 17:19:37,033:INFO:  Epoch 272/500:  train Loss: 18.1931   val Loss: 23.2463   time: 358.34s   best: 22.7587
2023-11-12 17:25:33,083:INFO:  Epoch 273/500:  train Loss: 18.7209   val Loss: 25.3637   time: 356.03s   best: 22.7587
2023-11-12 17:31:26,257:INFO:  Epoch 274/500:  train Loss: 18.2792   val Loss: 23.7885   time: 353.15s   best: 22.7587
2023-11-12 17:37:22,394:INFO:  Epoch 275/500:  train Loss: 18.6000   val Loss: 23.3106   time: 356.12s   best: 22.7587
2023-11-12 17:43:16,759:INFO:  Epoch 276/500:  train Loss: 18.3368   val Loss: 23.0166   time: 354.34s   best: 22.7587
2023-11-12 17:49:13,800:INFO:  Epoch 277/500:  train Loss: 18.4751   val Loss: 23.2593   time: 357.02s   best: 22.7587
2023-11-12 17:55:12,605:INFO:  Epoch 278/500:  train Loss: 18.4365   val Loss: 23.2769   time: 358.78s   best: 22.7587
2023-11-12 18:01:07,276:INFO:  Epoch 279/500:  train Loss: 18.2914   val Loss: 23.9299   time: 354.66s   best: 22.7587
2023-11-12 18:07:03,215:INFO:  Epoch 280/500:  train Loss: 18.2584   val Loss: 23.5631   time: 355.90s   best: 22.7587
2023-11-12 18:12:57,683:INFO:  Epoch 281/500:  train Loss: 18.2674   val Loss: 23.7321   time: 354.46s   best: 22.7587
2023-11-12 18:18:52,408:INFO:  Epoch 282/500:  train Loss: 18.2560   val Loss: 23.9264   time: 354.69s   best: 22.7587
2023-11-12 18:26:23,424:INFO:  Epoch 283/500:  train Loss: 18.2258   val Loss: 23.4288   time: 450.98s   best: 22.7587
2023-11-12 18:32:18,712:INFO:  Epoch 284/500:  train Loss: 18.2553   val Loss: 23.7851   time: 355.28s   best: 22.7587
2023-11-12 18:38:15,805:INFO:  Epoch 285/500:  train Loss: 18.1130   val Loss: 23.1444   time: 357.07s   best: 22.7587
2023-11-12 18:44:11,136:INFO:  Epoch 286/500:  train Loss: 18.2662   val Loss: 24.3432   time: 355.32s   best: 22.7587
2023-11-12 18:50:21,304:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-12 18:50:21,328:INFO:  Epoch 287/500:  train Loss: 18.2673   val Loss: 22.6931   time: 370.15s   best: 22.6931
2023-11-12 18:56:16,334:INFO:  Epoch 288/500:  train Loss: 18.3539   val Loss: 24.2578   time: 354.99s   best: 22.6931
2023-11-12 19:02:11,021:INFO:  Epoch 289/500:  train Loss: 18.1982   val Loss: 23.5133   time: 354.67s   best: 22.6931
2023-11-12 19:08:07,108:INFO:  Epoch 290/500:  train Loss: 18.1834   val Loss: 23.4246   time: 356.06s   best: 22.6931
2023-11-12 19:14:03,286:INFO:  Epoch 291/500:  train Loss: 18.3584   val Loss: 23.5883   time: 356.17s   best: 22.6931
2023-11-12 19:20:02,135:INFO:  Epoch 292/500:  train Loss: 18.1784   val Loss: 24.2020   time: 358.83s   best: 22.6931
2023-11-12 19:25:57,258:INFO:  Epoch 293/500:  train Loss: 18.1802   val Loss: 23.4055   time: 355.12s   best: 22.6931
2023-11-12 19:31:54,696:INFO:  Epoch 294/500:  train Loss: 18.0933   val Loss: 23.0093   time: 357.43s   best: 22.6931
2023-11-12 19:37:49,158:INFO:  Epoch 295/500:  train Loss: 18.4111   val Loss: 23.6109   time: 354.44s   best: 22.6931
2023-11-12 19:43:48,138:INFO:  Epoch 296/500:  train Loss: 18.2444   val Loss: 23.0916   time: 358.95s   best: 22.6931
2023-11-12 19:50:48,076:INFO:  Epoch 297/500:  train Loss: 18.0904   val Loss: 22.8847   time: 419.93s   best: 22.6931
2023-11-12 19:56:46,947:INFO:  Epoch 298/500:  train Loss: 18.1507   val Loss: 23.2619   time: 358.85s   best: 22.6931
2023-11-12 20:02:41,476:INFO:  Epoch 299/500:  train Loss: 18.1826   val Loss: 25.2498   time: 354.50s   best: 22.6931
2023-11-12 20:08:38,033:INFO:  Epoch 300/500:  train Loss: 18.1765   val Loss: 23.3176   time: 356.53s   best: 22.6931
2023-11-12 20:14:32,376:INFO:  Epoch 301/500:  train Loss: 18.0496   val Loss: 23.3904   time: 354.32s   best: 22.6931
2023-11-12 20:22:05,293:INFO:  Epoch 302/500:  train Loss: 18.4043   val Loss: 24.4382   time: 452.88s   best: 22.6931
2023-11-12 20:28:03,390:INFO:  Epoch 303/500:  train Loss: 18.0601   val Loss: 23.6927   time: 358.07s   best: 22.6931
2023-11-12 20:34:00,491:INFO:  Epoch 304/500:  train Loss: 18.3518   val Loss: 23.5234   time: 357.09s   best: 22.6931
2023-11-12 20:39:58,845:INFO:  Epoch 305/500:  train Loss: 18.2806   val Loss: 23.3137   time: 358.33s   best: 22.6931
2023-11-12 20:45:56,584:INFO:  Epoch 306/500:  train Loss: 18.0675   val Loss: 23.9651   time: 357.72s   best: 22.6931
2023-11-12 20:51:54,790:INFO:  Epoch 307/500:  train Loss: 18.1812   val Loss: 23.3442   time: 358.17s   best: 22.6931
2023-11-12 20:57:52,622:INFO:  Epoch 308/500:  train Loss: 17.9965   val Loss: 23.4681   time: 357.80s   best: 22.6931
2023-11-12 21:03:47,095:INFO:  Epoch 309/500:  train Loss: 18.1726   val Loss: 23.6796   time: 354.43s   best: 22.6931
2023-11-12 21:09:44,541:INFO:  Epoch 310/500:  train Loss: 18.0495   val Loss: 24.0506   time: 357.41s   best: 22.6931
2023-11-12 21:15:41,602:INFO:  Epoch 311/500:  train Loss: 18.2157   val Loss: 24.8968   time: 357.05s   best: 22.6931
2023-11-12 21:21:36,519:INFO:  Epoch 312/500:  train Loss: 18.0720   val Loss: 35.3605   time: 354.89s   best: 22.6931
2023-11-12 21:27:33,222:INFO:  Epoch 313/500:  train Loss: 18.5016   val Loss: 23.0805   time: 356.68s   best: 22.6931
2023-11-12 21:33:30,738:INFO:  Epoch 314/500:  train Loss: 18.0921   val Loss: 23.5725   time: 357.50s   best: 22.6931
2023-11-12 21:39:25,038:INFO:  Epoch 315/500:  train Loss: 18.0763   val Loss: 24.0569   time: 354.29s   best: 22.6931
2023-11-12 21:45:22,702:INFO:  Epoch 316/500:  train Loss: 18.1737   val Loss: 23.2541   time: 357.66s   best: 22.6931
2023-11-12 21:51:17,705:INFO:  Epoch 317/500:  train Loss: 17.9580   val Loss: 23.3128   time: 354.99s   best: 22.6931
2023-11-12 21:57:15,273:INFO:  Epoch 318/500:  train Loss: 18.1424   val Loss: 23.5280   time: 357.56s   best: 22.6931
2023-11-12 22:03:13,358:INFO:  Epoch 319/500:  train Loss: 17.9715   val Loss: 23.3906   time: 358.06s   best: 22.6931
2023-11-12 22:09:10,497:INFO:  Epoch 320/500:  train Loss: 18.0609   val Loss: 23.7424   time: 357.11s   best: 22.6931
2023-11-12 22:15:08,147:INFO:  Epoch 321/500:  train Loss: 18.1080   val Loss: 23.2778   time: 357.64s   best: 22.6931
2023-11-12 22:21:02,426:INFO:  Epoch 322/500:  train Loss: 17.9282   val Loss: 23.7933   time: 354.27s   best: 22.6931
2023-11-12 22:26:59,474:INFO:  Epoch 323/500:  train Loss: 18.5347   val Loss: 23.9741   time: 357.02s   best: 22.6931
2023-11-12 22:32:54,867:INFO:  Epoch 324/500:  train Loss: 18.0581   val Loss: 23.5558   time: 355.36s   best: 22.6931
2023-11-12 22:38:49,272:INFO:  Epoch 325/500:  train Loss: 17.8767   val Loss: 23.4843   time: 354.39s   best: 22.6931
2023-11-12 22:44:45,356:INFO:  Epoch 326/500:  train Loss: 18.1288   val Loss: 23.7274   time: 356.07s   best: 22.6931
2023-11-12 22:50:40,287:INFO:  Epoch 327/500:  train Loss: 18.0638   val Loss: 23.4465   time: 354.93s   best: 22.6931
2023-11-12 22:56:38,840:INFO:  Epoch 328/500:  train Loss: 18.4382   val Loss: 22.8340   time: 358.55s   best: 22.6931
2023-11-12 23:02:35,526:INFO:  Epoch 329/500:  train Loss: 17.8914   val Loss: 23.6966   time: 356.66s   best: 22.6931
2023-11-12 23:08:32,585:INFO:  Epoch 330/500:  train Loss: 18.0815   val Loss: 25.5964   time: 357.05s   best: 22.6931
2023-11-12 23:14:30,830:INFO:  Epoch 331/500:  train Loss: 18.0335   val Loss: 23.2785   time: 358.23s   best: 22.6931
2023-11-12 23:20:29,389:INFO:  Epoch 332/500:  train Loss: 17.9195   val Loss: 23.0253   time: 358.53s   best: 22.6931
2023-11-12 23:26:23,866:INFO:  Epoch 333/500:  train Loss: 17.9103   val Loss: 23.6161   time: 354.45s   best: 22.6931
2023-11-12 23:32:29,288:INFO:  Epoch 334/500:  train Loss: 17.8980   val Loss: 23.9107   time: 365.40s   best: 22.6931
2023-11-12 23:38:24,485:INFO:  Epoch 335/500:  train Loss: 17.9629   val Loss: 23.4176   time: 355.17s   best: 22.6931
2023-11-12 23:44:19,095:INFO:  Epoch 336/500:  train Loss: 18.0257   val Loss: 23.8252   time: 354.57s   best: 22.6931
2023-11-12 23:50:12,813:INFO:  Epoch 337/500:  train Loss: 17.8656   val Loss: 24.1439   time: 353.71s   best: 22.6931
2023-11-12 23:56:07,887:INFO:  Epoch 338/500:  train Loss: 17.8074   val Loss: 23.1784   time: 355.05s   best: 22.6931
2023-11-13 00:02:04,746:INFO:  Epoch 339/500:  train Loss: 18.0330   val Loss: 24.0368   time: 356.83s   best: 22.6931
2023-11-13 00:07:59,359:INFO:  Epoch 340/500:  train Loss: 17.9260   val Loss: 22.9618   time: 354.59s   best: 22.6931
2023-11-13 00:13:57,153:INFO:  Epoch 341/500:  train Loss: 18.0319   val Loss: 23.9032   time: 357.77s   best: 22.6931
2023-11-13 00:19:53,431:INFO:  Epoch 342/500:  train Loss: 17.8376   val Loss: 24.5163   time: 356.25s   best: 22.6931
2023-11-13 00:26:19,587:INFO:  Epoch 343/500:  train Loss: 17.7935   val Loss: 23.1846   time: 386.15s   best: 22.6931
2023-11-13 00:32:14,951:INFO:  Epoch 344/500:  train Loss: 17.8479   val Loss: 23.2739   time: 355.34s   best: 22.6931
2023-11-13 00:38:12,582:INFO:  Epoch 345/500:  train Loss: 18.0692   val Loss: 31.1969   time: 357.61s   best: 22.6931
2023-11-13 00:44:09,827:INFO:  Epoch 346/500:  train Loss: 18.0805   val Loss: 24.8346   time: 357.22s   best: 22.6931
2023-11-13 00:50:19,520:INFO:  Epoch 347/500:  train Loss: 18.0371   val Loss: 23.8494   time: 369.68s   best: 22.6931
2023-11-13 00:56:14,601:INFO:  Epoch 348/500:  train Loss: 18.1688   val Loss: 23.5022   time: 355.06s   best: 22.6931
2023-11-13 01:02:10,822:INFO:  Epoch 349/500:  train Loss: 17.8806   val Loss: 23.5067   time: 356.19s   best: 22.6931
2023-11-13 01:08:05,662:INFO:  Epoch 350/500:  train Loss: 17.7669   val Loss: 23.0001   time: 354.82s   best: 22.6931
2023-11-13 01:14:01,295:INFO:  Epoch 351/500:  train Loss: 18.1295   val Loss: 30.9192   time: 355.61s   best: 22.6931
2023-11-13 01:20:05,165:INFO:  Epoch 352/500:  train Loss: 17.8109   val Loss: 23.3863   time: 363.85s   best: 22.6931
2023-11-13 01:26:01,009:INFO:  Epoch 353/500:  train Loss: 18.0251   val Loss: 24.0870   time: 355.82s   best: 22.6931
2023-11-13 01:31:59,165:INFO:  Epoch 354/500:  train Loss: 17.8795   val Loss: 23.3347   time: 358.12s   best: 22.6931
2023-11-13 01:37:57,449:INFO:  Epoch 355/500:  train Loss: 17.9110   val Loss: 22.8112   time: 358.26s   best: 22.6931
2023-11-13 01:43:55,223:INFO:  Epoch 356/500:  train Loss: 17.9303   val Loss: 23.4209   time: 357.74s   best: 22.6931
2023-11-13 01:49:50,558:INFO:  Epoch 357/500:  train Loss: 17.7807   val Loss: 23.3047   time: 355.32s   best: 22.6931
2023-11-13 01:55:48,511:INFO:  Epoch 358/500:  train Loss: 17.7978   val Loss: 23.2153   time: 357.92s   best: 22.6931
2023-11-13 02:01:47,530:INFO:  Epoch 359/500:  train Loss: 17.7455   val Loss: 23.7704   time: 359.01s   best: 22.6931
2023-11-13 02:07:46,016:INFO:  Epoch 360/500:  train Loss: 17.7896   val Loss: 24.2207   time: 358.47s   best: 22.6931
2023-11-13 02:13:44,320:INFO:  Epoch 361/500:  train Loss: 17.7382   val Loss: 23.5965   time: 358.30s   best: 22.6931
2023-11-13 02:19:39,353:INFO:  Epoch 362/500:  train Loss: 17.7230   val Loss: 23.7749   time: 355.02s   best: 22.6931
2023-11-13 02:25:36,779:INFO:  Epoch 363/500:  train Loss: 17.8596   val Loss: 23.7367   time: 357.40s   best: 22.6931
2023-11-13 02:31:35,074:INFO:  Epoch 364/500:  train Loss: 17.8222   val Loss: 23.7551   time: 358.28s   best: 22.6931
2023-11-13 02:37:32,510:INFO:  Epoch 365/500:  train Loss: 17.8242   val Loss: 23.3400   time: 357.43s   best: 22.6931
2023-11-13 02:43:27,079:INFO:  Epoch 366/500:  train Loss: 17.7369   val Loss: 23.2241   time: 354.55s   best: 22.6931
2023-11-13 02:49:22,187:INFO:  Epoch 367/500:  train Loss: 17.8904   val Loss: 24.3429   time: 355.08s   best: 22.6931
2023-11-13 02:55:17,716:INFO:  Epoch 368/500:  train Loss: 17.7613   val Loss: 23.3159   time: 355.51s   best: 22.6931
2023-11-13 03:01:15,796:INFO:  Epoch 369/500:  train Loss: 17.8698   val Loss: 23.9703   time: 358.06s   best: 22.6931
2023-11-13 03:07:11,779:INFO:  Epoch 370/500:  train Loss: 17.7658   val Loss: 25.7211   time: 355.97s   best: 22.6931
2023-11-13 03:13:08,859:INFO:  Epoch 371/500:  train Loss: 17.6338   val Loss: 23.9812   time: 357.06s   best: 22.6931
2023-11-13 03:19:04,564:INFO:  Epoch 372/500:  train Loss: 17.7238   val Loss: 22.8524   time: 355.67s   best: 22.6931
2023-11-13 03:25:00,534:INFO:  Epoch 373/500:  train Loss: 17.8164   val Loss: 24.8186   time: 355.96s   best: 22.6931
2023-11-13 03:30:56,005:INFO:  Epoch 374/500:  train Loss: 17.7328   val Loss: 23.6654   time: 355.44s   best: 22.6931
2023-11-13 03:36:53,276:INFO:  Epoch 375/500:  train Loss: 17.6902   val Loss: 24.0482   time: 357.25s   best: 22.6931
2023-11-13 03:42:52,025:INFO:  Epoch 376/500:  train Loss: 17.6141   val Loss: 23.6944   time: 358.73s   best: 22.6931
2023-11-13 03:48:50,377:INFO:  Epoch 377/500:  train Loss: 17.7280   val Loss: 23.3028   time: 358.33s   best: 22.6931
2023-11-13 03:54:47,078:INFO:  Epoch 378/500:  train Loss: 17.8409   val Loss: 23.3245   time: 356.68s   best: 22.6931
2023-11-13 04:00:45,682:INFO:  Epoch 379/500:  train Loss: 17.8359   val Loss: 23.6249   time: 358.58s   best: 22.6931
2023-11-13 04:06:40,618:INFO:  Epoch 380/500:  train Loss: 17.7316   val Loss: 23.4581   time: 354.91s   best: 22.6931
2023-11-13 04:12:35,879:INFO:  Epoch 381/500:  train Loss: 17.7809   val Loss: 23.3306   time: 355.24s   best: 22.6931
2023-11-13 04:18:30,903:INFO:  Epoch 382/500:  train Loss: 17.6459   val Loss: 24.2296   time: 355.01s   best: 22.6931
2023-11-13 04:24:28,849:INFO:  Epoch 383/500:  train Loss: 17.7376   val Loss: 24.1168   time: 357.92s   best: 22.6931
2023-11-13 04:30:26,735:INFO:  Epoch 384/500:  train Loss: 17.6693   val Loss: 23.5538   time: 357.88s   best: 22.6931
2023-11-13 04:36:23,558:INFO:  Epoch 385/500:  train Loss: 17.7529   val Loss: 23.0583   time: 356.81s   best: 22.6931
2023-11-13 04:42:22,182:INFO:  Epoch 386/500:  train Loss: 17.6940   val Loss: 23.2024   time: 358.61s   best: 22.6931
2023-11-13 04:48:17,592:INFO:  Epoch 387/500:  train Loss: 17.6998   val Loss: 23.0260   time: 355.39s   best: 22.6931
2023-11-13 04:54:15,803:INFO:  Epoch 388/500:  train Loss: 17.6638   val Loss: 23.6287   time: 358.20s   best: 22.6931
2023-11-13 05:00:13,870:INFO:  Epoch 389/500:  train Loss: 17.6016   val Loss: 23.3588   time: 358.05s   best: 22.6931
2023-11-13 05:06:12,394:INFO:  Epoch 390/500:  train Loss: 17.7944   val Loss: 23.4155   time: 358.50s   best: 22.6931
2023-11-13 05:12:10,171:INFO:  Epoch 391/500:  train Loss: 17.6656   val Loss: 22.8082   time: 357.74s   best: 22.6931
2023-11-13 05:18:07,189:INFO:  Epoch 392/500:  train Loss: 17.9274   val Loss: 23.5525   time: 357.01s   best: 22.6931
2023-11-13 05:24:05,319:INFO:  Epoch 393/500:  train Loss: 17.8979   val Loss: 23.1470   time: 358.10s   best: 22.6931
2023-11-13 05:30:00,443:INFO:  Epoch 394/500:  train Loss: 18.2897   val Loss: 23.1640   time: 355.10s   best: 22.6931
2023-11-13 05:35:55,544:INFO:  Epoch 395/500:  train Loss: 17.8254   val Loss: 23.7193   time: 355.08s   best: 22.6931
2023-11-13 05:41:49,935:INFO:  Epoch 396/500:  train Loss: 18.0273   val Loss: 23.7390   time: 354.38s   best: 22.6931
2023-11-13 05:47:48,450:INFO:  Epoch 397/500:  train Loss: 17.6590   val Loss: 23.5302   time: 358.49s   best: 22.6931
2023-11-13 05:53:46,145:INFO:  Epoch 398/500:  train Loss: 17.5713   val Loss: 23.2064   time: 357.66s   best: 22.6931
2023-11-13 05:59:41,264:INFO:  Epoch 399/500:  train Loss: 17.6825   val Loss: 23.7727   time: 355.10s   best: 22.6931
2023-11-13 06:05:36,140:INFO:  Epoch 400/500:  train Loss: 17.6116   val Loss: 23.0748   time: 354.87s   best: 22.6931
2023-11-13 06:11:30,103:INFO:  Epoch 401/500:  train Loss: 17.6193   val Loss: 23.0992   time: 353.94s   best: 22.6931
2023-11-13 06:17:27,794:INFO:  Epoch 402/500:  train Loss: 17.6782   val Loss: 23.4757   time: 357.67s   best: 22.6931
2023-11-13 06:23:24,967:INFO:  Epoch 403/500:  train Loss: 17.6346   val Loss: 24.5511   time: 357.15s   best: 22.6931
2023-11-13 06:29:20,605:INFO:  Epoch 404/500:  train Loss: 18.1211   val Loss: 23.5736   time: 355.63s   best: 22.6931
2023-11-13 06:35:17,709:INFO:  Epoch 405/500:  train Loss: 17.6257   val Loss: 23.5390   time: 357.09s   best: 22.6931
2023-11-13 06:41:14,566:INFO:  Epoch 406/500:  train Loss: 17.5550   val Loss: 23.8272   time: 356.85s   best: 22.6931
2023-11-13 06:47:11,066:INFO:  Epoch 407/500:  train Loss: 17.4919   val Loss: 23.1417   time: 356.48s   best: 22.6931
2023-11-13 06:53:06,567:INFO:  Epoch 408/500:  train Loss: 17.7165   val Loss: 25.1656   time: 355.46s   best: 22.6931
2023-11-13 06:59:00,717:INFO:  Epoch 409/500:  train Loss: 17.7688   val Loss: 23.8036   time: 354.12s   best: 22.6931
2023-11-13 07:04:56,223:INFO:  Epoch 410/500:  train Loss: 17.6370   val Loss: 23.1469   time: 355.49s   best: 22.6931
2023-11-13 07:10:54,097:INFO:  Epoch 411/500:  train Loss: 17.8970   val Loss: 23.6505   time: 357.85s   best: 22.6931
2023-11-13 07:16:49,078:INFO:  Epoch 412/500:  train Loss: 17.6729   val Loss: 23.5991   time: 354.97s   best: 22.6931
2023-11-13 07:22:43,283:INFO:  Epoch 413/500:  train Loss: 17.6540   val Loss: 23.2181   time: 354.20s   best: 22.6931
2023-11-13 07:28:38,493:INFO:  Epoch 414/500:  train Loss: 17.4709   val Loss: 22.9559   time: 355.20s   best: 22.6931
2023-11-13 07:34:33,586:INFO:  Epoch 415/500:  train Loss: 17.6446   val Loss: 22.8493   time: 355.07s   best: 22.6931
2023-11-13 07:40:28,432:INFO:  Epoch 416/500:  train Loss: 17.5031   val Loss: 23.6012   time: 354.81s   best: 22.6931
2023-11-13 07:46:23,960:INFO:  Epoch 417/500:  train Loss: 17.5338   val Loss: 23.3701   time: 355.50s   best: 22.6931
2023-11-13 07:52:18,939:INFO:  Epoch 418/500:  train Loss: 17.8381   val Loss: 22.8943   time: 354.97s   best: 22.6931
2023-11-13 07:58:14,513:INFO:  Epoch 419/500:  train Loss: 17.5423   val Loss: 22.8395   time: 355.55s   best: 22.6931
2023-11-13 08:04:09,002:INFO:  Epoch 420/500:  train Loss: 17.5310   val Loss: 23.6175   time: 354.47s   best: 22.6931
2023-11-13 08:10:04,747:INFO:  Epoch 421/500:  train Loss: 17.4897   val Loss: 25.1045   time: 355.71s   best: 22.6931
2023-11-13 08:16:00,770:INFO:  Epoch 422/500:  train Loss: 17.6720   val Loss: 24.1473   time: 356.00s   best: 22.6931
2023-11-13 08:21:57,909:INFO:  Epoch 423/500:  train Loss: 17.5792   val Loss: 23.7874   time: 357.11s   best: 22.6931
2023-11-13 08:27:55,088:INFO:  Epoch 424/500:  train Loss: 17.6127   val Loss: 24.6945   time: 357.16s   best: 22.6931
2023-11-13 08:33:51,019:INFO:  Epoch 425/500:  train Loss: 17.6418   val Loss: 23.5443   time: 355.93s   best: 22.6931
2023-11-13 08:39:47,301:INFO:  Epoch 426/500:  train Loss: 17.6236   val Loss: 23.1648   time: 356.24s   best: 22.6931
2023-11-13 08:45:43,058:INFO:  Epoch 427/500:  train Loss: 17.5094   val Loss: 23.1725   time: 355.72s   best: 22.6931
2023-11-13 08:51:38,870:INFO:  Epoch 428/500:  train Loss: 17.5223   val Loss: 23.2817   time: 355.77s   best: 22.6931
2023-11-13 08:57:36,939:INFO:  Epoch 429/500:  train Loss: 17.4453   val Loss: 28.9995   time: 358.06s   best: 22.6931
2023-11-13 09:03:35,180:INFO:  Epoch 430/500:  train Loss: 17.7032   val Loss: 22.9831   time: 358.23s   best: 22.6931
2023-11-13 09:09:29,817:INFO:  Epoch 431/500:  train Loss: 17.4700   val Loss: 24.0188   time: 354.61s   best: 22.6931
2023-11-13 09:15:24,656:INFO:  Epoch 432/500:  train Loss: 17.4349   val Loss: 23.0244   time: 354.80s   best: 22.6931
2023-11-13 09:21:21,914:INFO:  Epoch 433/500:  train Loss: 17.9838   val Loss: 23.1375   time: 357.24s   best: 22.6931
2023-11-13 09:27:18,577:INFO:  Epoch 434/500:  train Loss: 17.6829   val Loss: 23.2255   time: 356.65s   best: 22.6931
2023-11-13 09:33:13,599:INFO:  Epoch 435/500:  train Loss: 17.7842   val Loss: 23.3341   time: 355.00s   best: 22.6931
2023-11-13 09:39:10,558:INFO:  Epoch 436/500:  train Loss: 17.6065   val Loss: 23.3318   time: 356.94s   best: 22.6931
2023-11-13 09:45:04,984:INFO:  Epoch 437/500:  train Loss: 17.4290   val Loss: 23.2523   time: 354.42s   best: 22.6931
2023-11-13 09:51:02,973:INFO:  Epoch 438/500:  train Loss: 17.5479   val Loss: 23.0095   time: 357.96s   best: 22.6931
2023-11-13 09:57:01,671:INFO:  Epoch 439/500:  train Loss: 17.5556   val Loss: 23.6864   time: 358.69s   best: 22.6931
2023-11-13 10:02:57,606:INFO:  Epoch 440/500:  train Loss: 17.4739   val Loss: 23.3341   time: 355.91s   best: 22.6931
2023-11-13 10:08:53,534:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder with 0.8 dataset (0.1 dropout)_65b9.pt
2023-11-13 10:08:53,565:INFO:  Epoch 441/500:  train Loss: 17.5649   val Loss: 22.3128   time: 355.90s   best: 22.3128
2023-11-13 10:14:52,952:INFO:  Epoch 442/500:  train Loss: 17.5931   val Loss: 23.3568   time: 359.36s   best: 22.3128
2023-11-13 10:20:51,625:INFO:  Epoch 443/500:  train Loss: 17.4387   val Loss: 23.5228   time: 358.67s   best: 22.3128
2023-11-13 10:26:48,057:INFO:  Epoch 444/500:  train Loss: 17.9584   val Loss: 23.0745   time: 356.41s   best: 22.3128
2023-11-13 10:32:42,715:INFO:  Epoch 445/500:  train Loss: 17.3726   val Loss: 23.2840   time: 354.65s   best: 22.3128
2023-11-13 10:38:37,833:INFO:  Epoch 446/500:  train Loss: 17.9033   val Loss: 23.1088   time: 355.09s   best: 22.3128
2023-11-13 10:44:35,142:INFO:  Epoch 447/500:  train Loss: 17.3950   val Loss: 24.2735   time: 357.30s   best: 22.3128
2023-11-13 10:50:33,039:INFO:  Epoch 448/500:  train Loss: 17.5538   val Loss: 23.8820   time: 357.87s   best: 22.3128
2023-11-13 10:56:29,392:INFO:  Epoch 449/500:  train Loss: 17.4785   val Loss: 23.5315   time: 356.32s   best: 22.3128
2023-11-13 11:02:26,454:INFO:  Epoch 450/500:  train Loss: 17.4407   val Loss: 23.2919   time: 357.02s   best: 22.3128
2023-11-13 11:08:21,256:INFO:  Epoch 451/500:  train Loss: 17.3375   val Loss: 23.6950   time: 354.79s   best: 22.3128
2023-11-13 11:14:19,670:INFO:  Epoch 452/500:  train Loss: 17.4858   val Loss: 23.3708   time: 358.39s   best: 22.3128
2023-11-13 11:20:14,014:INFO:  Epoch 453/500:  train Loss: 17.3461   val Loss: 23.4747   time: 354.32s   best: 22.3128
2023-11-13 11:26:12,487:INFO:  Epoch 454/500:  train Loss: 17.4192   val Loss: 23.0952   time: 358.45s   best: 22.3128
2023-11-13 11:32:10,695:INFO:  Epoch 455/500:  train Loss: 17.4001   val Loss: 23.4345   time: 358.08s   best: 22.3128
2023-11-13 11:38:09,282:INFO:  Epoch 456/500:  train Loss: 17.3886   val Loss: 23.7130   time: 358.57s   best: 22.3128
2023-11-13 11:44:08,116:INFO:  Epoch 457/500:  train Loss: 17.4113   val Loss: 22.8246   time: 358.81s   best: 22.3128
2023-11-13 11:50:03,089:INFO:  Epoch 458/500:  train Loss: 17.3513   val Loss: 23.3053   time: 354.95s   best: 22.3128
2023-11-13 11:55:58,145:INFO:  Epoch 459/500:  train Loss: 17.4130   val Loss: 23.8934   time: 355.05s   best: 22.3128
2023-11-13 12:01:56,063:INFO:  Epoch 460/500:  train Loss: 17.4241   val Loss: 25.4436   time: 357.89s   best: 22.3128
2023-11-13 12:07:50,891:INFO:  Epoch 461/500:  train Loss: 17.4831   val Loss: 24.2072   time: 354.82s   best: 22.3128
2023-11-13 12:13:48,514:INFO:  Epoch 462/500:  train Loss: 17.3164   val Loss: 23.6672   time: 357.62s   best: 22.3128
2023-11-13 12:19:46,465:INFO:  Epoch 463/500:  train Loss: 17.3223   val Loss: 23.5618   time: 357.93s   best: 22.3128
2023-11-13 12:25:42,471:INFO:  Epoch 464/500:  train Loss: 17.3850   val Loss: 24.0965   time: 355.99s   best: 22.3128
2023-11-13 12:31:40,526:INFO:  Epoch 465/500:  train Loss: 17.7184   val Loss: 22.5901   time: 358.04s   best: 22.3128
2023-11-13 12:37:38,120:INFO:  Epoch 466/500:  train Loss: 17.2927   val Loss: 23.5619   time: 357.57s   best: 22.3128
2023-11-13 12:43:35,136:INFO:  Epoch 467/500:  train Loss: 17.4374   val Loss: 23.1122   time: 356.98s   best: 22.3128
2023-11-13 12:49:31,633:INFO:  Epoch 468/500:  train Loss: 17.2503   val Loss: 23.1419   time: 356.49s   best: 22.3128
2023-11-13 12:55:29,485:INFO:  Epoch 469/500:  train Loss: 17.3861   val Loss: 23.5487   time: 357.84s   best: 22.3128
2023-11-13 13:01:26,272:INFO:  Epoch 470/500:  train Loss: 17.3526   val Loss: 23.0117   time: 356.75s   best: 22.3128
2023-11-13 13:07:23,011:INFO:  Epoch 471/500:  train Loss: 17.4320   val Loss: 22.7187   time: 356.72s   best: 22.3128
2023-11-13 13:13:19,092:INFO:  Epoch 472/500:  train Loss: 17.3683   val Loss: 23.1054   time: 356.05s   best: 22.3128
2023-11-13 13:19:16,955:INFO:  Epoch 473/500:  train Loss: 17.2680   val Loss: 23.6327   time: 357.85s   best: 22.3128
2023-11-13 13:25:12,971:INFO:  Epoch 474/500:  train Loss: 17.4806   val Loss: 24.0467   time: 355.99s   best: 22.3128
2023-11-13 13:31:08,222:INFO:  Epoch 475/500:  train Loss: 17.3225   val Loss: 23.9329   time: 355.23s   best: 22.3128
2023-11-13 13:37:03,451:INFO:  Epoch 476/500:  train Loss: 17.4112   val Loss: 23.2099   time: 355.20s   best: 22.3128
2023-11-13 13:42:59,542:INFO:  Epoch 477/500:  train Loss: 17.2222   val Loss: 23.6202   time: 356.07s   best: 22.3128
2023-11-13 13:48:55,364:INFO:  Epoch 478/500:  train Loss: 17.8659   val Loss: 23.1643   time: 355.79s   best: 22.3128
2023-11-13 13:54:53,321:INFO:  Epoch 479/500:  train Loss: 17.3638   val Loss: 23.6034   time: 357.94s   best: 22.3128
2023-11-13 14:00:48,745:INFO:  Epoch 480/500:  train Loss: 17.3485   val Loss: 23.5235   time: 355.40s   best: 22.3128
2023-11-13 14:06:45,830:INFO:  Epoch 481/500:  train Loss: 17.6202   val Loss: 23.2607   time: 357.06s   best: 22.3128
2023-11-13 14:12:42,864:INFO:  Epoch 482/500:  train Loss: 17.4587   val Loss: 23.4491   time: 357.00s   best: 22.3128
2023-11-13 14:18:41,386:INFO:  Epoch 483/500:  train Loss: 17.4233   val Loss: 23.7612   time: 358.49s   best: 22.3128
2023-11-13 14:24:37,626:INFO:  Epoch 484/500:  train Loss: 17.3722   val Loss: 22.5176   time: 356.15s   best: 22.3128
2023-11-13 14:30:32,524:INFO:  Epoch 485/500:  train Loss: 17.3966   val Loss: 22.9999   time: 354.87s   best: 22.3128
2023-11-13 14:36:28,212:INFO:  Epoch 486/500:  train Loss: 17.3289   val Loss: 23.3758   time: 355.68s   best: 22.3128
2023-11-13 14:42:26,381:INFO:  Epoch 487/500:  train Loss: 17.9702   val Loss: 23.1705   time: 358.14s   best: 22.3128
2023-11-13 14:48:23,970:INFO:  Epoch 488/500:  train Loss: 17.3759   val Loss: 23.1965   time: 357.58s   best: 22.3128
2023-11-13 14:54:22,541:INFO:  Epoch 489/500:  train Loss: 17.2499   val Loss: 23.1402   time: 358.56s   best: 22.3128
2023-11-13 15:00:20,735:INFO:  Epoch 490/500:  train Loss: 17.2794   val Loss: 23.5817   time: 358.17s   best: 22.3128
2023-11-13 15:06:15,729:INFO:  Epoch 491/500:  train Loss: 17.2493   val Loss: 22.7402   time: 354.97s   best: 22.3128
2023-11-13 15:12:11,302:INFO:  Epoch 492/500:  train Loss: 17.4632   val Loss: 22.8268   time: 355.55s   best: 22.3128
2023-11-13 15:18:06,256:INFO:  Epoch 493/500:  train Loss: 17.3799   val Loss: 23.9488   time: 354.93s   best: 22.3128
2023-11-13 15:24:01,269:INFO:  Epoch 494/500:  train Loss: 17.2352   val Loss: 23.9580   time: 354.99s   best: 22.3128
2023-11-13 15:29:59,400:INFO:  Epoch 495/500:  train Loss: 17.2188   val Loss: 22.9618   time: 358.11s   best: 22.3128
2023-11-13 15:35:56,700:INFO:  Epoch 496/500:  train Loss: 17.2655   val Loss: 23.2182   time: 357.27s   best: 22.3128
2023-11-13 15:41:55,152:INFO:  Epoch 497/500:  train Loss: 17.3703   val Loss: 22.9608   time: 358.43s   best: 22.3128
2023-11-13 15:47:52,606:INFO:  Epoch 498/500:  train Loss: 17.5730   val Loss: 23.6085   time: 357.42s   best: 22.3128
2023-11-13 15:53:47,550:INFO:  Epoch 499/500:  train Loss: 17.2625   val Loss: 23.9994   time: 354.93s   best: 22.3128
2023-11-13 15:59:45,934:INFO:  Epoch 500/500:  train Loss: 17.2981   val Loss: 23.3297   time: 358.36s   best: 22.3128
2023-11-13 15:59:45,973:INFO:  -----> Training complete in 3000m 25s   best validation loss: 22.3128
 
2023-11-14 09:45:42,281:INFO:  Starting experiment lstm autoencoder perm25 debug (0.05 dropout)
2023-11-14 09:45:42,296:INFO:  Defining the model
2023-11-14 09:45:42,339:INFO:  Reading the dataset
2023-11-14 09:45:49,130:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:45:49,151:INFO:  Epoch 1/500:  train Loss: 98.9885   val Loss: 96.4996   time: 1.69s   best: 96.4996
2023-11-14 09:45:49,437:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:45:49,522:INFO:  Epoch 2/500:  train Loss: 98.6159   val Loss: 95.8227   time: 0.28s   best: 95.8227
2023-11-14 09:45:49,765:INFO:  Epoch 3/500:  train Loss: 99.3323   val Loss: 100.0303   time: 0.24s   best: 95.8227
2023-11-14 09:45:50,022:INFO:  Epoch 4/500:  train Loss: 99.2662   val Loss: 100.0934   time: 0.25s   best: 95.8227
2023-11-14 09:45:50,265:INFO:  Epoch 5/500:  train Loss: 99.4799   val Loss: 100.0940   time: 0.24s   best: 95.8227
2023-11-14 09:45:50,526:INFO:  Epoch 6/500:  train Loss: 99.4073   val Loss: 99.9883   time: 0.26s   best: 95.8227
2023-11-14 09:45:50,767:INFO:  Epoch 7/500:  train Loss: 98.8709   val Loss: 99.5216   time: 0.24s   best: 95.8227
2023-11-14 09:45:51,008:INFO:  Epoch 8/500:  train Loss: 97.8131   val Loss: 96.1037   time: 0.24s   best: 95.8227
2023-11-14 09:45:51,271:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:45:51,293:INFO:  Epoch 9/500:  train Loss: 97.0191   val Loss: 94.3199   time: 0.26s   best: 94.3199
2023-11-14 09:45:51,591:INFO:  Epoch 10/500:  train Loss: 96.4604   val Loss: 94.4621   time: 0.30s   best: 94.3199
2023-11-14 09:45:51,834:INFO:  Epoch 11/500:  train Loss: 97.0078   val Loss: 96.7515   time: 0.24s   best: 94.3199
2023-11-14 09:45:52,094:INFO:  Epoch 12/500:  train Loss: 96.2840   val Loss: 95.8907   time: 0.26s   best: 94.3199
2023-11-14 09:45:52,337:INFO:  Epoch 13/500:  train Loss: 94.9475   val Loss: 94.6295   time: 0.24s   best: 94.3199
2023-11-14 09:45:52,601:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:45:52,684:INFO:  Epoch 14/500:  train Loss: 93.9179   val Loss: 93.6701   time: 0.24s   best: 93.6701
2023-11-14 09:45:52,927:INFO:  Epoch 15/500:  train Loss: 93.9785   val Loss: 94.2722   time: 0.24s   best: 93.6701
2023-11-14 09:45:53,185:INFO:  Epoch 16/500:  train Loss: 93.6913   val Loss: 94.2446   time: 0.26s   best: 93.6701
2023-11-14 09:45:53,428:INFO:  Epoch 17/500:  train Loss: 94.1356   val Loss: 94.2259   time: 0.24s   best: 93.6701
2023-11-14 09:45:53,731:INFO:  Epoch 18/500:  train Loss: 93.5641   val Loss: 94.1574   time: 0.30s   best: 93.6701
2023-11-14 09:45:53,973:INFO:  Epoch 19/500:  train Loss: 93.6111   val Loss: 94.2854   time: 0.24s   best: 93.6701
2023-11-14 09:45:54,235:INFO:  Epoch 20/500:  train Loss: 94.3819   val Loss: 94.3850   time: 0.26s   best: 93.6701
2023-11-14 09:45:54,477:INFO:  Epoch 21/500:  train Loss: 94.0852   val Loss: 94.2364   time: 0.24s   best: 93.6701
2023-11-14 09:45:54,737:INFO:  Epoch 22/500:  train Loss: 94.3758   val Loss: 94.1783   time: 0.26s   best: 93.6701
2023-11-14 09:45:54,979:INFO:  Epoch 23/500:  train Loss: 94.4210   val Loss: 94.1988   time: 0.24s   best: 93.6701
2023-11-14 09:45:55,240:INFO:  Epoch 24/500:  train Loss: 94.0707   val Loss: 94.1230   time: 0.26s   best: 93.6701
2023-11-14 09:45:55,490:INFO:  Epoch 25/500:  train Loss: 93.5637   val Loss: 93.8439   time: 0.25s   best: 93.6701
2023-11-14 09:45:55,787:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:45:55,846:INFO:  Epoch 26/500:  train Loss: 93.5116   val Loss: 92.7370   time: 0.29s   best: 92.7370
2023-11-14 09:45:56,088:INFO:  Epoch 27/500:  train Loss: 93.4614   val Loss: 93.7902   time: 0.24s   best: 92.7370
2023-11-14 09:45:56,346:INFO:  Epoch 28/500:  train Loss: 93.9570   val Loss: 94.1040   time: 0.26s   best: 92.7370
2023-11-14 09:45:56,587:INFO:  Epoch 29/500:  train Loss: 93.7001   val Loss: 94.2380   time: 0.24s   best: 92.7370
2023-11-14 09:45:56,848:INFO:  Epoch 30/500:  train Loss: 94.0346   val Loss: 94.3048   time: 0.26s   best: 92.7370
2023-11-14 09:45:57,089:INFO:  Epoch 31/500:  train Loss: 94.0151   val Loss: 94.2056   time: 0.24s   best: 92.7370
2023-11-14 09:45:57,350:INFO:  Epoch 32/500:  train Loss: 93.9231   val Loss: 94.0188   time: 0.26s   best: 92.7370
2023-11-14 09:45:57,632:INFO:  Epoch 33/500:  train Loss: 93.7976   val Loss: 94.0698   time: 0.28s   best: 92.7370
2023-11-14 09:45:57,897:INFO:  Epoch 34/500:  train Loss: 94.3253   val Loss: 94.2200   time: 0.26s   best: 92.7370
2023-11-14 09:45:58,140:INFO:  Epoch 35/500:  train Loss: 93.7766   val Loss: 94.0014   time: 0.24s   best: 92.7370
2023-11-14 09:45:58,399:INFO:  Epoch 36/500:  train Loss: 93.0946   val Loss: 93.6117   time: 0.26s   best: 92.7370
2023-11-14 09:45:58,642:INFO:  Epoch 37/500:  train Loss: 93.2911   val Loss: 93.5285   time: 0.24s   best: 92.7370
2023-11-14 09:45:58,903:INFO:  Epoch 38/500:  train Loss: 93.4182   val Loss: 93.4739   time: 0.26s   best: 92.7370
2023-11-14 09:45:59,145:INFO:  Epoch 39/500:  train Loss: 92.9511   val Loss: 93.2976   time: 0.24s   best: 92.7370
2023-11-14 09:45:59,385:INFO:  Epoch 40/500:  train Loss: 92.8304   val Loss: 93.0147   time: 0.24s   best: 92.7370
2023-11-14 09:45:59,681:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:45:59,746:INFO:  Epoch 41/500:  train Loss: 92.2754   val Loss: 92.3094   time: 0.27s   best: 92.3094
2023-11-14 09:46:00,004:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:00,076:INFO:  Epoch 42/500:  train Loss: 92.0225   val Loss: 92.2707   time: 0.25s   best: 92.2707
2023-11-14 09:46:00,319:INFO:  Epoch 43/500:  train Loss: 92.6280   val Loss: 92.3813   time: 0.24s   best: 92.2707
2023-11-14 09:46:00,578:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:00,638:INFO:  Epoch 44/500:  train Loss: 92.1262   val Loss: 92.0633   time: 0.25s   best: 92.0633
2023-11-14 09:46:00,881:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:00,997:INFO:  Epoch 45/500:  train Loss: 91.6952   val Loss: 91.4297   time: 0.24s   best: 91.4297
2023-11-14 09:46:01,242:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:01,352:INFO:  Epoch 46/500:  train Loss: 90.9960   val Loss: 90.8341   time: 0.24s   best: 90.8341
2023-11-14 09:46:01,614:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:01,678:INFO:  Epoch 47/500:  train Loss: 90.2365   val Loss: 90.4265   time: 0.26s   best: 90.4265
2023-11-14 09:46:01,962:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:02,034:INFO:  Epoch 48/500:  train Loss: 90.9954   val Loss: 90.0761   time: 0.28s   best: 90.0761
2023-11-14 09:46:02,280:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:02,304:INFO:  Epoch 49/500:  train Loss: 90.5436   val Loss: 89.7174   time: 0.24s   best: 89.7174
2023-11-14 09:46:02,562:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:02,675:INFO:  Epoch 50/500:  train Loss: 89.7431   val Loss: 89.1057   time: 0.25s   best: 89.1057
2023-11-14 09:46:02,931:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:02,995:INFO:  Epoch 51/500:  train Loss: 89.3236   val Loss: 88.3572   time: 0.25s   best: 88.3572
2023-11-14 09:46:03,255:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:03,327:INFO:  Epoch 52/500:  train Loss: 89.1565   val Loss: 88.3240   time: 0.25s   best: 88.3240
2023-11-14 09:46:03,591:INFO:  Epoch 53/500:  train Loss: 89.7985   val Loss: 88.9530   time: 0.26s   best: 88.3240
2023-11-14 09:46:03,873:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:03,932:INFO:  Epoch 54/500:  train Loss: 89.3273   val Loss: 88.1367   time: 0.28s   best: 88.1367
2023-11-14 09:46:04,265:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:04,307:INFO:  Epoch 55/500:  train Loss: 87.6463   val Loss: 87.4284   time: 0.26s   best: 87.4284
2023-11-14 09:46:04,550:INFO:  Epoch 56/500:  train Loss: 87.7215   val Loss: 87.4301   time: 0.24s   best: 87.4284
2023-11-14 09:46:04,807:INFO:  Epoch 57/500:  train Loss: 88.3174   val Loss: 87.9590   time: 0.25s   best: 87.4284
2023-11-14 09:46:05,052:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:05,118:INFO:  Epoch 58/500:  train Loss: 87.9194   val Loss: 86.5036   time: 0.24s   best: 86.5036
2023-11-14 09:46:05,363:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:05,501:INFO:  Epoch 59/500:  train Loss: 87.4196   val Loss: 86.3994   time: 0.24s   best: 86.3994
2023-11-14 09:46:05,759:INFO:  Epoch 60/500:  train Loss: 87.5876   val Loss: 86.6427   time: 0.25s   best: 86.3994
2023-11-14 09:46:06,041:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:06,094:INFO:  Epoch 61/500:  train Loss: 86.8210   val Loss: 85.9391   time: 0.28s   best: 85.9391
2023-11-14 09:46:06,353:INFO:  Epoch 62/500:  train Loss: 86.4565   val Loss: 86.5864   time: 0.26s   best: 85.9391
2023-11-14 09:46:06,596:INFO:  Epoch 63/500:  train Loss: 87.3513   val Loss: 86.6928   time: 0.24s   best: 85.9391
2023-11-14 09:46:06,855:INFO:  Epoch 64/500:  train Loss: 87.2496   val Loss: 86.0985   time: 0.26s   best: 85.9391
2023-11-14 09:46:07,099:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:07,179:INFO:  Epoch 65/500:  train Loss: 86.8150   val Loss: 85.8280   time: 0.24s   best: 85.8280
2023-11-14 09:46:07,423:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:07,487:INFO:  Epoch 66/500:  train Loss: 86.6922   val Loss: 85.4092   time: 0.24s   best: 85.4092
2023-11-14 09:46:07,748:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:07,803:INFO:  Epoch 67/500:  train Loss: 85.6956   val Loss: 85.1725   time: 0.26s   best: 85.1725
2023-11-14 09:46:08,082:INFO:  Epoch 68/500:  train Loss: 86.2328   val Loss: 85.4123   time: 0.28s   best: 85.1725
2023-11-14 09:46:08,339:INFO:  Epoch 69/500:  train Loss: 86.0712   val Loss: 85.2788   time: 0.26s   best: 85.1725
2023-11-14 09:46:08,582:INFO:  Epoch 70/500:  train Loss: 86.4162   val Loss: 86.2718   time: 0.24s   best: 85.1725
2023-11-14 09:46:08,840:INFO:  Epoch 71/500:  train Loss: 85.6982   val Loss: 85.6425   time: 0.26s   best: 85.1725
2023-11-14 09:46:09,085:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:09,190:INFO:  Epoch 72/500:  train Loss: 85.7149   val Loss: 85.0176   time: 0.24s   best: 85.0176
2023-11-14 09:46:09,451:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:09,480:INFO:  Epoch 73/500:  train Loss: 84.9161   val Loss: 84.5366   time: 0.26s   best: 84.5366
2023-11-14 09:46:09,731:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:09,840:INFO:  Epoch 74/500:  train Loss: 84.3591   val Loss: 84.0358   time: 0.25s   best: 84.0358
2023-11-14 09:46:10,121:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:10,175:INFO:  Epoch 75/500:  train Loss: 84.6147   val Loss: 83.7039   time: 0.28s   best: 83.7039
2023-11-14 09:46:10,430:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:10,505:INFO:  Epoch 76/500:  train Loss: 83.9046   val Loss: 83.3018   time: 0.25s   best: 83.3018
2023-11-14 09:46:10,749:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:10,880:INFO:  Epoch 77/500:  train Loss: 83.8306   val Loss: 83.1154   time: 0.24s   best: 83.1154
2023-11-14 09:46:11,123:INFO:  Epoch 78/500:  train Loss: 83.4731   val Loss: 83.4860   time: 0.24s   best: 83.1154
2023-11-14 09:46:11,380:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:11,443:INFO:  Epoch 79/500:  train Loss: 84.0987   val Loss: 83.0570   time: 0.25s   best: 83.0570
2023-11-14 09:46:11,691:INFO:  Epoch 80/500:  train Loss: 84.2438   val Loss: 83.1425   time: 0.25s   best: 83.0570
2023-11-14 09:46:11,949:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:12,019:INFO:  Epoch 81/500:  train Loss: 83.1222   val Loss: 82.9868   time: 0.25s   best: 82.9868
2023-11-14 09:46:12,300:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:12,355:INFO:  Epoch 82/500:  train Loss: 83.5684   val Loss: 82.3437   time: 0.28s   best: 82.3437
2023-11-14 09:46:12,599:INFO:  Epoch 83/500:  train Loss: 84.1254   val Loss: 84.8630   time: 0.24s   best: 82.3437
2023-11-14 09:46:12,856:INFO:  Epoch 84/500:  train Loss: 85.3671   val Loss: 89.4642   time: 0.25s   best: 82.3437
2023-11-14 09:46:13,098:INFO:  Epoch 85/500:  train Loss: 92.4665   val Loss: 93.5608   time: 0.24s   best: 82.3437
2023-11-14 09:46:13,340:INFO:  Epoch 86/500:  train Loss: 92.8670   val Loss: 91.0874   time: 0.24s   best: 82.3437
2023-11-14 09:46:13,611:INFO:  Epoch 87/500:  train Loss: 89.8689   val Loss: 88.0880   time: 0.27s   best: 82.3437
2023-11-14 09:46:13,852:INFO:  Epoch 88/500:  train Loss: 88.5823   val Loss: 88.6131   time: 0.24s   best: 82.3437
2023-11-14 09:46:14,111:INFO:  Epoch 89/500:  train Loss: 89.0228   val Loss: 89.0051   time: 0.26s   best: 82.3437
2023-11-14 09:46:14,444:INFO:  Epoch 90/500:  train Loss: 88.3981   val Loss: 86.7671   time: 0.33s   best: 82.3437
2023-11-14 09:46:14,691:INFO:  Epoch 91/500:  train Loss: 86.5326   val Loss: 85.7436   time: 0.24s   best: 82.3437
2023-11-14 09:46:14,962:INFO:  Epoch 92/500:  train Loss: 85.8589   val Loss: 84.9513   time: 0.27s   best: 82.3437
2023-11-14 09:46:15,215:INFO:  Epoch 93/500:  train Loss: 84.9763   val Loss: 83.9325   time: 0.25s   best: 82.3437
2023-11-14 09:46:15,476:INFO:  Epoch 94/500:  train Loss: 84.2008   val Loss: 82.9039   time: 0.26s   best: 82.3437
2023-11-14 09:46:15,728:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:15,815:INFO:  Epoch 95/500:  train Loss: 83.1199   val Loss: 82.3278   time: 0.25s   best: 82.3278
2023-11-14 09:46:16,072:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:16,135:INFO:  Epoch 96/500:  train Loss: 82.4983   val Loss: 81.6837   time: 0.25s   best: 81.6837
2023-11-14 09:46:16,414:INFO:  Epoch 97/500:  train Loss: 82.9346   val Loss: 81.7327   time: 0.28s   best: 81.6837
2023-11-14 09:46:16,673:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:16,693:INFO:  Epoch 98/500:  train Loss: 82.5970   val Loss: 81.3418   time: 0.25s   best: 81.3418
2023-11-14 09:46:16,937:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:17,031:INFO:  Epoch 99/500:  train Loss: 81.8299   val Loss: 80.9856   time: 0.24s   best: 80.9856
2023-11-14 09:46:17,341:INFO:  Epoch 100/500:  train Loss: 81.8913   val Loss: 81.1248   time: 0.29s   best: 80.9856
2023-11-14 09:46:17,613:INFO:  Epoch 101/500:  train Loss: 82.2910   val Loss: 80.9933   time: 0.26s   best: 80.9856
2023-11-14 09:46:17,857:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:17,911:INFO:  Epoch 102/500:  train Loss: 81.5192   val Loss: 80.7353   time: 0.24s   best: 80.7353
2023-11-14 09:46:18,187:INFO:  Epoch 103/500:  train Loss: 82.0053   val Loss: 81.6883   time: 0.27s   best: 80.7353
2023-11-14 09:46:18,475:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:18,577:INFO:  Epoch 104/500:  train Loss: 81.5143   val Loss: 80.1589   time: 0.28s   best: 80.1589
2023-11-14 09:46:18,821:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:18,974:INFO:  Epoch 105/500:  train Loss: 81.3042   val Loss: 80.1462   time: 0.24s   best: 80.1462
2023-11-14 09:46:19,238:INFO:  Epoch 106/500:  train Loss: 81.7596   val Loss: 80.6046   time: 0.25s   best: 80.1462
2023-11-14 09:46:19,483:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:19,522:INFO:  Epoch 107/500:  train Loss: 81.5191   val Loss: 80.0073   time: 0.24s   best: 80.0073
2023-11-14 09:46:19,787:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:19,895:INFO:  Epoch 108/500:  train Loss: 80.9374   val Loss: 79.9603   time: 0.26s   best: 79.9603
2023-11-14 09:46:20,155:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:20,185:INFO:  Epoch 109/500:  train Loss: 80.5886   val Loss: 79.7615   time: 0.26s   best: 79.7615
2023-11-14 09:46:20,430:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:20,549:INFO:  Epoch 110/500:  train Loss: 80.2762   val Loss: 79.4238   time: 0.24s   best: 79.4238
2023-11-14 09:46:20,815:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:20,903:INFO:  Epoch 111/500:  train Loss: 80.7735   val Loss: 79.4168   time: 0.26s   best: 79.4168
2023-11-14 09:46:21,170:INFO:  Epoch 112/500:  train Loss: 80.5891   val Loss: 80.3491   time: 0.24s   best: 79.4168
2023-11-14 09:46:21,423:INFO:  Epoch 113/500:  train Loss: 80.5812   val Loss: 79.7010   time: 0.24s   best: 79.4168
2023-11-14 09:46:21,690:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:21,720:INFO:  Epoch 114/500:  train Loss: 80.4282   val Loss: 79.2046   time: 0.24s   best: 79.2046
2023-11-14 09:46:21,971:INFO:  Epoch 115/500:  train Loss: 80.6617   val Loss: 79.2453   time: 0.24s   best: 79.2046
2023-11-14 09:46:22,241:INFO:  Epoch 116/500:  train Loss: 80.9248   val Loss: 80.9110   time: 0.26s   best: 79.2046
2023-11-14 09:46:22,491:INFO:  Epoch 117/500:  train Loss: 81.3647   val Loss: 80.4514   time: 0.24s   best: 79.2046
2023-11-14 09:46:22,794:INFO:  Epoch 118/500:  train Loss: 80.8631   val Loss: 79.7003   time: 0.29s   best: 79.2046
2023-11-14 09:46:23,043:INFO:  Epoch 119/500:  train Loss: 79.5621   val Loss: 79.6585   time: 0.24s   best: 79.2046
2023-11-14 09:46:23,305:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:23,333:INFO:  Epoch 120/500:  train Loss: 80.1417   val Loss: 78.7128   time: 0.26s   best: 78.7128
2023-11-14 09:46:23,586:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:23,700:INFO:  Epoch 121/500:  train Loss: 79.4518   val Loss: 78.4111   time: 0.25s   best: 78.4111
2023-11-14 09:46:23,957:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:23,986:INFO:  Epoch 122/500:  train Loss: 78.8943   val Loss: 78.3075   time: 0.25s   best: 78.3075
2023-11-14 09:46:24,233:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:24,299:INFO:  Epoch 123/500:  train Loss: 79.2002   val Loss: 77.9930   time: 0.24s   best: 77.9930
2023-11-14 09:46:24,543:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:24,601:INFO:  Epoch 124/500:  train Loss: 78.1297   val Loss: 77.3199   time: 0.24s   best: 77.3199
2023-11-14 09:46:24,894:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:24,963:INFO:  Epoch 125/500:  train Loss: 78.6078   val Loss: 76.7164   time: 0.29s   best: 76.7164
2023-11-14 09:46:25,215:INFO:  Epoch 126/500:  train Loss: 78.5973   val Loss: 76.8710   time: 0.24s   best: 76.7164
2023-11-14 09:46:25,480:INFO:  Epoch 127/500:  train Loss: 79.0877   val Loss: 77.5376   time: 0.25s   best: 76.7164
2023-11-14 09:46:25,738:INFO:  Epoch 128/500:  train Loss: 79.4912   val Loss: 78.3224   time: 0.25s   best: 76.7164
2023-11-14 09:46:26,006:INFO:  Epoch 129/500:  train Loss: 79.8732   val Loss: 79.6060   time: 0.26s   best: 76.7164
2023-11-14 09:46:26,258:INFO:  Epoch 130/500:  train Loss: 79.8198   val Loss: 78.6583   time: 0.24s   best: 76.7164
2023-11-14 09:46:26,525:INFO:  Epoch 131/500:  train Loss: 79.7231   val Loss: 77.6298   time: 0.26s   best: 76.7164
2023-11-14 09:46:26,814:INFO:  Epoch 132/500:  train Loss: 78.3660   val Loss: 77.0361   time: 0.28s   best: 76.7164
2023-11-14 09:46:27,082:INFO:  Epoch 133/500:  train Loss: 78.2518   val Loss: 78.6531   time: 0.26s   best: 76.7164
2023-11-14 09:46:27,333:INFO:  Epoch 134/500:  train Loss: 79.7821   val Loss: 78.5756   time: 0.24s   best: 76.7164
2023-11-14 09:46:27,608:INFO:  Epoch 135/500:  train Loss: 79.5926   val Loss: 78.9364   time: 0.26s   best: 76.7164
2023-11-14 09:46:27,857:INFO:  Epoch 136/500:  train Loss: 78.6850   val Loss: 77.5248   time: 0.24s   best: 76.7164
2023-11-14 09:46:28,127:INFO:  Epoch 137/500:  train Loss: 78.1483   val Loss: 76.9026   time: 0.26s   best: 76.7164
2023-11-14 09:46:28,374:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:28,471:INFO:  Epoch 138/500:  train Loss: 77.9446   val Loss: 76.3895   time: 0.24s   best: 76.3895
2023-11-14 09:46:28,722:INFO:  Epoch 139/500:  train Loss: 77.7903   val Loss: 76.4906   time: 0.24s   best: 76.3895
2023-11-14 09:46:29,019:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:29,049:INFO:  Epoch 140/500:  train Loss: 78.0196   val Loss: 75.9747   time: 0.29s   best: 75.9747
2023-11-14 09:46:29,296:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:29,424:INFO:  Epoch 141/500:  train Loss: 76.5404   val Loss: 74.6920   time: 0.24s   best: 74.6920
2023-11-14 09:46:29,694:INFO:  Epoch 142/500:  train Loss: 77.2731   val Loss: 76.0495   time: 0.26s   best: 74.6920
2023-11-14 09:46:29,944:INFO:  Epoch 143/500:  train Loss: 77.7316   val Loss: 77.0018   time: 0.24s   best: 74.6920
2023-11-14 09:46:30,212:INFO:  Epoch 144/500:  train Loss: 78.3519   val Loss: 77.3329   time: 0.26s   best: 74.6920
2023-11-14 09:46:30,466:INFO:  Epoch 145/500:  train Loss: 77.2948   val Loss: 76.4186   time: 0.24s   best: 74.6920
2023-11-14 09:46:30,734:INFO:  Epoch 146/500:  train Loss: 76.6919   val Loss: 75.3573   time: 0.26s   best: 74.6920
2023-11-14 09:46:31,020:INFO:  Epoch 147/500:  train Loss: 77.2830   val Loss: 75.4170   time: 0.27s   best: 74.6920
2023-11-14 09:46:31,289:INFO:  Epoch 148/500:  train Loss: 75.9117   val Loss: 74.9920   time: 0.26s   best: 74.6920
2023-11-14 09:46:31,537:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:31,615:INFO:  Epoch 149/500:  train Loss: 76.0065   val Loss: 74.1514   time: 0.24s   best: 74.1514
2023-11-14 09:46:31,866:INFO:  Epoch 150/500:  train Loss: 75.6862   val Loss: 74.3703   time: 0.24s   best: 74.1514
2023-11-14 09:46:32,134:INFO:  Epoch 151/500:  train Loss: 76.1371   val Loss: 74.7792   time: 0.26s   best: 74.1514
2023-11-14 09:46:32,389:INFO:  Epoch 152/500:  train Loss: 75.3064   val Loss: 74.2264   time: 0.24s   best: 74.1514
2023-11-14 09:46:32,658:INFO:  Epoch 153/500:  train Loss: 76.5727   val Loss: 74.2507   time: 0.26s   best: 74.1514
2023-11-14 09:46:32,908:INFO:  Epoch 154/500:  train Loss: 76.2276   val Loss: 74.4941   time: 0.24s   best: 74.1514
2023-11-14 09:46:33,212:INFO:  Epoch 155/500:  train Loss: 76.2458   val Loss: 74.6973   time: 0.29s   best: 74.1514
2023-11-14 09:46:33,463:INFO:  Epoch 156/500:  train Loss: 75.7260   val Loss: 74.6530   time: 0.24s   best: 74.1514
2023-11-14 09:46:33,742:INFO:  Epoch 157/500:  train Loss: 75.3700   val Loss: 75.6957   time: 0.27s   best: 74.1514
2023-11-14 09:46:34,002:INFO:  Epoch 158/500:  train Loss: 76.1512   val Loss: 75.2860   time: 0.25s   best: 74.1514
2023-11-14 09:46:34,265:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:34,336:INFO:  Epoch 159/500:  train Loss: 75.9452   val Loss: 73.8109   time: 0.26s   best: 73.8109
2023-11-14 09:46:34,582:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:34,675:INFO:  Epoch 160/500:  train Loss: 74.9251   val Loss: 73.6533   time: 0.24s   best: 73.6533
2023-11-14 09:46:34,940:INFO:  Epoch 161/500:  train Loss: 74.7765   val Loss: 73.8908   time: 0.25s   best: 73.6533
2023-11-14 09:46:35,236:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:35,303:INFO:  Epoch 162/500:  train Loss: 75.1441   val Loss: 73.3017   time: 0.29s   best: 73.3017
2023-11-14 09:46:35,556:INFO:  Epoch 163/500:  train Loss: 74.2646   val Loss: 73.8709   time: 0.24s   best: 73.3017
2023-11-14 09:46:35,823:INFO:  Epoch 164/500:  train Loss: 75.0273   val Loss: 74.1929   time: 0.26s   best: 73.3017
2023-11-14 09:46:36,076:INFO:  Epoch 165/500:  train Loss: 76.1597   val Loss: 73.7538   time: 0.24s   best: 73.3017
2023-11-14 09:46:36,344:INFO:  Epoch 166/500:  train Loss: 74.8706   val Loss: 73.6551   time: 0.26s   best: 73.3017
2023-11-14 09:46:36,589:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:36,699:INFO:  Epoch 167/500:  train Loss: 75.2143   val Loss: 73.2804   time: 0.24s   best: 73.2804
2023-11-14 09:46:36,959:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:36,999:INFO:  Epoch 168/500:  train Loss: 74.4628   val Loss: 73.2692   time: 0.26s   best: 73.2692
2023-11-14 09:46:37,296:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:37,384:INFO:  Epoch 169/500:  train Loss: 74.9299   val Loss: 72.8317   time: 0.29s   best: 72.8317
2023-11-14 09:46:37,634:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:37,761:INFO:  Epoch 170/500:  train Loss: 74.2008   val Loss: 72.6469   time: 0.24s   best: 72.6469
2023-11-14 09:46:38,022:INFO:  Epoch 171/500:  train Loss: 74.4792   val Loss: 73.0370   time: 0.25s   best: 72.6469
2023-11-14 09:46:38,276:INFO:  Epoch 172/500:  train Loss: 75.7442   val Loss: 73.7597   time: 0.24s   best: 72.6469
2023-11-14 09:46:38,545:INFO:  Epoch 173/500:  train Loss: 76.3951   val Loss: 76.9119   time: 0.26s   best: 72.6469
2023-11-14 09:46:38,790:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:38,859:INFO:  Epoch 174/500:  train Loss: 75.5193   val Loss: 72.2733   time: 0.24s   best: 72.2733
2023-11-14 09:46:39,111:INFO:  Epoch 175/500:  train Loss: 73.5656   val Loss: 72.4676   time: 0.24s   best: 72.2733
2023-11-14 09:46:39,417:INFO:  Epoch 176/500:  train Loss: 73.5008   val Loss: 72.7416   time: 0.29s   best: 72.2733
2023-11-14 09:46:39,671:INFO:  Epoch 177/500:  train Loss: 74.1385   val Loss: 73.5570   time: 0.24s   best: 72.2733
2023-11-14 09:46:39,939:INFO:  Epoch 178/500:  train Loss: 74.3433   val Loss: 72.3666   time: 0.26s   best: 72.2733
2023-11-14 09:46:40,184:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:40,364:INFO:  Epoch 179/500:  train Loss: 73.4234   val Loss: 71.3353   time: 0.24s   best: 71.3353
2023-11-14 09:46:40,629:INFO:  Epoch 180/500:  train Loss: 75.9496   val Loss: 79.6800   time: 0.25s   best: 71.3353
2023-11-14 09:46:40,881:INFO:  Epoch 181/500:  train Loss: 78.5846   val Loss: 77.3242   time: 0.24s   best: 71.3353
2023-11-14 09:46:41,149:INFO:  Epoch 182/500:  train Loss: 79.7038   val Loss: 78.2580   time: 0.26s   best: 71.3353
2023-11-14 09:46:41,453:INFO:  Epoch 183/500:  train Loss: 78.3517   val Loss: 79.9894   time: 0.29s   best: 71.3353
2023-11-14 09:46:41,709:INFO:  Epoch 184/500:  train Loss: 79.3654   val Loss: 74.9620   time: 0.24s   best: 71.3353
2023-11-14 09:46:41,976:INFO:  Epoch 185/500:  train Loss: 77.9130   val Loss: 78.3474   time: 0.26s   best: 71.3353
2023-11-14 09:46:42,229:INFO:  Epoch 186/500:  train Loss: 75.5160   val Loss: 72.6081   time: 0.24s   best: 71.3353
2023-11-14 09:46:42,491:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:42,592:INFO:  Epoch 187/500:  train Loss: 72.8033   val Loss: 71.2397   time: 0.26s   best: 71.2397
2023-11-14 09:46:42,844:INFO:  Epoch 188/500:  train Loss: 72.5438   val Loss: 71.7905   time: 0.24s   best: 71.2397
2023-11-14 09:46:43,107:INFO:  Epoch 189/500:  train Loss: 73.0979   val Loss: 72.3276   time: 0.25s   best: 71.2397
2023-11-14 09:46:43,373:INFO:  Epoch 190/500:  train Loss: 73.5599   val Loss: 72.8650   time: 0.26s   best: 71.2397
2023-11-14 09:46:43,668:INFO:  Epoch 191/500:  train Loss: 73.7112   val Loss: 71.8972   time: 0.28s   best: 71.2397
2023-11-14 09:46:43,919:INFO:  Epoch 192/500:  train Loss: 75.2067   val Loss: 74.7165   time: 0.24s   best: 71.2397
2023-11-14 09:46:44,186:INFO:  Epoch 193/500:  train Loss: 74.8259   val Loss: 75.1814   time: 0.26s   best: 71.2397
2023-11-14 09:46:44,439:INFO:  Epoch 194/500:  train Loss: 75.3506   val Loss: 73.2339   time: 0.24s   best: 71.2397
2023-11-14 09:46:44,708:INFO:  Epoch 195/500:  train Loss: 73.5724   val Loss: 72.2445   time: 0.26s   best: 71.2397
2023-11-14 09:46:44,958:INFO:  Epoch 196/500:  train Loss: 74.2211   val Loss: 71.4628   time: 0.24s   best: 71.2397
2023-11-14 09:46:45,227:INFO:  Epoch 197/500:  train Loss: 72.6199   val Loss: 71.8370   time: 0.26s   best: 71.2397
2023-11-14 09:46:45,532:INFO:  Epoch 198/500:  train Loss: 73.0529   val Loss: 71.8080   time: 0.30s   best: 71.2397
2023-11-14 09:46:45,819:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:45,873:INFO:  Epoch 199/500:  train Loss: 71.7666   val Loss: 70.4561   time: 0.28s   best: 70.4561
2023-11-14 09:46:46,198:INFO:  Epoch 200/500:  train Loss: 74.1516   val Loss: 77.5232   time: 0.30s   best: 70.4561
2023-11-14 09:46:46,450:INFO:  Epoch 201/500:  train Loss: 77.3451   val Loss: 74.6669   time: 0.24s   best: 70.4561
2023-11-14 09:46:46,720:INFO:  Epoch 202/500:  train Loss: 77.8677   val Loss: 77.5292   time: 0.26s   best: 70.4561
2023-11-14 09:46:46,971:INFO:  Epoch 203/500:  train Loss: 75.7821   val Loss: 74.0657   time: 0.24s   best: 70.4561
2023-11-14 09:46:47,238:INFO:  Epoch 204/500:  train Loss: 74.0378   val Loss: 72.0457   time: 0.26s   best: 70.4561
2023-11-14 09:46:47,488:INFO:  Epoch 205/500:  train Loss: 73.5051   val Loss: 71.4404   time: 0.24s   best: 70.4561
2023-11-14 09:46:47,794:INFO:  Epoch 206/500:  train Loss: 72.1627   val Loss: 70.6794   time: 0.30s   best: 70.4561
2023-11-14 09:46:48,046:INFO:  Epoch 207/500:  train Loss: 74.1167   val Loss: 71.1619   time: 0.24s   best: 70.4561
2023-11-14 09:46:48,316:INFO:  Epoch 208/500:  train Loss: 72.1843   val Loss: 70.6572   time: 0.26s   best: 70.4561
2023-11-14 09:46:48,566:INFO:  Epoch 209/500:  train Loss: 72.6855   val Loss: 71.5025   time: 0.24s   best: 70.4561
2023-11-14 09:46:48,836:INFO:  Epoch 210/500:  train Loss: 72.4925   val Loss: 71.4566   time: 0.26s   best: 70.4561
2023-11-14 09:46:49,094:INFO:  Epoch 211/500:  train Loss: 72.0796   val Loss: 70.7585   time: 0.25s   best: 70.4561
2023-11-14 09:46:49,371:INFO:  Epoch 212/500:  train Loss: 72.0711   val Loss: 70.6662   time: 0.27s   best: 70.4561
2023-11-14 09:46:49,626:INFO:  Epoch 213/500:  train Loss: 72.1651   val Loss: 70.6110   time: 0.24s   best: 70.4561
2023-11-14 09:46:49,924:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:50,055:INFO:  Epoch 214/500:  train Loss: 71.4319   val Loss: 69.8572   time: 0.29s   best: 69.8572
2023-11-14 09:46:50,302:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:50,429:INFO:  Epoch 215/500:  train Loss: 70.7032   val Loss: 68.8966   time: 0.24s   best: 68.8966
2023-11-14 09:46:50,680:INFO:  Epoch 216/500:  train Loss: 71.4738   val Loss: 70.2835   time: 0.24s   best: 68.8966
2023-11-14 09:46:50,944:INFO:  Epoch 217/500:  train Loss: 71.0476   val Loss: 70.2412   time: 0.25s   best: 68.8966
2023-11-14 09:46:51,193:INFO:  Epoch 218/500:  train Loss: 70.7459   val Loss: 70.5831   time: 0.24s   best: 68.8966
2023-11-14 09:46:51,464:INFO:  Epoch 219/500:  train Loss: 70.8754   val Loss: 69.9033   time: 0.26s   best: 68.8966
2023-11-14 09:46:51,719:INFO:  Epoch 220/500:  train Loss: 70.7515   val Loss: 69.5299   time: 0.24s   best: 68.8966
2023-11-14 09:46:52,023:INFO:  Epoch 221/500:  train Loss: 70.8256   val Loss: 69.1033   time: 0.29s   best: 68.8966
2023-11-14 09:46:52,269:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:52,309:INFO:  Epoch 222/500:  train Loss: 70.6196   val Loss: 68.5948   time: 0.24s   best: 68.5948
2023-11-14 09:46:52,575:INFO:  Epoch 223/500:  train Loss: 70.3056   val Loss: 68.9071   time: 0.26s   best: 68.5948
2023-11-14 09:46:52,829:INFO:  Epoch 224/500:  train Loss: 70.2417   val Loss: 69.1294   time: 0.24s   best: 68.5948
2023-11-14 09:46:53,090:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:53,242:INFO:  Epoch 225/500:  train Loss: 69.6310   val Loss: 68.2094   time: 0.26s   best: 68.2094
2023-11-14 09:46:53,505:INFO:  Epoch 226/500:  train Loss: 69.8791   val Loss: 68.3098   time: 0.25s   best: 68.2094
2023-11-14 09:46:53,760:INFO:  Epoch 227/500:  train Loss: 68.9574   val Loss: 68.6889   time: 0.24s   best: 68.2094
2023-11-14 09:46:54,055:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:54,153:INFO:  Epoch 228/500:  train Loss: 69.7270   val Loss: 68.0387   time: 0.29s   best: 68.0387
2023-11-14 09:46:54,407:INFO:  Epoch 229/500:  train Loss: 69.0326   val Loss: 68.2309   time: 0.24s   best: 68.0387
2023-11-14 09:46:54,672:INFO:  Epoch 230/500:  train Loss: 70.5211   val Loss: 68.3819   time: 0.25s   best: 68.0387
2023-11-14 09:46:54,916:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:46:55,019:INFO:  Epoch 231/500:  train Loss: 69.3056   val Loss: 66.8202   time: 0.24s   best: 66.8202
2023-11-14 09:46:55,271:INFO:  Epoch 232/500:  train Loss: 68.9770   val Loss: 67.9722   time: 0.24s   best: 66.8202
2023-11-14 09:46:55,537:INFO:  Epoch 233/500:  train Loss: 68.9390   val Loss: 67.9177   time: 0.25s   best: 66.8202
2023-11-14 09:46:55,793:INFO:  Epoch 234/500:  train Loss: 68.8878   val Loss: 67.1418   time: 0.24s   best: 66.8202
2023-11-14 09:46:56,094:INFO:  Epoch 235/500:  train Loss: 69.0106   val Loss: 67.2176   time: 0.29s   best: 66.8202
2023-11-14 09:46:56,346:INFO:  Epoch 236/500:  train Loss: 68.7080   val Loss: 68.3284   time: 0.24s   best: 66.8202
2023-11-14 09:46:56,613:INFO:  Epoch 237/500:  train Loss: 71.7125   val Loss: 72.9939   time: 0.26s   best: 66.8202
2023-11-14 09:46:56,863:INFO:  Epoch 238/500:  train Loss: 72.5439   val Loss: 73.1567   time: 0.24s   best: 66.8202
2023-11-14 09:46:57,130:INFO:  Epoch 239/500:  train Loss: 71.5353   val Loss: 70.4743   time: 0.26s   best: 66.8202
2023-11-14 09:46:57,383:INFO:  Epoch 240/500:  train Loss: 72.3352   val Loss: 67.8643   time: 0.24s   best: 66.8202
2023-11-14 09:46:57,653:INFO:  Epoch 241/500:  train Loss: 72.6347   val Loss: 73.4335   time: 0.26s   best: 66.8202
2023-11-14 09:46:57,904:INFO:  Epoch 242/500:  train Loss: 74.0658   val Loss: 73.4349   time: 0.24s   best: 66.8202
2023-11-14 09:46:58,205:INFO:  Epoch 243/500:  train Loss: 73.6682   val Loss: 71.8454   time: 0.29s   best: 66.8202
2023-11-14 09:46:58,457:INFO:  Epoch 244/500:  train Loss: 72.8570   val Loss: 70.4806   time: 0.24s   best: 66.8202
2023-11-14 09:46:58,726:INFO:  Epoch 245/500:  train Loss: 73.6111   val Loss: 76.2965   time: 0.26s   best: 66.8202
2023-11-14 09:46:58,976:INFO:  Epoch 246/500:  train Loss: 74.7294   val Loss: 75.1672   time: 0.24s   best: 66.8202
2023-11-14 09:46:59,245:INFO:  Epoch 247/500:  train Loss: 74.2588   val Loss: 71.8268   time: 0.26s   best: 66.8202
2023-11-14 09:46:59,496:INFO:  Epoch 248/500:  train Loss: 72.0646   val Loss: 72.4941   time: 0.24s   best: 66.8202
2023-11-14 09:46:59,768:INFO:  Epoch 249/500:  train Loss: 75.2454   val Loss: 69.8718   time: 0.26s   best: 66.8202
2023-11-14 09:47:00,018:INFO:  Epoch 250/500:  train Loss: 72.1025   val Loss: 69.8084   time: 0.24s   best: 66.8202
2023-11-14 09:47:00,323:INFO:  Epoch 251/500:  train Loss: 70.9021   val Loss: 68.6431   time: 0.29s   best: 66.8202
2023-11-14 09:47:00,573:INFO:  Epoch 252/500:  train Loss: 69.8923   val Loss: 68.6715   time: 0.24s   best: 66.8202
2023-11-14 09:47:00,841:INFO:  Epoch 253/500:  train Loss: 68.9896   val Loss: 68.0489   time: 0.26s   best: 66.8202
2023-11-14 09:47:01,092:INFO:  Epoch 254/500:  train Loss: 69.5109   val Loss: 68.3465   time: 0.24s   best: 66.8202
2023-11-14 09:47:01,361:INFO:  Epoch 255/500:  train Loss: 68.9512   val Loss: 67.5883   time: 0.26s   best: 66.8202
2023-11-14 09:47:01,613:INFO:  Epoch 256/500:  train Loss: 69.8164   val Loss: 69.8616   time: 0.24s   best: 66.8202
2023-11-14 09:47:01,884:INFO:  Epoch 257/500:  train Loss: 70.6669   val Loss: 69.0903   time: 0.26s   best: 66.8202
2023-11-14 09:47:02,135:INFO:  Epoch 258/500:  train Loss: 69.2144   val Loss: 67.4826   time: 0.24s   best: 66.8202
2023-11-14 09:47:02,432:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:02,461:INFO:  Epoch 259/500:  train Loss: 68.7339   val Loss: 66.6802   time: 0.29s   best: 66.6802
2023-11-14 09:47:02,706:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:02,829:INFO:  Epoch 260/500:  train Loss: 68.2209   val Loss: 66.2890   time: 0.24s   best: 66.2890
2023-11-14 09:47:03,080:INFO:  Epoch 261/500:  train Loss: 70.1067   val Loss: 71.4943   time: 0.24s   best: 66.2890
2023-11-14 09:47:03,348:INFO:  Epoch 262/500:  train Loss: 71.6414   val Loss: 71.4419   time: 0.26s   best: 66.2890
2023-11-14 09:47:03,602:INFO:  Epoch 263/500:  train Loss: 71.3876   val Loss: 68.4426   time: 0.24s   best: 66.2890
2023-11-14 09:47:03,873:INFO:  Epoch 264/500:  train Loss: 68.8186   val Loss: 67.3570   time: 0.26s   best: 66.2890
2023-11-14 09:47:04,123:INFO:  Epoch 265/500:  train Loss: 68.4380   val Loss: 66.6802   time: 0.24s   best: 66.2890
2023-11-14 09:47:04,435:INFO:  Epoch 266/500:  train Loss: 71.0571   val Loss: 69.2915   time: 0.31s   best: 66.2890
2023-11-14 09:47:04,693:INFO:  Epoch 267/500:  train Loss: 70.5241   val Loss: 70.6804   time: 0.25s   best: 66.2890
2023-11-14 09:47:04,963:INFO:  Epoch 268/500:  train Loss: 70.6691   val Loss: 68.9429   time: 0.26s   best: 66.2890
2023-11-14 09:47:05,213:INFO:  Epoch 269/500:  train Loss: 70.1258   val Loss: 67.7729   time: 0.24s   best: 66.2890
2023-11-14 09:47:05,482:INFO:  Epoch 270/500:  train Loss: 68.6886   val Loss: 67.7799   time: 0.26s   best: 66.2890
2023-11-14 09:47:05,736:INFO:  Epoch 271/500:  train Loss: 68.0461   val Loss: 66.7362   time: 0.24s   best: 66.2890
2023-11-14 09:47:06,004:INFO:  Epoch 272/500:  train Loss: 68.8702   val Loss: 68.1673   time: 0.26s   best: 66.2890
2023-11-14 09:47:06,255:INFO:  Epoch 273/500:  train Loss: 69.1058   val Loss: 67.0538   time: 0.24s   best: 66.2890
2023-11-14 09:47:06,553:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:06,582:INFO:  Epoch 274/500:  train Loss: 67.7245   val Loss: 65.2421   time: 0.29s   best: 65.2421
2023-11-14 09:47:06,834:INFO:  Epoch 275/500:  train Loss: 67.2671   val Loss: 66.4422   time: 0.24s   best: 65.2421
2023-11-14 09:47:07,100:INFO:  Epoch 276/500:  train Loss: 67.4873   val Loss: 66.3545   time: 0.26s   best: 65.2421
2023-11-14 09:47:07,353:INFO:  Epoch 277/500:  train Loss: 66.8117   val Loss: 65.3416   time: 0.24s   best: 65.2421
2023-11-14 09:47:07,622:INFO:  Epoch 278/500:  train Loss: 67.8348   val Loss: 68.3462   time: 0.26s   best: 65.2421
2023-11-14 09:47:07,876:INFO:  Epoch 279/500:  train Loss: 69.6582   val Loss: 69.7684   time: 0.24s   best: 65.2421
2023-11-14 09:47:08,144:INFO:  Epoch 280/500:  train Loss: 70.1093   val Loss: 68.5910   time: 0.26s   best: 65.2421
2023-11-14 09:47:08,396:INFO:  Epoch 281/500:  train Loss: 70.6976   val Loss: 67.6790   time: 0.24s   best: 65.2421
2023-11-14 09:47:08,697:INFO:  Epoch 282/500:  train Loss: 68.9823   val Loss: 68.4198   time: 0.29s   best: 65.2421
2023-11-14 09:47:08,947:INFO:  Epoch 283/500:  train Loss: 69.0830   val Loss: 68.4863   time: 0.24s   best: 65.2421
2023-11-14 09:47:09,213:INFO:  Epoch 284/500:  train Loss: 70.2009   val Loss: 71.2291   time: 0.25s   best: 65.2421
2023-11-14 09:47:09,465:INFO:  Epoch 285/500:  train Loss: 71.2025   val Loss: 69.6700   time: 0.24s   best: 65.2421
2023-11-14 09:47:09,736:INFO:  Epoch 286/500:  train Loss: 71.4728   val Loss: 73.0113   time: 0.26s   best: 65.2421
2023-11-14 09:47:09,987:INFO:  Epoch 287/500:  train Loss: 71.6507   val Loss: 68.4579   time: 0.24s   best: 65.2421
2023-11-14 09:47:10,255:INFO:  Epoch 288/500:  train Loss: 69.2774   val Loss: 67.5451   time: 0.26s   best: 65.2421
2023-11-14 09:47:10,533:INFO:  Epoch 289/500:  train Loss: 68.8111   val Loss: 66.5915   time: 0.27s   best: 65.2421
2023-11-14 09:47:10,813:INFO:  Epoch 290/500:  train Loss: 67.4722   val Loss: 66.1986   time: 0.27s   best: 65.2421
2023-11-14 09:47:11,063:INFO:  Epoch 291/500:  train Loss: 67.1609   val Loss: 66.1508   time: 0.24s   best: 65.2421
2023-11-14 09:47:11,332:INFO:  Epoch 292/500:  train Loss: 66.7947   val Loss: 65.3403   time: 0.26s   best: 65.2421
2023-11-14 09:47:11,582:INFO:  Epoch 293/500:  train Loss: 67.2199   val Loss: 67.0302   time: 0.24s   best: 65.2421
2023-11-14 09:47:11,854:INFO:  Epoch 294/500:  train Loss: 68.9846   val Loss: 67.9524   time: 0.26s   best: 65.2421
2023-11-14 09:47:12,105:INFO:  Epoch 295/500:  train Loss: 68.3184   val Loss: 67.5799   time: 0.24s   best: 65.2421
2023-11-14 09:47:12,375:INFO:  Epoch 296/500:  train Loss: 70.6869   val Loss: 67.4554   time: 0.26s   best: 65.2421
2023-11-14 09:47:12,660:INFO:  Epoch 297/500:  train Loss: 71.8948   val Loss: 71.6568   time: 0.27s   best: 65.2421
2023-11-14 09:47:12,931:INFO:  Epoch 298/500:  train Loss: 72.4849   val Loss: 71.7516   time: 0.26s   best: 65.2421
2023-11-14 09:47:13,182:INFO:  Epoch 299/500:  train Loss: 75.2754   val Loss: 76.1792   time: 0.24s   best: 65.2421
2023-11-14 09:47:13,510:INFO:  Epoch 300/500:  train Loss: 73.7738   val Loss: 73.8228   time: 0.30s   best: 65.2421
2023-11-14 09:47:13,782:INFO:  Epoch 301/500:  train Loss: 72.6281   val Loss: 70.0407   time: 0.24s   best: 65.2421
2023-11-14 09:47:14,031:INFO:  Epoch 302/500:  train Loss: 70.0795   val Loss: 68.1350   time: 0.24s   best: 65.2421
2023-11-14 09:47:14,283:INFO:  Epoch 303/500:  train Loss: 69.6537   val Loss: 68.1888   time: 0.24s   best: 65.2421
2023-11-14 09:47:14,551:INFO:  Epoch 304/500:  train Loss: 68.2704   val Loss: 67.2076   time: 0.24s   best: 65.2421
2023-11-14 09:47:14,853:INFO:  Epoch 305/500:  train Loss: 75.4440   val Loss: 82.0512   time: 0.29s   best: 65.2421
2023-11-14 09:47:15,103:INFO:  Epoch 306/500:  train Loss: 82.5909   val Loss: 80.9208   time: 0.24s   best: 65.2421
2023-11-14 09:47:15,370:INFO:  Epoch 307/500:  train Loss: 82.1034   val Loss: 83.2898   time: 0.26s   best: 65.2421
2023-11-14 09:47:15,622:INFO:  Epoch 308/500:  train Loss: 83.1997   val Loss: 80.3033   time: 0.24s   best: 65.2421
2023-11-14 09:47:15,892:INFO:  Epoch 309/500:  train Loss: 81.6692   val Loss: 79.5026   time: 0.26s   best: 65.2421
2023-11-14 09:47:16,142:INFO:  Epoch 310/500:  train Loss: 78.8356   val Loss: 81.8103   time: 0.24s   best: 65.2421
2023-11-14 09:47:16,412:INFO:  Epoch 311/500:  train Loss: 82.3386   val Loss: 80.4960   time: 0.26s   best: 65.2421
2023-11-14 09:47:16,662:INFO:  Epoch 312/500:  train Loss: 81.1850   val Loss: 78.9198   time: 0.24s   best: 65.2421
2023-11-14 09:47:16,997:INFO:  Epoch 313/500:  train Loss: 79.6996   val Loss: 78.7838   time: 0.32s   best: 65.2421
2023-11-14 09:47:17,248:INFO:  Epoch 314/500:  train Loss: 78.3963   val Loss: 76.9274   time: 0.24s   best: 65.2421
2023-11-14 09:47:17,517:INFO:  Epoch 315/500:  train Loss: 76.7956   val Loss: 75.4704   time: 0.26s   best: 65.2421
2023-11-14 09:47:17,771:INFO:  Epoch 316/500:  train Loss: 75.9546   val Loss: 75.2120   time: 0.24s   best: 65.2421
2023-11-14 09:47:18,038:INFO:  Epoch 317/500:  train Loss: 75.2235   val Loss: 74.1325   time: 0.26s   best: 65.2421
2023-11-14 09:47:18,289:INFO:  Epoch 318/500:  train Loss: 74.0387   val Loss: 72.4188   time: 0.24s   best: 65.2421
2023-11-14 09:47:18,561:INFO:  Epoch 319/500:  train Loss: 74.0221   val Loss: 74.2203   time: 0.26s   best: 65.2421
2023-11-14 09:47:18,811:INFO:  Epoch 320/500:  train Loss: 75.1728   val Loss: 73.8614   time: 0.24s   best: 65.2421
2023-11-14 09:47:19,112:INFO:  Epoch 321/500:  train Loss: 74.3720   val Loss: 72.6729   time: 0.29s   best: 65.2421
2023-11-14 09:47:19,364:INFO:  Epoch 322/500:  train Loss: 72.0039   val Loss: 72.3258   time: 0.24s   best: 65.2421
2023-11-14 09:47:19,716:INFO:  Epoch 323/500:  train Loss: 72.2642   val Loss: 70.7006   time: 0.35s   best: 65.2421
2023-11-14 09:47:19,975:INFO:  Epoch 324/500:  train Loss: 73.0852   val Loss: 70.0782   time: 0.25s   best: 65.2421
2023-11-14 09:47:20,242:INFO:  Epoch 325/500:  train Loss: 73.3015   val Loss: 73.7644   time: 0.26s   best: 65.2421
2023-11-14 09:47:20,494:INFO:  Epoch 326/500:  train Loss: 75.1108   val Loss: 74.5746   time: 0.24s   best: 65.2421
2023-11-14 09:47:20,762:INFO:  Epoch 327/500:  train Loss: 73.3219   val Loss: 73.9489   time: 0.26s   best: 65.2421
2023-11-14 09:47:21,027:INFO:  Epoch 328/500:  train Loss: 72.2065   val Loss: 73.1141   time: 0.24s   best: 65.2421
2023-11-14 09:47:21,319:INFO:  Epoch 329/500:  train Loss: 72.3645   val Loss: 71.5006   time: 0.28s   best: 65.2421
2023-11-14 09:47:21,570:INFO:  Epoch 330/500:  train Loss: 69.9415   val Loss: 69.4246   time: 0.24s   best: 65.2421
2023-11-14 09:47:21,843:INFO:  Epoch 331/500:  train Loss: 69.9390   val Loss: 69.2389   time: 0.26s   best: 65.2421
2023-11-14 09:47:22,093:INFO:  Epoch 332/500:  train Loss: 68.3382   val Loss: 67.8940   time: 0.24s   best: 65.2421
2023-11-14 09:47:22,364:INFO:  Epoch 333/500:  train Loss: 68.8280   val Loss: 68.6002   time: 0.26s   best: 65.2421
2023-11-14 09:47:22,614:INFO:  Epoch 334/500:  train Loss: 68.2912   val Loss: 67.4028   time: 0.24s   best: 65.2421
2023-11-14 09:47:22,882:INFO:  Epoch 335/500:  train Loss: 69.6197   val Loss: 70.5068   time: 0.26s   best: 65.2421
2023-11-14 09:47:23,144:INFO:  Epoch 336/500:  train Loss: 68.4582   val Loss: 67.2233   time: 0.26s   best: 65.2421
2023-11-14 09:47:23,442:INFO:  Epoch 337/500:  train Loss: 68.2458   val Loss: 68.0505   time: 0.29s   best: 65.2421
2023-11-14 09:47:23,714:INFO:  Epoch 338/500:  train Loss: 67.9532   val Loss: 67.3908   time: 0.24s   best: 65.2421
2023-11-14 09:47:23,964:INFO:  Epoch 339/500:  train Loss: 69.3780   val Loss: 69.0130   time: 0.24s   best: 65.2421
2023-11-14 09:47:24,216:INFO:  Epoch 340/500:  train Loss: 67.0692   val Loss: 66.0998   time: 0.24s   best: 65.2421
2023-11-14 09:47:24,487:INFO:  Epoch 341/500:  train Loss: 68.1358   val Loss: 67.8221   time: 0.26s   best: 65.2421
2023-11-14 09:47:24,738:INFO:  Epoch 342/500:  train Loss: 67.1744   val Loss: 66.2733   time: 0.24s   best: 65.2421
2023-11-14 09:47:25,007:INFO:  Epoch 343/500:  train Loss: 67.1742   val Loss: 68.1394   time: 0.26s   best: 65.2421
2023-11-14 09:47:25,269:INFO:  Epoch 344/500:  train Loss: 68.3540   val Loss: 65.8429   time: 0.26s   best: 65.2421
2023-11-14 09:47:25,563:INFO:  Epoch 345/500:  train Loss: 68.0615   val Loss: 68.9407   time: 0.28s   best: 65.2421
2023-11-14 09:47:25,834:INFO:  Epoch 346/500:  train Loss: 67.4330   val Loss: 67.0098   time: 0.24s   best: 65.2421
2023-11-14 09:47:26,085:INFO:  Epoch 347/500:  train Loss: 67.3190   val Loss: 66.1360   time: 0.24s   best: 65.2421
2023-11-14 09:47:26,337:INFO:  Epoch 348/500:  train Loss: 66.4897   val Loss: 66.2185   time: 0.24s   best: 65.2421
2023-11-14 09:47:26,606:INFO:  Epoch 349/500:  train Loss: 67.2233   val Loss: 65.4666   time: 0.26s   best: 65.2421
2023-11-14 09:47:26,851:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:26,931:INFO:  Epoch 350/500:  train Loss: 68.1919   val Loss: 64.2467   time: 0.24s   best: 64.2467
2023-11-14 09:47:27,182:INFO:  Epoch 351/500:  train Loss: 66.2413   val Loss: 65.6363   time: 0.24s   best: 64.2467
2023-11-14 09:47:27,486:INFO:  Epoch 352/500:  train Loss: 65.9094   val Loss: 67.4404   time: 0.29s   best: 64.2467
2023-11-14 09:47:27,741:INFO:  Epoch 353/500:  train Loss: 75.8696   val Loss: 87.9610   time: 0.24s   best: 64.2467
2023-11-14 09:47:28,008:INFO:  Epoch 354/500:  train Loss: 88.6165   val Loss: 82.8104   time: 0.26s   best: 64.2467
2023-11-14 09:47:28,258:INFO:  Epoch 355/500:  train Loss: 81.8794   val Loss: 84.9306   time: 0.24s   best: 64.2467
2023-11-14 09:47:28,527:INFO:  Epoch 356/500:  train Loss: 81.6239   val Loss: 77.6088   time: 0.26s   best: 64.2467
2023-11-14 09:47:28,779:INFO:  Epoch 357/500:  train Loss: 77.3231   val Loss: 75.5938   time: 0.24s   best: 64.2467
2023-11-14 09:47:29,047:INFO:  Epoch 358/500:  train Loss: 75.4819   val Loss: 74.4618   time: 0.26s   best: 64.2467
2023-11-14 09:47:29,298:INFO:  Epoch 359/500:  train Loss: 73.3975   val Loss: 70.8693   time: 0.24s   best: 64.2467
2023-11-14 09:47:29,605:INFO:  Epoch 360/500:  train Loss: 71.0926   val Loss: 69.2227   time: 0.30s   best: 64.2467
2023-11-14 09:47:29,860:INFO:  Epoch 361/500:  train Loss: 69.6879   val Loss: 68.5310   time: 0.24s   best: 64.2467
2023-11-14 09:47:30,128:INFO:  Epoch 362/500:  train Loss: 69.2549   val Loss: 67.5676   time: 0.26s   best: 64.2467
2023-11-14 09:47:30,381:INFO:  Epoch 363/500:  train Loss: 68.7099   val Loss: 67.9241   time: 0.24s   best: 64.2467
2023-11-14 09:47:30,650:INFO:  Epoch 364/500:  train Loss: 68.1862   val Loss: 66.4859   time: 0.26s   best: 64.2467
2023-11-14 09:47:30,901:INFO:  Epoch 365/500:  train Loss: 67.1688   val Loss: 66.3063   time: 0.24s   best: 64.2467
2023-11-14 09:47:31,168:INFO:  Epoch 366/500:  train Loss: 67.9788   val Loss: 66.6720   time: 0.26s   best: 64.2467
2023-11-14 09:47:31,422:INFO:  Epoch 367/500:  train Loss: 67.1500   val Loss: 66.1806   time: 0.24s   best: 64.2467
2023-11-14 09:47:31,729:INFO:  Epoch 368/500:  train Loss: 67.6207   val Loss: 66.8575   time: 0.30s   best: 64.2467
2023-11-14 09:47:31,979:INFO:  Epoch 369/500:  train Loss: 67.4158   val Loss: 66.2400   time: 0.24s   best: 64.2467
2023-11-14 09:47:32,246:INFO:  Epoch 370/500:  train Loss: 66.9016   val Loss: 66.2608   time: 0.26s   best: 64.2467
2023-11-14 09:47:32,498:INFO:  Epoch 371/500:  train Loss: 66.7702   val Loss: 67.6223   time: 0.24s   best: 64.2467
2023-11-14 09:47:32,764:INFO:  Epoch 372/500:  train Loss: 68.3902   val Loss: 68.4531   time: 0.26s   best: 64.2467
2023-11-14 09:47:33,015:INFO:  Epoch 373/500:  train Loss: 66.9239   val Loss: 64.8749   time: 0.24s   best: 64.2467
2023-11-14 09:47:33,286:INFO:  Epoch 374/500:  train Loss: 68.6190   val Loss: 68.6790   time: 0.26s   best: 64.2467
2023-11-14 09:47:33,538:INFO:  Epoch 375/500:  train Loss: 66.5132   val Loss: 65.7293   time: 0.24s   best: 64.2467
2023-11-14 09:47:33,845:INFO:  Epoch 376/500:  train Loss: 66.9835   val Loss: 65.9371   time: 0.30s   best: 64.2467
2023-11-14 09:47:34,096:INFO:  Epoch 377/500:  train Loss: 65.9029   val Loss: 65.8157   time: 0.24s   best: 64.2467
2023-11-14 09:47:34,367:INFO:  Epoch 378/500:  train Loss: 66.8115   val Loss: 66.9881   time: 0.26s   best: 64.2467
2023-11-14 09:47:34,617:INFO:  Epoch 379/500:  train Loss: 66.3413   val Loss: 64.6776   time: 0.24s   best: 64.2467
2023-11-14 09:47:34,884:INFO:  Epoch 380/500:  train Loss: 66.3250   val Loss: 66.2052   time: 0.26s   best: 64.2467
2023-11-14 09:47:35,142:INFO:  Epoch 381/500:  train Loss: 66.9092   val Loss: 66.6788   time: 0.25s   best: 64.2467
2023-11-14 09:47:35,420:INFO:  Epoch 382/500:  train Loss: 66.6841   val Loss: 66.1559   time: 0.27s   best: 64.2467
2023-11-14 09:47:35,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:35,697:INFO:  Epoch 383/500:  train Loss: 65.5757   val Loss: 64.1658   time: 0.24s   best: 64.1658
2023-11-14 09:47:36,002:INFO:  Epoch 384/500:  train Loss: 66.5351   val Loss: 65.7722   time: 0.29s   best: 64.1658
2023-11-14 09:47:36,252:INFO:  Epoch 385/500:  train Loss: 65.3329   val Loss: 64.8570   time: 0.24s   best: 64.1658
2023-11-14 09:47:36,522:INFO:  Epoch 386/500:  train Loss: 66.2197   val Loss: 65.9171   time: 0.26s   best: 64.1658
2023-11-14 09:47:36,772:INFO:  Epoch 387/500:  train Loss: 67.4874   val Loss: 66.1086   time: 0.24s   best: 64.1658
2023-11-14 09:47:37,040:INFO:  Epoch 388/500:  train Loss: 69.3890   val Loss: 69.1379   time: 0.26s   best: 64.1658
2023-11-14 09:47:37,291:INFO:  Epoch 389/500:  train Loss: 68.1725   val Loss: 69.2632   time: 0.24s   best: 64.1658
2023-11-14 09:47:37,561:INFO:  Epoch 390/500:  train Loss: 68.5713   val Loss: 67.2840   time: 0.26s   best: 64.1658
2023-11-14 09:47:37,809:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:38,198:INFO:  Epoch 391/500:  train Loss: 66.5844   val Loss: 63.8430   time: 0.24s   best: 63.8430
2023-11-14 09:47:38,459:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:38,488:INFO:  Epoch 392/500:  train Loss: 65.3619   val Loss: 63.7911   time: 0.26s   best: 63.7911
2023-11-14 09:47:38,733:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:38,830:INFO:  Epoch 393/500:  train Loss: 65.1629   val Loss: 63.7262   time: 0.24s   best: 63.7262
2023-11-14 09:47:39,096:INFO:  Epoch 394/500:  train Loss: 65.7287   val Loss: 63.7514   time: 0.25s   best: 63.7262
2023-11-14 09:47:39,347:INFO:  Epoch 395/500:  train Loss: 65.1878   val Loss: 64.8783   time: 0.24s   best: 63.7262
2023-11-14 09:47:39,615:INFO:  Epoch 396/500:  train Loss: 64.8914   val Loss: 64.7749   time: 0.26s   best: 63.7262
2023-11-14 09:47:39,870:INFO:  Epoch 397/500:  train Loss: 64.5694   val Loss: 64.3360   time: 0.24s   best: 63.7262
2023-11-14 09:47:40,167:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:40,246:INFO:  Epoch 398/500:  train Loss: 65.0928   val Loss: 63.6632   time: 0.29s   best: 63.6632
2023-11-14 09:47:40,508:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:40,582:INFO:  Epoch 399/500:  train Loss: 65.7234   val Loss: 63.2346   time: 0.26s   best: 63.2346
2023-11-14 09:47:40,892:INFO:  Epoch 400/500:  train Loss: 64.1244   val Loss: 63.8847   time: 0.29s   best: 63.2346
2023-11-14 09:47:41,156:INFO:  Epoch 401/500:  train Loss: 64.8525   val Loss: 65.2000   time: 0.25s   best: 63.2346
2023-11-14 09:47:41,408:INFO:  Epoch 402/500:  train Loss: 67.6714   val Loss: 66.5635   time: 0.24s   best: 63.2346
2023-11-14 09:47:41,677:INFO:  Epoch 403/500:  train Loss: 66.0772   val Loss: 63.8812   time: 0.26s   best: 63.2346
2023-11-14 09:47:41,930:INFO:  Epoch 404/500:  train Loss: 64.6868   val Loss: 63.7115   time: 0.24s   best: 63.2346
2023-11-14 09:47:42,235:INFO:  Epoch 405/500:  train Loss: 65.3578   val Loss: 63.3311   time: 0.29s   best: 63.2346
2023-11-14 09:47:42,480:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:42,597:INFO:  Epoch 406/500:  train Loss: 64.7380   val Loss: 63.0506   time: 0.24s   best: 63.0506
2023-11-14 09:47:42,842:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:42,872:INFO:  Epoch 407/500:  train Loss: 64.7467   val Loss: 62.5868   time: 0.24s   best: 62.5868
2023-11-14 09:47:43,165:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:43,243:INFO:  Epoch 408/500:  train Loss: 63.8586   val Loss: 62.0597   time: 0.29s   best: 62.0597
2023-11-14 09:47:43,498:INFO:  Epoch 409/500:  train Loss: 64.2676   val Loss: 64.4416   time: 0.24s   best: 62.0597
2023-11-14 09:47:43,766:INFO:  Epoch 410/500:  train Loss: 65.2254   val Loss: 62.7772   time: 0.26s   best: 62.0597
2023-11-14 09:47:44,017:INFO:  Epoch 411/500:  train Loss: 65.6280   val Loss: 65.8181   time: 0.24s   best: 62.0597
2023-11-14 09:47:44,321:INFO:  Epoch 412/500:  train Loss: 65.6355   val Loss: 64.5616   time: 0.29s   best: 62.0597
2023-11-14 09:47:44,574:INFO:  Epoch 413/500:  train Loss: 65.1555   val Loss: 63.7876   time: 0.24s   best: 62.0597
2023-11-14 09:47:44,841:INFO:  Epoch 414/500:  train Loss: 64.4847   val Loss: 63.6024   time: 0.26s   best: 62.0597
2023-11-14 09:47:45,093:INFO:  Epoch 415/500:  train Loss: 65.8178   val Loss: 68.4451   time: 0.24s   best: 62.0597
2023-11-14 09:47:45,362:INFO:  Epoch 416/500:  train Loss: 66.0307   val Loss: 64.0914   time: 0.26s   best: 62.0597
2023-11-14 09:47:45,613:INFO:  Epoch 417/500:  train Loss: 65.9200   val Loss: 67.4704   time: 0.24s   best: 62.0597
2023-11-14 09:47:45,885:INFO:  Epoch 418/500:  train Loss: 66.3122   val Loss: 65.4858   time: 0.26s   best: 62.0597
2023-11-14 09:47:46,135:INFO:  Epoch 419/500:  train Loss: 65.9094   val Loss: 63.6798   time: 0.24s   best: 62.0597
2023-11-14 09:47:46,442:INFO:  Epoch 420/500:  train Loss: 64.0268   val Loss: 62.4326   time: 0.30s   best: 62.0597
2023-11-14 09:47:46,692:INFO:  Epoch 421/500:  train Loss: 63.6188   val Loss: 63.4882   time: 0.24s   best: 62.0597
2023-11-14 09:47:46,960:INFO:  Epoch 422/500:  train Loss: 64.7890   val Loss: 64.8597   time: 0.26s   best: 62.0597
2023-11-14 09:47:47,209:INFO:  Epoch 423/500:  train Loss: 63.8895   val Loss: 63.7839   time: 0.24s   best: 62.0597
2023-11-14 09:47:47,474:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:47,548:INFO:  Epoch 424/500:  train Loss: 63.7135   val Loss: 61.5533   time: 0.26s   best: 61.5533
2023-11-14 09:47:47,816:INFO:  Epoch 425/500:  train Loss: 66.0841   val Loss: 67.2437   time: 0.26s   best: 61.5533
2023-11-14 09:47:48,066:INFO:  Epoch 426/500:  train Loss: 68.6635   val Loss: 67.3307   time: 0.24s   best: 61.5533
2023-11-14 09:47:48,377:INFO:  Epoch 427/500:  train Loss: 73.2765   val Loss: 77.9430   time: 0.30s   best: 61.5533
2023-11-14 09:47:48,660:INFO:  Epoch 428/500:  train Loss: 73.5573   val Loss: 73.6609   time: 0.27s   best: 61.5533
2023-11-14 09:47:48,925:INFO:  Epoch 429/500:  train Loss: 71.8703   val Loss: 68.3477   time: 0.25s   best: 61.5533
2023-11-14 09:47:49,177:INFO:  Epoch 430/500:  train Loss: 68.5927   val Loss: 65.3293   time: 0.24s   best: 61.5533
2023-11-14 09:47:49,446:INFO:  Epoch 431/500:  train Loss: 66.5384   val Loss: 65.9640   time: 0.26s   best: 61.5533
2023-11-14 09:47:49,698:INFO:  Epoch 432/500:  train Loss: 65.7709   val Loss: 63.2800   time: 0.24s   best: 61.5533
2023-11-14 09:47:49,969:INFO:  Epoch 433/500:  train Loss: 65.5000   val Loss: 65.8435   time: 0.26s   best: 61.5533
2023-11-14 09:47:50,219:INFO:  Epoch 434/500:  train Loss: 68.0468   val Loss: 67.6524   time: 0.24s   best: 61.5533
2023-11-14 09:47:50,532:INFO:  Epoch 435/500:  train Loss: 68.2310   val Loss: 67.4946   time: 0.31s   best: 61.5533
2023-11-14 09:47:50,790:INFO:  Epoch 436/500:  train Loss: 67.3467   val Loss: 66.3871   time: 0.25s   best: 61.5533
2023-11-14 09:47:51,057:INFO:  Epoch 437/500:  train Loss: 67.0689   val Loss: 68.0953   time: 0.26s   best: 61.5533
2023-11-14 09:47:51,307:INFO:  Epoch 438/500:  train Loss: 65.6288   val Loss: 66.7908   time: 0.24s   best: 61.5533
2023-11-14 09:47:51,575:INFO:  Epoch 439/500:  train Loss: 66.2122   val Loss: 66.3173   time: 0.26s   best: 61.5533
2023-11-14 09:47:51,829:INFO:  Epoch 440/500:  train Loss: 65.0909   val Loss: 64.0458   time: 0.24s   best: 61.5533
2023-11-14 09:47:52,096:INFO:  Epoch 441/500:  train Loss: 65.4195   val Loss: 64.6236   time: 0.26s   best: 61.5533
2023-11-14 09:47:52,347:INFO:  Epoch 442/500:  train Loss: 64.4879   val Loss: 63.5434   time: 0.24s   best: 61.5533
2023-11-14 09:47:52,655:INFO:  Epoch 443/500:  train Loss: 64.4217   val Loss: 63.3606   time: 0.30s   best: 61.5533
2023-11-14 09:47:52,905:INFO:  Epoch 444/500:  train Loss: 63.2356   val Loss: 63.7278   time: 0.24s   best: 61.5533
2023-11-14 09:47:53,172:INFO:  Epoch 445/500:  train Loss: 64.4931   val Loss: 61.8681   time: 0.26s   best: 61.5533
2023-11-14 09:47:53,423:INFO:  Epoch 446/500:  train Loss: 63.1428   val Loss: 66.0016   time: 0.24s   best: 61.5533
2023-11-14 09:47:53,696:INFO:  Epoch 447/500:  train Loss: 66.0640   val Loss: 64.1113   time: 0.26s   best: 61.5533
2023-11-14 09:47:53,948:INFO:  Epoch 448/500:  train Loss: 67.3540   val Loss: 63.7187   time: 0.24s   best: 61.5533
2023-11-14 09:47:54,215:INFO:  Epoch 449/500:  train Loss: 64.9534   val Loss: 64.0313   time: 0.26s   best: 61.5533
2023-11-14 09:47:54,470:INFO:  Epoch 450/500:  train Loss: 65.1011   val Loss: 62.2977   time: 0.24s   best: 61.5533
2023-11-14 09:47:54,789:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:54,831:INFO:  Epoch 451/500:  train Loss: 63.3583   val Loss: 61.5212   time: 0.29s   best: 61.5212
2023-11-14 09:47:55,076:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:55,229:INFO:  Epoch 452/500:  train Loss: 62.2186   val Loss: 60.8655   time: 0.24s   best: 60.8655
2023-11-14 09:47:55,482:INFO:  Epoch 453/500:  train Loss: 62.4927   val Loss: 62.2331   time: 0.24s   best: 60.8655
2023-11-14 09:47:55,749:INFO:  Epoch 454/500:  train Loss: 63.3904   val Loss: 61.4194   time: 0.25s   best: 60.8655
2023-11-14 09:47:55,999:INFO:  Epoch 455/500:  train Loss: 62.0524   val Loss: 61.0047   time: 0.24s   best: 60.8655
2023-11-14 09:47:56,261:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:56,315:INFO:  Epoch 456/500:  train Loss: 61.3727   val Loss: 60.5446   time: 0.26s   best: 60.5446
2023-11-14 09:47:56,568:INFO:  Epoch 457/500:  train Loss: 62.2603   val Loss: 61.1869   time: 0.24s   best: 60.5446
2023-11-14 09:47:56,874:INFO:  Epoch 458/500:  train Loss: 63.8984   val Loss: 65.1343   time: 0.29s   best: 60.5446
2023-11-14 09:47:57,124:INFO:  Epoch 459/500:  train Loss: 64.6325   val Loss: 62.5903   time: 0.24s   best: 60.5446
2023-11-14 09:47:57,391:INFO:  Epoch 460/500:  train Loss: 63.4199   val Loss: 61.5899   time: 0.26s   best: 60.5446
2023-11-14 09:47:57,644:INFO:  Epoch 461/500:  train Loss: 64.5172   val Loss: 64.4618   time: 0.24s   best: 60.5446
2023-11-14 09:47:57,915:INFO:  Epoch 462/500:  train Loss: 67.3216   val Loss: 64.8295   time: 0.26s   best: 60.5446
2023-11-14 09:47:58,166:INFO:  Epoch 463/500:  train Loss: 65.2705   val Loss: 62.5076   time: 0.24s   best: 60.5446
2023-11-14 09:47:58,435:INFO:  Epoch 464/500:  train Loss: 62.8267   val Loss: 62.8653   time: 0.26s   best: 60.5446
2023-11-14 09:47:58,686:INFO:  Epoch 465/500:  train Loss: 63.2516   val Loss: 60.9941   time: 0.24s   best: 60.5446
2023-11-14 09:47:58,982:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:59,062:INFO:  Epoch 466/500:  train Loss: 64.0085   val Loss: 60.5064   time: 0.29s   best: 60.5064
2023-11-14 09:47:59,321:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:47:59,400:INFO:  Epoch 467/500:  train Loss: 61.3810   val Loss: 60.0175   time: 0.25s   best: 60.0175
2023-11-14 09:47:59,653:INFO:  Epoch 468/500:  train Loss: 61.3542   val Loss: 60.5090   time: 0.24s   best: 60.0175
2023-11-14 09:47:59,922:INFO:  Epoch 469/500:  train Loss: 62.2098   val Loss: 63.4741   time: 0.26s   best: 60.0175
2023-11-14 09:48:00,173:INFO:  Epoch 470/500:  train Loss: 65.0997   val Loss: 63.7761   time: 0.24s   best: 60.0175
2023-11-14 09:48:00,442:INFO:  Epoch 471/500:  train Loss: 63.3688   val Loss: 61.0143   time: 0.26s   best: 60.0175
2023-11-14 09:48:00,694:INFO:  Epoch 472/500:  train Loss: 63.4887   val Loss: 63.3361   time: 0.24s   best: 60.0175
2023-11-14 09:48:00,999:INFO:  Epoch 473/500:  train Loss: 63.8371   val Loss: 62.6064   time: 0.29s   best: 60.0175
2023-11-14 09:48:01,249:INFO:  Epoch 474/500:  train Loss: 62.9742   val Loss: 60.6609   time: 0.24s   best: 60.0175
2023-11-14 09:48:01,520:INFO:  Epoch 475/500:  train Loss: 61.7149   val Loss: 60.1409   time: 0.26s   best: 60.0175
2023-11-14 09:48:01,775:INFO:  Epoch 476/500:  train Loss: 62.1608   val Loss: 62.1454   time: 0.24s   best: 60.0175
2023-11-14 09:48:02,043:INFO:  Epoch 477/500:  train Loss: 64.0998   val Loss: 62.0800   time: 0.26s   best: 60.0175
2023-11-14 09:48:02,294:INFO:  Epoch 478/500:  train Loss: 63.0595   val Loss: 62.9949   time: 0.24s   best: 60.0175
2023-11-14 09:48:02,564:INFO:  Epoch 479/500:  train Loss: 63.2271   val Loss: 63.1237   time: 0.26s   best: 60.0175
2023-11-14 09:48:02,815:INFO:  Epoch 480/500:  train Loss: 72.4106   val Loss: 78.0816   time: 0.24s   best: 60.0175
2023-11-14 09:48:03,120:INFO:  Epoch 481/500:  train Loss: 76.6143   val Loss: 70.9743   time: 0.29s   best: 60.0175
2023-11-14 09:48:03,371:INFO:  Epoch 482/500:  train Loss: 67.1563   val Loss: 64.8768   time: 0.24s   best: 60.0175
2023-11-14 09:48:03,638:INFO:  Epoch 483/500:  train Loss: 69.1377   val Loss: 64.8909   time: 0.26s   best: 60.0175
2023-11-14 09:48:03,896:INFO:  Epoch 484/500:  train Loss: 66.6536   val Loss: 67.0178   time: 0.25s   best: 60.0175
2023-11-14 09:48:04,163:INFO:  Epoch 485/500:  train Loss: 66.0540   val Loss: 64.4414   time: 0.26s   best: 60.0175
2023-11-14 09:48:04,416:INFO:  Epoch 486/500:  train Loss: 63.8441   val Loss: 61.8453   time: 0.24s   best: 60.0175
2023-11-14 09:48:04,685:INFO:  Epoch 487/500:  train Loss: 64.4235   val Loss: 61.5771   time: 0.26s   best: 60.0175
2023-11-14 09:48:04,935:INFO:  Epoch 488/500:  train Loss: 64.2334   val Loss: 62.0192   time: 0.24s   best: 60.0175
2023-11-14 09:48:05,242:INFO:  Epoch 489/500:  train Loss: 63.9554   val Loss: 60.5665   time: 0.30s   best: 60.0175
2023-11-14 09:48:05,493:INFO:  Epoch 490/500:  train Loss: 61.2893   val Loss: 60.5341   time: 0.24s   best: 60.0175
2023-11-14 09:48:05,772:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:48:05,829:INFO:  Epoch 491/500:  train Loss: 61.9511   val Loss: 59.6845   time: 0.27s   best: 59.6845
2023-11-14 09:48:06,082:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 debug (0.05 dropout)_3ff8.pt
2023-11-14 09:48:06,189:INFO:  Epoch 492/500:  train Loss: 61.3389   val Loss: 59.2746   time: 0.25s   best: 59.2746
2023-11-14 09:48:06,441:INFO:  Epoch 493/500:  train Loss: 60.5196   val Loss: 60.8293   time: 0.24s   best: 59.2746
2023-11-14 09:48:06,706:INFO:  Epoch 494/500:  train Loss: 61.7856   val Loss: 60.4406   time: 0.25s   best: 59.2746
2023-11-14 09:48:06,957:INFO:  Epoch 495/500:  train Loss: 62.5079   val Loss: 61.7790   time: 0.24s   best: 59.2746
2023-11-14 09:48:07,259:INFO:  Epoch 496/500:  train Loss: 63.8884   val Loss: 60.8973   time: 0.29s   best: 59.2746
2023-11-14 09:48:07,511:INFO:  Epoch 497/500:  train Loss: 61.7246   val Loss: 60.2425   time: 0.24s   best: 59.2746
2023-11-14 09:48:07,782:INFO:  Epoch 498/500:  train Loss: 61.3881   val Loss: 60.4663   time: 0.26s   best: 59.2746
2023-11-14 09:48:08,033:INFO:  Epoch 499/500:  train Loss: 60.9884   val Loss: 59.4042   time: 0.24s   best: 59.2746
2023-11-14 09:48:08,361:INFO:  Epoch 500/500:  train Loss: 61.5521   val Loss: 60.5322   time: 0.30s   best: 59.2746
2023-11-14 09:48:08,361:INFO:  -----> Training complete in 2m 21s   best validation loss: 59.2746
 
2023-11-14 16:46:48,900:INFO:  Starting experiment lstm autoencoder perm25 (0.05 dropout)
2023-11-14 16:46:48,920:INFO:  Defining the model
2023-11-14 16:46:49,011:INFO:  Reading the dataset
2023-11-14 17:15:22,866:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 17:15:22,898:INFO:  Epoch 1/500:  train Loss: 75.0068   val Loss: 68.3537   time: 425.69s   best: 68.3537
2023-11-14 17:22:32,413:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 17:22:32,450:INFO:  Epoch 2/500:  train Loss: 62.2618   val Loss: 57.4123   time: 429.51s   best: 57.4123
2023-11-14 17:29:38,784:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 17:29:38,805:INFO:  Epoch 3/500:  train Loss: 55.1254   val Loss: 52.0148   time: 426.33s   best: 52.0148
2023-11-14 17:36:48,758:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 17:36:48,783:INFO:  Epoch 4/500:  train Loss: 49.3546   val Loss: 46.9366   time: 429.94s   best: 46.9366
2023-11-14 17:43:59,533:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 17:43:59,554:INFO:  Epoch 5/500:  train Loss: 45.1181   val Loss: 44.1529   time: 430.72s   best: 44.1529
2023-11-14 17:51:10,463:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 17:51:10,491:INFO:  Epoch 6/500:  train Loss: 41.7502   val Loss: 40.8790   time: 430.90s   best: 40.8790
2023-11-14 17:58:18,562:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 17:58:18,587:INFO:  Epoch 7/500:  train Loss: 38.8924   val Loss: 38.4788   time: 428.06s   best: 38.4788
2023-11-14 18:05:28,766:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 18:05:28,790:INFO:  Epoch 8/500:  train Loss: 36.7626   val Loss: 38.1679   time: 430.17s   best: 38.1679
2023-11-14 18:12:38,525:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 18:12:38,553:INFO:  Epoch 9/500:  train Loss: 35.1602   val Loss: 35.3780   time: 429.73s   best: 35.3780
2023-11-14 18:19:49,705:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 18:19:49,732:INFO:  Epoch 10/500:  train Loss: 34.0035   val Loss: 35.3143   time: 431.15s   best: 35.3143
2023-11-14 18:27:00,527:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 18:27:00,561:INFO:  Epoch 11/500:  train Loss: 32.8525   val Loss: 33.1580   time: 430.77s   best: 33.1580
2023-11-14 18:34:11,828:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 18:34:11,859:INFO:  Epoch 12/500:  train Loss: 32.2670   val Loss: 32.1356   time: 431.25s   best: 32.1356
2023-11-14 18:41:20,749:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 18:41:20,777:INFO:  Epoch 13/500:  train Loss: 31.3568   val Loss: 31.7457   time: 428.88s   best: 31.7457
2023-11-14 18:48:33,255:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 18:48:33,289:INFO:  Epoch 14/500:  train Loss: 30.6271   val Loss: 31.3003   time: 432.47s   best: 31.3003
2023-11-14 18:55:41,822:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 18:55:41,852:INFO:  Epoch 15/500:  train Loss: 30.2463   val Loss: 31.1739   time: 428.51s   best: 31.1739
2023-11-14 19:02:52,379:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 19:02:52,410:INFO:  Epoch 16/500:  train Loss: 29.7578   val Loss: 30.2033   time: 430.52s   best: 30.2033
2023-11-14 19:10:04,875:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 19:10:04,901:INFO:  Epoch 17/500:  train Loss: 29.1429   val Loss: 29.9999   time: 432.46s   best: 29.9999
2023-11-14 19:17:13,175:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 19:17:13,214:INFO:  Epoch 18/500:  train Loss: 28.8583   val Loss: 29.5726   time: 428.26s   best: 29.5726
2023-11-14 19:24:21,613:INFO:  Epoch 19/500:  train Loss: 28.5282   val Loss: 30.2402   time: 428.39s   best: 29.5726
2023-11-14 19:31:32,424:INFO:  Epoch 20/500:  train Loss: 28.0406   val Loss: 29.6790   time: 430.81s   best: 29.5726
2023-11-14 19:38:44,714:INFO:  Epoch 21/500:  train Loss: 27.7479   val Loss: 30.0366   time: 432.27s   best: 29.5726
2023-11-14 19:45:55,645:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 19:45:55,679:INFO:  Epoch 22/500:  train Loss: 27.3001   val Loss: 28.6807   time: 430.91s   best: 28.6807
2023-11-14 19:53:03,854:INFO:  Epoch 23/500:  train Loss: 27.0644   val Loss: 29.6493   time: 428.16s   best: 28.6807
2023-11-14 20:00:14,839:INFO:  Epoch 24/500:  train Loss: 26.8107   val Loss: 28.7477   time: 430.97s   best: 28.6807
2023-11-14 20:07:26,318:INFO:  Epoch 25/500:  train Loss: 26.5366   val Loss: 29.0045   time: 431.44s   best: 28.6807
2023-11-14 20:14:36,055:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 20:14:36,078:INFO:  Epoch 26/500:  train Loss: 26.4256   val Loss: 28.2390   time: 429.73s   best: 28.2390
2023-11-14 20:21:47,491:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 20:21:47,515:INFO:  Epoch 27/500:  train Loss: 26.2048   val Loss: 27.6024   time: 431.39s   best: 27.6024
2023-11-14 20:28:57,027:INFO:  Epoch 28/500:  train Loss: 26.1514   val Loss: 28.2271   time: 429.51s   best: 27.6024
2023-11-14 20:36:08,357:INFO:  Epoch 29/500:  train Loss: 26.4395   val Loss: 27.6055   time: 431.32s   best: 27.6024
2023-11-14 20:43:20,230:INFO:  Epoch 30/500:  train Loss: 26.0597   val Loss: 28.5104   time: 431.86s   best: 27.6024
2023-11-14 20:50:32,045:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 20:50:32,089:INFO:  Epoch 31/500:  train Loss: 25.4264   val Loss: 27.4886   time: 431.79s   best: 27.4886
2023-11-14 20:57:40,423:INFO:  Epoch 32/500:  train Loss: 25.3521   val Loss: 27.7964   time: 428.33s   best: 27.4886
2023-11-14 21:04:50,451:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 21:04:50,481:INFO:  Epoch 33/500:  train Loss: 25.3677   val Loss: 27.4569   time: 430.01s   best: 27.4569
2023-11-14 21:12:02,018:INFO:  Epoch 34/500:  train Loss: 24.8871   val Loss: 29.0310   time: 431.52s   best: 27.4569
2023-11-14 21:19:12,187:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 21:19:12,213:INFO:  Epoch 35/500:  train Loss: 24.9200   val Loss: 27.3204   time: 430.15s   best: 27.3204
2023-11-14 21:26:21,642:INFO:  Epoch 36/500:  train Loss: 24.5902   val Loss: 27.4523   time: 429.43s   best: 27.3204
2023-11-14 21:33:30,842:INFO:  Epoch 37/500:  train Loss: 24.3982   val Loss: 27.6912   time: 429.17s   best: 27.3204
2023-11-14 21:40:40,265:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 21:40:40,301:INFO:  Epoch 38/500:  train Loss: 24.3580   val Loss: 26.4968   time: 429.39s   best: 26.4968
2023-11-14 21:47:48,118:INFO:  Epoch 39/500:  train Loss: 24.6339   val Loss: 27.3760   time: 427.80s   best: 26.4968
2023-11-14 21:54:55,759:INFO:  Epoch 40/500:  train Loss: 24.2506   val Loss: 26.7733   time: 427.62s   best: 26.4968
2023-11-14 22:02:07,836:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 22:02:07,863:INFO:  Epoch 41/500:  train Loss: 24.1321   val Loss: 26.1524   time: 432.06s   best: 26.1524
2023-11-14 22:09:20,456:INFO:  Epoch 42/500:  train Loss: 23.7451   val Loss: 27.7849   time: 432.59s   best: 26.1524
2023-11-14 22:16:32,244:INFO:  Epoch 43/500:  train Loss: 23.9758   val Loss: 27.2007   time: 431.76s   best: 26.1524
2023-11-14 22:23:43,847:INFO:  Epoch 44/500:  train Loss: 23.5657   val Loss: 26.4633   time: 431.57s   best: 26.1524
2023-11-14 22:30:54,884:INFO:  Epoch 45/500:  train Loss: 23.4526   val Loss: 27.6259   time: 431.01s   best: 26.1524
2023-11-14 22:38:07,324:INFO:  Epoch 46/500:  train Loss: 23.4460   val Loss: 26.2501   time: 432.43s   best: 26.1524
2023-11-14 22:45:14,691:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 22:45:14,719:INFO:  Epoch 47/500:  train Loss: 23.6109   val Loss: 26.0171   time: 427.36s   best: 26.0171
2023-11-14 22:52:25,790:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 22:52:26,310:INFO:  Epoch 48/500:  train Loss: 23.1928   val Loss: 25.6339   time: 431.06s   best: 25.6339
2023-11-14 22:59:35,788:INFO:  Epoch 49/500:  train Loss: 23.3943   val Loss: 27.1902   time: 429.48s   best: 25.6339
2023-11-14 23:06:44,023:INFO:  Epoch 50/500:  train Loss: 23.3086   val Loss: 25.8037   time: 428.23s   best: 25.6339
2023-11-14 23:13:52,165:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 23:13:52,193:INFO:  Epoch 51/500:  train Loss: 23.0278   val Loss: 25.4184   time: 428.13s   best: 25.4184
2023-11-14 23:21:02,787:INFO:  Epoch 52/500:  train Loss: 22.8784   val Loss: 26.1823   time: 430.58s   best: 25.4184
2023-11-14 23:28:14,344:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 23:28:14,368:INFO:  Epoch 53/500:  train Loss: 23.0520   val Loss: 25.3815   time: 431.53s   best: 25.3815
2023-11-14 23:35:26,760:INFO:  Epoch 54/500:  train Loss: 22.9646   val Loss: 25.6502   time: 432.39s   best: 25.3815
2023-11-14 23:42:38,297:INFO:  Epoch 55/500:  train Loss: 22.5709   val Loss: 25.8819   time: 431.51s   best: 25.3815
2023-11-14 23:49:47,631:INFO:  Epoch 56/500:  train Loss: 22.5522   val Loss: 27.7231   time: 429.31s   best: 25.3815
2023-11-14 23:56:58,840:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-14 23:56:58,866:INFO:  Epoch 57/500:  train Loss: 22.4167   val Loss: 25.1789   time: 431.20s   best: 25.1789
2023-11-15 00:04:07,244:INFO:  Epoch 58/500:  train Loss: 22.3636   val Loss: 25.1906   time: 428.38s   best: 25.1789
2023-11-15 00:11:19,659:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 00:11:19,699:INFO:  Epoch 59/500:  train Loss: 22.2597   val Loss: 25.1723   time: 432.39s   best: 25.1723
2023-11-15 00:18:30,873:INFO:  Epoch 60/500:  train Loss: 22.2011   val Loss: 25.4146   time: 431.16s   best: 25.1723
2023-11-15 00:25:42,446:INFO:  Epoch 61/500:  train Loss: 22.2180   val Loss: 25.2257   time: 431.56s   best: 25.1723
2023-11-15 00:32:50,727:INFO:  Epoch 62/500:  train Loss: 22.0815   val Loss: 26.4912   time: 428.25s   best: 25.1723
2023-11-15 00:40:01,943:INFO:  Epoch 63/500:  train Loss: 22.0650   val Loss: 25.3986   time: 431.18s   best: 25.1723
2023-11-15 00:47:10,574:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 00:47:10,627:INFO:  Epoch 64/500:  train Loss: 21.9250   val Loss: 24.9413   time: 428.59s   best: 24.9413
2023-11-15 00:54:21,482:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 00:54:21,506:INFO:  Epoch 65/500:  train Loss: 22.5953   val Loss: 24.1643   time: 430.84s   best: 24.1643
2023-11-15 01:01:31,472:INFO:  Epoch 66/500:  train Loss: 21.9070   val Loss: 25.0663   time: 429.95s   best: 24.1643
2023-11-15 01:08:43,698:INFO:  Epoch 67/500:  train Loss: 21.7538   val Loss: 25.1676   time: 432.21s   best: 24.1643
2023-11-15 01:15:52,981:INFO:  Epoch 68/500:  train Loss: 21.8203   val Loss: 25.3551   time: 429.26s   best: 24.1643
2023-11-15 01:23:06,313:INFO:  Epoch 69/500:  train Loss: 21.6382   val Loss: 30.6095   time: 433.31s   best: 24.1643
2023-11-15 01:30:19,179:INFO:  Epoch 70/500:  train Loss: 21.7643   val Loss: 25.7107   time: 432.86s   best: 24.1643
2023-11-15 01:37:28,786:INFO:  Epoch 71/500:  train Loss: 22.2433   val Loss: 24.2100   time: 429.58s   best: 24.1643
2023-11-15 01:44:36,464:INFO:  Epoch 72/500:  train Loss: 21.5174   val Loss: 24.7366   time: 427.64s   best: 24.1643
2023-11-15 01:51:43,894:INFO:  Epoch 73/500:  train Loss: 21.4192   val Loss: 24.8494   time: 427.42s   best: 24.1643
2023-11-15 01:58:52,245:INFO:  Epoch 74/500:  train Loss: 21.8608   val Loss: 24.4928   time: 428.33s   best: 24.1643
2023-11-15 02:06:03,970:INFO:  Epoch 75/500:  train Loss: 21.4942   val Loss: 24.2173   time: 431.71s   best: 24.1643
2023-11-15 02:13:15,006:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 02:13:15,032:INFO:  Epoch 76/500:  train Loss: 21.3148   val Loss: 24.0138   time: 431.00s   best: 24.0138
2023-11-15 02:20:26,253:INFO:  Epoch 77/500:  train Loss: 21.2895   val Loss: 24.5186   time: 431.21s   best: 24.0138
2023-11-15 02:27:37,145:INFO:  Epoch 78/500:  train Loss: 21.2755   val Loss: 24.5315   time: 430.89s   best: 24.0138
2023-11-15 02:34:48,524:INFO:  Epoch 79/500:  train Loss: 21.2207   val Loss: 26.1359   time: 431.36s   best: 24.0138
2023-11-15 02:41:59,466:INFO:  Epoch 80/500:  train Loss: 21.1732   val Loss: 24.6951   time: 430.92s   best: 24.0138
2023-11-15 02:49:08,229:INFO:  Epoch 81/500:  train Loss: 21.0664   val Loss: 24.3143   time: 428.74s   best: 24.0138
2023-11-15 02:56:17,535:INFO:  Epoch 82/500:  train Loss: 21.1510   val Loss: 24.4827   time: 429.28s   best: 24.0138
2023-11-15 03:03:29,330:INFO:  Epoch 83/500:  train Loss: 21.0031   val Loss: 24.6114   time: 431.77s   best: 24.0138
2023-11-15 03:10:38,267:INFO:  Epoch 84/500:  train Loss: 20.9265   val Loss: 24.7469   time: 428.92s   best: 24.0138
2023-11-15 03:17:51,012:INFO:  Epoch 85/500:  train Loss: 20.9088   val Loss: 24.4331   time: 432.72s   best: 24.0138
2023-11-15 03:24:59,783:INFO:  Epoch 86/500:  train Loss: 20.9927   val Loss: 24.2972   time: 428.75s   best: 24.0138
2023-11-15 03:32:11,111:INFO:  Epoch 87/500:  train Loss: 20.7562   val Loss: 24.6791   time: 431.32s   best: 24.0138
2023-11-15 03:39:22,603:INFO:  Epoch 88/500:  train Loss: 20.8219   val Loss: 24.3990   time: 431.47s   best: 24.0138
2023-11-15 03:46:34,160:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 03:46:34,199:INFO:  Epoch 89/500:  train Loss: 20.7814   val Loss: 24.0030   time: 431.54s   best: 24.0030
2023-11-15 03:53:45,570:INFO:  Epoch 90/500:  train Loss: 20.7865   val Loss: 24.4349   time: 431.36s   best: 24.0030
2023-11-15 04:00:53,310:INFO:  Epoch 91/500:  train Loss: 20.7925   val Loss: 24.2972   time: 427.73s   best: 24.0030
2023-11-15 04:08:03,386:INFO:  Epoch 92/500:  train Loss: 20.6656   val Loss: 24.2854   time: 430.05s   best: 24.0030
2023-11-15 04:15:11,523:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 04:15:11,556:INFO:  Epoch 93/500:  train Loss: 20.7391   val Loss: 23.9718   time: 428.13s   best: 23.9718
2023-11-15 04:22:22,402:INFO:  Epoch 94/500:  train Loss: 20.6584   val Loss: 24.0538   time: 430.83s   best: 23.9718
2023-11-15 04:29:31,972:INFO:  Epoch 95/500:  train Loss: 20.5571   val Loss: 24.2303   time: 429.56s   best: 23.9718
2023-11-15 04:36:36,918:INFO:  Epoch 96/500:  train Loss: 20.5334   val Loss: 24.6396   time: 424.93s   best: 23.9718
2023-11-15 04:43:42,255:INFO:  Epoch 97/500:  train Loss: 20.6986   val Loss: 24.3569   time: 425.33s   best: 23.9718
2023-11-15 04:50:52,201:INFO:  Epoch 98/500:  train Loss: 20.5062   val Loss: 24.0657   time: 429.94s   best: 23.9718
2023-11-15 04:57:59,112:INFO:  Epoch 99/500:  train Loss: 20.4046   val Loss: 24.7467   time: 426.90s   best: 23.9718
2023-11-15 05:05:08,213:INFO:  Epoch 100/500:  train Loss: 20.3424   val Loss: 24.1921   time: 429.09s   best: 23.9718
2023-11-15 05:12:15,490:INFO:  Epoch 101/500:  train Loss: 20.3672   val Loss: 24.1293   time: 427.27s   best: 23.9718
2023-11-15 05:19:21,838:INFO:  Epoch 102/500:  train Loss: 20.2726   val Loss: 24.4994   time: 426.33s   best: 23.9718
2023-11-15 05:26:27,427:INFO:  Epoch 103/500:  train Loss: 20.3269   val Loss: 24.3363   time: 425.55s   best: 23.9718
2023-11-15 05:33:34,791:INFO:  Epoch 104/500:  train Loss: 20.1943   val Loss: 24.0023   time: 427.33s   best: 23.9718
2023-11-15 05:40:40,469:INFO:  Epoch 105/500:  train Loss: 20.2441   val Loss: 25.0641   time: 425.67s   best: 23.9718
2023-11-15 05:47:47,605:INFO:  Epoch 106/500:  train Loss: 20.6000   val Loss: 24.0012   time: 427.11s   best: 23.9718
2023-11-15 05:54:57,421:INFO:  Epoch 107/500:  train Loss: 20.2628   val Loss: 24.1434   time: 429.79s   best: 23.9718
2023-11-15 06:02:05,803:INFO:  Epoch 108/500:  train Loss: 20.1624   val Loss: 24.0779   time: 428.36s   best: 23.9718
2023-11-15 06:09:17,441:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 06:09:17,466:INFO:  Epoch 109/500:  train Loss: 20.2102   val Loss: 23.5582   time: 431.62s   best: 23.5582
2023-11-15 06:16:28,356:INFO:  Epoch 110/500:  train Loss: 20.3103   val Loss: 23.6452   time: 430.89s   best: 23.5582
2023-11-15 06:23:38,747:INFO:  Epoch 111/500:  train Loss: 20.1653   val Loss: 24.0716   time: 430.36s   best: 23.5582
2023-11-15 06:30:49,233:INFO:  Epoch 112/500:  train Loss: 20.3133   val Loss: 23.9987   time: 430.47s   best: 23.5582
2023-11-15 06:37:55,931:INFO:  Epoch 113/500:  train Loss: 20.0347   val Loss: 24.0741   time: 426.69s   best: 23.5582
2023-11-15 06:45:03,408:INFO:  Epoch 114/500:  train Loss: 20.0155   val Loss: 24.0308   time: 427.47s   best: 23.5582
2023-11-15 06:52:13,298:INFO:  Epoch 115/500:  train Loss: 19.9725   val Loss: 24.3333   time: 429.88s   best: 23.5582
2023-11-15 06:59:23,715:INFO:  Epoch 116/500:  train Loss: 20.1217   val Loss: 23.8816   time: 430.39s   best: 23.5582
2023-11-15 07:06:33,329:INFO:  Epoch 117/500:  train Loss: 20.0067   val Loss: 24.1414   time: 429.59s   best: 23.5582
2023-11-15 07:13:43,351:INFO:  Epoch 118/500:  train Loss: 20.0887   val Loss: 23.7399   time: 430.01s   best: 23.5582
2023-11-15 07:20:51,680:INFO:  Epoch 119/500:  train Loss: 19.9485   val Loss: 23.8031   time: 428.32s   best: 23.5582
2023-11-15 07:27:57,769:INFO:  Epoch 120/500:  train Loss: 20.1516   val Loss: 24.0880   time: 426.06s   best: 23.5582
2023-11-15 07:35:07,411:INFO:  Epoch 121/500:  train Loss: 19.7895   val Loss: 23.9850   time: 429.63s   best: 23.5582
2023-11-15 07:42:14,730:INFO:  Epoch 122/500:  train Loss: 20.0433   val Loss: 23.8932   time: 427.29s   best: 23.5582
2023-11-15 07:49:24,597:INFO:  Epoch 123/500:  train Loss: 19.9670   val Loss: 25.0560   time: 429.83s   best: 23.5582
2023-11-15 07:56:32,932:INFO:  Epoch 124/500:  train Loss: 19.8854   val Loss: 24.0006   time: 428.32s   best: 23.5582
2023-11-15 08:03:42,885:INFO:  Epoch 125/500:  train Loss: 19.9668   val Loss: 23.6968   time: 429.94s   best: 23.5582
2023-11-15 08:10:51,055:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 08:10:51,094:INFO:  Epoch 126/500:  train Loss: 20.1906   val Loss: 23.4816   time: 428.14s   best: 23.4816
2023-11-15 08:17:56,753:INFO:  Epoch 127/500:  train Loss: 19.8891   val Loss: 23.5644   time: 425.65s   best: 23.4816
2023-11-15 08:25:06,480:INFO:  Epoch 128/500:  train Loss: 19.8373   val Loss: 24.9595   time: 429.71s   best: 23.4816
2023-11-15 08:32:16,056:INFO:  Epoch 129/500:  train Loss: 19.6403   val Loss: 23.9816   time: 429.55s   best: 23.4816
2023-11-15 08:39:21,841:INFO:  Epoch 130/500:  train Loss: 19.6569   val Loss: 25.5298   time: 425.78s   best: 23.4816
2023-11-15 08:46:27,789:INFO:  Epoch 131/500:  train Loss: 19.6119   val Loss: 23.9727   time: 425.93s   best: 23.4816
2023-11-15 08:53:38,085:INFO:  Epoch 132/500:  train Loss: 19.7185   val Loss: 23.9311   time: 430.29s   best: 23.4816
2023-11-15 09:00:50,162:INFO:  Epoch 133/500:  train Loss: 19.7755   val Loss: 24.2614   time: 432.05s   best: 23.4816
2023-11-15 09:08:01,292:INFO:  Epoch 134/500:  train Loss: 19.8770   val Loss: 24.0103   time: 431.10s   best: 23.4816
2023-11-15 09:15:14,160:INFO:  Epoch 135/500:  train Loss: 19.6301   val Loss: 23.8352   time: 432.84s   best: 23.4816
2023-11-15 09:22:22,080:INFO:  Epoch 136/500:  train Loss: 19.6805   val Loss: 24.1449   time: 427.90s   best: 23.4816
2023-11-15 09:29:34,171:INFO:  Epoch 137/500:  train Loss: 19.8718   val Loss: 24.3169   time: 432.08s   best: 23.4816
2023-11-15 09:36:45,549:INFO:  Epoch 138/500:  train Loss: 19.9424   val Loss: 23.9980   time: 431.36s   best: 23.4816
2023-11-15 09:43:54,916:INFO:  Epoch 139/500:  train Loss: 19.5254   val Loss: 23.8225   time: 429.35s   best: 23.4816
2023-11-15 09:51:01,955:INFO:  Epoch 140/500:  train Loss: 19.4902   val Loss: 24.1122   time: 427.01s   best: 23.4816
2023-11-15 09:58:13,682:INFO:  Epoch 141/500:  train Loss: 19.5429   val Loss: 23.6093   time: 431.72s   best: 23.4816
2023-11-15 10:05:21,372:INFO:  Epoch 142/500:  train Loss: 19.3815   val Loss: 24.5745   time: 427.65s   best: 23.4816
2023-11-15 10:12:29,075:INFO:  Epoch 143/500:  train Loss: 19.4953   val Loss: 23.7613   time: 427.68s   best: 23.4816
2023-11-15 10:19:36,495:INFO:  Epoch 144/500:  train Loss: 19.3956   val Loss: 28.5483   time: 427.40s   best: 23.4816
2023-11-15 10:26:43,974:INFO:  Epoch 145/500:  train Loss: 19.3694   val Loss: 24.1849   time: 427.45s   best: 23.4816
2023-11-15 10:33:53,377:INFO:  Epoch 146/500:  train Loss: 19.5898   val Loss: 23.7713   time: 429.38s   best: 23.4816
2023-11-15 10:41:03,159:INFO:  Epoch 147/500:  train Loss: 19.4171   val Loss: 23.8453   time: 429.76s   best: 23.4816
2023-11-15 10:48:15,231:INFO:  Epoch 148/500:  train Loss: 19.4348   val Loss: 23.7843   time: 432.05s   best: 23.4816
2023-11-15 10:55:26,137:INFO:  Epoch 149/500:  train Loss: 19.3223   val Loss: 23.7251   time: 430.88s   best: 23.4816
2023-11-15 11:02:33,548:INFO:  Epoch 150/500:  train Loss: 19.3582   val Loss: 23.6620   time: 427.38s   best: 23.4816
2023-11-15 11:09:42,475:INFO:  Epoch 151/500:  train Loss: 19.7122   val Loss: 23.9934   time: 428.92s   best: 23.4816
2023-11-15 11:16:53,498:INFO:  Epoch 152/500:  train Loss: 19.2547   val Loss: 23.8201   time: 431.00s   best: 23.4816
2023-11-15 11:24:04,634:INFO:  Epoch 153/500:  train Loss: 19.3373   val Loss: 23.8760   time: 431.13s   best: 23.4816
2023-11-15 11:31:15,164:INFO:  Epoch 154/500:  train Loss: 19.2514   val Loss: 23.6370   time: 430.51s   best: 23.4816
2023-11-15 11:38:22,070:INFO:  Epoch 155/500:  train Loss: 19.2167   val Loss: 23.9893   time: 426.87s   best: 23.4816
2023-11-15 11:45:29,913:INFO:  Epoch 156/500:  train Loss: 19.2818   val Loss: 23.8199   time: 427.82s   best: 23.4816
2023-11-15 11:52:40,228:INFO:  Epoch 157/500:  train Loss: 19.0550   val Loss: 24.2218   time: 430.31s   best: 23.4816
2023-11-15 11:59:47,431:INFO:  Epoch 158/500:  train Loss: 19.1052   val Loss: 23.9919   time: 427.20s   best: 23.4816
2023-11-15 12:06:58,786:INFO:  Epoch 159/500:  train Loss: 19.1069   val Loss: 23.6972   time: 431.35s   best: 23.4816
2023-11-15 12:14:08,315:INFO:  Epoch 160/500:  train Loss: 19.1345   val Loss: 23.9704   time: 429.52s   best: 23.4816
2023-11-15 12:21:19,710:INFO:  Epoch 161/500:  train Loss: 19.1563   val Loss: 23.9965   time: 431.36s   best: 23.4816
2023-11-15 12:28:30,688:INFO:  Epoch 162/500:  train Loss: 19.0944   val Loss: 25.7915   time: 430.97s   best: 23.4816
2023-11-15 12:35:43,342:INFO:  Epoch 163/500:  train Loss: 19.0867   val Loss: 23.8915   time: 432.64s   best: 23.4816
2023-11-15 12:42:55,661:INFO:  Epoch 164/500:  train Loss: 19.1993   val Loss: 23.7620   time: 432.28s   best: 23.4816
2023-11-15 12:50:05,256:INFO:  Epoch 165/500:  train Loss: 19.1003   val Loss: 24.0780   time: 429.59s   best: 23.4816
2023-11-15 12:57:17,721:INFO:  Epoch 166/500:  train Loss: 19.1480   val Loss: 24.0637   time: 432.42s   best: 23.4816
2023-11-15 13:04:26,369:INFO:  Epoch 167/500:  train Loss: 18.9994   val Loss: 24.1019   time: 428.61s   best: 23.4816
2023-11-15 13:11:35,752:INFO:  Epoch 168/500:  train Loss: 19.3627   val Loss: 24.1759   time: 429.36s   best: 23.4816
2023-11-15 13:18:44,001:INFO:  Epoch 169/500:  train Loss: 19.1063   val Loss: 24.3776   time: 428.21s   best: 23.4816
2023-11-15 13:25:54,298:INFO:  Epoch 170/500:  train Loss: 19.0608   val Loss: 23.6705   time: 430.27s   best: 23.4816
2023-11-15 13:33:05,029:INFO:  Epoch 171/500:  train Loss: 18.9430   val Loss: 24.0608   time: 430.69s   best: 23.4816
2023-11-15 13:40:15,575:INFO:  Epoch 172/500:  train Loss: 19.1673   val Loss: 24.0243   time: 430.53s   best: 23.4816
2023-11-15 13:47:26,564:INFO:  Epoch 173/500:  train Loss: 19.3995   val Loss: 23.9433   time: 430.97s   best: 23.4816
2023-11-15 13:54:37,533:INFO:  Epoch 174/500:  train Loss: 18.9132   val Loss: 24.0131   time: 430.96s   best: 23.4816
2023-11-15 14:01:49,157:INFO:  Epoch 175/500:  train Loss: 19.0896   val Loss: 24.1278   time: 431.60s   best: 23.4816
2023-11-15 14:08:57,285:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 14:08:57,340:INFO:  Epoch 176/500:  train Loss: 18.9173   val Loss: 23.4068   time: 428.08s   best: 23.4068
2023-11-15 14:16:08,636:INFO:  Epoch 177/500:  train Loss: 18.9118   val Loss: 24.4470   time: 431.27s   best: 23.4068
2023-11-15 14:23:20,238:INFO:  Epoch 178/500:  train Loss: 18.9710   val Loss: 24.0544   time: 431.57s   best: 23.4068
2023-11-15 14:30:33,208:INFO:  Epoch 179/500:  train Loss: 19.0926   val Loss: 24.0496   time: 432.95s   best: 23.4068
2023-11-15 14:37:42,324:INFO:  Epoch 180/500:  train Loss: 18.9335   val Loss: 23.6979   time: 429.08s   best: 23.4068
2023-11-15 14:44:55,388:INFO:  Epoch 181/500:  train Loss: 18.9936   val Loss: 23.5265   time: 433.03s   best: 23.4068
2023-11-15 14:52:07,272:INFO:  Epoch 182/500:  train Loss: 18.8485   val Loss: 23.7846   time: 431.85s   best: 23.4068
2023-11-15 14:59:19,749:INFO:  Epoch 183/500:  train Loss: 18.8150   val Loss: 23.6230   time: 432.45s   best: 23.4068
2023-11-15 15:06:31,265:INFO:  Epoch 184/500:  train Loss: 18.8599   val Loss: 23.5500   time: 431.51s   best: 23.4068
2023-11-15 15:13:42,272:INFO:  Epoch 185/500:  train Loss: 18.8845   val Loss: 24.5825   time: 430.98s   best: 23.4068
2023-11-15 15:20:50,710:INFO:  Epoch 186/500:  train Loss: 18.9436   val Loss: 23.4802   time: 428.40s   best: 23.4068
2023-11-15 15:28:00,952:INFO:  Epoch 187/500:  train Loss: 18.7705   val Loss: 24.9150   time: 430.21s   best: 23.4068
2023-11-15 15:35:13,058:INFO:  Epoch 188/500:  train Loss: 18.7638   val Loss: 24.6893   time: 432.08s   best: 23.4068
2023-11-15 15:42:24,974:INFO:  Epoch 189/500:  train Loss: 18.9215   val Loss: 23.7826   time: 431.89s   best: 23.4068
2023-11-15 15:49:35,724:INFO:  Epoch 190/500:  train Loss: 18.8154   val Loss: 24.3233   time: 430.73s   best: 23.4068
2023-11-15 15:56:44,435:INFO:  Epoch 191/500:  train Loss: 18.7785   val Loss: 24.0555   time: 428.70s   best: 23.4068
2023-11-15 16:03:52,924:INFO:  Epoch 192/500:  train Loss: 19.1771   val Loss: 23.4622   time: 428.46s   best: 23.4068
2023-11-15 16:11:01,699:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 16:11:01,740:INFO:  Epoch 193/500:  train Loss: 19.1459   val Loss: 23.2870   time: 428.75s   best: 23.2870
2023-11-15 16:18:11,099:INFO:  Epoch 194/500:  train Loss: 18.7249   val Loss: 23.6956   time: 429.35s   best: 23.2870
2023-11-15 16:25:25,111:INFO:  Epoch 195/500:  train Loss: 18.8744   val Loss: 24.1001   time: 434.00s   best: 23.2870
2023-11-15 16:32:38,466:INFO:  Epoch 196/500:  train Loss: 18.9370   val Loss: 23.9350   time: 433.33s   best: 23.2870
2023-11-15 16:39:47,846:INFO:  Epoch 197/500:  train Loss: 18.9413   val Loss: 23.8155   time: 429.36s   best: 23.2870
2023-11-15 16:46:56,095:INFO:  Epoch 198/500:  train Loss: 18.7446   val Loss: 24.1144   time: 428.22s   best: 23.2870
2023-11-15 16:54:04,333:INFO:  Epoch 199/500:  train Loss: 18.7094   val Loss: 24.0704   time: 428.21s   best: 23.2870
2023-11-15 17:01:13,160:INFO:  Epoch 200/500:  train Loss: 19.0174   val Loss: 23.7889   time: 428.81s   best: 23.2870
2023-11-15 17:08:21,854:INFO:  Epoch 201/500:  train Loss: 18.6061   val Loss: 24.0086   time: 428.69s   best: 23.2870
2023-11-15 17:15:32,672:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-15 17:15:32,700:INFO:  Epoch 202/500:  train Loss: 18.6744   val Loss: 23.1361   time: 430.81s   best: 23.1361
2023-11-15 17:22:41,573:INFO:  Epoch 203/500:  train Loss: 18.5783   val Loss: 23.6011   time: 428.86s   best: 23.1361
2023-11-15 17:29:53,467:INFO:  Epoch 204/500:  train Loss: 18.6340   val Loss: 24.1673   time: 431.87s   best: 23.1361
2023-11-15 17:37:06,784:INFO:  Epoch 205/500:  train Loss: 18.6879   val Loss: 23.6397   time: 433.29s   best: 23.1361
2023-11-15 17:44:19,463:INFO:  Epoch 206/500:  train Loss: 18.6290   val Loss: 24.0968   time: 432.65s   best: 23.1361
2023-11-15 17:51:29,031:INFO:  Epoch 207/500:  train Loss: 18.6885   val Loss: 23.9283   time: 429.55s   best: 23.1361
2023-11-15 17:58:41,367:INFO:  Epoch 208/500:  train Loss: 18.5870   val Loss: 23.1737   time: 432.33s   best: 23.1361
2023-11-15 18:05:50,551:INFO:  Epoch 209/500:  train Loss: 18.5083   val Loss: 23.7180   time: 429.16s   best: 23.1361
2023-11-15 18:13:02,836:INFO:  Epoch 210/500:  train Loss: 18.5097   val Loss: 23.9914   time: 432.27s   best: 23.1361
2023-11-15 18:20:12,110:INFO:  Epoch 211/500:  train Loss: 18.4696   val Loss: 23.6866   time: 429.25s   best: 23.1361
2023-11-15 18:27:22,510:INFO:  Epoch 212/500:  train Loss: 18.7850   val Loss: 24.4242   time: 430.37s   best: 23.1361
2023-11-15 18:34:31,119:INFO:  Epoch 213/500:  train Loss: 18.6513   val Loss: 23.4158   time: 428.58s   best: 23.1361
2023-11-15 18:41:44,382:INFO:  Epoch 214/500:  train Loss: 18.6137   val Loss: 24.1842   time: 433.26s   best: 23.1361
2023-11-15 18:48:57,288:INFO:  Epoch 215/500:  train Loss: 18.5909   val Loss: 23.8246   time: 432.88s   best: 23.1361
2023-11-15 18:56:06,659:INFO:  Epoch 216/500:  train Loss: 18.5626   val Loss: 24.1640   time: 429.36s   best: 23.1361
2023-11-15 19:03:18,894:INFO:  Epoch 217/500:  train Loss: 18.5052   val Loss: 23.6255   time: 432.21s   best: 23.1361
2023-11-15 19:10:30,524:INFO:  Epoch 218/500:  train Loss: 18.4243   val Loss: 23.9757   time: 431.62s   best: 23.1361
2023-11-15 19:17:42,786:INFO:  Epoch 219/500:  train Loss: 18.5323   val Loss: 23.4918   time: 432.24s   best: 23.1361
2023-11-15 19:24:56,065:INFO:  Epoch 220/500:  train Loss: 18.7581   val Loss: 24.3707   time: 433.27s   best: 23.1361
2023-11-15 19:32:04,779:INFO:  Epoch 221/500:  train Loss: 18.9603   val Loss: 23.3038   time: 428.69s   best: 23.1361
2023-11-15 19:39:16,110:INFO:  Epoch 222/500:  train Loss: 18.3662   val Loss: 23.5966   time: 431.32s   best: 23.1361
2023-11-15 19:46:29,075:INFO:  Epoch 223/500:  train Loss: 18.4280   val Loss: 23.4526   time: 432.94s   best: 23.1361
2023-11-15 19:53:42,679:INFO:  Epoch 224/500:  train Loss: 18.4081   val Loss: 23.7757   time: 433.59s   best: 23.1361
2023-11-15 20:00:55,422:INFO:  Epoch 225/500:  train Loss: 18.4202   val Loss: 23.6594   time: 432.73s   best: 23.1361
2023-11-15 20:08:04,299:INFO:  Epoch 226/500:  train Loss: 18.5544   val Loss: 24.3256   time: 428.84s   best: 23.1361
2023-11-15 20:15:16,177:INFO:  Epoch 227/500:  train Loss: 18.6925   val Loss: 23.6362   time: 431.86s   best: 23.1361
2023-11-15 20:22:28,216:INFO:  Epoch 228/500:  train Loss: 18.3995   val Loss: 23.7045   time: 432.02s   best: 23.1361
2023-11-15 20:29:38,951:INFO:  Epoch 229/500:  train Loss: 18.5331   val Loss: 23.5424   time: 430.69s   best: 23.1361
2023-11-15 20:36:48,843:INFO:  Epoch 230/500:  train Loss: 18.6437   val Loss: 23.4062   time: 429.86s   best: 23.1361
2023-11-15 20:44:01,663:INFO:  Epoch 231/500:  train Loss: 18.4131   val Loss: 23.2168   time: 432.80s   best: 23.1361
2023-11-15 20:51:11,196:INFO:  Epoch 232/500:  train Loss: 18.3097   val Loss: 24.7097   time: 429.53s   best: 23.1361
2023-11-15 20:58:22,075:INFO:  Epoch 233/500:  train Loss: 18.3792   val Loss: 24.0342   time: 430.87s   best: 23.1361
2023-11-15 21:05:33,670:INFO:  Epoch 234/500:  train Loss: 18.2406   val Loss: 23.9275   time: 431.57s   best: 23.1361
2023-11-15 21:12:47,125:INFO:  Epoch 235/500:  train Loss: 18.3121   val Loss: 23.8866   time: 433.43s   best: 23.1361
2023-11-15 21:19:55,993:INFO:  Epoch 236/500:  train Loss: 18.2698   val Loss: 23.6475   time: 428.86s   best: 23.1361
2023-11-15 21:27:09,745:INFO:  Epoch 237/500:  train Loss: 18.2825   val Loss: 23.5972   time: 433.73s   best: 23.1361
2023-11-15 21:34:18,698:INFO:  Epoch 238/500:  train Loss: 18.3714   val Loss: 24.4301   time: 428.93s   best: 23.1361
2023-11-15 21:41:30,416:INFO:  Epoch 239/500:  train Loss: 18.3594   val Loss: 23.4168   time: 431.71s   best: 23.1361
2023-11-15 21:48:42,026:INFO:  Epoch 240/500:  train Loss: 18.2436   val Loss: 23.8095   time: 431.60s   best: 23.1361
2023-11-15 21:55:50,063:INFO:  Epoch 241/500:  train Loss: 18.2991   val Loss: 23.2750   time: 427.99s   best: 23.1361
2023-11-15 22:02:58,506:INFO:  Epoch 242/500:  train Loss: 18.1876   val Loss: 23.5017   time: 428.41s   best: 23.1361
2023-11-15 22:10:07,369:INFO:  Epoch 243/500:  train Loss: 18.1884   val Loss: 23.5041   time: 428.84s   best: 23.1361
2023-11-15 22:17:15,728:INFO:  Epoch 244/500:  train Loss: 18.2111   val Loss: 23.4528   time: 428.33s   best: 23.1361
2023-11-15 22:24:23,666:INFO:  Epoch 245/500:  train Loss: 18.2476   val Loss: 23.2854   time: 427.92s   best: 23.1361
2023-11-15 22:31:34,292:INFO:  Epoch 246/500:  train Loss: 18.5617   val Loss: 23.4503   time: 430.62s   best: 23.1361
2023-11-15 22:38:43,121:INFO:  Epoch 247/500:  train Loss: 18.1900   val Loss: 23.7878   time: 428.80s   best: 23.1361
2023-11-15 22:45:57,216:INFO:  Epoch 248/500:  train Loss: 18.3490   val Loss: 23.7049   time: 434.07s   best: 23.1361
2023-11-15 22:53:05,937:INFO:  Epoch 249/500:  train Loss: 18.6012   val Loss: 23.8790   time: 428.71s   best: 23.1361
2023-11-15 23:00:18,293:INFO:  Epoch 250/500:  train Loss: 18.1152   val Loss: 23.6648   time: 432.34s   best: 23.1361
2023-11-15 23:07:28,274:INFO:  Epoch 251/500:  train Loss: 18.1568   val Loss: 23.7003   time: 429.96s   best: 23.1361
2023-11-15 23:14:36,303:INFO:  Epoch 252/500:  train Loss: 18.2340   val Loss: 23.5346   time: 428.00s   best: 23.1361
2023-11-15 23:21:47,858:INFO:  Epoch 253/500:  train Loss: 18.3397   val Loss: 24.1091   time: 431.54s   best: 23.1361
2023-11-15 23:29:00,769:INFO:  Epoch 254/500:  train Loss: 18.2157   val Loss: 24.0348   time: 432.90s   best: 23.1361
2023-11-15 23:36:11,446:INFO:  Epoch 255/500:  train Loss: 18.1452   val Loss: 23.6978   time: 430.66s   best: 23.1361
2023-11-15 23:43:24,706:INFO:  Epoch 256/500:  train Loss: 18.2036   val Loss: 23.9872   time: 433.24s   best: 23.1361
2023-11-15 23:50:35,945:INFO:  Epoch 257/500:  train Loss: 18.0667   val Loss: 25.6795   time: 431.21s   best: 23.1361
2023-11-15 23:57:48,914:INFO:  Epoch 258/500:  train Loss: 18.1461   val Loss: 23.7286   time: 432.96s   best: 23.1361
2023-11-16 00:05:01,966:INFO:  Epoch 259/500:  train Loss: 18.1566   val Loss: 23.3881   time: 433.03s   best: 23.1361
2023-11-16 00:12:11,456:INFO:  Epoch 260/500:  train Loss: 18.1331   val Loss: 27.7229   time: 429.46s   best: 23.1361
2023-11-16 00:19:24,301:INFO:  Epoch 261/500:  train Loss: 18.3166   val Loss: 23.3850   time: 432.82s   best: 23.1361
2023-11-16 00:26:34,733:INFO:  Epoch 262/500:  train Loss: 18.0833   val Loss: 23.4739   time: 430.42s   best: 23.1361
2023-11-16 00:33:43,032:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 00:33:43,079:INFO:  Epoch 263/500:  train Loss: 18.3624   val Loss: 23.1265   time: 428.29s   best: 23.1265
2023-11-16 00:40:54,873:INFO:  Epoch 264/500:  train Loss: 18.1001   val Loss: 23.8082   time: 431.79s   best: 23.1265
2023-11-16 00:48:03,850:INFO:  Epoch 265/500:  train Loss: 18.3080   val Loss: 23.2758   time: 428.95s   best: 23.1265
2023-11-16 00:55:16,508:INFO:  Epoch 266/500:  train Loss: 18.1507   val Loss: 23.9980   time: 432.65s   best: 23.1265
2023-11-16 01:02:24,706:INFO:  Epoch 267/500:  train Loss: 18.2901   val Loss: 23.6763   time: 428.19s   best: 23.1265
2023-11-16 01:09:36,762:INFO:  Epoch 268/500:  train Loss: 18.0814   val Loss: 23.6889   time: 432.03s   best: 23.1265
2023-11-16 01:16:48,015:INFO:  Epoch 269/500:  train Loss: 18.0943   val Loss: 23.4229   time: 431.23s   best: 23.1265
2023-11-16 01:23:59,227:INFO:  Epoch 270/500:  train Loss: 18.1583   val Loss: 24.2636   time: 431.20s   best: 23.1265
2023-11-16 01:31:08,436:INFO:  Epoch 271/500:  train Loss: 18.1453   val Loss: 23.3592   time: 429.19s   best: 23.1265
2023-11-16 01:38:20,061:INFO:  Epoch 272/500:  train Loss: 18.1427   val Loss: 23.2383   time: 431.59s   best: 23.1265
2023-11-16 01:45:32,396:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 01:45:32,426:INFO:  Epoch 273/500:  train Loss: 18.2001   val Loss: 23.0716   time: 432.29s   best: 23.0716
2023-11-16 01:52:44,231:INFO:  Epoch 274/500:  train Loss: 17.9599   val Loss: 23.7543   time: 431.80s   best: 23.0716
2023-11-16 01:59:57,282:INFO:  Epoch 275/500:  train Loss: 17.9506   val Loss: 23.5089   time: 433.02s   best: 23.0716
2023-11-16 02:07:11,133:INFO:  Epoch 276/500:  train Loss: 18.4167   val Loss: 23.7993   time: 433.82s   best: 23.0716
2023-11-16 02:14:20,448:INFO:  Epoch 277/500:  train Loss: 18.0281   val Loss: 23.6055   time: 429.29s   best: 23.0716
2023-11-16 02:21:32,593:INFO:  Epoch 278/500:  train Loss: 18.0450   val Loss: 23.4455   time: 432.13s   best: 23.0716
2023-11-16 02:28:44,515:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 02:28:44,547:INFO:  Epoch 279/500:  train Loss: 18.1928   val Loss: 22.9032   time: 431.88s   best: 22.9032
2023-11-16 02:35:53,050:INFO:  Epoch 280/500:  train Loss: 18.1425   val Loss: 23.9248   time: 428.49s   best: 22.9032
2023-11-16 02:43:04,052:INFO:  Epoch 281/500:  train Loss: 18.0461   val Loss: 41.8564   time: 430.98s   best: 22.9032
2023-11-16 02:50:12,940:INFO:  Epoch 282/500:  train Loss: 18.3954   val Loss: 23.5313   time: 428.86s   best: 22.9032
2023-11-16 02:57:21,240:INFO:  Epoch 283/500:  train Loss: 18.0687   val Loss: 23.0026   time: 428.26s   best: 22.9032
2023-11-16 03:04:29,001:INFO:  Epoch 284/500:  train Loss: 18.1798   val Loss: 24.2817   time: 427.73s   best: 22.9032
2023-11-16 03:11:37,755:INFO:  Epoch 285/500:  train Loss: 17.9602   val Loss: 23.5465   time: 428.75s   best: 22.9032
2023-11-16 03:18:48,358:INFO:  Epoch 286/500:  train Loss: 18.0528   val Loss: 23.4618   time: 430.57s   best: 22.9032
2023-11-16 03:25:56,833:INFO:  Epoch 287/500:  train Loss: 17.9430   val Loss: 23.3952   time: 428.46s   best: 22.9032
2023-11-16 03:33:06,115:INFO:  Epoch 288/500:  train Loss: 18.0332   val Loss: 23.4279   time: 429.27s   best: 22.9032
2023-11-16 03:40:18,732:INFO:  Epoch 289/500:  train Loss: 18.0108   val Loss: 23.0105   time: 432.60s   best: 22.9032
2023-11-16 03:47:31,038:INFO:  Epoch 290/500:  train Loss: 17.9295   val Loss: 23.2835   time: 432.28s   best: 22.9032
2023-11-16 03:54:43,136:INFO:  Epoch 291/500:  train Loss: 17.9075   val Loss: 23.7482   time: 432.08s   best: 22.9032
2023-11-16 04:01:56,178:INFO:  Epoch 292/500:  train Loss: 17.9777   val Loss: 23.3208   time: 433.01s   best: 22.9032
2023-11-16 04:09:03,989:INFO:  Epoch 293/500:  train Loss: 18.0702   val Loss: 24.2808   time: 427.79s   best: 22.9032
2023-11-16 04:16:14,485:INFO:  Epoch 294/500:  train Loss: 18.2261   val Loss: 24.4798   time: 430.47s   best: 22.9032
2023-11-16 04:23:23,458:INFO:  Epoch 295/500:  train Loss: 17.9119   val Loss: 23.4302   time: 428.95s   best: 22.9032
2023-11-16 04:30:35,258:INFO:  Epoch 296/500:  train Loss: 17.9407   val Loss: 23.3267   time: 431.78s   best: 22.9032
2023-11-16 04:37:46,569:INFO:  Epoch 297/500:  train Loss: 18.0790   val Loss: 23.7038   time: 431.29s   best: 22.9032
2023-11-16 04:44:58,015:INFO:  Epoch 298/500:  train Loss: 17.8909   val Loss: 22.9447   time: 431.42s   best: 22.9032
2023-11-16 04:52:06,646:INFO:  Epoch 299/500:  train Loss: 17.9190   val Loss: 30.7622   time: 428.61s   best: 22.9032
2023-11-16 04:59:18,326:INFO:  Epoch 300/500:  train Loss: 18.1651   val Loss: 23.2969   time: 431.65s   best: 22.9032
2023-11-16 05:06:26,980:INFO:  Epoch 301/500:  train Loss: 17.8862   val Loss: 25.5366   time: 428.64s   best: 22.9032
2023-11-16 05:13:39,503:INFO:  Epoch 302/500:  train Loss: 18.0161   val Loss: 23.2307   time: 432.50s   best: 22.9032
2023-11-16 05:20:47,729:INFO:  Epoch 303/500:  train Loss: 18.3067   val Loss: 23.8870   time: 428.21s   best: 22.9032
2023-11-16 05:28:01,322:INFO:  Epoch 304/500:  train Loss: 17.9414   val Loss: 24.0645   time: 433.58s   best: 22.9032
2023-11-16 05:35:12,690:INFO:  Epoch 305/500:  train Loss: 17.7837   val Loss: 25.8467   time: 431.36s   best: 22.9032
2023-11-16 05:42:21,495:INFO:  Epoch 306/500:  train Loss: 17.9401   val Loss: 23.7126   time: 428.76s   best: 22.9032
2023-11-16 05:49:32,575:INFO:  Epoch 307/500:  train Loss: 17.8433   val Loss: 23.8302   time: 431.07s   best: 22.9032
2023-11-16 05:56:41,677:INFO:  Epoch 308/500:  train Loss: 17.8682   val Loss: 24.6647   time: 429.10s   best: 22.9032
2023-11-16 06:03:49,468:INFO:  Epoch 309/500:  train Loss: 18.0288   val Loss: 23.0752   time: 427.77s   best: 22.9032
2023-11-16 06:10:59,447:INFO:  Epoch 310/500:  train Loss: 17.9647   val Loss: 23.4400   time: 429.97s   best: 22.9032
2023-11-16 06:18:08,026:INFO:  Epoch 311/500:  train Loss: 17.8388   val Loss: 22.9332   time: 428.55s   best: 22.9032
2023-11-16 06:25:20,755:INFO:  Epoch 312/500:  train Loss: 17.9311   val Loss: 23.2328   time: 432.72s   best: 22.9032
2023-11-16 06:32:32,256:INFO:  Epoch 313/500:  train Loss: 17.9950   val Loss: 23.0791   time: 431.48s   best: 22.9032
2023-11-16 06:39:43,312:INFO:  Epoch 314/500:  train Loss: 17.6993   val Loss: 23.3431   time: 431.03s   best: 22.9032
2023-11-16 06:46:54,235:INFO:  Epoch 315/500:  train Loss: 17.8995   val Loss: 23.6103   time: 430.92s   best: 22.9032
2023-11-16 06:54:04,967:INFO:  Epoch 316/500:  train Loss: 17.6587   val Loss: 23.1041   time: 430.72s   best: 22.9032
2023-11-16 07:01:17,909:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 07:01:17,939:INFO:  Epoch 317/500:  train Loss: 18.1276   val Loss: 22.8828   time: 432.91s   best: 22.8828
2023-11-16 07:08:30,395:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 07:08:30,424:INFO:  Epoch 318/500:  train Loss: 17.9468   val Loss: 22.8576   time: 432.44s   best: 22.8576
2023-11-16 07:15:40,166:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 07:15:40,211:INFO:  Epoch 319/500:  train Loss: 17.9905   val Loss: 22.8187   time: 429.74s   best: 22.8187
2023-11-16 07:22:50,313:INFO:  Epoch 320/500:  train Loss: 17.8024   val Loss: 22.8484   time: 430.10s   best: 22.8187
2023-11-16 07:30:02,329:INFO:  Epoch 321/500:  train Loss: 17.7833   val Loss: 23.3223   time: 432.00s   best: 22.8187
2023-11-16 07:37:13,654:INFO:  Epoch 322/500:  train Loss: 17.9107   val Loss: 23.3979   time: 431.32s   best: 22.8187
2023-11-16 07:44:22,230:INFO:  Epoch 323/500:  train Loss: 17.8340   val Loss: 23.3105   time: 428.55s   best: 22.8187
2023-11-16 07:51:32,779:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 07:51:32,870:INFO:  Epoch 324/500:  train Loss: 17.7012   val Loss: 22.4906   time: 430.52s   best: 22.4906
2023-11-16 07:58:41,366:INFO:  Epoch 325/500:  train Loss: 17.6720   val Loss: 23.1878   time: 428.47s   best: 22.4906
2023-11-16 07:59:16,545:INFO:  Starting experiment lstm autoencoder perm50 debug (0.05 dropout)
2023-11-16 07:59:16,546:INFO:  Defining the model
2023-11-16 07:59:16,590:INFO:  Reading the dataset
2023-11-16 07:59:22,967:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:22,988:INFO:  Epoch 1/500:  train Loss: 99.4441   val Loss: 96.5694   time: 1.45s   best: 96.5694
2023-11-16 07:59:23,242:INFO:  Epoch 2/500:  train Loss: 99.1093   val Loss: 100.0345   time: 0.25s   best: 96.5694
2023-11-16 07:59:23,511:INFO:  Epoch 3/500:  train Loss: 99.1155   val Loss: 99.9610   time: 0.26s   best: 96.5694
2023-11-16 07:59:23,782:INFO:  Epoch 4/500:  train Loss: 99.1619   val Loss: 99.7854   time: 0.27s   best: 96.5694
2023-11-16 07:59:24,027:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:24,152:INFO:  Epoch 5/500:  train Loss: 97.9507   val Loss: 94.1273   time: 0.24s   best: 94.1273
2023-11-16 07:59:24,394:INFO:  Epoch 6/500:  train Loss: 96.9883   val Loss: 94.7855   time: 0.24s   best: 94.1273
2023-11-16 07:59:24,650:INFO:  Epoch 7/500:  train Loss: 97.0748   val Loss: 95.6897   time: 0.25s   best: 94.1273
2023-11-16 07:59:24,890:INFO:  Epoch 8/500:  train Loss: 96.5202   val Loss: 96.2968   time: 0.24s   best: 94.1273
2023-11-16 07:59:25,151:INFO:  Epoch 9/500:  train Loss: 96.4499   val Loss: 95.9757   time: 0.26s   best: 94.1273
2023-11-16 07:59:25,391:INFO:  Epoch 10/500:  train Loss: 95.7047   val Loss: 95.6965   time: 0.24s   best: 94.1273
2023-11-16 07:59:25,696:INFO:  Epoch 11/500:  train Loss: 95.5000   val Loss: 95.6040   time: 0.30s   best: 94.1273
2023-11-16 07:59:25,936:INFO:  Epoch 12/500:  train Loss: 95.7008   val Loss: 94.8944   time: 0.24s   best: 94.1273
2023-11-16 07:59:26,198:INFO:  Epoch 13/500:  train Loss: 94.8939   val Loss: 94.7477   time: 0.26s   best: 94.1273
2023-11-16 07:59:26,438:INFO:  Epoch 14/500:  train Loss: 94.8801   val Loss: 94.5640   time: 0.24s   best: 94.1273
2023-11-16 07:59:26,697:INFO:  Epoch 15/500:  train Loss: 94.5343   val Loss: 94.3919   time: 0.26s   best: 94.1273
2023-11-16 07:59:26,937:INFO:  Epoch 16/500:  train Loss: 94.6013   val Loss: 94.3100   time: 0.24s   best: 94.1273
2023-11-16 07:59:27,177:INFO:  Epoch 17/500:  train Loss: 94.7269   val Loss: 94.3590   time: 0.24s   best: 94.1273
2023-11-16 07:59:27,437:INFO:  Epoch 18/500:  train Loss: 94.3317   val Loss: 94.2646   time: 0.26s   best: 94.1273
2023-11-16 07:59:27,742:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:27,822:INFO:  Epoch 19/500:  train Loss: 94.3971   val Loss: 94.0713   time: 0.28s   best: 94.0713
2023-11-16 07:59:28,064:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:28,173:INFO:  Epoch 20/500:  train Loss: 94.1343   val Loss: 94.0216   time: 0.24s   best: 94.0216
2023-11-16 07:59:28,428:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:28,539:INFO:  Epoch 21/500:  train Loss: 93.9669   val Loss: 93.9595   time: 0.25s   best: 93.9595
2023-11-16 07:59:28,797:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:28,917:INFO:  Epoch 22/500:  train Loss: 93.6220   val Loss: 93.9217   time: 0.25s   best: 93.9217
2023-11-16 07:59:29,160:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:29,231:INFO:  Epoch 23/500:  train Loss: 93.5634   val Loss: 93.8042   time: 0.24s   best: 93.8042
2023-11-16 07:59:29,482:INFO:  Epoch 24/500:  train Loss: 93.4573   val Loss: 93.8597   time: 0.25s   best: 93.8042
2023-11-16 07:59:29,730:INFO:  Epoch 25/500:  train Loss: 93.6018   val Loss: 94.0487   time: 0.25s   best: 93.8042
2023-11-16 07:59:30,026:INFO:  Epoch 26/500:  train Loss: 93.7667   val Loss: 94.0927   time: 0.29s   best: 93.8042
2023-11-16 07:59:30,267:INFO:  Epoch 27/500:  train Loss: 93.9294   val Loss: 94.1940   time: 0.24s   best: 93.8042
2023-11-16 07:59:30,528:INFO:  Epoch 28/500:  train Loss: 94.0009   val Loss: 94.4075   time: 0.26s   best: 93.8042
2023-11-16 07:59:30,769:INFO:  Epoch 29/500:  train Loss: 94.5447   val Loss: 94.3570   time: 0.24s   best: 93.8042
2023-11-16 07:59:31,029:INFO:  Epoch 30/500:  train Loss: 94.5217   val Loss: 94.2850   time: 0.26s   best: 93.8042
2023-11-16 07:59:31,269:INFO:  Epoch 31/500:  train Loss: 94.1935   val Loss: 94.1759   time: 0.24s   best: 93.8042
2023-11-16 07:59:31,535:INFO:  Epoch 32/500:  train Loss: 93.9291   val Loss: 94.0398   time: 0.26s   best: 93.8042
2023-11-16 07:59:31,778:INFO:  Epoch 33/500:  train Loss: 93.9002   val Loss: 93.9380   time: 0.24s   best: 93.8042
2023-11-16 07:59:32,077:INFO:  Epoch 34/500:  train Loss: 93.6763   val Loss: 93.8330   time: 0.30s   best: 93.8042
2023-11-16 07:59:32,320:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:32,469:INFO:  Epoch 35/500:  train Loss: 93.6543   val Loss: 93.7473   time: 0.24s   best: 93.7473
2023-11-16 07:59:32,713:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:32,893:INFO:  Epoch 36/500:  train Loss: 93.3986   val Loss: 93.6826   time: 0.24s   best: 93.6826
2023-11-16 07:59:33,146:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:33,209:INFO:  Epoch 37/500:  train Loss: 93.4025   val Loss: 93.5036   time: 0.25s   best: 93.5036
2023-11-16 07:59:33,452:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:33,571:INFO:  Epoch 38/500:  train Loss: 93.5097   val Loss: 93.3452   time: 0.24s   best: 93.3452
2023-11-16 07:59:33,816:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:34,010:INFO:  Epoch 39/500:  train Loss: 93.1345   val Loss: 92.9410   time: 0.24s   best: 92.9410
2023-11-16 07:59:34,255:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:34,275:INFO:  Epoch 40/500:  train Loss: 92.5917   val Loss: 91.6045   time: 0.24s   best: 91.6045
2023-11-16 07:59:34,533:INFO:  Epoch 41/500:  train Loss: 92.3545   val Loss: 92.0275   time: 0.26s   best: 91.6045
2023-11-16 07:59:34,774:INFO:  Epoch 42/500:  train Loss: 92.7645   val Loss: 93.0262   time: 0.24s   best: 91.6045
2023-11-16 07:59:35,014:INFO:  Epoch 43/500:  train Loss: 92.9771   val Loss: 93.2227   time: 0.24s   best: 91.6045
2023-11-16 07:59:35,275:INFO:  Epoch 44/500:  train Loss: 92.9727   val Loss: 92.9656   time: 0.26s   best: 91.6045
2023-11-16 07:59:35,515:INFO:  Epoch 45/500:  train Loss: 92.5278   val Loss: 92.5486   time: 0.24s   best: 91.6045
2023-11-16 07:59:35,783:INFO:  Epoch 46/500:  train Loss: 92.3503   val Loss: 92.0143   time: 0.27s   best: 91.6045
2023-11-16 07:59:36,065:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:36,163:INFO:  Epoch 47/500:  train Loss: 92.1279   val Loss: 91.5920   time: 0.28s   best: 91.5920
2023-11-16 07:59:36,406:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:36,641:INFO:  Epoch 48/500:  train Loss: 91.4947   val Loss: 91.1051   time: 0.24s   best: 91.1051
2023-11-16 07:59:36,970:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:36,989:INFO:  Epoch 49/500:  train Loss: 91.0105   val Loss: 90.9263   time: 0.25s   best: 90.9263
2023-11-16 07:59:37,247:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:37,334:INFO:  Epoch 50/500:  train Loss: 91.0563   val Loss: 90.4887   time: 0.25s   best: 90.4887
2023-11-16 07:59:37,584:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:38,123:INFO:  Epoch 51/500:  train Loss: 90.3055   val Loss: 89.9650   time: 0.24s   best: 89.9650
2023-11-16 07:59:38,368:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:38,403:INFO:  Epoch 52/500:  train Loss: 89.7835   val Loss: 89.2659   time: 0.24s   best: 89.2659
2023-11-16 07:59:38,646:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:38,774:INFO:  Epoch 53/500:  train Loss: 89.6678   val Loss: 88.5478   time: 0.24s   best: 88.5478
2023-11-16 07:59:39,016:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:39,102:INFO:  Epoch 54/500:  train Loss: 89.1240   val Loss: 88.2406   time: 0.24s   best: 88.2406
2023-11-16 07:59:39,355:INFO:  Epoch 55/500:  train Loss: 88.7872   val Loss: 88.8970   time: 0.25s   best: 88.2406
2023-11-16 07:59:39,604:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:39,666:INFO:  Epoch 56/500:  train Loss: 89.1226   val Loss: 88.0957   time: 0.24s   best: 88.0957
2023-11-16 07:59:39,928:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:39,981:INFO:  Epoch 57/500:  train Loss: 88.1849   val Loss: 87.2327   time: 0.26s   best: 87.2327
2023-11-16 07:59:40,273:INFO:  Epoch 58/500:  train Loss: 87.6725   val Loss: 88.2429   time: 0.29s   best: 87.2327
2023-11-16 07:59:40,514:INFO:  Epoch 59/500:  train Loss: 88.7838   val Loss: 87.7045   time: 0.24s   best: 87.2327
2023-11-16 07:59:40,776:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:40,883:INFO:  Epoch 60/500:  train Loss: 88.0875   val Loss: 87.0548   time: 0.26s   best: 87.0548
2023-11-16 07:59:41,126:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:41,205:INFO:  Epoch 61/500:  train Loss: 87.6886   val Loss: 86.8388   time: 0.24s   best: 86.8388
2023-11-16 07:59:41,458:INFO:  Epoch 62/500:  train Loss: 87.5658   val Loss: 87.2606   time: 0.25s   best: 86.8388
2023-11-16 07:59:41,707:INFO:  Epoch 63/500:  train Loss: 87.6949   val Loss: 87.7803   time: 0.25s   best: 86.8388
2023-11-16 07:59:41,968:INFO:  Epoch 64/500:  train Loss: 87.7789   val Loss: 87.2936   time: 0.26s   best: 86.8388
2023-11-16 07:59:42,229:INFO:  Epoch 65/500:  train Loss: 87.5884   val Loss: 87.0350   time: 0.26s   best: 86.8388
2023-11-16 07:59:42,505:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:42,571:INFO:  Epoch 66/500:  train Loss: 87.4878   val Loss: 86.5535   time: 0.27s   best: 86.5535
2023-11-16 07:59:42,814:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:42,962:INFO:  Epoch 67/500:  train Loss: 86.9494   val Loss: 86.3670   time: 0.24s   best: 86.3670
2023-11-16 07:59:43,204:INFO:  Epoch 68/500:  train Loss: 86.9239   val Loss: 86.6599   time: 0.24s   best: 86.3670
2023-11-16 07:59:43,459:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:43,507:INFO:  Epoch 69/500:  train Loss: 87.3522   val Loss: 86.2362   time: 0.25s   best: 86.2362
2023-11-16 07:59:43,758:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:43,847:INFO:  Epoch 70/500:  train Loss: 86.6704   val Loss: 85.6071   time: 0.25s   best: 85.6071
2023-11-16 07:59:44,106:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:44,224:INFO:  Epoch 71/500:  train Loss: 86.2103   val Loss: 85.5096   time: 0.25s   best: 85.5096
2023-11-16 07:59:44,516:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:44,569:INFO:  Epoch 72/500:  train Loss: 86.1302   val Loss: 84.9719   time: 0.29s   best: 84.9719
2023-11-16 07:59:44,810:INFO:  Epoch 73/500:  train Loss: 86.4492   val Loss: 84.9900   time: 0.24s   best: 84.9719
2023-11-16 07:59:45,067:INFO:  Epoch 74/500:  train Loss: 85.3729   val Loss: 85.6342   time: 0.25s   best: 84.9719
2023-11-16 07:59:45,308:INFO:  Epoch 75/500:  train Loss: 86.0288   val Loss: 84.9954   time: 0.24s   best: 84.9719
2023-11-16 07:59:45,571:INFO:  Epoch 76/500:  train Loss: 86.1159   val Loss: 84.9792   time: 0.26s   best: 84.9719
2023-11-16 07:59:45,816:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:45,966:INFO:  Epoch 77/500:  train Loss: 85.2641   val Loss: 84.9197   time: 0.24s   best: 84.9197
2023-11-16 07:59:46,208:INFO:  Epoch 78/500:  train Loss: 85.2244   val Loss: 88.2249   time: 0.24s   best: 84.9197
2023-11-16 07:59:46,503:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:46,659:INFO:  Epoch 79/500:  train Loss: 86.5858   val Loss: 84.8900   time: 0.29s   best: 84.8900
2023-11-16 07:59:46,900:INFO:  Epoch 80/500:  train Loss: 85.7286   val Loss: 85.4605   time: 0.24s   best: 84.8900
2023-11-16 07:59:47,155:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:47,290:INFO:  Epoch 81/500:  train Loss: 85.7434   val Loss: 84.6497   time: 0.25s   best: 84.6497
2023-11-16 07:59:47,547:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:47,695:INFO:  Epoch 82/500:  train Loss: 85.5614   val Loss: 84.0774   time: 0.25s   best: 84.0774
2023-11-16 07:59:47,941:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:48,049:INFO:  Epoch 83/500:  train Loss: 84.4698   val Loss: 83.4935   time: 0.24s   best: 83.4935
2023-11-16 07:59:48,290:INFO:  Epoch 84/500:  train Loss: 84.0680   val Loss: 83.9471   time: 0.24s   best: 83.4935
2023-11-16 07:59:48,621:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:48,719:INFO:  Epoch 85/500:  train Loss: 83.8621   val Loss: 83.3019   time: 0.32s   best: 83.3019
2023-11-16 07:59:48,964:INFO:  Epoch 86/500:  train Loss: 84.8710   val Loss: 84.6573   time: 0.24s   best: 83.3019
2023-11-16 07:59:49,225:INFO:  Epoch 87/500:  train Loss: 85.6077   val Loss: 89.6059   time: 0.26s   best: 83.3019
2023-11-16 07:59:49,476:INFO:  Epoch 88/500:  train Loss: 90.6558   val Loss: 90.0410   time: 0.25s   best: 83.3019
2023-11-16 07:59:49,744:INFO:  Epoch 89/500:  train Loss: 88.8419   val Loss: 86.6062   time: 0.26s   best: 83.3019
2023-11-16 07:59:49,986:INFO:  Epoch 90/500:  train Loss: 86.8506   val Loss: 85.8032   time: 0.24s   best: 83.3019
2023-11-16 07:59:50,246:INFO:  Epoch 91/500:  train Loss: 86.3657   val Loss: 85.4449   time: 0.26s   best: 83.3019
2023-11-16 07:59:50,488:INFO:  Epoch 92/500:  train Loss: 84.8876   val Loss: 84.1792   time: 0.24s   best: 83.3019
2023-11-16 07:59:50,785:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:50,874:INFO:  Epoch 93/500:  train Loss: 84.0574   val Loss: 83.1864   time: 0.29s   best: 83.1864
2023-11-16 07:59:51,115:INFO:  Epoch 94/500:  train Loss: 83.5140   val Loss: 83.2026   time: 0.24s   best: 83.1864
2023-11-16 07:59:51,374:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:51,546:INFO:  Epoch 95/500:  train Loss: 84.0523   val Loss: 82.5786   time: 0.25s   best: 82.5786
2023-11-16 07:59:51,809:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:51,946:INFO:  Epoch 96/500:  train Loss: 83.4604   val Loss: 82.4476   time: 0.26s   best: 82.4476
2023-11-16 07:59:52,205:INFO:  Epoch 97/500:  train Loss: 82.9890   val Loss: 82.7036   time: 0.25s   best: 82.4476
2023-11-16 07:59:52,456:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:52,537:INFO:  Epoch 98/500:  train Loss: 83.8838   val Loss: 82.3596   time: 0.25s   best: 82.3596
2023-11-16 07:59:52,827:INFO:  Epoch 99/500:  train Loss: 83.4608   val Loss: 83.3699   time: 0.29s   best: 82.3596
2023-11-16 07:59:53,136:INFO:  Epoch 100/500:  train Loss: 83.5898   val Loss: 82.8105   time: 0.28s   best: 82.3596
2023-11-16 07:59:53,402:INFO:  Epoch 101/500:  train Loss: 85.6192   val Loss: 83.4658   time: 0.26s   best: 82.3596
2023-11-16 07:59:53,654:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:53,808:INFO:  Epoch 102/500:  train Loss: 82.6001   val Loss: 82.3539   time: 0.25s   best: 82.3539
2023-11-16 07:59:54,050:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:54,157:INFO:  Epoch 103/500:  train Loss: 82.5719   val Loss: 81.5409   time: 0.24s   best: 81.5409
2023-11-16 07:59:54,421:INFO:  Epoch 104/500:  train Loss: 81.9353   val Loss: 81.5953   time: 0.25s   best: 81.5409
2023-11-16 07:59:54,671:INFO:  Epoch 105/500:  train Loss: 82.1153   val Loss: 83.6127   time: 0.24s   best: 81.5409
2023-11-16 07:59:54,969:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:55,072:INFO:  Epoch 106/500:  train Loss: 81.8311   val Loss: 81.3805   time: 0.29s   best: 81.3805
2023-11-16 07:59:55,316:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:55,513:INFO:  Epoch 107/500:  train Loss: 83.1476   val Loss: 81.2298   time: 0.24s   best: 81.2298
2023-11-16 07:59:55,764:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:55,834:INFO:  Epoch 108/500:  train Loss: 81.6982   val Loss: 81.0387   time: 0.25s   best: 81.0387
2023-11-16 07:59:56,092:INFO:  Epoch 109/500:  train Loss: 81.8818   val Loss: 82.3560   time: 0.25s   best: 81.0387
2023-11-16 07:59:56,335:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:56,398:INFO:  Epoch 110/500:  train Loss: 81.7886   val Loss: 80.3130   time: 0.24s   best: 80.3130
2023-11-16 07:59:56,645:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:57,207:INFO:  Epoch 111/500:  train Loss: 80.5005   val Loss: 80.1602   time: 0.24s   best: 80.1602
2023-11-16 07:59:57,472:INFO:  Epoch 112/500:  train Loss: 81.0137   val Loss: 81.3078   time: 0.25s   best: 80.1602
2023-11-16 07:59:57,728:INFO:  Epoch 113/500:  train Loss: 81.7318   val Loss: 80.7673   time: 0.25s   best: 80.1602
2023-11-16 07:59:57,994:INFO:  Epoch 114/500:  train Loss: 81.7922   val Loss: 80.8741   time: 0.25s   best: 80.1602
2023-11-16 07:59:58,242:INFO:  Epoch 115/500:  train Loss: 81.3956   val Loss: 80.6757   time: 0.24s   best: 80.1602
2023-11-16 07:59:58,507:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 07:59:58,662:INFO:  Epoch 116/500:  train Loss: 82.3875   val Loss: 80.1210   time: 0.26s   best: 80.1210
2023-11-16 07:59:58,911:INFO:  Epoch 117/500:  train Loss: 80.6715   val Loss: 80.9829   time: 0.24s   best: 80.1210
2023-11-16 07:59:59,210:INFO:  Epoch 118/500:  train Loss: 81.4592   val Loss: 81.7667   time: 0.29s   best: 80.1210
2023-11-16 07:59:59,475:INFO:  Epoch 119/500:  train Loss: 84.9094   val Loss: 80.2728   time: 0.25s   best: 80.1210
2023-11-16 07:59:59,732:INFO:  Epoch 120/500:  train Loss: 80.8124   val Loss: 80.7114   time: 0.25s   best: 80.1210
2023-11-16 07:59:59,999:INFO:  Epoch 121/500:  train Loss: 81.8322   val Loss: 80.6249   time: 0.24s   best: 80.1210
2023-11-16 08:00:00,244:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:00,318:INFO:  Epoch 122/500:  train Loss: 80.7354   val Loss: 80.0539   time: 0.24s   best: 80.0539
2023-11-16 08:00:00,578:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:00,955:INFO:  Epoch 123/500:  train Loss: 81.2364   val Loss: 79.6247   time: 0.26s   best: 79.6247
2023-11-16 08:00:01,245:INFO:  Epoch 124/500:  train Loss: 80.5053   val Loss: 80.3948   time: 0.28s   best: 79.6247
2023-11-16 08:00:01,489:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:01,607:INFO:  Epoch 125/500:  train Loss: 80.8935   val Loss: 79.4119   time: 0.24s   best: 79.4119
2023-11-16 08:00:01,858:INFO:  Epoch 126/500:  train Loss: 79.9382   val Loss: 79.5976   time: 0.24s   best: 79.4119
2023-11-16 08:00:02,121:INFO:  Epoch 127/500:  train Loss: 81.0149   val Loss: 80.5107   time: 0.25s   best: 79.4119
2023-11-16 08:00:02,372:INFO:  Epoch 128/500:  train Loss: 80.6083   val Loss: 80.0509   time: 0.24s   best: 79.4119
2023-11-16 08:00:02,643:INFO:  Epoch 129/500:  train Loss: 80.8124   val Loss: 80.0488   time: 0.26s   best: 79.4119
2023-11-16 08:00:02,893:INFO:  Epoch 130/500:  train Loss: 80.8433   val Loss: 79.7786   time: 0.24s   best: 79.4119
2023-11-16 08:00:03,197:INFO:  Epoch 131/500:  train Loss: 81.2803   val Loss: 83.8862   time: 0.29s   best: 79.4119
2023-11-16 08:00:03,447:INFO:  Epoch 132/500:  train Loss: 81.2928   val Loss: 80.6010   time: 0.24s   best: 79.4119
2023-11-16 08:00:03,715:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:03,841:INFO:  Epoch 133/500:  train Loss: 81.1605   val Loss: 79.1118   time: 0.26s   best: 79.1118
2023-11-16 08:00:04,084:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:04,222:INFO:  Epoch 134/500:  train Loss: 82.4588   val Loss: 79.0064   time: 0.24s   best: 79.0064
2023-11-16 08:00:04,474:INFO:  Epoch 135/500:  train Loss: 80.3084   val Loss: 81.8781   time: 0.24s   best: 79.0064
2023-11-16 08:00:04,740:INFO:  Epoch 136/500:  train Loss: 81.2407   val Loss: 79.4310   time: 0.25s   best: 79.0064
2023-11-16 08:00:04,988:INFO:  Epoch 137/500:  train Loss: 80.9986   val Loss: 79.7190   time: 0.24s   best: 79.0064
2023-11-16 08:00:05,292:INFO:  Epoch 138/500:  train Loss: 79.6148   val Loss: 79.2448   time: 0.29s   best: 79.0064
2023-11-16 08:00:05,539:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:05,667:INFO:  Epoch 139/500:  train Loss: 80.0301   val Loss: 78.4881   time: 0.24s   best: 78.4881
2023-11-16 08:00:05,930:INFO:  Epoch 140/500:  train Loss: 80.6410   val Loss: 78.8961   time: 0.25s   best: 78.4881
2023-11-16 08:00:06,179:INFO:  Epoch 141/500:  train Loss: 80.0430   val Loss: 79.2700   time: 0.24s   best: 78.4881
2023-11-16 08:00:06,449:INFO:  Epoch 142/500:  train Loss: 80.1049   val Loss: 79.3416   time: 0.26s   best: 78.4881
2023-11-16 08:00:06,699:INFO:  Epoch 143/500:  train Loss: 80.9061   val Loss: 78.9712   time: 0.24s   best: 78.4881
2023-11-16 08:00:06,964:INFO:  Epoch 144/500:  train Loss: 78.5112   val Loss: 78.7805   time: 0.26s   best: 78.4881
2023-11-16 08:00:07,213:INFO:  Epoch 145/500:  train Loss: 79.5148   val Loss: 79.1834   time: 0.24s   best: 78.4881
2023-11-16 08:00:07,518:INFO:  Epoch 146/500:  train Loss: 80.4466   val Loss: 79.1878   time: 0.29s   best: 78.4881
2023-11-16 08:00:07,776:INFO:  Epoch 147/500:  train Loss: 79.8078   val Loss: 79.2249   time: 0.25s   best: 78.4881
2023-11-16 08:00:08,052:INFO:  Epoch 148/500:  train Loss: 82.5132   val Loss: 80.2943   time: 0.27s   best: 78.4881
2023-11-16 08:00:08,300:INFO:  Epoch 149/500:  train Loss: 83.7229   val Loss: 90.5456   time: 0.24s   best: 78.4881
2023-11-16 08:00:08,568:INFO:  Epoch 150/500:  train Loss: 91.7285   val Loss: 90.5478   time: 0.26s   best: 78.4881
2023-11-16 08:00:08,817:INFO:  Epoch 151/500:  train Loss: 88.2844   val Loss: 85.5095   time: 0.24s   best: 78.4881
2023-11-16 08:00:09,083:INFO:  Epoch 152/500:  train Loss: 85.6835   val Loss: 85.1794   time: 0.26s   best: 78.4881
2023-11-16 08:00:09,331:INFO:  Epoch 153/500:  train Loss: 85.1858   val Loss: 84.0899   time: 0.24s   best: 78.4881
2023-11-16 08:00:09,638:INFO:  Epoch 154/500:  train Loss: 83.8224   val Loss: 84.2970   time: 0.30s   best: 78.4881
2023-11-16 08:00:09,891:INFO:  Epoch 155/500:  train Loss: 84.3740   val Loss: 82.8116   time: 0.24s   best: 78.4881
2023-11-16 08:00:10,157:INFO:  Epoch 156/500:  train Loss: 82.7675   val Loss: 81.3671   time: 0.25s   best: 78.4881
2023-11-16 08:00:10,405:INFO:  Epoch 157/500:  train Loss: 82.0512   val Loss: 79.1415   time: 0.24s   best: 78.4881
2023-11-16 08:00:10,674:INFO:  Epoch 158/500:  train Loss: 79.4822   val Loss: 79.0880   time: 0.26s   best: 78.4881
2023-11-16 08:00:10,915:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:11,058:INFO:  Epoch 159/500:  train Loss: 79.6028   val Loss: 77.6737   time: 0.24s   best: 77.6737
2023-11-16 08:00:11,307:INFO:  Epoch 160/500:  train Loss: 78.3636   val Loss: 77.7805   time: 0.24s   best: 77.6737
2023-11-16 08:00:11,609:INFO:  Epoch 161/500:  train Loss: 78.1716   val Loss: 78.4557   time: 0.29s   best: 77.6737
2023-11-16 08:00:11,861:INFO:  Epoch 162/500:  train Loss: 79.0994   val Loss: 77.7997   time: 0.24s   best: 77.6737
2023-11-16 08:00:12,129:INFO:  Epoch 163/500:  train Loss: 78.5408   val Loss: 77.8874   time: 0.25s   best: 77.6737
2023-11-16 08:00:12,371:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:12,449:INFO:  Epoch 164/500:  train Loss: 79.9815   val Loss: 77.2102   time: 0.24s   best: 77.2102
2023-11-16 08:00:12,707:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:12,849:INFO:  Epoch 165/500:  train Loss: 78.6417   val Loss: 76.7437   time: 0.25s   best: 76.7437
2023-11-16 08:00:13,114:INFO:  Epoch 166/500:  train Loss: 78.0549   val Loss: 76.8556   time: 0.24s   best: 76.7437
2023-11-16 08:00:13,363:INFO:  Epoch 167/500:  train Loss: 79.1388   val Loss: 77.0182   time: 0.24s   best: 76.7437
2023-11-16 08:00:13,659:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:13,765:INFO:  Epoch 168/500:  train Loss: 78.4126   val Loss: 76.3444   time: 0.29s   best: 76.3444
2023-11-16 08:00:14,014:INFO:  Epoch 169/500:  train Loss: 78.0130   val Loss: 77.2225   time: 0.24s   best: 76.3444
2023-11-16 08:00:14,278:INFO:  Epoch 170/500:  train Loss: 77.7949   val Loss: 78.5536   time: 0.25s   best: 76.3444
2023-11-16 08:00:14,529:INFO:  Epoch 171/500:  train Loss: 78.8603   val Loss: 76.7334   time: 0.24s   best: 76.3444
2023-11-16 08:00:14,797:INFO:  Epoch 172/500:  train Loss: 77.5749   val Loss: 76.4724   time: 0.26s   best: 76.3444
2023-11-16 08:00:15,046:INFO:  Epoch 173/500:  train Loss: 77.6672   val Loss: 76.6250   time: 0.24s   best: 76.3444
2023-11-16 08:00:15,314:INFO:  Epoch 174/500:  train Loss: 77.5615   val Loss: 76.9909   time: 0.26s   best: 76.3444
2023-11-16 08:00:15,564:INFO:  Epoch 175/500:  train Loss: 78.3750   val Loss: 76.3879   time: 0.24s   best: 76.3444
2023-11-16 08:00:15,865:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:15,984:INFO:  Epoch 176/500:  train Loss: 78.3433   val Loss: 76.1303   time: 0.30s   best: 76.1303
2023-11-16 08:00:16,249:INFO:  Epoch 177/500:  train Loss: 77.3999   val Loss: 77.0014   time: 0.24s   best: 76.1303
2023-11-16 08:00:16,500:INFO:  Epoch 178/500:  train Loss: 77.9066   val Loss: 77.6928   time: 0.24s   best: 76.1303
2023-11-16 08:00:16,768:INFO:  Epoch 179/500:  train Loss: 78.6463   val Loss: 77.0058   time: 0.24s   best: 76.1303
2023-11-16 08:00:17,016:INFO:  Epoch 180/500:  train Loss: 77.7284   val Loss: 76.5848   time: 0.24s   best: 76.1303
2023-11-16 08:00:17,260:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:17,481:INFO:  Epoch 181/500:  train Loss: 77.8862   val Loss: 76.0146   time: 0.24s   best: 76.0146
2023-11-16 08:00:17,745:INFO:  Epoch 182/500:  train Loss: 77.1978   val Loss: 76.1593   time: 0.26s   best: 76.0146
2023-11-16 08:00:18,028:INFO:  Epoch 183/500:  train Loss: 77.4366   val Loss: 76.4724   time: 0.27s   best: 76.0146
2023-11-16 08:00:18,277:INFO:  Epoch 184/500:  train Loss: 77.7900   val Loss: 76.1273   time: 0.24s   best: 76.0146
2023-11-16 08:00:18,537:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:18,650:INFO:  Epoch 185/500:  train Loss: 76.3069   val Loss: 75.5845   time: 0.26s   best: 75.5845
2023-11-16 08:00:18,913:INFO:  Epoch 186/500:  train Loss: 77.8351   val Loss: 81.3298   time: 0.25s   best: 75.5845
2023-11-16 08:00:19,161:INFO:  Epoch 187/500:  train Loss: 79.9203   val Loss: 77.4874   time: 0.24s   best: 75.5845
2023-11-16 08:00:19,428:INFO:  Epoch 188/500:  train Loss: 77.1015   val Loss: 76.7094   time: 0.26s   best: 75.5845
2023-11-16 08:00:19,682:INFO:  Epoch 189/500:  train Loss: 77.6972   val Loss: 78.0812   time: 0.24s   best: 75.5845
2023-11-16 08:00:20,023:INFO:  Epoch 190/500:  train Loss: 78.6895   val Loss: 78.3320   time: 0.33s   best: 75.5845
2023-11-16 08:00:20,273:INFO:  Epoch 191/500:  train Loss: 78.7091   val Loss: 77.5833   time: 0.24s   best: 75.5845
2023-11-16 08:00:20,541:INFO:  Epoch 192/500:  train Loss: 78.0165   val Loss: 76.8799   time: 0.26s   best: 75.5845
2023-11-16 08:00:20,790:INFO:  Epoch 193/500:  train Loss: 77.5622   val Loss: 76.5709   time: 0.24s   best: 75.5845
2023-11-16 08:00:21,057:INFO:  Epoch 194/500:  train Loss: 77.4264   val Loss: 75.8686   time: 0.26s   best: 75.5845
2023-11-16 08:00:21,300:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:21,400:INFO:  Epoch 195/500:  train Loss: 76.5515   val Loss: 75.2173   time: 0.24s   best: 75.2173
2023-11-16 08:00:21,661:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:21,799:INFO:  Epoch 196/500:  train Loss: 75.8959   val Loss: 75.1884   time: 0.26s   best: 75.1884
2023-11-16 08:00:22,090:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:22,227:INFO:  Epoch 197/500:  train Loss: 75.4077   val Loss: 74.9131   time: 0.29s   best: 74.9131
2023-11-16 08:00:22,471:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:22,522:INFO:  Epoch 198/500:  train Loss: 77.5528   val Loss: 74.8150   time: 0.24s   best: 74.8150
2023-11-16 08:00:22,771:INFO:  Epoch 199/500:  train Loss: 76.6769   val Loss: 75.0423   time: 0.24s   best: 74.8150
2023-11-16 08:00:23,096:INFO:  Epoch 200/500:  train Loss: 76.4417   val Loss: 74.8208   time: 0.31s   best: 74.8150
2023-11-16 08:00:23,358:INFO:  Epoch 201/500:  train Loss: 75.8278   val Loss: 75.1100   time: 0.25s   best: 74.8150
2023-11-16 08:00:23,628:INFO:  Epoch 202/500:  train Loss: 77.0033   val Loss: 75.2244   time: 0.26s   best: 74.8150
2023-11-16 08:00:23,879:INFO:  Epoch 203/500:  train Loss: 76.4479   val Loss: 76.1744   time: 0.24s   best: 74.8150
2023-11-16 08:00:24,183:INFO:  Epoch 204/500:  train Loss: 76.9652   val Loss: 76.2266   time: 0.29s   best: 74.8150
2023-11-16 08:00:24,432:INFO:  Epoch 205/500:  train Loss: 76.0175   val Loss: 74.9580   time: 0.24s   best: 74.8150
2023-11-16 08:00:24,693:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:24,855:INFO:  Epoch 206/500:  train Loss: 75.4352   val Loss: 74.2798   time: 0.26s   best: 74.2798
2023-11-16 08:00:25,104:INFO:  Epoch 207/500:  train Loss: 76.5512   val Loss: 74.4429   time: 0.24s   best: 74.2798
2023-11-16 08:00:25,367:INFO:  Epoch 208/500:  train Loss: 75.5652   val Loss: 75.5361   time: 0.25s   best: 74.2798
2023-11-16 08:00:25,616:INFO:  Epoch 209/500:  train Loss: 76.3154   val Loss: 77.3429   time: 0.24s   best: 74.2798
2023-11-16 08:00:25,888:INFO:  Epoch 210/500:  train Loss: 84.3105   val Loss: 87.8098   time: 0.26s   best: 74.2798
2023-11-16 08:00:26,139:INFO:  Epoch 211/500:  train Loss: 87.4245   val Loss: 84.6493   time: 0.24s   best: 74.2798
2023-11-16 08:00:26,442:INFO:  Epoch 212/500:  train Loss: 85.7667   val Loss: 84.9208   time: 0.29s   best: 74.2798
2023-11-16 08:00:26,693:INFO:  Epoch 213/500:  train Loss: 84.2663   val Loss: 83.5688   time: 0.24s   best: 74.2798
2023-11-16 08:00:26,960:INFO:  Epoch 214/500:  train Loss: 83.5681   val Loss: 83.1356   time: 0.26s   best: 74.2798
2023-11-16 08:00:27,207:INFO:  Epoch 215/500:  train Loss: 82.9455   val Loss: 81.3200   time: 0.24s   best: 74.2798
2023-11-16 08:00:27,477:INFO:  Epoch 216/500:  train Loss: 81.4144   val Loss: 79.5724   time: 0.26s   best: 74.2798
2023-11-16 08:00:27,729:INFO:  Epoch 217/500:  train Loss: 79.7991   val Loss: 78.0662   time: 0.24s   best: 74.2798
2023-11-16 08:00:27,995:INFO:  Epoch 218/500:  train Loss: 77.9464   val Loss: 76.4092   time: 0.26s   best: 74.2798
2023-11-16 08:00:28,245:INFO:  Epoch 219/500:  train Loss: 77.8077   val Loss: 75.9331   time: 0.24s   best: 74.2798
2023-11-16 08:00:28,548:INFO:  Epoch 220/500:  train Loss: 76.3939   val Loss: 75.6024   time: 0.29s   best: 74.2798
2023-11-16 08:00:28,797:INFO:  Epoch 221/500:  train Loss: 76.4737   val Loss: 75.0365   time: 0.24s   best: 74.2798
2023-11-16 08:00:29,063:INFO:  Epoch 222/500:  train Loss: 75.4425   val Loss: 74.5225   time: 0.26s   best: 74.2798
2023-11-16 08:00:29,305:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:29,334:INFO:  Epoch 223/500:  train Loss: 75.1281   val Loss: 74.1007   time: 0.24s   best: 74.1007
2023-11-16 08:00:29,595:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:29,773:INFO:  Epoch 224/500:  train Loss: 75.5441   val Loss: 73.7931   time: 0.26s   best: 73.7931
2023-11-16 08:00:30,026:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:30,102:INFO:  Epoch 225/500:  train Loss: 74.3727   val Loss: 73.4297   time: 0.25s   best: 73.4297
2023-11-16 08:00:30,351:INFO:  Epoch 226/500:  train Loss: 74.6083   val Loss: 73.7687   time: 0.24s   best: 73.4297
2023-11-16 08:00:30,656:INFO:  Epoch 227/500:  train Loss: 74.5423   val Loss: 74.2188   time: 0.29s   best: 73.4297
2023-11-16 08:00:30,905:INFO:  Epoch 228/500:  train Loss: 74.5100   val Loss: 73.6204   time: 0.24s   best: 73.4297
2023-11-16 08:00:31,171:INFO:  Epoch 229/500:  train Loss: 74.5895   val Loss: 74.1443   time: 0.26s   best: 73.4297
2023-11-16 08:00:31,420:INFO:  Epoch 230/500:  train Loss: 75.2026   val Loss: 74.0277   time: 0.24s   best: 73.4297
2023-11-16 08:00:31,690:INFO:  Epoch 231/500:  train Loss: 74.5953   val Loss: 74.2795   time: 0.26s   best: 73.4297
2023-11-16 08:00:31,941:INFO:  Epoch 232/500:  train Loss: 74.4715   val Loss: 73.9770   time: 0.24s   best: 73.4297
2023-11-16 08:00:32,200:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:32,285:INFO:  Epoch 233/500:  train Loss: 74.2305   val Loss: 72.8560   time: 0.26s   best: 72.8560
2023-11-16 08:00:32,589:INFO:  Epoch 234/500:  train Loss: 74.4088   val Loss: 73.1837   time: 0.29s   best: 72.8560
2023-11-16 08:00:32,839:INFO:  Epoch 235/500:  train Loss: 73.5346   val Loss: 73.3013   time: 0.24s   best: 72.8560
2023-11-16 08:00:33,105:INFO:  Epoch 236/500:  train Loss: 74.2318   val Loss: 73.2958   time: 0.25s   best: 72.8560
2023-11-16 08:00:33,349:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:33,453:INFO:  Epoch 237/500:  train Loss: 74.3789   val Loss: 72.7277   time: 0.24s   best: 72.7277
2023-11-16 08:00:33,716:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:33,807:INFO:  Epoch 238/500:  train Loss: 73.4314   val Loss: 72.4886   time: 0.26s   best: 72.4886
2023-11-16 08:00:34,073:INFO:  Epoch 239/500:  train Loss: 73.2283   val Loss: 72.9107   time: 0.24s   best: 72.4886
2023-11-16 08:00:34,321:INFO:  Epoch 240/500:  train Loss: 73.7967   val Loss: 72.9800   time: 0.24s   best: 72.4886
2023-11-16 08:00:34,565:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:34,707:INFO:  Epoch 241/500:  train Loss: 74.0596   val Loss: 72.4618   time: 0.24s   best: 72.4618
2023-11-16 08:00:34,956:INFO:  Epoch 242/500:  train Loss: 74.5231   val Loss: 73.2257   time: 0.24s   best: 72.4618
2023-11-16 08:00:35,217:INFO:  Epoch 243/500:  train Loss: 74.3558   val Loss: 73.0810   time: 0.25s   best: 72.4618
2023-11-16 08:00:35,461:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:35,547:INFO:  Epoch 244/500:  train Loss: 73.0002   val Loss: 72.2862   time: 0.24s   best: 72.2862
2023-11-16 08:00:35,817:INFO:  Epoch 245/500:  train Loss: 73.1725   val Loss: 72.8093   time: 0.26s   best: 72.2862
2023-11-16 08:00:36,065:INFO:  Epoch 246/500:  train Loss: 73.4069   val Loss: 73.2200   time: 0.24s   best: 72.2862
2023-11-16 08:00:36,336:INFO:  Epoch 247/500:  train Loss: 76.9472   val Loss: 77.1282   time: 0.26s   best: 72.2862
2023-11-16 08:00:36,586:INFO:  Epoch 248/500:  train Loss: 78.0682   val Loss: 78.2008   time: 0.24s   best: 72.2862
2023-11-16 08:00:36,891:INFO:  Epoch 249/500:  train Loss: 77.6385   val Loss: 75.5112   time: 0.29s   best: 72.2862
2023-11-16 08:00:37,139:INFO:  Epoch 250/500:  train Loss: 75.9375   val Loss: 74.8592   time: 0.24s   best: 72.2862
2023-11-16 08:00:37,405:INFO:  Epoch 251/500:  train Loss: 75.1614   val Loss: 74.6145   time: 0.26s   best: 72.2862
2023-11-16 08:00:37,654:INFO:  Epoch 252/500:  train Loss: 75.9379   val Loss: 74.4791   time: 0.24s   best: 72.2862
2023-11-16 08:00:37,926:INFO:  Epoch 253/500:  train Loss: 75.7249   val Loss: 74.7797   time: 0.26s   best: 72.2862
2023-11-16 08:00:38,174:INFO:  Epoch 254/500:  train Loss: 76.1417   val Loss: 73.9628   time: 0.24s   best: 72.2862
2023-11-16 08:00:38,446:INFO:  Epoch 255/500:  train Loss: 75.8600   val Loss: 74.9750   time: 0.27s   best: 72.2862
2023-11-16 08:00:38,702:INFO:  Epoch 256/500:  train Loss: 75.8881   val Loss: 74.8854   time: 0.25s   best: 72.2862
2023-11-16 08:00:39,007:INFO:  Epoch 257/500:  train Loss: 74.9408   val Loss: 73.1848   time: 0.29s   best: 72.2862
2023-11-16 08:00:39,256:INFO:  Epoch 258/500:  train Loss: 73.4703   val Loss: 72.7647   time: 0.24s   best: 72.2862
2023-11-16 08:00:39,523:INFO:  Epoch 259/500:  train Loss: 74.0239   val Loss: 72.7694   time: 0.26s   best: 72.2862
2023-11-16 08:00:39,777:INFO:  Epoch 260/500:  train Loss: 73.7759   val Loss: 72.6227   time: 0.24s   best: 72.2862
2023-11-16 08:00:40,039:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:40,241:INFO:  Epoch 261/500:  train Loss: 73.8633   val Loss: 72.1106   time: 0.26s   best: 72.1106
2023-11-16 08:00:40,494:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:40,539:INFO:  Epoch 262/500:  train Loss: 72.8041   val Loss: 71.8679   time: 0.25s   best: 71.8679
2023-11-16 08:00:40,787:INFO:  Epoch 263/500:  train Loss: 72.6915   val Loss: 72.3202   time: 0.24s   best: 71.8679
2023-11-16 08:00:41,083:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:41,217:INFO:  Epoch 264/500:  train Loss: 73.0544   val Loss: 71.2468   time: 0.29s   best: 71.2468
2023-11-16 08:00:41,473:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:41,634:INFO:  Epoch 265/500:  train Loss: 72.1103   val Loss: 71.0990   time: 0.25s   best: 71.0990
2023-11-16 08:00:41,887:INFO:  Epoch 266/500:  train Loss: 71.7591   val Loss: 71.3744   time: 0.24s   best: 71.0990
2023-11-16 08:00:42,146:INFO:  Epoch 267/500:  train Loss: 72.4191   val Loss: 72.0495   time: 0.25s   best: 71.0990
2023-11-16 08:00:42,395:INFO:  Epoch 268/500:  train Loss: 76.1876   val Loss: 75.7889   time: 0.24s   best: 71.0990
2023-11-16 08:00:42,660:INFO:  Epoch 269/500:  train Loss: 77.0735   val Loss: 74.5474   time: 0.25s   best: 71.0990
2023-11-16 08:00:42,910:INFO:  Epoch 270/500:  train Loss: 74.6905   val Loss: 73.6595   time: 0.24s   best: 71.0990
2023-11-16 08:00:43,213:INFO:  Epoch 271/500:  train Loss: 73.9114   val Loss: 73.2886   time: 0.29s   best: 71.0990
2023-11-16 08:00:43,462:INFO:  Epoch 272/500:  train Loss: 73.8142   val Loss: 72.4590   time: 0.24s   best: 71.0990
2023-11-16 08:00:43,732:INFO:  Epoch 273/500:  train Loss: 74.1822   val Loss: 73.7702   time: 0.26s   best: 71.0990
2023-11-16 08:00:43,981:INFO:  Epoch 274/500:  train Loss: 75.1618   val Loss: 75.1041   time: 0.24s   best: 71.0990
2023-11-16 08:00:44,247:INFO:  Epoch 275/500:  train Loss: 74.0656   val Loss: 72.2297   time: 0.25s   best: 71.0990
2023-11-16 08:00:44,497:INFO:  Epoch 276/500:  train Loss: 72.7564   val Loss: 71.7557   time: 0.24s   best: 71.0990
2023-11-16 08:00:44,764:INFO:  Epoch 277/500:  train Loss: 74.8544   val Loss: 73.3320   time: 0.26s   best: 71.0990
2023-11-16 08:00:45,012:INFO:  Epoch 278/500:  train Loss: 74.5175   val Loss: 75.2881   time: 0.24s   best: 71.0990
2023-11-16 08:00:45,313:INFO:  Epoch 279/500:  train Loss: 75.0665   val Loss: 74.2670   time: 0.29s   best: 71.0990
2023-11-16 08:00:45,562:INFO:  Epoch 280/500:  train Loss: 73.5204   val Loss: 72.5800   time: 0.24s   best: 71.0990
2023-11-16 08:00:45,827:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:45,864:INFO:  Epoch 281/500:  train Loss: 74.1896   val Loss: 70.9990   time: 0.26s   best: 70.9990
2023-11-16 08:00:46,112:INFO:  Epoch 282/500:  train Loss: 72.1155   val Loss: 71.1280   time: 0.24s   best: 70.9990
2023-11-16 08:00:46,377:INFO:  Epoch 283/500:  train Loss: 72.6340   val Loss: 71.1499   time: 0.25s   best: 70.9990
2023-11-16 08:00:46,624:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:46,779:INFO:  Epoch 284/500:  train Loss: 72.0771   val Loss: 70.2187   time: 0.24s   best: 70.2187
2023-11-16 08:00:47,030:INFO:  Epoch 285/500:  train Loss: 72.5264   val Loss: 70.8294   time: 0.24s   best: 70.2187
2023-11-16 08:00:47,321:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:47,498:INFO:  Epoch 286/500:  train Loss: 71.9120   val Loss: 69.9535   time: 0.29s   best: 69.9535
2023-11-16 08:00:47,756:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:47,785:INFO:  Epoch 287/500:  train Loss: 71.1604   val Loss: 69.7675   time: 0.25s   best: 69.7675
2023-11-16 08:00:48,033:INFO:  Epoch 288/500:  train Loss: 71.4344   val Loss: 70.2218   time: 0.24s   best: 69.7675
2023-11-16 08:00:48,297:INFO:  Epoch 289/500:  train Loss: 70.6995   val Loss: 70.0217   time: 0.25s   best: 69.7675
2023-11-16 08:00:48,542:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:48,705:INFO:  Epoch 290/500:  train Loss: 72.2965   val Loss: 69.4616   time: 0.24s   best: 69.4616
2023-11-16 08:00:48,960:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:49,094:INFO:  Epoch 291/500:  train Loss: 70.6887   val Loss: 68.9180   time: 0.25s   best: 68.9180
2023-11-16 08:00:49,391:INFO:  Epoch 292/500:  train Loss: 70.4041   val Loss: 70.8512   time: 0.29s   best: 68.9180
2023-11-16 08:00:49,639:INFO:  Epoch 293/500:  train Loss: 71.8784   val Loss: 70.3674   time: 0.24s   best: 68.9180
2023-11-16 08:00:49,908:INFO:  Epoch 294/500:  train Loss: 72.5777   val Loss: 71.7319   time: 0.26s   best: 68.9180
2023-11-16 08:00:50,155:INFO:  Epoch 295/500:  train Loss: 73.2115   val Loss: 72.4903   time: 0.24s   best: 68.9180
2023-11-16 08:00:50,422:INFO:  Epoch 296/500:  train Loss: 72.5979   val Loss: 70.4672   time: 0.26s   best: 68.9180
2023-11-16 08:00:50,671:INFO:  Epoch 297/500:  train Loss: 72.9916   val Loss: 70.0143   time: 0.24s   best: 68.9180
2023-11-16 08:00:50,939:INFO:  Epoch 298/500:  train Loss: 73.0970   val Loss: 70.1560   time: 0.26s   best: 68.9180
2023-11-16 08:00:51,186:INFO:  Epoch 299/500:  train Loss: 75.0158   val Loss: 72.5575   time: 0.24s   best: 68.9180
2023-11-16 08:00:51,583:INFO:  Epoch 300/500:  train Loss: 73.1784   val Loss: 72.1294   time: 0.37s   best: 68.9180
2023-11-16 08:00:51,835:INFO:  Epoch 301/500:  train Loss: 71.8333   val Loss: 71.5312   time: 0.24s   best: 68.9180
2023-11-16 08:00:52,099:INFO:  Epoch 302/500:  train Loss: 72.3795   val Loss: 72.3413   time: 0.25s   best: 68.9180
2023-11-16 08:00:52,346:INFO:  Epoch 303/500:  train Loss: 82.3287   val Loss: 87.1043   time: 0.24s   best: 68.9180
2023-11-16 08:00:52,614:INFO:  Epoch 304/500:  train Loss: 83.7744   val Loss: 78.2728   time: 0.26s   best: 68.9180
2023-11-16 08:00:52,862:INFO:  Epoch 305/500:  train Loss: 78.5987   val Loss: 77.9646   time: 0.24s   best: 68.9180
2023-11-16 08:00:53,128:INFO:  Epoch 306/500:  train Loss: 75.8167   val Loss: 73.9072   time: 0.26s   best: 68.9180
2023-11-16 08:00:53,376:INFO:  Epoch 307/500:  train Loss: 73.6984   val Loss: 72.9992   time: 0.24s   best: 68.9180
2023-11-16 08:00:53,689:INFO:  Epoch 308/500:  train Loss: 73.7909   val Loss: 71.5611   time: 0.31s   best: 68.9180
2023-11-16 08:00:53,950:INFO:  Epoch 309/500:  train Loss: 72.0560   val Loss: 71.0523   time: 0.25s   best: 68.9180
2023-11-16 08:00:54,293:INFO:  Epoch 310/500:  train Loss: 72.2033   val Loss: 71.8354   time: 0.33s   best: 68.9180
2023-11-16 08:00:54,543:INFO:  Epoch 311/500:  train Loss: 72.9694   val Loss: 71.6275   time: 0.24s   best: 68.9180
2023-11-16 08:00:54,809:INFO:  Epoch 312/500:  train Loss: 72.5236   val Loss: 71.6442   time: 0.25s   best: 68.9180
2023-11-16 08:00:55,056:INFO:  Epoch 313/500:  train Loss: 72.3089   val Loss: 71.4591   time: 0.24s   best: 68.9180
2023-11-16 08:00:55,323:INFO:  Epoch 314/500:  train Loss: 72.0311   val Loss: 71.5160   time: 0.26s   best: 68.9180
2023-11-16 08:00:55,573:INFO:  Epoch 315/500:  train Loss: 72.2354   val Loss: 71.3376   time: 0.24s   best: 68.9180
2023-11-16 08:00:55,878:INFO:  Epoch 316/500:  train Loss: 71.6621   val Loss: 70.2592   time: 0.30s   best: 68.9180
2023-11-16 08:00:56,126:INFO:  Epoch 317/500:  train Loss: 70.8350   val Loss: 70.2309   time: 0.24s   best: 68.9180
2023-11-16 08:00:56,392:INFO:  Epoch 318/500:  train Loss: 71.5693   val Loss: 69.9855   time: 0.26s   best: 68.9180
2023-11-16 08:00:56,635:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:56,756:INFO:  Epoch 319/500:  train Loss: 70.4270   val Loss: 68.5170   time: 0.24s   best: 68.5170
2023-11-16 08:00:57,004:INFO:  Epoch 320/500:  train Loss: 69.7530   val Loss: 69.4148   time: 0.24s   best: 68.5170
2023-11-16 08:00:57,255:INFO:  Epoch 321/500:  train Loss: 70.4873   val Loss: 70.6167   time: 0.24s   best: 68.5170
2023-11-16 08:00:57,522:INFO:  Epoch 322/500:  train Loss: 71.4309   val Loss: 69.8785   time: 0.26s   best: 68.5170
2023-11-16 08:00:57,825:INFO:  Epoch 323/500:  train Loss: 70.0062   val Loss: 69.1145   time: 0.29s   best: 68.5170
2023-11-16 08:00:58,073:INFO:  Epoch 324/500:  train Loss: 70.4021   val Loss: 68.6364   time: 0.24s   best: 68.5170
2023-11-16 08:00:58,338:INFO:  Epoch 325/500:  train Loss: 70.7941   val Loss: 70.3383   time: 0.25s   best: 68.5170
2023-11-16 08:00:58,589:INFO:  Epoch 326/500:  train Loss: 71.3567   val Loss: 70.2722   time: 0.24s   best: 68.5170
2023-11-16 08:00:58,837:INFO:  Epoch 327/500:  train Loss: 70.8121   val Loss: 69.3332   time: 0.24s   best: 68.5170
2023-11-16 08:00:59,097:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:00:59,230:INFO:  Epoch 328/500:  train Loss: 69.6010   val Loss: 68.0760   time: 0.26s   best: 68.0760
2023-11-16 08:00:59,491:INFO:  Epoch 329/500:  train Loss: 71.2357   val Loss: 70.7900   time: 0.25s   best: 68.0760
2023-11-16 08:00:59,742:INFO:  Epoch 330/500:  train Loss: 71.8776   val Loss: 69.3669   time: 0.24s   best: 68.0760
2023-11-16 08:01:00,045:INFO:  Epoch 331/500:  train Loss: 71.5903   val Loss: 69.2611   time: 0.29s   best: 68.0760
2023-11-16 08:01:00,293:INFO:  Epoch 332/500:  train Loss: 70.2338   val Loss: 68.1597   time: 0.24s   best: 68.0760
2023-11-16 08:01:00,561:INFO:  Epoch 333/500:  train Loss: 69.4912   val Loss: 68.1001   time: 0.26s   best: 68.0760
2023-11-16 08:01:00,802:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:00,896:INFO:  Epoch 334/500:  train Loss: 70.2562   val Loss: 67.7732   time: 0.24s   best: 67.7732
2023-11-16 08:01:01,159:INFO:  Epoch 335/500:  train Loss: 70.8998   val Loss: 69.6135   time: 0.25s   best: 67.7732
2023-11-16 08:01:01,401:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:01,454:INFO:  Epoch 336/500:  train Loss: 69.6999   val Loss: 67.6353   time: 0.24s   best: 67.6353
2023-11-16 08:01:01,720:INFO:  Epoch 337/500:  train Loss: 70.2081   val Loss: 69.6406   time: 0.26s   best: 67.6353
2023-11-16 08:01:02,024:INFO:  Epoch 338/500:  train Loss: 69.4156   val Loss: 69.9655   time: 0.29s   best: 67.6353
2023-11-16 08:01:02,271:INFO:  Epoch 339/500:  train Loss: 72.5106   val Loss: 73.4486   time: 0.24s   best: 67.6353
2023-11-16 08:01:02,520:INFO:  Epoch 340/500:  train Loss: 70.6850   val Loss: 71.2997   time: 0.24s   best: 67.6353
2023-11-16 08:01:02,787:INFO:  Epoch 341/500:  train Loss: 69.8004   val Loss: 69.8862   time: 0.26s   best: 67.6353
2023-11-16 08:01:03,036:INFO:  Epoch 342/500:  train Loss: 70.2239   val Loss: 68.8336   time: 0.24s   best: 67.6353
2023-11-16 08:01:03,294:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:03,322:INFO:  Epoch 343/500:  train Loss: 69.4277   val Loss: 67.2453   time: 0.25s   best: 67.2453
2023-11-16 08:01:03,566:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:03,689:INFO:  Epoch 344/500:  train Loss: 68.1670   val Loss: 66.8597   time: 0.24s   best: 66.8597
2023-11-16 08:01:03,935:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:04,052:INFO:  Epoch 345/500:  train Loss: 68.2930   val Loss: 66.2649   time: 0.24s   best: 66.2649
2023-11-16 08:01:04,317:INFO:  Epoch 346/500:  train Loss: 67.4441   val Loss: 67.3429   time: 0.26s   best: 66.2649
2023-11-16 08:01:04,567:INFO:  Epoch 347/500:  train Loss: 68.5800   val Loss: 66.2918   time: 0.24s   best: 66.2649
2023-11-16 08:01:04,833:INFO:  Epoch 348/500:  train Loss: 67.7251   val Loss: 66.7403   time: 0.26s   best: 66.2649
2023-11-16 08:01:05,082:INFO:  Epoch 349/500:  train Loss: 68.6217   val Loss: 67.1257   time: 0.24s   best: 66.2649
2023-11-16 08:01:05,348:INFO:  Epoch 350/500:  train Loss: 68.5783   val Loss: 66.5266   time: 0.25s   best: 66.2649
2023-11-16 08:01:05,597:INFO:  Epoch 351/500:  train Loss: 69.6022   val Loss: 67.5864   time: 0.24s   best: 66.2649
2023-11-16 08:01:05,868:INFO:  Epoch 352/500:  train Loss: 69.2944   val Loss: 67.1298   time: 0.26s   best: 66.2649
2023-11-16 08:01:06,151:INFO:  Epoch 353/500:  train Loss: 70.1125   val Loss: 67.9224   time: 0.27s   best: 66.2649
2023-11-16 08:01:06,415:INFO:  Epoch 354/500:  train Loss: 68.8184   val Loss: 68.2369   time: 0.25s   best: 66.2649
2023-11-16 08:01:06,664:INFO:  Epoch 355/500:  train Loss: 69.6231   val Loss: 67.7575   time: 0.24s   best: 66.2649
2023-11-16 08:01:06,933:INFO:  Epoch 356/500:  train Loss: 69.4795   val Loss: 66.8435   time: 0.26s   best: 66.2649
2023-11-16 08:01:07,182:INFO:  Epoch 357/500:  train Loss: 68.8003   val Loss: 66.4711   time: 0.24s   best: 66.2649
2023-11-16 08:01:07,443:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:07,548:INFO:  Epoch 358/500:  train Loss: 67.1817   val Loss: 65.3384   time: 0.26s   best: 65.3384
2023-11-16 08:01:07,815:INFO:  Epoch 359/500:  train Loss: 66.6497   val Loss: 65.8004   time: 0.26s   best: 65.3384
2023-11-16 08:01:08,063:INFO:  Epoch 360/500:  train Loss: 67.8964   val Loss: 65.6292   time: 0.24s   best: 65.3384
2023-11-16 08:01:08,356:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:08,521:INFO:  Epoch 361/500:  train Loss: 66.4452   val Loss: 64.6097   time: 0.29s   best: 64.6097
2023-11-16 08:01:08,764:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:08,844:INFO:  Epoch 362/500:  train Loss: 66.1537   val Loss: 64.4871   time: 0.24s   best: 64.4871
2023-11-16 08:01:09,096:INFO:  Epoch 363/500:  train Loss: 66.2486   val Loss: 66.8311   time: 0.25s   best: 64.4871
2023-11-16 08:01:09,375:INFO:  Epoch 364/500:  train Loss: 69.0453   val Loss: 65.9851   time: 0.27s   best: 64.4871
2023-11-16 08:01:09,624:INFO:  Epoch 365/500:  train Loss: 69.8863   val Loss: 66.8733   time: 0.24s   best: 64.4871
2023-11-16 08:01:09,893:INFO:  Epoch 366/500:  train Loss: 72.5475   val Loss: 71.8607   time: 0.26s   best: 64.4871
2023-11-16 08:01:10,141:INFO:  Epoch 367/500:  train Loss: 73.4557   val Loss: 68.7297   time: 0.24s   best: 64.4871
2023-11-16 08:01:10,442:INFO:  Epoch 368/500:  train Loss: 73.9278   val Loss: 71.2944   time: 0.29s   best: 64.4871
2023-11-16 08:01:10,691:INFO:  Epoch 369/500:  train Loss: 69.9866   val Loss: 71.3426   time: 0.24s   best: 64.4871
2023-11-16 08:01:10,958:INFO:  Epoch 370/500:  train Loss: 71.9175   val Loss: 70.0806   time: 0.24s   best: 64.4871
2023-11-16 08:01:11,205:INFO:  Epoch 371/500:  train Loss: 69.5228   val Loss: 66.8699   time: 0.24s   best: 64.4871
2023-11-16 08:01:11,452:INFO:  Epoch 372/500:  train Loss: 68.0632   val Loss: 68.4803   time: 0.24s   best: 64.4871
2023-11-16 08:01:11,720:INFO:  Epoch 373/500:  train Loss: 71.5007   val Loss: 68.2749   time: 0.26s   best: 64.4871
2023-11-16 08:01:11,973:INFO:  Epoch 374/500:  train Loss: 70.4399   val Loss: 68.8992   time: 0.24s   best: 64.4871
2023-11-16 08:01:12,238:INFO:  Epoch 375/500:  train Loss: 70.4169   val Loss: 68.1186   time: 0.25s   best: 64.4871
2023-11-16 08:01:12,540:INFO:  Epoch 376/500:  train Loss: 73.2546   val Loss: 75.5179   time: 0.27s   best: 64.4871
2023-11-16 08:01:12,789:INFO:  Epoch 377/500:  train Loss: 77.0490   val Loss: 76.7253   time: 0.24s   best: 64.4871
2023-11-16 08:01:13,037:INFO:  Epoch 378/500:  train Loss: 76.5240   val Loss: 75.5092   time: 0.24s   best: 64.4871
2023-11-16 08:01:13,303:INFO:  Epoch 379/500:  train Loss: 74.8919   val Loss: 72.7237   time: 0.25s   best: 64.4871
2023-11-16 08:01:13,552:INFO:  Epoch 380/500:  train Loss: 72.6859   val Loss: 70.5064   time: 0.24s   best: 64.4871
2023-11-16 08:01:13,822:INFO:  Epoch 381/500:  train Loss: 70.7430   val Loss: 68.9058   time: 0.26s   best: 64.4871
2023-11-16 08:01:14,071:INFO:  Epoch 382/500:  train Loss: 71.0154   val Loss: 67.9839   time: 0.24s   best: 64.4871
2023-11-16 08:01:14,336:INFO:  Epoch 383/500:  train Loss: 69.6385   val Loss: 68.9538   time: 0.25s   best: 64.4871
2023-11-16 08:01:14,620:INFO:  Epoch 384/500:  train Loss: 73.9880   val Loss: 70.0332   time: 0.27s   best: 64.4871
2023-11-16 08:01:14,884:INFO:  Epoch 385/500:  train Loss: 71.9158   val Loss: 74.7478   time: 0.25s   best: 64.4871
2023-11-16 08:01:15,132:INFO:  Epoch 386/500:  train Loss: 75.8917   val Loss: 72.9269   time: 0.24s   best: 64.4871
2023-11-16 08:01:15,398:INFO:  Epoch 387/500:  train Loss: 73.6923   val Loss: 73.2285   time: 0.25s   best: 64.4871
2023-11-16 08:01:15,647:INFO:  Epoch 388/500:  train Loss: 72.7774   val Loss: 71.0959   time: 0.24s   best: 64.4871
2023-11-16 08:01:15,918:INFO:  Epoch 389/500:  train Loss: 71.3115   val Loss: 70.0090   time: 0.26s   best: 64.4871
2023-11-16 08:01:16,165:INFO:  Epoch 390/500:  train Loss: 69.9756   val Loss: 68.7260   time: 0.24s   best: 64.4871
2023-11-16 08:01:16,431:INFO:  Epoch 391/500:  train Loss: 69.8474   val Loss: 67.3731   time: 0.26s   best: 64.4871
2023-11-16 08:01:16,715:INFO:  Epoch 392/500:  train Loss: 68.7248   val Loss: 66.9202   time: 0.27s   best: 64.4871
2023-11-16 08:01:16,979:INFO:  Epoch 393/500:  train Loss: 68.4842   val Loss: 65.9724   time: 0.25s   best: 64.4871
2023-11-16 08:01:17,226:INFO:  Epoch 394/500:  train Loss: 68.0351   val Loss: 67.2150   time: 0.24s   best: 64.4871
2023-11-16 08:01:17,493:INFO:  Epoch 395/500:  train Loss: 85.3960   val Loss: 88.8294   time: 0.26s   best: 64.4871
2023-11-16 08:01:17,744:INFO:  Epoch 396/500:  train Loss: 89.3436   val Loss: 87.4762   time: 0.24s   best: 64.4871
2023-11-16 08:01:18,013:INFO:  Epoch 397/500:  train Loss: 84.5179   val Loss: 80.9126   time: 0.26s   best: 64.4871
2023-11-16 08:01:18,260:INFO:  Epoch 398/500:  train Loss: 78.9740   val Loss: 78.8053   time: 0.24s   best: 64.4871
2023-11-16 08:01:18,528:INFO:  Epoch 399/500:  train Loss: 77.6860   val Loss: 75.7429   time: 0.26s   best: 64.4871
2023-11-16 08:01:18,885:INFO:  Epoch 400/500:  train Loss: 74.8434   val Loss: 73.5207   time: 0.33s   best: 64.4871
2023-11-16 08:01:19,133:INFO:  Epoch 401/500:  train Loss: 74.5420   val Loss: 72.0514   time: 0.24s   best: 64.4871
2023-11-16 08:01:19,399:INFO:  Epoch 402/500:  train Loss: 72.4224   val Loss: 71.9648   time: 0.26s   best: 64.4871
2023-11-16 08:01:19,648:INFO:  Epoch 403/500:  train Loss: 72.5423   val Loss: 71.2007   time: 0.24s   best: 64.4871
2023-11-16 08:01:19,919:INFO:  Epoch 404/500:  train Loss: 71.8082   val Loss: 70.7831   time: 0.26s   best: 64.4871
2023-11-16 08:01:20,167:INFO:  Epoch 405/500:  train Loss: 71.1842   val Loss: 70.7635   time: 0.24s   best: 64.4871
2023-11-16 08:01:20,431:INFO:  Epoch 406/500:  train Loss: 71.1761   val Loss: 69.8806   time: 0.24s   best: 64.4871
2023-11-16 08:01:20,718:INFO:  Epoch 407/500:  train Loss: 71.6490   val Loss: 68.7888   time: 0.28s   best: 64.4871
2023-11-16 08:01:20,983:INFO:  Epoch 408/500:  train Loss: 69.7966   val Loss: 67.6238   time: 0.25s   best: 64.4871
2023-11-16 08:01:21,272:INFO:  Epoch 409/500:  train Loss: 68.9211   val Loss: 67.9279   time: 0.27s   best: 64.4871
2023-11-16 08:01:21,538:INFO:  Epoch 410/500:  train Loss: 69.5345   val Loss: 67.2487   time: 0.26s   best: 64.4871
2023-11-16 08:01:21,787:INFO:  Epoch 411/500:  train Loss: 68.5144   val Loss: 68.1612   time: 0.24s   best: 64.4871
2023-11-16 08:01:22,058:INFO:  Epoch 412/500:  train Loss: 69.7268   val Loss: 67.3110   time: 0.26s   best: 64.4871
2023-11-16 08:01:22,304:INFO:  Epoch 413/500:  train Loss: 69.7408   val Loss: 65.6943   time: 0.24s   best: 64.4871
2023-11-16 08:01:22,572:INFO:  Epoch 414/500:  train Loss: 68.5167   val Loss: 67.9231   time: 0.26s   best: 64.4871
2023-11-16 08:01:22,891:INFO:  Epoch 415/500:  train Loss: 68.5853   val Loss: 66.1894   time: 0.31s   best: 64.4871
2023-11-16 08:01:23,153:INFO:  Epoch 416/500:  train Loss: 67.2939   val Loss: 66.3413   time: 0.25s   best: 64.4871
2023-11-16 08:01:23,399:INFO:  Epoch 417/500:  train Loss: 67.3298   val Loss: 65.7743   time: 0.24s   best: 64.4871
2023-11-16 08:01:23,667:INFO:  Epoch 418/500:  train Loss: 67.5325   val Loss: 65.6959   time: 0.26s   best: 64.4871
2023-11-16 08:01:23,921:INFO:  Epoch 419/500:  train Loss: 66.3287   val Loss: 65.2984   time: 0.24s   best: 64.4871
2023-11-16 08:01:24,193:INFO:  Epoch 420/500:  train Loss: 66.6837   val Loss: 64.7222   time: 0.27s   best: 64.4871
2023-11-16 08:01:24,434:INFO:  Epoch 421/500:  train Loss: 66.2467   val Loss: 65.4373   time: 0.24s   best: 64.4871
2023-11-16 08:01:24,708:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:24,737:INFO:  Epoch 422/500:  train Loss: 66.3714   val Loss: 64.3649   time: 0.27s   best: 64.3649
2023-11-16 08:01:25,015:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:25,167:INFO:  Epoch 423/500:  train Loss: 67.6360   val Loss: 63.7582   time: 0.27s   best: 63.7582
2023-11-16 08:01:25,414:INFO:  Epoch 424/500:  train Loss: 67.2205   val Loss: 65.8444   time: 0.24s   best: 63.7582
2023-11-16 08:01:25,681:INFO:  Epoch 425/500:  train Loss: 67.5088   val Loss: 63.8553   time: 0.24s   best: 63.7582
2023-11-16 08:01:25,935:INFO:  Epoch 426/500:  train Loss: 69.0347   val Loss: 66.5730   time: 0.24s   best: 63.7582
2023-11-16 08:01:26,182:INFO:  Epoch 427/500:  train Loss: 67.7527   val Loss: 63.9962   time: 0.24s   best: 63.7582
2023-11-16 08:01:26,448:INFO:  Epoch 428/500:  train Loss: 68.8032   val Loss: 66.6527   time: 0.26s   best: 63.7582
2023-11-16 08:01:26,697:INFO:  Epoch 429/500:  train Loss: 69.2864   val Loss: 66.1382   time: 0.24s   best: 63.7582
2023-11-16 08:01:26,998:INFO:  Epoch 430/500:  train Loss: 73.3432   val Loss: 67.0399   time: 0.29s   best: 63.7582
2023-11-16 08:01:27,264:INFO:  Epoch 431/500:  train Loss: 79.0860   val Loss: 75.5065   time: 0.24s   best: 63.7582
2023-11-16 08:01:27,511:INFO:  Epoch 432/500:  train Loss: 76.4951   val Loss: 76.3362   time: 0.24s   best: 63.7582
2023-11-16 08:01:27,762:INFO:  Epoch 433/500:  train Loss: 73.3224   val Loss: 71.4210   time: 0.24s   best: 63.7582
2023-11-16 08:01:28,034:INFO:  Epoch 434/500:  train Loss: 69.8214   val Loss: 80.4265   time: 0.26s   best: 63.7582
2023-11-16 08:01:28,281:INFO:  Epoch 435/500:  train Loss: 97.5819   val Loss: 84.9415   time: 0.24s   best: 63.7582
2023-11-16 08:01:28,547:INFO:  Epoch 436/500:  train Loss: 82.1386   val Loss: 76.0548   time: 0.26s   best: 63.7582
2023-11-16 08:01:28,796:INFO:  Epoch 437/500:  train Loss: 79.4819   val Loss: 77.3337   time: 0.24s   best: 63.7582
2023-11-16 08:01:29,097:INFO:  Epoch 438/500:  train Loss: 76.9670   val Loss: 76.8849   time: 0.29s   best: 63.7582
2023-11-16 08:01:29,344:INFO:  Epoch 439/500:  train Loss: 78.5506   val Loss: 75.8679   time: 0.24s   best: 63.7582
2023-11-16 08:01:29,611:INFO:  Epoch 440/500:  train Loss: 80.9496   val Loss: 78.7831   time: 0.26s   best: 63.7582
2023-11-16 08:01:29,864:INFO:  Epoch 441/500:  train Loss: 78.0301   val Loss: 76.6566   time: 0.24s   best: 63.7582
2023-11-16 08:01:30,130:INFO:  Epoch 442/500:  train Loss: 76.1125   val Loss: 74.2724   time: 0.25s   best: 63.7582
2023-11-16 08:01:30,377:INFO:  Epoch 443/500:  train Loss: 75.6392   val Loss: 71.0588   time: 0.24s   best: 63.7582
2023-11-16 08:01:30,645:INFO:  Epoch 444/500:  train Loss: 73.7470   val Loss: 71.7464   time: 0.26s   best: 63.7582
2023-11-16 08:01:30,892:INFO:  Epoch 445/500:  train Loss: 75.2984   val Loss: 71.7076   time: 0.24s   best: 63.7582
2023-11-16 08:01:31,194:INFO:  Epoch 446/500:  train Loss: 72.7566   val Loss: 68.8725   time: 0.29s   best: 63.7582
2023-11-16 08:01:31,441:INFO:  Epoch 447/500:  train Loss: 73.1739   val Loss: 70.5697   time: 0.24s   best: 63.7582
2023-11-16 08:01:31,707:INFO:  Epoch 448/500:  train Loss: 71.1895   val Loss: 72.2991   time: 0.26s   best: 63.7582
2023-11-16 08:01:31,961:INFO:  Epoch 449/500:  train Loss: 71.6280   val Loss: 69.8877   time: 0.24s   best: 63.7582
2023-11-16 08:01:32,228:INFO:  Epoch 450/500:  train Loss: 70.6804   val Loss: 69.2583   time: 0.26s   best: 63.7582
2023-11-16 08:01:32,475:INFO:  Epoch 451/500:  train Loss: 70.7274   val Loss: 68.1313   time: 0.24s   best: 63.7582
2023-11-16 08:01:32,741:INFO:  Epoch 452/500:  train Loss: 68.7631   val Loss: 65.9988   time: 0.26s   best: 63.7582
2023-11-16 08:01:32,989:INFO:  Epoch 453/500:  train Loss: 67.3777   val Loss: 64.2071   time: 0.24s   best: 63.7582
2023-11-16 08:01:33,290:INFO:  Epoch 454/500:  train Loss: 66.3876   val Loss: 66.4828   time: 0.29s   best: 63.7582
2023-11-16 08:01:33,538:INFO:  Epoch 455/500:  train Loss: 67.0042   val Loss: 65.3024   time: 0.24s   best: 63.7582
2023-11-16 08:01:33,806:INFO:  Epoch 456/500:  train Loss: 66.3624   val Loss: 64.7684   time: 0.26s   best: 63.7582
2023-11-16 08:01:34,057:INFO:  Epoch 457/500:  train Loss: 66.4290   val Loss: 64.2331   time: 0.24s   best: 63.7582
2023-11-16 08:01:34,316:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:34,345:INFO:  Epoch 458/500:  train Loss: 66.5215   val Loss: 63.0520   time: 0.25s   best: 63.0520
2023-11-16 08:01:34,595:INFO:  Epoch 459/500:  train Loss: 65.8841   val Loss: 65.7151   time: 0.24s   best: 63.0520
2023-11-16 08:01:34,861:INFO:  Epoch 460/500:  train Loss: 66.8889   val Loss: 65.0466   time: 0.26s   best: 63.0520
2023-11-16 08:01:35,103:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:35,280:INFO:  Epoch 461/500:  train Loss: 66.3586   val Loss: 62.9075   time: 0.24s   best: 62.9075
2023-11-16 08:01:35,538:INFO:  Epoch 462/500:  train Loss: 65.4137   val Loss: 64.4682   time: 0.25s   best: 62.9075
2023-11-16 08:01:35,792:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:35,872:INFO:  Epoch 463/500:  train Loss: 65.9162   val Loss: 62.2078   time: 0.25s   best: 62.2078
2023-11-16 08:01:36,120:INFO:  Epoch 464/500:  train Loss: 63.8505   val Loss: 63.9878   time: 0.24s   best: 62.2078
2023-11-16 08:01:36,377:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:36,492:INFO:  Epoch 465/500:  train Loss: 64.0799   val Loss: 61.8792   time: 0.25s   best: 61.8792
2023-11-16 08:01:36,755:INFO:  Epoch 466/500:  train Loss: 65.0952   val Loss: 64.3809   time: 0.25s   best: 61.8792
2023-11-16 08:01:37,003:INFO:  Epoch 467/500:  train Loss: 65.3663   val Loss: 62.9423   time: 0.24s   best: 61.8792
2023-11-16 08:01:37,267:INFO:  Epoch 468/500:  train Loss: 65.2315   val Loss: 64.2091   time: 0.25s   best: 61.8792
2023-11-16 08:01:37,550:INFO:  Epoch 469/500:  train Loss: 65.2095   val Loss: 63.5730   time: 0.27s   best: 61.8792
2023-11-16 08:01:37,820:INFO:  Epoch 470/500:  train Loss: 66.3104   val Loss: 64.7792   time: 0.26s   best: 61.8792
2023-11-16 08:01:38,071:INFO:  Epoch 471/500:  train Loss: 64.3618   val Loss: 63.0532   time: 0.24s   best: 61.8792
2023-11-16 08:01:38,337:INFO:  Epoch 472/500:  train Loss: 65.0280   val Loss: 63.0809   time: 0.25s   best: 61.8792
2023-11-16 08:01:38,586:INFO:  Epoch 473/500:  train Loss: 65.9683   val Loss: 65.9052   time: 0.24s   best: 61.8792
2023-11-16 08:01:38,851:INFO:  Epoch 474/500:  train Loss: 67.3390   val Loss: 65.0631   time: 0.25s   best: 61.8792
2023-11-16 08:01:39,099:INFO:  Epoch 475/500:  train Loss: 66.4429   val Loss: 64.5647   time: 0.24s   best: 61.8792
2023-11-16 08:01:39,374:INFO:  Epoch 476/500:  train Loss: 67.4680   val Loss: 69.3247   time: 0.27s   best: 61.8792
2023-11-16 08:01:39,658:INFO:  Epoch 477/500:  train Loss: 71.0280   val Loss: 67.9996   time: 0.28s   best: 61.8792
2023-11-16 08:01:39,935:INFO:  Epoch 478/500:  train Loss: 69.0163   val Loss: 66.5118   time: 0.27s   best: 61.8792
2023-11-16 08:01:40,183:INFO:  Epoch 479/500:  train Loss: 66.2277   val Loss: 64.9953   time: 0.24s   best: 61.8792
2023-11-16 08:01:40,449:INFO:  Epoch 480/500:  train Loss: 66.7681   val Loss: 64.4516   time: 0.25s   best: 61.8792
2023-11-16 08:01:40,698:INFO:  Epoch 481/500:  train Loss: 67.8938   val Loss: 64.9214   time: 0.24s   best: 61.8792
2023-11-16 08:01:40,964:INFO:  Epoch 482/500:  train Loss: 67.4779   val Loss: 65.5088   time: 0.26s   best: 61.8792
2023-11-16 08:01:41,211:INFO:  Epoch 483/500:  train Loss: 67.0251   val Loss: 64.5088   time: 0.24s   best: 61.8792
2023-11-16 08:01:41,500:INFO:  Epoch 484/500:  train Loss: 65.2459   val Loss: 63.1721   time: 0.27s   best: 61.8792
2023-11-16 08:01:41,762:INFO:  Epoch 485/500:  train Loss: 64.7174   val Loss: 64.0368   time: 0.25s   best: 61.8792
2023-11-16 08:01:42,030:INFO:  Epoch 486/500:  train Loss: 64.7752   val Loss: 63.0070   time: 0.26s   best: 61.8792
2023-11-16 08:01:42,277:INFO:  Epoch 487/500:  train Loss: 64.6149   val Loss: 61.8968   time: 0.24s   best: 61.8792
2023-11-16 08:01:42,543:INFO:  Epoch 488/500:  train Loss: 63.7304   val Loss: 62.5609   time: 0.26s   best: 61.8792
2023-11-16 08:01:42,786:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:42,936:INFO:  Epoch 489/500:  train Loss: 63.8098   val Loss: 61.2879   time: 0.24s   best: 61.2879
2023-11-16 08:01:43,198:INFO:  Epoch 490/500:  train Loss: 65.6496   val Loss: 62.1540   time: 0.25s   best: 61.2879
2023-11-16 08:01:43,446:INFO:  Epoch 491/500:  train Loss: 63.2046   val Loss: 61.7036   time: 0.24s   best: 61.2879
2023-11-16 08:01:43,750:INFO:  Epoch 492/500:  train Loss: 63.9114   val Loss: 62.6151   time: 0.29s   best: 61.2879
2023-11-16 08:01:44,002:INFO:  Epoch 493/500:  train Loss: 66.4308   val Loss: 62.6179   time: 0.24s   best: 61.2879
2023-11-16 08:01:44,268:INFO:  Epoch 494/500:  train Loss: 68.5419   val Loss: 62.8480   time: 0.25s   best: 61.2879
2023-11-16 08:01:44,516:INFO:  Epoch 495/500:  train Loss: 67.3296   val Loss: 61.7290   time: 0.24s   best: 61.2879
2023-11-16 08:01:44,783:INFO:  Epoch 496/500:  train Loss: 63.5592   val Loss: 62.7082   time: 0.26s   best: 61.2879
2023-11-16 08:01:45,025:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:45,159:INFO:  Epoch 497/500:  train Loss: 65.1669   val Loss: 60.8315   time: 0.24s   best: 60.8315
2023-11-16 08:01:45,401:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:45,490:INFO:  Epoch 498/500:  train Loss: 61.9866   val Loss: 60.6314   time: 0.24s   best: 60.6314
2023-11-16 08:01:45,786:INFO:  Epoch 499/500:  train Loss: 62.0367   val Loss: 62.6683   time: 0.29s   best: 60.6314
2023-11-16 08:01:46,076:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 debug (0.05 dropout)_4008.pt
2023-11-16 08:01:46,200:INFO:  Epoch 500/500:  train Loss: 63.6994   val Loss: 60.3672   time: 0.29s   best: 60.3672
2023-11-16 08:01:46,201:INFO:  -----> Training complete in 2m 25s   best validation loss: 60.3672
 
2023-11-16 08:05:50,576:INFO:  Epoch 326/500:  train Loss: 18.0012   val Loss: 23.1099   time: 429.19s   best: 22.4906
2023-11-16 08:13:03,893:INFO:  Epoch 327/500:  train Loss: 17.9506   val Loss: 25.5621   time: 433.29s   best: 22.4906
2023-11-16 08:20:12,737:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 08:20:12,772:INFO:  Epoch 328/500:  train Loss: 17.7670   val Loss: 22.4812   time: 428.83s   best: 22.4812
2023-11-16 08:27:26,374:INFO:  Epoch 329/500:  train Loss: 17.7311   val Loss: 23.2011   time: 433.60s   best: 22.4812
2023-11-16 08:34:38,338:INFO:  Epoch 330/500:  train Loss: 17.6851   val Loss: 24.6609   time: 431.94s   best: 22.4812
2023-11-16 08:41:50,463:INFO:  Epoch 331/500:  train Loss: 17.9972   val Loss: 23.6563   time: 432.10s   best: 22.4812
2023-11-16 08:48:59,628:INFO:  Epoch 332/500:  train Loss: 18.2992   val Loss: 26.4555   time: 429.14s   best: 22.4812
2023-11-16 08:56:11,217:INFO:  Epoch 333/500:  train Loss: 18.0451   val Loss: 23.2454   time: 431.58s   best: 22.4812
2023-11-16 09:03:19,886:INFO:  Epoch 334/500:  train Loss: 17.7327   val Loss: 23.4484   time: 428.63s   best: 22.4812
2023-11-16 09:10:28,948:INFO:  Epoch 335/500:  train Loss: 18.0514   val Loss: 22.7869   time: 429.04s   best: 22.4812
2023-11-16 09:17:42,359:INFO:  Epoch 336/500:  train Loss: 17.6889   val Loss: 23.4952   time: 433.39s   best: 22.4812
2023-11-16 09:24:53,903:INFO:  Epoch 337/500:  train Loss: 17.7222   val Loss: 23.1408   time: 431.51s   best: 22.4812
2023-11-16 09:32:05,398:INFO:  Epoch 338/500:  train Loss: 17.6161   val Loss: 22.9776   time: 431.46s   best: 22.4812
2023-11-16 09:39:19,056:INFO:  Epoch 339/500:  train Loss: 17.5863   val Loss: 23.0394   time: 433.63s   best: 22.4812
2023-11-16 09:46:30,815:INFO:  Epoch 340/500:  train Loss: 17.8240   val Loss: 24.8977   time: 431.75s   best: 22.4812
2023-11-16 09:53:39,190:INFO:  Epoch 341/500:  train Loss: 17.8540   val Loss: 22.8135   time: 428.35s   best: 22.4812
2023-11-16 10:00:51,140:INFO:  Epoch 342/500:  train Loss: 17.6238   val Loss: 22.8902   time: 431.92s   best: 22.4812
2023-11-16 10:08:03,736:INFO:  Epoch 343/500:  train Loss: 17.6535   val Loss: 23.9147   time: 432.58s   best: 22.4812
2023-11-16 10:15:12,819:INFO:  Epoch 344/500:  train Loss: 17.7893   val Loss: 22.9185   time: 429.06s   best: 22.4812
2023-11-16 10:22:21,450:INFO:  Epoch 345/500:  train Loss: 17.7139   val Loss: 26.2374   time: 428.61s   best: 22.4812
2023-11-16 10:29:33,669:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 10:29:33,702:INFO:  Epoch 346/500:  train Loss: 17.7928   val Loss: 22.4631   time: 432.21s   best: 22.4631
2023-11-16 10:36:45,456:INFO:  Epoch 347/500:  train Loss: 17.8119   val Loss: 22.8634   time: 431.75s   best: 22.4631
2023-11-16 10:43:58,468:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 10:43:58,507:INFO:  Epoch 348/500:  train Loss: 17.7804   val Loss: 22.2357   time: 432.99s   best: 22.2357
2023-11-16 10:51:10,555:INFO:  Epoch 349/500:  train Loss: 17.6987   val Loss: 23.1097   time: 432.04s   best: 22.2357
2023-11-16 10:58:18,733:INFO:  Epoch 350/500:  train Loss: 17.6737   val Loss: 23.0850   time: 428.14s   best: 22.2357
2023-11-16 11:05:28,491:INFO:  Epoch 351/500:  train Loss: 17.6964   val Loss: 23.4504   time: 429.73s   best: 22.2357
2023-11-16 11:12:39,802:INFO:  Epoch 352/500:  train Loss: 17.6431   val Loss: 22.6088   time: 431.30s   best: 22.2357
2023-11-16 11:19:50,502:INFO:  Epoch 353/500:  train Loss: 17.7429   val Loss: 23.1848   time: 430.69s   best: 22.2357
2023-11-16 11:26:58,576:INFO:  Epoch 354/500:  train Loss: 17.8378   val Loss: 23.0666   time: 428.07s   best: 22.2357
2023-11-16 11:34:10,365:INFO:  Epoch 355/500:  train Loss: 17.7568   val Loss: 23.5108   time: 431.75s   best: 22.2357
2023-11-16 11:41:19,044:INFO:  Epoch 356/500:  train Loss: 17.8227   val Loss: 23.7025   time: 428.64s   best: 22.2357
2023-11-16 11:48:27,376:INFO:  Epoch 357/500:  train Loss: 18.0465   val Loss: 22.8412   time: 428.31s   best: 22.2357
2023-11-16 11:55:38,429:INFO:  Epoch 358/500:  train Loss: 17.6215   val Loss: 22.4312   time: 431.03s   best: 22.2357
2023-11-16 12:02:46,742:INFO:  Epoch 359/500:  train Loss: 17.8262   val Loss: 23.3106   time: 428.28s   best: 22.2357
2023-11-16 12:09:58,061:INFO:  Epoch 360/500:  train Loss: 17.5501   val Loss: 23.3601   time: 431.30s   best: 22.2357
2023-11-16 12:17:05,808:INFO:  Epoch 361/500:  train Loss: 17.5263   val Loss: 22.9885   time: 427.71s   best: 22.2357
2023-11-16 12:24:18,228:INFO:  Epoch 362/500:  train Loss: 17.5504   val Loss: 23.0013   time: 432.39s   best: 22.2357
2023-11-16 12:31:26,634:INFO:  Epoch 363/500:  train Loss: 17.9255   val Loss: 22.9398   time: 428.37s   best: 22.2357
2023-11-16 12:38:35,040:INFO:  Epoch 364/500:  train Loss: 17.8268   val Loss: 22.8499   time: 428.38s   best: 22.2357
2023-11-16 12:45:47,302:INFO:  Epoch 365/500:  train Loss: 17.5151   val Loss: 23.1625   time: 432.24s   best: 22.2357
2023-11-16 12:53:00,285:INFO:  Epoch 366/500:  train Loss: 17.5835   val Loss: 24.8067   time: 432.96s   best: 22.2357
2023-11-16 13:00:12,119:INFO:  Epoch 367/500:  train Loss: 17.4465   val Loss: 23.0356   time: 431.82s   best: 22.2357
2023-11-16 13:07:24,515:INFO:  Epoch 368/500:  train Loss: 17.4805   val Loss: 22.8242   time: 432.38s   best: 22.2357
2023-11-16 13:14:33,263:INFO:  Epoch 369/500:  train Loss: 17.4808   val Loss: 22.6761   time: 428.72s   best: 22.2357
2023-11-16 13:21:41,891:INFO:  Epoch 370/500:  train Loss: 17.8609   val Loss: 22.8848   time: 428.61s   best: 22.2357
2023-11-16 13:28:54,384:INFO:  Epoch 371/500:  train Loss: 17.5318   val Loss: 23.0980   time: 432.49s   best: 22.2357
2023-11-16 13:36:06,881:INFO:  Epoch 372/500:  train Loss: 17.8630   val Loss: 23.1847   time: 432.47s   best: 22.2357
2023-11-16 13:43:17,153:INFO:  Epoch 373/500:  train Loss: 18.2221   val Loss: 22.4939   time: 430.27s   best: 22.2357
2023-11-16 13:50:26,272:INFO:  Epoch 374/500:  train Loss: 17.6299   val Loss: 22.9004   time: 429.09s   best: 22.2357
2023-11-16 13:57:35,156:INFO:  Epoch 375/500:  train Loss: 17.5852   val Loss: 23.1054   time: 428.88s   best: 22.2357
2023-11-16 14:04:47,821:INFO:  Epoch 376/500:  train Loss: 17.4698   val Loss: 23.7755   time: 432.64s   best: 22.2357
2023-11-16 14:11:59,839:INFO:  Epoch 377/500:  train Loss: 17.8251   val Loss: 23.3932   time: 432.00s   best: 22.2357
2023-11-16 14:19:11,870:INFO:  Epoch 378/500:  train Loss: 17.5567   val Loss: 25.2293   time: 432.01s   best: 22.2357
2023-11-16 14:26:23,161:INFO:  Epoch 379/500:  train Loss: 17.5663   val Loss: 23.1095   time: 431.24s   best: 22.2357
2023-11-16 14:33:36,328:INFO:  Epoch 380/500:  train Loss: 17.5515   val Loss: 23.0214   time: 433.15s   best: 22.2357
2023-11-16 14:40:49,357:INFO:  Epoch 381/500:  train Loss: 17.4819   val Loss: 22.7523   time: 432.99s   best: 22.2357
2023-11-16 14:48:01,433:INFO:  Epoch 382/500:  train Loss: 17.5988   val Loss: 22.8688   time: 432.06s   best: 22.2357
2023-11-16 14:55:13,689:INFO:  Epoch 383/500:  train Loss: 17.9920   val Loss: 23.1549   time: 432.24s   best: 22.2357
2023-11-16 15:02:27,130:INFO:  Epoch 384/500:  train Loss: 17.5679   val Loss: 22.6825   time: 433.42s   best: 22.2357
2023-11-16 15:09:39,234:INFO:  Epoch 385/500:  train Loss: 17.6364   val Loss: 23.2123   time: 432.09s   best: 22.2357
2023-11-16 15:16:51,004:INFO:  Epoch 386/500:  train Loss: 17.7461   val Loss: 22.5413   time: 431.76s   best: 22.2357
2023-11-16 15:23:59,488:INFO:  Epoch 387/500:  train Loss: 17.5716   val Loss: 23.0804   time: 428.45s   best: 22.2357
2023-11-16 15:31:11,988:INFO:  Epoch 388/500:  train Loss: 17.6907   val Loss: 22.5725   time: 432.49s   best: 22.2357
2023-11-16 15:38:20,289:INFO:  Epoch 389/500:  train Loss: 17.5118   val Loss: 22.8874   time: 428.29s   best: 22.2357
2023-11-16 15:45:32,562:INFO:  Epoch 390/500:  train Loss: 17.4666   val Loss: 23.0629   time: 432.25s   best: 22.2357
2023-11-16 15:52:43,603:INFO:  Epoch 391/500:  train Loss: 17.6499   val Loss: 22.5825   time: 431.01s   best: 22.2357
2023-11-16 15:59:52,775:INFO:  Epoch 392/500:  train Loss: 17.5894   val Loss: 22.3923   time: 429.15s   best: 22.2357
2023-11-16 16:07:04,875:INFO:  Epoch 393/500:  train Loss: 17.5771   val Loss: 23.0808   time: 432.09s   best: 22.2357
2023-11-16 16:14:17,276:INFO:  Epoch 394/500:  train Loss: 17.9401   val Loss: 23.9631   time: 432.36s   best: 22.2357
2023-11-16 16:21:27,371:INFO:  Epoch 395/500:  train Loss: 17.4796   val Loss: 22.7262   time: 430.08s   best: 22.2357
2023-11-16 16:28:39,445:INFO:  Epoch 396/500:  train Loss: 17.5069   val Loss: 23.2092   time: 432.05s   best: 22.2357
2023-11-16 16:35:50,118:INFO:  Epoch 397/500:  train Loss: 17.3556   val Loss: 22.9199   time: 430.65s   best: 22.2357
2023-11-16 16:40:41,344:INFO:  Starting experiment lstm autoencoder perm50 (0.05 dropout)
2023-11-16 16:40:41,352:INFO:  Defining the model
2023-11-16 16:40:41,429:INFO:  Reading the dataset
2023-11-16 16:43:03,344:INFO:  Epoch 398/500:  train Loss: 17.4912   val Loss: 22.4929   time: 433.20s   best: 22.2357
2023-11-16 16:50:13,771:INFO:  Epoch 399/500:  train Loss: 17.4538   val Loss: 23.8568   time: 430.39s   best: 22.2357
2023-11-16 16:57:27,245:INFO:  Epoch 400/500:  train Loss: 17.7360   val Loss: 23.1847   time: 433.45s   best: 22.2357
2023-11-16 17:04:36,792:INFO:  Epoch 401/500:  train Loss: 17.4472   val Loss: 23.0054   time: 429.52s   best: 22.2357
2023-11-16 17:08:55,180:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 17:08:55,206:INFO:  Epoch 1/500:  train Loss: 74.7316   val Loss: 67.2034   time: 426.65s   best: 67.2034
2023-11-16 17:11:49,260:INFO:  Epoch 402/500:  train Loss: 17.4866   val Loss: 22.9380   time: 432.44s   best: 22.2357
2023-11-16 17:15:59,853:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 17:15:59,884:INFO:  Epoch 2/500:  train Loss: 63.1744   val Loss: 59.0379   time: 424.63s   best: 59.0379
2023-11-16 17:19:02,783:INFO:  Epoch 403/500:  train Loss: 17.6089   val Loss: 22.4566   time: 433.52s   best: 22.2357
2023-11-16 17:23:02,600:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 17:23:02,631:INFO:  Epoch 3/500:  train Loss: 56.6261   val Loss: 53.8626   time: 422.71s   best: 53.8626
2023-11-16 17:26:12,751:INFO:  Epoch 404/500:  train Loss: 17.3900   val Loss: 23.2118   time: 429.96s   best: 22.2357
2023-11-16 17:30:07,061:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 17:30:07,100:INFO:  Epoch 4/500:  train Loss: 50.9562   val Loss: 50.8142   time: 424.41s   best: 50.8142
2023-11-16 17:33:25,079:INFO:  Epoch 405/500:  train Loss: 17.4529   val Loss: 23.2546   time: 432.30s   best: 22.2357
2023-11-16 17:37:12,888:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 17:37:12,925:INFO:  Epoch 5/500:  train Loss: 46.5544   val Loss: 46.0677   time: 425.78s   best: 46.0677
2023-11-16 17:40:35,919:INFO:  Epoch 406/500:  train Loss: 17.5839   val Loss: 23.0031   time: 430.82s   best: 22.2357
2023-11-16 17:44:17,891:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 17:44:17,919:INFO:  Epoch 6/500:  train Loss: 43.0926   val Loss: 42.7475   time: 424.95s   best: 42.7475
2023-11-16 17:47:46,461:INFO:  Epoch 407/500:  train Loss: 17.5166   val Loss: 22.6991   time: 430.52s   best: 22.2357
2023-11-16 17:51:23,849:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 17:51:23,874:INFO:  Epoch 7/500:  train Loss: 40.3803   val Loss: 40.5538   time: 425.93s   best: 40.5538
2023-11-16 17:54:54,788:INFO:  Epoch 408/500:  train Loss: 17.3535   val Loss: 23.3191   time: 428.31s   best: 22.2357
2023-11-16 17:58:27,766:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 17:58:27,795:INFO:  Epoch 8/500:  train Loss: 38.1112   val Loss: 40.1660   time: 423.88s   best: 40.1660
2023-11-16 18:02:05,419:INFO:  Epoch 409/500:  train Loss: 17.4630   val Loss: 22.7244   time: 430.61s   best: 22.2357
2023-11-16 18:05:33,005:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 18:05:33,160:INFO:  Epoch 9/500:  train Loss: 36.4930   val Loss: 37.2926   time: 425.21s   best: 37.2926
2023-11-16 18:09:16,540:INFO:  Epoch 410/500:  train Loss: 17.7562   val Loss: 22.6144   time: 431.09s   best: 22.2357
2023-11-16 18:12:38,480:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 18:12:38,504:INFO:  Epoch 10/500:  train Loss: 35.1364   val Loss: 35.5913   time: 425.30s   best: 35.5913
2023-11-16 18:16:25,743:INFO:  Epoch 411/500:  train Loss: 17.4329   val Loss: 22.5253   time: 429.19s   best: 22.2357
2023-11-16 18:19:41,900:INFO:  Epoch 11/500:  train Loss: 34.0438   val Loss: 37.3019   time: 423.40s   best: 35.5913
2023-11-16 18:23:38,861:INFO:  Epoch 412/500:  train Loss: 17.3830   val Loss: 22.8247   time: 433.10s   best: 22.2357
2023-11-16 18:26:47,809:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 18:26:47,847:INFO:  Epoch 12/500:  train Loss: 33.1137   val Loss: 35.1622   time: 425.89s   best: 35.1622
2023-11-16 18:30:51,022:INFO:  Epoch 413/500:  train Loss: 17.5526   val Loss: 22.5719   time: 432.15s   best: 22.2357
2023-11-16 18:33:54,618:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 18:33:54,643:INFO:  Epoch 13/500:  train Loss: 32.1787   val Loss: 32.2117   time: 426.76s   best: 32.2117
2023-11-16 18:38:00,607:INFO:  Epoch 414/500:  train Loss: 17.6829   val Loss: 22.8343   time: 429.58s   best: 22.2357
2023-11-16 18:41:00,631:INFO:  Epoch 14/500:  train Loss: 31.4403   val Loss: 32.9720   time: 425.98s   best: 32.2117
2023-11-16 18:45:11,899:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 18:45:11,947:INFO:  Epoch 415/500:  train Loss: 17.6149   val Loss: 22.1488   time: 431.24s   best: 22.1488
2023-11-16 18:48:04,620:INFO:  Epoch 15/500:  train Loss: 30.9529   val Loss: 36.1671   time: 423.98s   best: 32.2117
2023-11-16 18:52:24,317:INFO:  Epoch 416/500:  train Loss: 17.6258   val Loss: 22.9560   time: 432.36s   best: 22.1488
2023-11-16 18:55:09,279:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 18:55:09,307:INFO:  Epoch 16/500:  train Loss: 30.5092   val Loss: 31.2693   time: 424.63s   best: 31.2693
2023-11-16 18:59:37,058:INFO:  Epoch 417/500:  train Loss: 17.4086   val Loss: 22.4096   time: 432.72s   best: 22.1488
2023-11-16 19:02:13,194:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 19:02:13,220:INFO:  Epoch 17/500:  train Loss: 30.0101   val Loss: 30.2306   time: 423.87s   best: 30.2306
2023-11-16 19:06:46,760:INFO:  Epoch 418/500:  train Loss: 17.4976   val Loss: 22.5364   time: 429.68s   best: 22.1488
2023-11-16 19:09:18,556:INFO:  Epoch 18/500:  train Loss: 29.5759   val Loss: 30.5597   time: 425.32s   best: 30.2306
2023-11-16 19:13:57,914:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 19:13:57,945:INFO:  Epoch 419/500:  train Loss: 17.3708   val Loss: 21.9190   time: 431.12s   best: 21.9190
2023-11-16 19:16:23,443:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 19:16:23,471:INFO:  Epoch 19/500:  train Loss: 28.9817   val Loss: 30.0245   time: 424.84s   best: 30.0245
2023-11-16 19:21:07,954:INFO:  Epoch 420/500:  train Loss: 17.4494   val Loss: 38.7034   time: 430.00s   best: 21.9190
2023-11-16 19:23:26,934:INFO:  Epoch 20/500:  train Loss: 28.5962   val Loss: 30.3776   time: 423.46s   best: 30.0245
2023-11-16 19:28:14,173:INFO:  Epoch 421/500:  train Loss: 18.0768   val Loss: 22.3618   time: 426.21s   best: 21.9190
2023-11-16 19:30:34,050:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 19:30:34,106:INFO:  Epoch 21/500:  train Loss: 28.1776   val Loss: 29.3662   time: 427.10s   best: 29.3662
2023-11-16 19:35:20,918:INFO:  Epoch 422/500:  train Loss: 17.4476   val Loss: 21.9631   time: 426.71s   best: 21.9190
2023-11-16 19:37:41,986:INFO:  Epoch 22/500:  train Loss: 27.7364   val Loss: 29.7938   time: 427.87s   best: 29.3662
2023-11-16 19:42:30,680:INFO:  Epoch 423/500:  train Loss: 17.3441   val Loss: 24.1223   time: 429.74s   best: 21.9190
2023-11-16 19:44:48,164:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 19:44:48,205:INFO:  Epoch 23/500:  train Loss: 27.4592   val Loss: 28.6058   time: 426.08s   best: 28.6058
2023-11-16 19:49:37,977:INFO:  Epoch 424/500:  train Loss: 17.3598   val Loss: 22.3306   time: 427.28s   best: 21.9190
2023-11-16 19:51:55,648:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 19:51:55,675:INFO:  Epoch 24/500:  train Loss: 27.2760   val Loss: 28.0351   time: 427.42s   best: 28.0351
2023-11-16 19:56:45,710:INFO:  Epoch 425/500:  train Loss: 17.3855   val Loss: 22.9326   time: 427.71s   best: 21.9190
2023-11-16 19:59:00,142:INFO:  Epoch 25/500:  train Loss: 26.7142   val Loss: 28.2597   time: 424.46s   best: 28.0351
2023-11-16 20:03:57,822:INFO:  Epoch 426/500:  train Loss: 17.3578   val Loss: 22.4090   time: 432.09s   best: 21.9190
2023-11-16 20:06:03,514:INFO:  Epoch 26/500:  train Loss: 26.5994   val Loss: 28.1567   time: 423.32s   best: 28.0351
2023-11-16 20:11:10,561:INFO:  Epoch 427/500:  train Loss: 17.2665   val Loss: 23.1134   time: 432.72s   best: 21.9190
2023-11-16 20:13:09,141:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 20:13:09,364:INFO:  Epoch 27/500:  train Loss: 26.3915   val Loss: 27.5196   time: 425.61s   best: 27.5196
2023-11-16 20:18:19,468:INFO:  Epoch 428/500:  train Loss: 17.7050   val Loss: 23.0033   time: 428.88s   best: 21.9190
2023-11-16 20:20:15,264:INFO:  Epoch 28/500:  train Loss: 25.9557   val Loss: 27.7262   time: 425.90s   best: 27.5196
2023-11-16 20:25:31,584:INFO:  Epoch 429/500:  train Loss: 17.5097   val Loss: 22.3224   time: 432.09s   best: 21.9190
2023-11-16 20:27:17,722:INFO:  Epoch 29/500:  train Loss: 25.9681   val Loss: 27.8507   time: 422.43s   best: 27.5196
2023-11-16 20:32:39,132:INFO:  Epoch 430/500:  train Loss: 17.6633   val Loss: 22.9363   time: 427.52s   best: 21.9190
2023-11-16 20:34:22,830:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 20:34:22,875:INFO:  Epoch 30/500:  train Loss: 25.6478   val Loss: 27.3478   time: 425.08s   best: 27.3478
2023-11-16 20:39:50,412:INFO:  Epoch 431/500:  train Loss: 17.4086   val Loss: 22.7107   time: 431.27s   best: 21.9190
2023-11-16 20:41:24,789:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 20:41:24,884:INFO:  Epoch 31/500:  train Loss: 25.5449   val Loss: 27.1738   time: 421.91s   best: 27.1738
2023-11-16 20:47:02,047:INFO:  Epoch 432/500:  train Loss: 17.2886   val Loss: 22.8109   time: 431.61s   best: 21.9190
2023-11-16 20:48:30,893:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 20:48:30,922:INFO:  Epoch 32/500:  train Loss: 25.2093   val Loss: 26.8500   time: 425.97s   best: 26.8500
2023-11-16 20:54:13,274:INFO:  Epoch 433/500:  train Loss: 17.2920   val Loss: 22.7256   time: 431.20s   best: 21.9190
2023-11-16 20:55:35,903:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 20:55:35,989:INFO:  Epoch 33/500:  train Loss: 24.9616   val Loss: 26.6314   time: 424.98s   best: 26.6314
2023-11-16 21:01:23,506:INFO:  Epoch 434/500:  train Loss: 17.3619   val Loss: 22.4298   time: 430.22s   best: 21.9190
2023-11-16 21:02:41,383:INFO:  Epoch 34/500:  train Loss: 25.1172   val Loss: 26.9185   time: 425.38s   best: 26.6314
2023-11-16 21:08:35,193:INFO:  Epoch 435/500:  train Loss: 17.3259   val Loss: 22.8228   time: 431.68s   best: 21.9190
2023-11-16 21:09:48,839:INFO:  Epoch 35/500:  train Loss: 24.7142   val Loss: 27.0467   time: 427.43s   best: 26.6314
2023-11-16 21:15:46,117:INFO:  Epoch 436/500:  train Loss: 17.4214   val Loss: 22.6698   time: 430.90s   best: 21.9190
2023-11-16 21:16:55,605:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 21:16:55,630:INFO:  Epoch 36/500:  train Loss: 24.5300   val Loss: 26.6286   time: 426.75s   best: 26.6286
2023-11-16 21:22:54,079:INFO:  Epoch 437/500:  train Loss: 17.3482   val Loss: 22.5852   time: 427.95s   best: 21.9190
2023-11-16 21:24:03,193:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 21:24:03,231:INFO:  Epoch 37/500:  train Loss: 24.3112   val Loss: 26.1941   time: 427.55s   best: 26.1941
2023-11-16 21:30:01,694:INFO:  Epoch 438/500:  train Loss: 17.2964   val Loss: 22.6628   time: 427.61s   best: 21.9190
2023-11-16 21:31:08,865:INFO:  Epoch 38/500:  train Loss: 24.2097   val Loss: 26.7180   time: 425.62s   best: 26.1941
2023-11-16 21:37:12,280:INFO:  Epoch 439/500:  train Loss: 17.7147   val Loss: 22.5011   time: 430.54s   best: 21.9190
2023-11-16 21:38:14,119:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 21:38:14,143:INFO:  Epoch 39/500:  train Loss: 24.0986   val Loss: 26.0631   time: 425.22s   best: 26.0631
2023-11-16 21:44:20,435:INFO:  Epoch 440/500:  train Loss: 17.4803   val Loss: 23.2695   time: 428.15s   best: 21.9190
2023-11-16 21:45:18,368:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 21:45:18,419:INFO:  Epoch 40/500:  train Loss: 23.9141   val Loss: 25.8440   time: 424.22s   best: 25.8440
2023-11-16 21:51:31,699:INFO:  Epoch 441/500:  train Loss: 17.3201   val Loss: 22.0960   time: 431.23s   best: 21.9190
2023-11-16 21:52:23,709:INFO:  Epoch 41/500:  train Loss: 23.7186   val Loss: 25.9220   time: 425.29s   best: 25.8440
2023-11-16 21:58:40,431:INFO:  Epoch 442/500:  train Loss: 17.3305   val Loss: 22.3690   time: 428.73s   best: 21.9190
2023-11-16 21:59:26,416:INFO:  Epoch 42/500:  train Loss: 23.7247   val Loss: 26.6259   time: 422.68s   best: 25.8440
2023-11-16 22:05:50,711:INFO:  Epoch 443/500:  train Loss: 17.2689   val Loss: 23.0189   time: 430.26s   best: 21.9190
2023-11-16 22:06:33,935:INFO:  Epoch 43/500:  train Loss: 23.7478   val Loss: 26.5085   time: 427.51s   best: 25.8440
2023-11-16 22:13:00,198:INFO:  Epoch 444/500:  train Loss: 17.4271   val Loss: 23.0979   time: 429.48s   best: 21.9190
2023-11-16 22:13:40,042:INFO:  Epoch 44/500:  train Loss: 23.5434   val Loss: 25.8476   time: 426.10s   best: 25.8440
2023-11-16 22:20:10,049:INFO:  Epoch 445/500:  train Loss: 17.2256   val Loss: 22.9513   time: 429.83s   best: 21.9190
2023-11-16 22:20:44,728:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 22:20:44,754:INFO:  Epoch 45/500:  train Loss: 23.3465   val Loss: 25.8153   time: 424.64s   best: 25.8153
2023-11-16 22:27:17,987:INFO:  Epoch 446/500:  train Loss: 17.5142   val Loss: 22.7845   time: 427.91s   best: 21.9190
2023-11-16 22:27:52,192:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 22:27:52,225:INFO:  Epoch 46/500:  train Loss: 23.4181   val Loss: 25.7499   time: 427.43s   best: 25.7499
2023-11-16 22:34:29,050:INFO:  Epoch 447/500:  train Loss: 17.3231   val Loss: 22.5289   time: 431.04s   best: 21.9190
2023-11-16 22:34:55,242:INFO:  Epoch 47/500:  train Loss: 23.1281   val Loss: 25.9689   time: 423.01s   best: 25.7499
2023-11-16 22:41:37,049:INFO:  Epoch 448/500:  train Loss: 17.3218   val Loss: 22.5559   time: 427.98s   best: 21.9190
2023-11-16 22:42:00,833:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 22:42:00,859:INFO:  Epoch 48/500:  train Loss: 23.1339   val Loss: 25.3100   time: 425.57s   best: 25.3100
2023-11-16 22:48:49,837:INFO:  Epoch 449/500:  train Loss: 17.2743   val Loss: 22.9864   time: 432.77s   best: 21.9190
2023-11-16 22:49:06,488:INFO:  Epoch 49/500:  train Loss: 23.0017   val Loss: 26.6779   time: 425.62s   best: 25.3100
2023-11-16 22:56:02,847:INFO:  Epoch 450/500:  train Loss: 17.2365   val Loss: 22.8714   time: 432.99s   best: 21.9190
2023-11-16 22:56:12,434:INFO:  Epoch 50/500:  train Loss: 22.8369   val Loss: 25.8635   time: 425.92s   best: 25.3100
2023-11-16 23:03:14,998:INFO:  Epoch 451/500:  train Loss: 17.2689   val Loss: 23.1748   time: 432.13s   best: 21.9190
2023-11-16 23:03:19,731:INFO:  Epoch 51/500:  train Loss: 22.8116   val Loss: 25.6462   time: 427.28s   best: 25.3100
2023-11-16 23:10:25,283:INFO:  Epoch 52/500:  train Loss: 22.6410   val Loss: 25.6358   time: 425.53s   best: 25.3100
2023-11-16 23:10:25,423:INFO:  Epoch 452/500:  train Loss: 17.2026   val Loss: 22.5328   time: 430.42s   best: 21.9190
2023-11-16 23:17:32,570:INFO:  Epoch 53/500:  train Loss: 22.7732   val Loss: 25.6186   time: 427.28s   best: 25.3100
2023-11-16 23:17:33,017:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-16 23:17:33,104:INFO:  Epoch 453/500:  train Loss: 17.2429   val Loss: 21.8178   time: 427.52s   best: 21.8178
2023-11-16 23:24:36,950:INFO:  Epoch 54/500:  train Loss: 22.7080   val Loss: 26.0949   time: 424.37s   best: 25.3100
2023-11-16 23:24:42,665:INFO:  Epoch 454/500:  train Loss: 17.3385   val Loss: 22.5243   time: 429.54s   best: 21.8178
2023-11-16 23:31:43,942:INFO:  Epoch 55/500:  train Loss: 22.7824   val Loss: 25.3473   time: 426.98s   best: 25.3100
2023-11-16 23:31:53,582:INFO:  Epoch 455/500:  train Loss: 17.2459   val Loss: 22.6790   time: 430.89s   best: 21.8178
2023-11-16 23:38:47,890:INFO:  Epoch 56/500:  train Loss: 22.4517   val Loss: 25.3900   time: 423.94s   best: 25.3100
2023-11-16 23:39:04,377:INFO:  Epoch 456/500:  train Loss: 17.2956   val Loss: 22.3303   time: 430.79s   best: 21.8178
2023-11-16 23:45:51,815:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 23:45:51,850:INFO:  Epoch 57/500:  train Loss: 22.5601   val Loss: 24.9870   time: 423.91s   best: 24.9870
2023-11-16 23:46:16,978:INFO:  Epoch 457/500:  train Loss: 17.6114   val Loss: 22.6802   time: 432.57s   best: 21.8178
2023-11-16 23:52:57,454:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-16 23:52:57,493:INFO:  Epoch 58/500:  train Loss: 22.3559   val Loss: 24.9023   time: 425.60s   best: 24.9023
2023-11-16 23:53:25,580:INFO:  Epoch 458/500:  train Loss: 17.3936   val Loss: 22.0558   time: 428.60s   best: 21.8178
2023-11-17 00:00:05,346:INFO:  Epoch 59/500:  train Loss: 22.4251   val Loss: 25.9680   time: 427.85s   best: 24.9023
2023-11-17 00:00:36,860:INFO:  Epoch 459/500:  train Loss: 17.4356   val Loss: 24.3635   time: 431.28s   best: 21.8178
2023-11-17 00:07:12,284:INFO:  Epoch 60/500:  train Loss: 22.3641   val Loss: 24.9439   time: 426.93s   best: 24.9023
2023-11-17 00:07:47,567:INFO:  Epoch 460/500:  train Loss: 17.4425   val Loss: 23.6072   time: 430.68s   best: 21.8178
2023-11-17 00:14:19,282:INFO:  Epoch 61/500:  train Loss: 21.9725   val Loss: 24.9187   time: 426.99s   best: 24.9023
2023-11-17 00:15:00,831:INFO:  Epoch 461/500:  train Loss: 17.1443   val Loss: 22.7954   time: 433.26s   best: 21.8178
2023-11-17 00:21:25,766:INFO:  Epoch 62/500:  train Loss: 22.1258   val Loss: 25.2282   time: 426.44s   best: 24.9023
2023-11-17 00:22:12,126:INFO:  Epoch 462/500:  train Loss: 17.1307   val Loss: 22.4230   time: 431.28s   best: 21.8178
2023-11-17 00:28:32,750:INFO:  Epoch 63/500:  train Loss: 22.1017   val Loss: 25.2651   time: 426.96s   best: 24.9023
2023-11-17 00:29:19,635:INFO:  Epoch 463/500:  train Loss: 17.3448   val Loss: 22.7115   time: 427.48s   best: 21.8178
2023-11-17 00:35:40,492:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 00:35:40,529:INFO:  Epoch 64/500:  train Loss: 22.2862   val Loss: 24.6448   time: 427.70s   best: 24.6448
2023-11-17 00:36:30,587:INFO:  Epoch 464/500:  train Loss: 17.1502   val Loss: 22.2382   time: 430.94s   best: 21.8178
2023-11-17 00:42:47,325:INFO:  Epoch 65/500:  train Loss: 21.8735   val Loss: 25.3067   time: 426.78s   best: 24.6448
2023-11-17 00:43:42,123:INFO:  Epoch 465/500:  train Loss: 17.1671   val Loss: 21.9774   time: 431.52s   best: 21.8178
2023-11-17 00:49:51,699:INFO:  Epoch 66/500:  train Loss: 21.8337   val Loss: 25.1990   time: 424.32s   best: 24.6448
2023-11-17 00:50:54,426:INFO:  Epoch 466/500:  train Loss: 17.4121   val Loss: 22.0514   time: 432.28s   best: 21.8178
2023-11-17 00:56:54,392:INFO:  Epoch 67/500:  train Loss: 21.8540   val Loss: 24.8779   time: 422.66s   best: 24.6448
2023-11-17 00:58:02,364:INFO:  Epoch 467/500:  train Loss: 17.2426   val Loss: 22.5677   time: 427.91s   best: 21.8178
2023-11-17 01:03:57,940:INFO:  Epoch 68/500:  train Loss: 21.8076   val Loss: 25.1380   time: 423.54s   best: 24.6448
2023-11-17 01:05:13,590:INFO:  Epoch 468/500:  train Loss: 17.5124   val Loss: 23.4257   time: 431.22s   best: 21.8178
2023-11-17 01:11:02,565:INFO:  Epoch 69/500:  train Loss: 22.0061   val Loss: 25.0959   time: 424.62s   best: 24.6448
2023-11-17 01:12:21,519:INFO:  Epoch 469/500:  train Loss: 17.5770   val Loss: 23.1623   time: 427.92s   best: 21.8178
2023-11-17 01:18:08,330:INFO:  Epoch 70/500:  train Loss: 21.5607   val Loss: 25.1865   time: 425.74s   best: 24.6448
2023-11-17 01:19:33,172:INFO:  Epoch 470/500:  train Loss: 17.3727   val Loss: 22.5960   time: 431.63s   best: 21.8178
2023-11-17 01:25:12,360:INFO:  Epoch 71/500:  train Loss: 21.5178   val Loss: 25.3258   time: 424.02s   best: 24.6448
2023-11-17 01:26:44,937:INFO:  Epoch 471/500:  train Loss: 17.2619   val Loss: 22.8870   time: 431.74s   best: 21.8178
2023-11-17 01:32:16,514:INFO:  Epoch 72/500:  train Loss: 21.5495   val Loss: 25.2654   time: 424.14s   best: 24.6448
2023-11-17 01:33:55,416:INFO:  Epoch 472/500:  train Loss: 17.4018   val Loss: 22.2969   time: 430.47s   best: 21.8178
2023-11-17 01:39:21,645:INFO:  Epoch 73/500:  train Loss: 21.4890   val Loss: 24.7934   time: 425.12s   best: 24.6448
2023-11-17 01:41:07,794:INFO:  Epoch 473/500:  train Loss: 17.3517   val Loss: 22.5268   time: 432.35s   best: 21.8178
2023-11-17 01:46:25,356:INFO:  Epoch 74/500:  train Loss: 21.3636   val Loss: 26.8851   time: 423.68s   best: 24.6448
2023-11-17 01:48:15,234:INFO:  Epoch 474/500:  train Loss: 17.2459   val Loss: 23.4822   time: 427.43s   best: 21.8178
2023-11-17 01:53:30,800:INFO:  Epoch 75/500:  train Loss: 21.3617   val Loss: 25.2899   time: 425.43s   best: 24.6448
2023-11-17 01:55:26,619:INFO:  Epoch 475/500:  train Loss: 17.2282   val Loss: 22.9721   time: 431.38s   best: 21.8178
2023-11-17 02:00:35,812:INFO:  Epoch 76/500:  train Loss: 21.4257   val Loss: 24.8733   time: 424.98s   best: 24.6448
2023-11-17 02:02:38,339:INFO:  Epoch 476/500:  train Loss: 17.2876   val Loss: 22.5996   time: 431.71s   best: 21.8178
2023-11-17 02:07:39,885:INFO:  Epoch 77/500:  train Loss: 21.2620   val Loss: 25.6939   time: 424.05s   best: 24.6448
2023-11-17 02:09:50,061:INFO:  Epoch 477/500:  train Loss: 17.3070   val Loss: 22.6299   time: 431.71s   best: 21.8178
2023-11-17 02:14:45,623:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 02:14:45,653:INFO:  Epoch 78/500:  train Loss: 21.1673   val Loss: 24.5756   time: 425.72s   best: 24.5756
2023-11-17 02:16:58,195:INFO:  Epoch 478/500:  train Loss: 17.2754   val Loss: 22.7191   time: 428.13s   best: 21.8178
2023-11-17 02:21:51,815:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 02:21:51,840:INFO:  Epoch 79/500:  train Loss: 21.1760   val Loss: 24.2532   time: 426.16s   best: 24.2532
2023-11-17 02:24:09,472:INFO:  Epoch 479/500:  train Loss: 17.4122   val Loss: 22.5780   time: 431.27s   best: 21.8178
2023-11-17 02:28:57,409:INFO:  Epoch 80/500:  train Loss: 21.1458   val Loss: 24.5551   time: 425.57s   best: 24.2532
2023-11-17 02:31:18,347:INFO:  Epoch 480/500:  train Loss: 17.1836   val Loss: 22.3676   time: 428.86s   best: 21.8178
2023-11-17 02:36:04,397:INFO:  Epoch 81/500:  train Loss: 21.0937   val Loss: 24.7761   time: 426.97s   best: 24.2532
2023-11-17 02:38:27,306:INFO:  Epoch 481/500:  train Loss: 17.2586   val Loss: 22.4716   time: 428.95s   best: 21.8178
2023-11-17 02:43:09,467:INFO:  Epoch 82/500:  train Loss: 21.0188   val Loss: 24.6714   time: 425.05s   best: 24.2532
2023-11-17 02:45:40,301:INFO:  Epoch 482/500:  train Loss: 17.1728   val Loss: 22.6850   time: 432.97s   best: 21.8178
2023-11-17 02:50:14,300:INFO:  Epoch 83/500:  train Loss: 21.0063   val Loss: 25.5774   time: 424.82s   best: 24.2532
2023-11-17 02:52:48,225:INFO:  Epoch 483/500:  train Loss: 17.2911   val Loss: 22.7807   time: 427.91s   best: 21.8178
2023-11-17 02:57:19,251:INFO:  Epoch 84/500:  train Loss: 21.1252   val Loss: 24.9025   time: 424.92s   best: 24.2532
2023-11-17 03:00:00,464:INFO:  Epoch 484/500:  train Loss: 17.2314   val Loss: 22.6836   time: 432.21s   best: 21.8178
2023-11-17 03:04:23,560:INFO:  Epoch 85/500:  train Loss: 20.9163   val Loss: 24.6107   time: 424.27s   best: 24.2532
2023-11-17 03:07:10,529:INFO:  Epoch 485/500:  train Loss: 17.2354   val Loss: 22.5036   time: 430.05s   best: 21.8178
2023-11-17 03:11:28,983:INFO:  Epoch 86/500:  train Loss: 21.0320   val Loss: 24.8842   time: 425.42s   best: 24.2532
2023-11-17 03:14:17,995:INFO:  Epoch 486/500:  train Loss: 17.2278   val Loss: 22.9486   time: 427.44s   best: 21.8178
2023-11-17 03:18:33,662:INFO:  Epoch 87/500:  train Loss: 20.8235   val Loss: 24.5531   time: 424.65s   best: 24.2532
2023-11-17 03:21:25,594:INFO:  Epoch 487/500:  train Loss: 17.4225   val Loss: 22.2776   time: 427.59s   best: 21.8178
2023-11-17 03:25:37,104:INFO:  Epoch 88/500:  train Loss: 20.7637   val Loss: 24.9892   time: 423.44s   best: 24.2532
2023-11-17 03:28:34,352:INFO:  Epoch 488/500:  train Loss: 17.2080   val Loss: 23.8018   time: 428.75s   best: 21.8178
2023-11-17 03:32:40,906:INFO:  Epoch 89/500:  train Loss: 21.0159   val Loss: 24.8761   time: 423.77s   best: 24.2532
2023-11-17 03:35:44,449:INFO:  Epoch 489/500:  train Loss: 17.3032   val Loss: 22.3749   time: 430.09s   best: 21.8178
2023-11-17 03:39:44,832:INFO:  Epoch 90/500:  train Loss: 20.7612   val Loss: 24.6543   time: 423.89s   best: 24.2532
2023-11-17 03:42:54,477:INFO:  Epoch 490/500:  train Loss: 17.3046   val Loss: 22.4367   time: 430.00s   best: 21.8178
2023-11-17 03:46:47,930:INFO:  Epoch 91/500:  train Loss: 20.6208   val Loss: 24.3569   time: 423.08s   best: 24.2532
2023-11-17 03:50:00,577:INFO:  Epoch 491/500:  train Loss: 17.3041   val Loss: 22.3984   time: 426.08s   best: 21.8178
2023-11-17 03:53:52,259:INFO:  Epoch 92/500:  train Loss: 20.6339   val Loss: 24.3343   time: 424.31s   best: 24.2532
2023-11-17 03:57:07,470:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm25 (0.05 dropout)_b5c2.pt
2023-11-17 03:57:07,500:INFO:  Epoch 492/500:  train Loss: 17.2704   val Loss: 21.8135   time: 426.87s   best: 21.8135
2023-11-17 04:00:55,517:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 04:00:55,547:INFO:  Epoch 93/500:  train Loss: 20.7340   val Loss: 24.0639   time: 423.24s   best: 24.0639
2023-11-17 04:04:19,770:INFO:  Epoch 493/500:  train Loss: 17.1877   val Loss: 22.3743   time: 432.26s   best: 21.8135
2023-11-17 04:08:02,896:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 04:08:02,927:INFO:  Epoch 94/500:  train Loss: 20.8969   val Loss: 23.9526   time: 427.33s   best: 23.9526
2023-11-17 04:11:31,641:INFO:  Epoch 494/500:  train Loss: 17.1334   val Loss: 24.1121   time: 431.85s   best: 21.8135
2023-11-17 04:15:09,256:INFO:  Epoch 95/500:  train Loss: 20.4371   val Loss: 24.5313   time: 426.32s   best: 23.9526
2023-11-17 04:18:43,027:INFO:  Epoch 495/500:  train Loss: 17.3370   val Loss: 22.6114   time: 431.35s   best: 21.8135
2023-11-17 04:22:13,484:INFO:  Epoch 96/500:  train Loss: 20.4204   val Loss: 24.3378   time: 424.20s   best: 23.9526
2023-11-17 04:25:51,134:INFO:  Epoch 496/500:  train Loss: 17.4508   val Loss: 22.4713   time: 428.09s   best: 21.8135
2023-11-17 04:29:17,949:INFO:  Epoch 97/500:  train Loss: 20.5111   val Loss: 24.2699   time: 424.44s   best: 23.9526
2023-11-17 04:33:02,052:INFO:  Epoch 497/500:  train Loss: 17.0737   val Loss: 22.3253   time: 430.89s   best: 21.8135
2023-11-17 04:36:26,259:INFO:  Epoch 98/500:  train Loss: 20.4306   val Loss: 24.3300   time: 428.29s   best: 23.9526
2023-11-17 04:40:09,917:INFO:  Epoch 498/500:  train Loss: 17.1717   val Loss: 22.2663   time: 427.86s   best: 21.8135
2023-11-17 04:43:32,075:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 04:43:32,099:INFO:  Epoch 99/500:  train Loss: 20.9324   val Loss: 23.3890   time: 425.79s   best: 23.3890
2023-11-17 04:47:22,276:INFO:  Epoch 499/500:  train Loss: 17.6065   val Loss: 22.2799   time: 432.35s   best: 21.8135
2023-11-17 04:50:37,317:INFO:  Epoch 100/500:  train Loss: 20.4687   val Loss: 24.3131   time: 425.20s   best: 23.3890
2023-11-17 04:54:32,796:INFO:  Epoch 500/500:  train Loss: 17.1996   val Loss: 22.5092   time: 430.51s   best: 21.8135
2023-11-17 04:54:32,822:INFO:  -----> Training complete in 3586m 16s   best validation loss: 21.8135
 
2023-11-17 04:57:42,089:INFO:  Epoch 101/500:  train Loss: 20.3311   val Loss: 24.2213   time: 424.73s   best: 23.3890
2023-11-17 05:04:47,740:INFO:  Epoch 102/500:  train Loss: 20.2202   val Loss: 27.1685   time: 425.63s   best: 23.3890
2023-11-17 05:11:54,937:INFO:  Epoch 103/500:  train Loss: 20.6135   val Loss: 23.9601   time: 427.17s   best: 23.3890
2023-11-17 05:19:01,919:INFO:  Epoch 104/500:  train Loss: 20.1908   val Loss: 23.6412   time: 426.97s   best: 23.3890
2023-11-17 05:26:09,789:INFO:  Epoch 105/500:  train Loss: 20.4622   val Loss: 24.2788   time: 427.86s   best: 23.3890
2023-11-17 05:33:17,416:INFO:  Epoch 106/500:  train Loss: 20.1473   val Loss: 23.7459   time: 427.60s   best: 23.3890
2023-11-17 05:40:25,887:INFO:  Epoch 107/500:  train Loss: 20.1165   val Loss: 23.8117   time: 428.44s   best: 23.3890
2023-11-17 05:47:34,706:INFO:  Epoch 108/500:  train Loss: 20.1598   val Loss: 23.9066   time: 428.81s   best: 23.3890
2023-11-17 05:54:38,504:INFO:  Epoch 109/500:  train Loss: 20.3696   val Loss: 24.1754   time: 423.77s   best: 23.3890
2023-11-17 06:01:43,758:INFO:  Epoch 110/500:  train Loss: 20.0727   val Loss: 23.6628   time: 425.24s   best: 23.3890
2023-11-17 06:08:51,533:INFO:  Epoch 111/500:  train Loss: 20.2529   val Loss: 23.5720   time: 427.76s   best: 23.3890
2023-11-17 06:15:59,324:INFO:  Epoch 112/500:  train Loss: 20.1180   val Loss: 24.0309   time: 427.78s   best: 23.3890
2023-11-17 06:23:04,440:INFO:  Epoch 113/500:  train Loss: 19.9173   val Loss: 23.4147   time: 425.10s   best: 23.3890
2023-11-17 06:30:12,358:INFO:  Epoch 114/500:  train Loss: 19.9682   val Loss: 23.6636   time: 427.89s   best: 23.3890
2023-11-17 06:37:16,895:INFO:  Epoch 115/500:  train Loss: 20.0799   val Loss: 23.9194   time: 424.52s   best: 23.3890
2023-11-17 06:44:25,678:INFO:  Epoch 116/500:  train Loss: 20.0210   val Loss: 23.7155   time: 428.77s   best: 23.3890
2023-11-17 06:51:33,114:INFO:  Epoch 117/500:  train Loss: 19.9347   val Loss: 23.8494   time: 427.42s   best: 23.3890
2023-11-17 06:58:37,839:INFO:  Epoch 118/500:  train Loss: 19.9313   val Loss: 23.8172   time: 424.70s   best: 23.3890
2023-11-17 07:05:45,696:INFO:  Epoch 119/500:  train Loss: 19.9202   val Loss: 23.6617   time: 427.85s   best: 23.3890
2023-11-17 07:12:53,876:INFO:  Epoch 120/500:  train Loss: 19.7882   val Loss: 24.7477   time: 428.17s   best: 23.3890
2023-11-17 07:20:00,466:INFO:  Epoch 121/500:  train Loss: 19.7428   val Loss: 24.5220   time: 426.57s   best: 23.3890
2023-11-17 07:27:06,289:INFO:  Epoch 122/500:  train Loss: 19.7475   val Loss: 23.4183   time: 425.80s   best: 23.3890
2023-11-17 07:34:11,604:INFO:  Epoch 123/500:  train Loss: 20.0044   val Loss: 23.8510   time: 425.31s   best: 23.3890
2023-11-17 07:41:17,602:INFO:  Epoch 124/500:  train Loss: 19.6892   val Loss: 25.3859   time: 425.99s   best: 23.3890
2023-11-17 07:48:20,867:INFO:  Epoch 125/500:  train Loss: 19.8137   val Loss: 24.1192   time: 423.24s   best: 23.3890
2023-11-17 07:55:25,624:INFO:  Epoch 126/500:  train Loss: 19.7118   val Loss: 24.8692   time: 424.73s   best: 23.3890
2023-11-17 08:02:32,128:INFO:  Epoch 127/500:  train Loss: 19.6073   val Loss: 24.4683   time: 426.50s   best: 23.3890
2023-11-17 08:09:38,928:INFO:  Epoch 128/500:  train Loss: 19.6711   val Loss: 23.7530   time: 426.79s   best: 23.3890
2023-11-17 08:16:42,687:INFO:  Epoch 129/500:  train Loss: 19.6230   val Loss: 24.0442   time: 423.74s   best: 23.3890
2023-11-17 08:23:49,928:INFO:  Epoch 130/500:  train Loss: 19.8789   val Loss: 24.0003   time: 427.23s   best: 23.3890
2023-11-17 08:30:59,031:INFO:  Epoch 131/500:  train Loss: 19.8302   val Loss: 23.6514   time: 429.09s   best: 23.3890
2023-11-17 08:38:03,774:INFO:  Epoch 132/500:  train Loss: 19.7886   val Loss: 23.5366   time: 424.72s   best: 23.3890
2023-11-17 08:45:08,728:INFO:  Epoch 133/500:  train Loss: 19.5621   val Loss: 23.9859   time: 424.94s   best: 23.3890
2023-11-17 08:52:13,433:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 08:52:13,487:INFO:  Epoch 134/500:  train Loss: 19.5071   val Loss: 23.1760   time: 424.66s   best: 23.1760
2023-11-17 08:59:22,603:INFO:  Epoch 135/500:  train Loss: 19.9515   val Loss: 23.9644   time: 429.12s   best: 23.1760
2023-11-17 09:06:31,408:INFO:  Epoch 136/500:  train Loss: 19.5392   val Loss: 23.7210   time: 428.77s   best: 23.1760
2023-11-17 09:13:40,040:INFO:  Epoch 137/500:  train Loss: 19.4545   val Loss: 23.8852   time: 428.62s   best: 23.1760
2023-11-17 09:20:48,100:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 09:20:48,189:INFO:  Epoch 138/500:  train Loss: 19.4227   val Loss: 23.0764   time: 427.99s   best: 23.0764
2023-11-17 09:27:55,706:INFO:  Epoch 139/500:  train Loss: 19.3650   val Loss: 23.4461   time: 427.50s   best: 23.0764
2023-11-17 09:35:03,508:INFO:  Epoch 140/500:  train Loss: 19.3544   val Loss: 23.5532   time: 427.78s   best: 23.0764
2023-11-17 09:42:12,828:INFO:  Epoch 141/500:  train Loss: 19.4918   val Loss: 23.4223   time: 429.30s   best: 23.0764
2023-11-17 09:49:20,794:INFO:  Epoch 142/500:  train Loss: 19.4140   val Loss: 23.1375   time: 427.94s   best: 23.0764
2023-11-17 09:56:29,248:INFO:  Epoch 143/500:  train Loss: 19.3759   val Loss: 23.4855   time: 428.42s   best: 23.0764
2023-11-17 10:03:36,781:INFO:  Epoch 144/500:  train Loss: 19.5552   val Loss: 23.1730   time: 427.51s   best: 23.0764
2023-11-17 10:10:43,534:INFO:  Epoch 145/500:  train Loss: 19.4842   val Loss: 23.5586   time: 426.74s   best: 23.0764
2023-11-17 10:17:50,070:INFO:  Epoch 146/500:  train Loss: 19.6846   val Loss: 25.0105   time: 426.51s   best: 23.0764
2023-11-17 10:24:58,790:INFO:  Epoch 147/500:  train Loss: 19.4835   val Loss: 23.5361   time: 428.69s   best: 23.0764
2023-11-17 10:32:04,241:INFO:  Epoch 148/500:  train Loss: 19.3078   val Loss: 23.5258   time: 425.43s   best: 23.0764
2023-11-17 10:39:12,549:INFO:  Epoch 149/500:  train Loss: 19.4302   val Loss: 23.7360   time: 428.28s   best: 23.0764
2023-11-17 10:46:16,486:INFO:  Epoch 150/500:  train Loss: 19.3552   val Loss: 23.2693   time: 423.92s   best: 23.0764
2023-11-17 10:53:22,045:INFO:  Epoch 151/500:  train Loss: 19.2237   val Loss: 23.4459   time: 425.54s   best: 23.0764
2023-11-17 11:00:29,080:INFO:  Epoch 152/500:  train Loss: 19.4221   val Loss: 24.0989   time: 427.03s   best: 23.0764
2023-11-17 11:07:32,657:INFO:  Epoch 153/500:  train Loss: 19.6386   val Loss: 25.4204   time: 423.56s   best: 23.0764
2023-11-17 11:14:40,561:INFO:  Epoch 154/500:  train Loss: 19.1716   val Loss: 23.1794   time: 427.87s   best: 23.0764
2023-11-17 11:21:48,614:INFO:  Epoch 155/500:  train Loss: 19.2152   val Loss: 23.4720   time: 428.03s   best: 23.0764
2023-11-17 11:28:57,197:INFO:  Epoch 156/500:  train Loss: 19.2670   val Loss: 24.2740   time: 428.57s   best: 23.0764
2023-11-17 11:36:03,256:INFO:  Epoch 157/500:  train Loss: 19.1813   val Loss: 23.1754   time: 426.05s   best: 23.0764
2023-11-17 11:43:11,420:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 11:43:11,452:INFO:  Epoch 158/500:  train Loss: 19.1560   val Loss: 22.9988   time: 428.15s   best: 22.9988
2023-11-17 11:50:16,542:INFO:  Epoch 159/500:  train Loss: 19.0942   val Loss: 23.1159   time: 425.08s   best: 22.9988
2023-11-17 11:57:25,131:INFO:  Epoch 160/500:  train Loss: 19.1577   val Loss: 23.5249   time: 428.58s   best: 22.9988
2023-11-17 12:04:31,691:INFO:  Epoch 161/500:  train Loss: 19.6671   val Loss: 23.4629   time: 426.52s   best: 22.9988
2023-11-17 12:11:35,702:INFO:  Epoch 162/500:  train Loss: 19.1877   val Loss: 23.5449   time: 424.00s   best: 22.9988
2023-11-17 12:18:42,873:INFO:  Epoch 163/500:  train Loss: 18.9995   val Loss: 23.3945   time: 427.15s   best: 22.9988
2023-11-17 12:25:50,375:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 12:25:50,402:INFO:  Epoch 164/500:  train Loss: 19.2417   val Loss: 22.9788   time: 427.49s   best: 22.9788
2023-11-17 12:32:59,082:INFO:  Epoch 165/500:  train Loss: 19.0091   val Loss: 23.0740   time: 428.67s   best: 22.9788
2023-11-17 12:40:07,961:INFO:  Epoch 166/500:  train Loss: 19.0219   val Loss: 23.3386   time: 428.85s   best: 22.9788
2023-11-17 12:47:16,582:INFO:  Epoch 167/500:  train Loss: 18.9044   val Loss: 23.7949   time: 428.60s   best: 22.9788
2023-11-17 12:54:25,747:INFO:  Epoch 168/500:  train Loss: 19.0850   val Loss: 23.3943   time: 429.16s   best: 22.9788
2023-11-17 13:01:33,809:INFO:  Epoch 169/500:  train Loss: 19.0492   val Loss: 23.2734   time: 428.06s   best: 22.9788
2023-11-17 13:08:42,630:INFO:  Epoch 170/500:  train Loss: 18.9841   val Loss: 26.0497   time: 428.80s   best: 22.9788
2023-11-17 13:15:51,278:INFO:  Epoch 171/500:  train Loss: 18.9862   val Loss: 23.2598   time: 428.64s   best: 22.9788
2023-11-17 13:22:57,003:INFO:  Epoch 172/500:  train Loss: 18.9312   val Loss: 23.5466   time: 425.71s   best: 22.9788
2023-11-17 13:30:05,777:INFO:  Epoch 173/500:  train Loss: 18.9066   val Loss: 23.1049   time: 428.77s   best: 22.9788
2023-11-17 13:37:14,209:INFO:  Epoch 174/500:  train Loss: 18.9308   val Loss: 23.7208   time: 428.40s   best: 22.9788
2023-11-17 13:39:25,078:INFO:  Starting experiment lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)
2023-11-17 13:39:25,087:INFO:  Defining the model
2023-11-17 13:39:25,199:INFO:  Reading the dataset
2023-11-17 13:44:22,856:INFO:  Epoch 175/500:  train Loss: 18.8480   val Loss: 24.4200   time: 428.63s   best: 22.9788
2023-11-17 13:51:29,684:INFO:  Epoch 176/500:  train Loss: 18.9698   val Loss: 23.5771   time: 426.80s   best: 22.9788
2023-11-17 13:58:38,587:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 13:58:38,628:INFO:  Epoch 177/500:  train Loss: 18.8577   val Loss: 22.9162   time: 428.87s   best: 22.9162
2023-11-17 14:05:45,094:INFO:  Epoch 178/500:  train Loss: 19.2418   val Loss: 23.0876   time: 426.45s   best: 22.9162
2023-11-17 14:06:52,840:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:06:52,866:INFO:  Epoch 1/500:  train Loss: 87.8278   val Loss: 84.7073   time: 130.07s   best: 84.7073
2023-11-17 14:09:00,014:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:09:00,061:INFO:  Epoch 2/500:  train Loss: 79.4175   val Loss: 75.8284   time: 127.14s   best: 75.8284
2023-11-17 14:11:09,053:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:11:09,104:INFO:  Epoch 3/500:  train Loss: 73.0664   val Loss: 71.1674   time: 128.99s   best: 71.1674
2023-11-17 14:12:52,777:INFO:  Epoch 179/500:  train Loss: 18.8104   val Loss: 23.7064   time: 427.68s   best: 22.9162
2023-11-17 14:13:18,545:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:13:18,772:INFO:  Epoch 4/500:  train Loss: 70.4776   val Loss: 69.5004   time: 129.41s   best: 69.5004
2023-11-17 14:15:27,697:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:15:27,717:INFO:  Epoch 5/500:  train Loss: 68.3126   val Loss: 67.1991   time: 128.92s   best: 67.1991
2023-11-17 14:17:36,085:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:17:36,105:INFO:  Epoch 6/500:  train Loss: 66.6366   val Loss: 65.3198   time: 128.35s   best: 65.3198
2023-11-17 14:19:44,317:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:19:44,337:INFO:  Epoch 7/500:  train Loss: 65.4668   val Loss: 64.1905   time: 128.20s   best: 64.1905
2023-11-17 14:19:59,396:INFO:  Epoch 180/500:  train Loss: 18.9050   val Loss: 23.3801   time: 426.61s   best: 22.9162
2023-11-17 14:21:53,216:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:21:53,242:INFO:  Epoch 8/500:  train Loss: 64.4114   val Loss: 63.6790   time: 128.86s   best: 63.6790
2023-11-17 14:24:01,735:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:24:01,755:INFO:  Epoch 9/500:  train Loss: 63.3729   val Loss: 61.6722   time: 128.48s   best: 61.6722
2023-11-17 14:26:10,645:INFO:  Epoch 10/500:  train Loss: 62.4696   val Loss: 62.7063   time: 128.88s   best: 61.6722
2023-11-17 14:27:06,671:INFO:  Epoch 181/500:  train Loss: 18.7936   val Loss: 23.3598   time: 427.25s   best: 22.9162
2023-11-17 14:28:19,939:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:28:19,970:INFO:  Epoch 11/500:  train Loss: 61.4354   val Loss: 60.0217   time: 129.28s   best: 60.0217
2023-11-17 14:30:28,896:INFO:  Epoch 12/500:  train Loss: 60.7826   val Loss: 60.1209   time: 128.93s   best: 60.0217
2023-11-17 14:32:37,839:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:32:37,859:INFO:  Epoch 13/500:  train Loss: 59.8265   val Loss: 59.1346   time: 128.93s   best: 59.1346
2023-11-17 14:34:14,561:INFO:  Epoch 182/500:  train Loss: 18.7712   val Loss: 23.6995   time: 427.86s   best: 22.9162
2023-11-17 14:34:45,838:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:34:45,864:INFO:  Epoch 14/500:  train Loss: 58.9863   val Loss: 57.5126   time: 127.98s   best: 57.5126
2023-11-17 14:36:54,731:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:36:54,750:INFO:  Epoch 15/500:  train Loss: 58.4796   val Loss: 57.2930   time: 128.86s   best: 57.2930
2023-11-17 14:39:02,718:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:39:02,737:INFO:  Epoch 16/500:  train Loss: 57.2100   val Loss: 55.7987   time: 127.96s   best: 55.7987
2023-11-17 14:41:11,416:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:41:11,436:INFO:  Epoch 17/500:  train Loss: 56.5445   val Loss: 55.4995   time: 128.67s   best: 55.4995
2023-11-17 14:41:19,368:INFO:  Epoch 183/500:  train Loss: 18.8069   val Loss: 23.5824   time: 424.79s   best: 22.9162
2023-11-17 14:43:19,685:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:43:19,729:INFO:  Epoch 18/500:  train Loss: 56.1852   val Loss: 55.1269   time: 128.23s   best: 55.1269
2023-11-17 14:45:28,122:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:45:28,159:INFO:  Epoch 19/500:  train Loss: 54.8517   val Loss: 54.2764   time: 128.39s   best: 54.2764
2023-11-17 14:47:36,700:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:47:36,719:INFO:  Epoch 20/500:  train Loss: 53.9271   val Loss: 53.6827   time: 128.54s   best: 53.6827
2023-11-17 14:48:23,340:INFO:  Epoch 184/500:  train Loss: 18.7631   val Loss: 23.2997   time: 423.96s   best: 22.9162
2023-11-17 14:49:45,185:INFO:  Epoch 21/500:  train Loss: 53.9592   val Loss: 54.1302   time: 128.47s   best: 53.6827
2023-11-17 14:51:53,560:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:51:53,579:INFO:  Epoch 22/500:  train Loss: 53.1623   val Loss: 52.4645   time: 128.35s   best: 52.4645
2023-11-17 14:54:01,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:54:02,016:INFO:  Epoch 23/500:  train Loss: 52.0586   val Loss: 52.0632   time: 128.41s   best: 52.0632
2023-11-17 14:55:29,009:INFO:  Epoch 185/500:  train Loss: 18.7993   val Loss: 24.6380   time: 425.63s   best: 22.9162
2023-11-17 14:56:10,501:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:56:10,543:INFO:  Epoch 24/500:  train Loss: 51.0094   val Loss: 50.7957   time: 128.47s   best: 50.7957
2023-11-17 14:58:19,819:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 14:58:19,838:INFO:  Epoch 25/500:  train Loss: 50.3503   val Loss: 49.8905   time: 129.26s   best: 49.8905
2023-11-17 15:00:28,318:INFO:  Epoch 26/500:  train Loss: 49.8114   val Loss: 50.2643   time: 128.47s   best: 49.8905
2023-11-17 15:02:34,958:INFO:  Epoch 186/500:  train Loss: 18.9573   val Loss: 23.1090   time: 425.94s   best: 22.9162
2023-11-17 15:02:36,438:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:02:36,477:INFO:  Epoch 27/500:  train Loss: 48.9322   val Loss: 48.5987   time: 128.12s   best: 48.5987
2023-11-17 15:04:45,340:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:04:45,360:INFO:  Epoch 28/500:  train Loss: 48.3259   val Loss: 48.4774   time: 128.85s   best: 48.4774
2023-11-17 15:06:54,554:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:06:54,573:INFO:  Epoch 29/500:  train Loss: 47.2703   val Loss: 47.6843   time: 129.18s   best: 47.6843
2023-11-17 15:09:02,944:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:09:02,964:INFO:  Epoch 30/500:  train Loss: 46.5673   val Loss: 46.5123   time: 128.36s   best: 46.5123
2023-11-17 15:09:38,914:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 15:09:38,933:INFO:  Epoch 187/500:  train Loss: 18.9232   val Loss: 22.9140   time: 423.93s   best: 22.9140
2023-11-17 15:11:11,458:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:11:11,502:INFO:  Epoch 31/500:  train Loss: 45.9595   val Loss: 45.7456   time: 128.48s   best: 45.7456
2023-11-17 15:13:20,356:INFO:  Epoch 32/500:  train Loss: 45.4014   val Loss: 46.3073   time: 128.84s   best: 45.7456
2023-11-17 15:15:28,840:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:15:28,859:INFO:  Epoch 33/500:  train Loss: 44.6649   val Loss: 45.1754   time: 128.48s   best: 45.1754
2023-11-17 15:16:43,961:INFO:  Epoch 188/500:  train Loss: 18.7942   val Loss: 23.9342   time: 425.02s   best: 22.9140
2023-11-17 15:17:38,271:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:17:38,296:INFO:  Epoch 34/500:  train Loss: 44.1585   val Loss: 44.5446   time: 129.41s   best: 44.5446
2023-11-17 15:19:47,404:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:19:47,423:INFO:  Epoch 35/500:  train Loss: 43.4568   val Loss: 43.8965   time: 129.10s   best: 43.8965
2023-11-17 15:21:55,795:INFO:  Epoch 36/500:  train Loss: 43.4328   val Loss: 44.7815   time: 128.36s   best: 43.8965
2023-11-17 15:23:52,359:INFO:  Epoch 189/500:  train Loss: 18.9979   val Loss: 23.3181   time: 428.36s   best: 22.9140
2023-11-17 15:24:04,457:INFO:  Epoch 37/500:  train Loss: 42.5934   val Loss: 44.0599   time: 128.65s   best: 43.8965
2023-11-17 15:26:12,871:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:26:12,890:INFO:  Epoch 38/500:  train Loss: 42.3202   val Loss: 43.4092   time: 128.40s   best: 43.4092
2023-11-17 15:28:21,512:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:28:21,540:INFO:  Epoch 39/500:  train Loss: 41.8836   val Loss: 43.1561   time: 128.61s   best: 43.1561
2023-11-17 15:30:30,321:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:30:30,350:INFO:  Epoch 40/500:  train Loss: 41.3305   val Loss: 41.9239   time: 128.77s   best: 41.9239
2023-11-17 15:31:01,999:INFO:  Epoch 190/500:  train Loss: 18.8077   val Loss: 23.5599   time: 429.62s   best: 22.9140
2023-11-17 15:32:38,760:INFO:  Epoch 41/500:  train Loss: 41.0160   val Loss: 42.2180   time: 128.41s   best: 41.9239
2023-11-17 15:34:47,840:INFO:  Epoch 42/500:  train Loss: 40.5335   val Loss: 42.0626   time: 129.06s   best: 41.9239
2023-11-17 15:36:56,412:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:36:56,432:INFO:  Epoch 43/500:  train Loss: 40.4653   val Loss: 41.3871   time: 128.57s   best: 41.3871
2023-11-17 15:38:11,030:INFO:  Epoch 191/500:  train Loss: 18.7690   val Loss: 23.0407   time: 428.99s   best: 22.9140
2023-11-17 15:39:05,078:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:39:05,125:INFO:  Epoch 44/500:  train Loss: 39.7805   val Loss: 41.0594   time: 128.64s   best: 41.0594
2023-11-17 15:41:13,465:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:41:13,494:INFO:  Epoch 45/500:  train Loss: 39.5023   val Loss: 41.0041   time: 128.32s   best: 41.0041
2023-11-17 15:43:22,209:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:43:22,229:INFO:  Epoch 46/500:  train Loss: 38.9944   val Loss: 39.6823   time: 128.71s   best: 39.6823
2023-11-17 15:45:18,695:INFO:  Epoch 192/500:  train Loss: 18.6609   val Loss: 24.0741   time: 427.64s   best: 22.9140
2023-11-17 15:45:31,765:INFO:  Epoch 47/500:  train Loss: 38.4579   val Loss: 39.9265   time: 129.52s   best: 39.6823
2023-11-17 15:47:40,734:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:47:40,755:INFO:  Epoch 48/500:  train Loss: 38.1944   val Loss: 38.6639   time: 128.95s   best: 38.6639
2023-11-17 15:49:49,273:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:49:49,299:INFO:  Epoch 49/500:  train Loss: 37.7481   val Loss: 38.0873   time: 128.50s   best: 38.0873
2023-11-17 15:51:58,490:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:51:58,509:INFO:  Epoch 50/500:  train Loss: 37.2927   val Loss: 38.0232   time: 129.17s   best: 38.0232
2023-11-17 15:52:23,569:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-17 15:52:23,599:INFO:  Epoch 193/500:  train Loss: 18.8884   val Loss: 22.3811   time: 424.85s   best: 22.3811
2023-11-17 15:54:06,687:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 15:54:06,726:INFO:  Epoch 51/500:  train Loss: 37.0501   val Loss: 37.5703   time: 128.16s   best: 37.5703
2023-11-17 15:56:14,647:INFO:  Epoch 52/500:  train Loss: 36.7350   val Loss: 37.9243   time: 127.92s   best: 37.5703
2023-11-17 15:58:22,955:INFO:  Epoch 53/500:  train Loss: 36.3175   val Loss: 37.6992   time: 128.30s   best: 37.5703
2023-11-17 15:59:29,253:INFO:  Epoch 194/500:  train Loss: 18.6138   val Loss: 25.1873   time: 425.64s   best: 22.3811
2023-11-17 16:00:32,391:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 16:00:32,420:INFO:  Epoch 54/500:  train Loss: 36.1337   val Loss: 36.8488   time: 129.43s   best: 36.8488
2023-11-17 16:02:41,174:INFO:  Epoch 55/500:  train Loss: 35.8222   val Loss: 37.0199   time: 128.75s   best: 36.8488
2023-11-17 16:04:49,374:INFO:  Epoch 56/500:  train Loss: 35.7501   val Loss: 36.8740   time: 128.20s   best: 36.8488
2023-11-17 16:06:36,184:INFO:  Epoch 195/500:  train Loss: 18.7143   val Loss: 22.7664   time: 426.91s   best: 22.3811
2023-11-17 16:06:58,649:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 16:06:58,670:INFO:  Epoch 57/500:  train Loss: 35.2671   val Loss: 35.5924   time: 129.27s   best: 35.5924
2023-11-17 16:09:07,732:INFO:  Epoch 58/500:  train Loss: 34.8848   val Loss: 35.8982   time: 129.05s   best: 35.5924
2023-11-17 16:11:16,552:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 16:11:16,572:INFO:  Epoch 59/500:  train Loss: 34.9392   val Loss: 34.8719   time: 128.80s   best: 34.8719
2023-11-17 16:13:26,233:INFO:  Epoch 60/500:  train Loss: 34.4443   val Loss: 35.8212   time: 129.65s   best: 34.8719
2023-11-17 16:13:44,131:INFO:  Epoch 196/500:  train Loss: 18.6708   val Loss: 22.9050   time: 427.93s   best: 22.3811
2023-11-17 16:15:34,588:INFO:  Epoch 61/500:  train Loss: 34.2844   val Loss: 35.0241   time: 128.35s   best: 34.8719
2023-11-17 16:17:43,246:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 16:17:43,266:INFO:  Epoch 62/500:  train Loss: 34.0538   val Loss: 34.7479   time: 128.63s   best: 34.7479
2023-11-17 16:19:52,532:INFO:  Epoch 63/500:  train Loss: 33.7465   val Loss: 36.5179   time: 129.25s   best: 34.7479
2023-11-17 16:20:52,063:INFO:  Epoch 197/500:  train Loss: 18.8872   val Loss: 23.4114   time: 427.93s   best: 22.3811
2023-11-17 16:22:01,114:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 16:22:01,157:INFO:  Epoch 64/500:  train Loss: 33.6911   val Loss: 33.9208   time: 128.57s   best: 33.9208
2023-11-17 16:24:09,586:INFO:  Epoch 65/500:  train Loss: 33.5706   val Loss: 34.9121   time: 128.43s   best: 33.9208
2023-11-17 16:26:18,669:INFO:  Epoch 66/500:  train Loss: 33.7834   val Loss: 34.1259   time: 129.07s   best: 33.9208
2023-11-17 16:27:59,867:INFO:  Epoch 198/500:  train Loss: 18.7728   val Loss: 24.0889   time: 427.78s   best: 22.3811
2023-11-17 16:28:27,400:INFO:  Epoch 67/500:  train Loss: 33.0911   val Loss: 33.9446   time: 128.73s   best: 33.9208
2023-11-17 16:30:35,494:INFO:  Epoch 68/500:  train Loss: 32.7949   val Loss: 34.1156   time: 128.09s   best: 33.9208
2023-11-17 16:32:44,730:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 16:32:44,750:INFO:  Epoch 69/500:  train Loss: 32.5062   val Loss: 33.0585   time: 129.23s   best: 33.0585
2023-11-17 16:34:53,211:INFO:  Epoch 70/500:  train Loss: 32.3569   val Loss: 33.8673   time: 128.46s   best: 33.0585
2023-11-17 16:35:05,747:INFO:  Epoch 199/500:  train Loss: 18.6670   val Loss: 23.4051   time: 425.87s   best: 22.3811
2023-11-17 16:37:01,969:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 16:37:01,999:INFO:  Epoch 71/500:  train Loss: 32.3610   val Loss: 32.5858   time: 128.74s   best: 32.5858
2023-11-17 16:39:10,261:INFO:  Epoch 72/500:  train Loss: 32.1424   val Loss: 32.7888   time: 128.25s   best: 32.5858
2023-11-17 16:41:18,984:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 16:41:19,004:INFO:  Epoch 73/500:  train Loss: 31.8303   val Loss: 32.5307   time: 128.72s   best: 32.5307
2023-11-17 16:42:10,498:INFO:  Epoch 200/500:  train Loss: 18.6831   val Loss: 22.9044   time: 424.73s   best: 22.3811
2023-11-17 16:43:28,449:INFO:  Epoch 74/500:  train Loss: 32.0669   val Loss: 32.5375   time: 129.42s   best: 32.5307
2023-11-17 16:45:37,545:INFO:  Epoch 75/500:  train Loss: 32.4266   val Loss: 32.7135   time: 129.07s   best: 32.5307
2023-11-17 16:47:46,601:INFO:  Epoch 76/500:  train Loss: 31.4942   val Loss: 32.5991   time: 129.05s   best: 32.5307
2023-11-17 16:49:18,852:INFO:  Epoch 201/500:  train Loss: 18.6683   val Loss: 23.8938   time: 428.33s   best: 22.3811
2023-11-17 16:49:55,206:INFO:  Epoch 77/500:  train Loss: 31.4641   val Loss: 33.1169   time: 128.60s   best: 32.5307
2023-11-17 16:52:04,080:INFO:  Epoch 78/500:  train Loss: 31.2027   val Loss: 33.5202   time: 128.85s   best: 32.5307
2023-11-17 16:54:13,277:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 16:54:13,298:INFO:  Epoch 79/500:  train Loss: 31.0193   val Loss: 31.6360   time: 129.19s   best: 31.6360
2023-11-17 16:56:21,882:INFO:  Epoch 80/500:  train Loss: 31.3454   val Loss: 31.9676   time: 128.58s   best: 31.6360
2023-11-17 16:56:26,936:INFO:  Epoch 202/500:  train Loss: 18.5302   val Loss: 23.9822   time: 428.07s   best: 22.3811
2023-11-17 16:58:29,995:INFO:  Epoch 81/500:  train Loss: 30.8610   val Loss: 31.6574   time: 128.10s   best: 31.6360
2023-11-17 17:00:37,949:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:00:37,968:INFO:  Epoch 82/500:  train Loss: 30.8538   val Loss: 31.4810   time: 127.94s   best: 31.4810
2023-11-17 17:02:46,170:INFO:  Epoch 83/500:  train Loss: 30.6142   val Loss: 33.2565   time: 128.20s   best: 31.4810
2023-11-17 17:03:34,171:INFO:  Epoch 203/500:  train Loss: 18.7351   val Loss: 23.4239   time: 427.23s   best: 22.3811
2023-11-17 17:04:54,146:INFO:  Epoch 84/500:  train Loss: 30.6271   val Loss: 31.8172   time: 127.97s   best: 31.4810
2023-11-17 17:07:02,032:INFO:  Epoch 85/500:  train Loss: 30.8868   val Loss: 31.9920   time: 127.85s   best: 31.4810
2023-11-17 17:09:10,800:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:09:10,819:INFO:  Epoch 86/500:  train Loss: 30.2052   val Loss: 31.3612   time: 128.75s   best: 31.3612
2023-11-17 17:10:37,381:INFO:  Epoch 204/500:  train Loss: 18.5408   val Loss: 22.8766   time: 423.18s   best: 22.3811
2023-11-17 17:11:18,799:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:11:18,849:INFO:  Epoch 87/500:  train Loss: 30.0795   val Loss: 31.2625   time: 127.98s   best: 31.2625
2023-11-17 17:13:26,953:INFO:  Epoch 88/500:  train Loss: 30.5274   val Loss: 31.4346   time: 128.10s   best: 31.2625
2023-11-17 17:15:35,742:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:15:35,761:INFO:  Epoch 89/500:  train Loss: 29.8821   val Loss: 31.1899   time: 128.78s   best: 31.1899
2023-11-17 17:17:44,056:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:17:44,075:INFO:  Epoch 90/500:  train Loss: 30.2717   val Loss: 30.9860   time: 128.29s   best: 30.9860
2023-11-17 17:17:45,356:INFO:  Epoch 205/500:  train Loss: 18.5666   val Loss: 23.4297   time: 427.95s   best: 22.3811
2023-11-17 17:19:52,717:INFO:  Epoch 91/500:  train Loss: 29.7772   val Loss: 31.1133   time: 128.63s   best: 30.9860
2023-11-17 17:22:02,066:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:22:02,085:INFO:  Epoch 92/500:  train Loss: 29.6832   val Loss: 30.8762   time: 129.33s   best: 30.8762
2023-11-17 17:24:10,344:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:24:10,363:INFO:  Epoch 93/500:  train Loss: 29.5878   val Loss: 30.4937   time: 128.25s   best: 30.4937
2023-11-17 17:24:53,228:INFO:  Epoch 206/500:  train Loss: 18.8301   val Loss: 23.5250   time: 427.86s   best: 22.3811
2023-11-17 17:26:18,600:INFO:  Epoch 94/500:  train Loss: 29.4976   val Loss: 30.8945   time: 128.23s   best: 30.4937
2023-11-17 17:28:27,263:INFO:  Epoch 95/500:  train Loss: 29.2820   val Loss: 31.8628   time: 128.64s   best: 30.4937
2023-11-17 17:30:36,904:INFO:  Epoch 96/500:  train Loss: 29.4351   val Loss: 30.8626   time: 129.64s   best: 30.4937
2023-11-17 17:31:59,206:INFO:  Epoch 207/500:  train Loss: 18.7298   val Loss: 22.8261   time: 425.97s   best: 22.3811
2023-11-17 17:32:46,039:INFO:  Epoch 97/500:  train Loss: 29.1646   val Loss: 30.5509   time: 129.12s   best: 30.4937
2023-11-17 17:34:54,931:INFO:  Epoch 98/500:  train Loss: 29.0965   val Loss: 32.6288   time: 128.87s   best: 30.4937
2023-11-17 17:37:04,470:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:37:04,489:INFO:  Epoch 99/500:  train Loss: 29.0538   val Loss: 30.2508   time: 129.53s   best: 30.2508
2023-11-17 17:39:03,515:INFO:  Epoch 208/500:  train Loss: 18.4356   val Loss: 23.5070   time: 424.29s   best: 22.3811
2023-11-17 17:39:13,491:INFO:  Epoch 100/500:  train Loss: 29.3415   val Loss: 30.5179   time: 128.99s   best: 30.2508
2023-11-17 17:41:22,960:INFO:  Epoch 101/500:  train Loss: 28.9285   val Loss: 30.5654   time: 129.47s   best: 30.2508
2023-11-17 17:43:32,515:INFO:  Epoch 102/500:  train Loss: 29.0389   val Loss: 32.0240   time: 129.55s   best: 30.2508
2023-11-17 17:45:41,195:INFO:  Epoch 103/500:  train Loss: 28.7361   val Loss: 31.6460   time: 128.67s   best: 30.2508
2023-11-17 17:46:07,666:INFO:  Epoch 209/500:  train Loss: 18.5618   val Loss: 23.2013   time: 424.13s   best: 22.3811
2023-11-17 17:47:48,937:INFO:  Epoch 104/500:  train Loss: 28.5601   val Loss: 30.2922   time: 127.73s   best: 30.2508
2023-11-17 17:49:56,815:INFO:  Epoch 105/500:  train Loss: 28.5238   val Loss: 30.3117   time: 127.85s   best: 30.2508
2023-11-17 17:52:05,169:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:52:05,188:INFO:  Epoch 106/500:  train Loss: 28.3590   val Loss: 30.0375   time: 128.34s   best: 30.0375
2023-11-17 17:53:13,816:INFO:  Epoch 210/500:  train Loss: 18.4416   val Loss: 22.6737   time: 426.14s   best: 22.3811
2023-11-17 17:54:14,451:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:54:14,497:INFO:  Epoch 107/500:  train Loss: 28.3160   val Loss: 29.7572   time: 129.25s   best: 29.7572
2023-11-17 17:56:22,626:INFO:  Epoch 108/500:  train Loss: 28.2741   val Loss: 29.7696   time: 128.13s   best: 29.7572
2023-11-17 17:58:31,983:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 17:58:32,002:INFO:  Epoch 109/500:  train Loss: 28.1728   val Loss: 29.7507   time: 129.35s   best: 29.7507
2023-11-17 18:00:18,648:INFO:  Epoch 211/500:  train Loss: 18.6977   val Loss: 24.1421   time: 424.80s   best: 22.3811
2023-11-17 18:00:40,622:INFO:  Epoch 110/500:  train Loss: 28.1307   val Loss: 30.0120   time: 128.62s   best: 29.7507
2023-11-17 18:02:49,132:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 18:02:49,151:INFO:  Epoch 111/500:  train Loss: 28.0250   val Loss: 29.6407   time: 128.49s   best: 29.6407
2023-11-17 18:04:57,945:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 18:04:57,964:INFO:  Epoch 112/500:  train Loss: 28.4505   val Loss: 29.4727   time: 128.78s   best: 29.4727
2023-11-17 18:07:06,593:INFO:  Epoch 113/500:  train Loss: 27.6590   val Loss: 29.5430   time: 128.63s   best: 29.4727
2023-11-17 18:07:24,652:INFO:  Epoch 212/500:  train Loss: 18.4745   val Loss: 23.9388   time: 425.99s   best: 22.3811
2023-11-17 18:09:15,628:INFO:  Epoch 114/500:  train Loss: 28.1148   val Loss: 29.8068   time: 129.03s   best: 29.4727
2023-11-17 18:11:24,127:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 18:11:24,146:INFO:  Epoch 115/500:  train Loss: 27.7071   val Loss: 29.0107   time: 128.48s   best: 29.0107
2023-11-17 18:13:33,119:INFO:  Epoch 116/500:  train Loss: 27.6570   val Loss: 31.7494   time: 128.96s   best: 29.0107
2023-11-17 18:14:27,910:INFO:  Epoch 213/500:  train Loss: 18.5269   val Loss: 23.1705   time: 423.26s   best: 22.3811
2023-11-17 18:15:41,283:INFO:  Epoch 117/500:  train Loss: 27.6407   val Loss: 29.3670   time: 128.16s   best: 29.0107
2023-11-17 18:17:49,792:INFO:  Epoch 118/500:  train Loss: 27.6384   val Loss: 29.2235   time: 128.49s   best: 29.0107
2023-11-17 18:19:57,972:INFO:  Epoch 119/500:  train Loss: 27.4043   val Loss: 29.6464   time: 128.18s   best: 29.0107
2023-11-17 18:21:34,526:INFO:  Epoch 214/500:  train Loss: 18.4665   val Loss: 22.6590   time: 426.61s   best: 22.3811
2023-11-17 18:22:06,302:INFO:  Epoch 120/500:  train Loss: 27.5528   val Loss: 29.7008   time: 128.33s   best: 29.0107
2023-11-17 18:24:14,503:INFO:  Epoch 121/500:  train Loss: 27.7474   val Loss: 29.8087   time: 128.17s   best: 29.0107
2023-11-17 18:26:22,326:INFO:  Epoch 122/500:  train Loss: 27.3808   val Loss: 29.4652   time: 127.82s   best: 29.0107
2023-11-17 18:28:30,395:INFO:  Epoch 123/500:  train Loss: 27.0607   val Loss: 29.5564   time: 128.07s   best: 29.0107
2023-11-17 18:28:38,909:INFO:  Epoch 215/500:  train Loss: 18.4505   val Loss: 22.7721   time: 424.36s   best: 22.3811
2023-11-17 18:30:38,056:INFO:  Epoch 124/500:  train Loss: 27.1211   val Loss: 29.9116   time: 127.64s   best: 29.0107
2023-11-17 18:32:46,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 18:32:46,107:INFO:  Epoch 125/500:  train Loss: 27.0498   val Loss: 28.8102   time: 128.00s   best: 28.8102
2023-11-17 18:34:53,860:INFO:  Epoch 126/500:  train Loss: 26.9576   val Loss: 29.1059   time: 127.74s   best: 28.8102
2023-11-17 18:35:43,787:INFO:  Epoch 216/500:  train Loss: 18.5136   val Loss: 23.1015   time: 424.86s   best: 22.3811
2023-11-17 18:37:01,570:INFO:  Epoch 127/500:  train Loss: 26.8979   val Loss: 29.5323   time: 127.71s   best: 28.8102
2023-11-17 18:39:09,941:INFO:  Epoch 128/500:  train Loss: 27.0064   val Loss: 29.1340   time: 128.34s   best: 28.8102
2023-11-17 18:41:18,047:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 18:41:18,066:INFO:  Epoch 129/500:  train Loss: 26.9456   val Loss: 28.4699   time: 128.10s   best: 28.4699
2023-11-17 18:42:48,151:INFO:  Epoch 217/500:  train Loss: 18.5375   val Loss: 23.4102   time: 424.34s   best: 22.3811
2023-11-17 18:43:26,626:INFO:  Epoch 130/500:  train Loss: 26.9978   val Loss: 29.0084   time: 128.56s   best: 28.4699
2023-11-17 18:45:34,458:INFO:  Epoch 131/500:  train Loss: 26.9812   val Loss: 29.3242   time: 127.82s   best: 28.4699
2023-11-17 18:47:42,192:INFO:  Epoch 132/500:  train Loss: 26.9261   val Loss: 29.6273   time: 127.72s   best: 28.4699
2023-11-17 18:49:49,963:INFO:  Epoch 133/500:  train Loss: 26.7827   val Loss: 29.5964   time: 127.76s   best: 28.4699
2023-11-17 18:49:54,517:INFO:  Epoch 218/500:  train Loss: 18.5098   val Loss: 25.1826   time: 426.34s   best: 22.3811
2023-11-17 18:51:57,590:INFO:  Epoch 134/500:  train Loss: 26.5080   val Loss: 30.1434   time: 127.62s   best: 28.4699
2023-11-17 18:54:05,395:INFO:  Epoch 135/500:  train Loss: 26.6033   val Loss: 30.1216   time: 127.79s   best: 28.4699
2023-11-17 18:56:13,183:INFO:  Epoch 136/500:  train Loss: 26.5930   val Loss: 28.5254   time: 127.79s   best: 28.4699
2023-11-17 18:57:03,062:INFO:  Epoch 219/500:  train Loss: 18.5264   val Loss: 23.3229   time: 428.53s   best: 22.3811
2023-11-17 18:58:21,087:INFO:  Epoch 137/500:  train Loss: 26.4213   val Loss: 28.9204   time: 127.90s   best: 28.4699
2023-11-17 19:00:28,913:INFO:  Epoch 138/500:  train Loss: 26.6924   val Loss: 29.7463   time: 127.80s   best: 28.4699
2023-11-17 19:02:37,325:INFO:  Epoch 139/500:  train Loss: 26.2054   val Loss: 29.5688   time: 128.40s   best: 28.4699
2023-11-17 19:04:10,699:INFO:  Epoch 220/500:  train Loss: 18.6670   val Loss: 23.2311   time: 427.63s   best: 22.3811
2023-11-17 19:04:45,170:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 19:04:45,190:INFO:  Epoch 140/500:  train Loss: 26.5135   val Loss: 28.1369   time: 127.83s   best: 28.1369
2023-11-17 19:06:53,561:INFO:  Epoch 141/500:  train Loss: 26.0943   val Loss: 29.1589   time: 128.36s   best: 28.1369
2023-11-17 19:09:01,201:INFO:  Epoch 142/500:  train Loss: 26.4195   val Loss: 28.1688   time: 127.64s   best: 28.1369
2023-11-17 19:11:09,089:INFO:  Epoch 143/500:  train Loss: 26.1989   val Loss: 28.3176   time: 127.89s   best: 28.1369
2023-11-17 19:11:18,451:INFO:  Epoch 221/500:  train Loss: 18.3992   val Loss: 23.7755   time: 427.73s   best: 22.3811
2023-11-17 19:13:17,677:INFO:  Epoch 144/500:  train Loss: 26.0662   val Loss: 28.5191   time: 128.59s   best: 28.1369
2023-11-17 19:15:25,753:INFO:  Epoch 145/500:  train Loss: 25.9814   val Loss: 28.5226   time: 128.05s   best: 28.1369
2023-11-17 19:17:33,440:INFO:  Epoch 146/500:  train Loss: 26.0515   val Loss: 28.5500   time: 127.68s   best: 28.1369
2023-11-17 19:18:26,466:INFO:  Epoch 222/500:  train Loss: 18.5381   val Loss: 23.2758   time: 428.01s   best: 22.3811
2023-11-17 19:19:42,112:INFO:  Epoch 147/500:  train Loss: 26.2254   val Loss: 28.3188   time: 128.66s   best: 28.1369
2023-11-17 19:21:50,680:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 19:21:50,829:INFO:  Epoch 148/500:  train Loss: 25.7544   val Loss: 27.8733   time: 128.55s   best: 27.8733
2023-11-17 19:23:59,114:INFO:  Epoch 149/500:  train Loss: 25.7814   val Loss: 28.3140   time: 128.27s   best: 27.8733
2023-11-17 19:25:34,488:INFO:  Epoch 223/500:  train Loss: 18.6159   val Loss: 24.1930   time: 428.01s   best: 22.3811
2023-11-17 19:26:07,546:INFO:  Epoch 150/500:  train Loss: 25.7022   val Loss: 28.5017   time: 128.43s   best: 27.8733
2023-11-17 19:28:15,360:INFO:  Epoch 151/500:  train Loss: 25.7186   val Loss: 28.9158   time: 127.79s   best: 27.8733
2023-11-17 19:30:23,028:INFO:  Epoch 152/500:  train Loss: 25.6407   val Loss: 28.8781   time: 127.66s   best: 27.8733
2023-11-17 19:32:31,850:INFO:  Epoch 153/500:  train Loss: 25.5980   val Loss: 28.6418   time: 128.82s   best: 27.8733
2023-11-17 19:32:39,556:INFO:  Epoch 224/500:  train Loss: 18.4250   val Loss: 25.3447   time: 425.06s   best: 22.3811
2023-11-17 19:34:39,550:INFO:  Epoch 154/500:  train Loss: 25.8520   val Loss: 27.9214   time: 127.69s   best: 27.8733
2023-11-17 19:36:48,310:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 19:36:48,329:INFO:  Epoch 155/500:  train Loss: 25.5392   val Loss: 27.3900   time: 128.74s   best: 27.3900
2023-11-17 19:38:56,778:INFO:  Epoch 156/500:  train Loss: 25.4860   val Loss: 27.9130   time: 128.45s   best: 27.3900
2023-11-17 19:39:45,505:INFO:  Epoch 225/500:  train Loss: 18.6860   val Loss: 25.3562   time: 425.95s   best: 22.3811
2023-11-17 19:41:05,116:INFO:  Epoch 157/500:  train Loss: 25.9672   val Loss: 28.1328   time: 128.33s   best: 27.3900
2023-11-17 19:43:14,031:INFO:  Epoch 158/500:  train Loss: 25.3863   val Loss: 27.9864   time: 128.88s   best: 27.3900
2023-11-17 19:45:22,082:INFO:  Epoch 159/500:  train Loss: 25.4621   val Loss: 28.4687   time: 128.05s   best: 27.3900
2023-11-17 19:46:52,659:INFO:  Epoch 226/500:  train Loss: 18.5367   val Loss: 22.7211   time: 427.15s   best: 22.3811
2023-11-17 19:47:30,267:INFO:  Epoch 160/500:  train Loss: 25.2113   val Loss: 27.9293   time: 128.18s   best: 27.3900
2023-11-17 19:49:38,252:INFO:  Epoch 161/500:  train Loss: 25.3799   val Loss: 29.0259   time: 127.96s   best: 27.3900
2023-11-17 19:51:46,620:INFO:  Epoch 162/500:  train Loss: 25.2094   val Loss: 27.4054   time: 128.37s   best: 27.3900
2023-11-17 19:53:54,543:INFO:  Epoch 163/500:  train Loss: 25.3392   val Loss: 29.0195   time: 127.92s   best: 27.3900
2023-11-17 19:53:59,701:INFO:  Epoch 227/500:  train Loss: 18.4446   val Loss: 23.9396   time: 427.02s   best: 22.3811
2023-11-17 19:56:02,286:INFO:  Epoch 164/500:  train Loss: 25.1357   val Loss: 27.7965   time: 127.74s   best: 27.3900
2023-11-17 19:58:10,304:INFO:  Epoch 165/500:  train Loss: 25.1658   val Loss: 27.8136   time: 127.99s   best: 27.3900
2023-11-17 20:00:18,103:INFO:  Epoch 166/500:  train Loss: 25.2463   val Loss: 28.2676   time: 127.80s   best: 27.3900
2023-11-17 20:01:07,876:INFO:  Epoch 228/500:  train Loss: 18.3960   val Loss: 24.8642   time: 428.16s   best: 22.3811
2023-11-17 20:02:26,856:INFO:  Epoch 167/500:  train Loss: 25.1995   val Loss: 31.1436   time: 128.74s   best: 27.3900
2023-11-17 20:04:35,233:INFO:  Epoch 168/500:  train Loss: 25.4283   val Loss: 27.6847   time: 128.35s   best: 27.3900
2023-11-17 20:06:44,160:INFO:  Epoch 169/500:  train Loss: 25.1949   val Loss: 27.7254   time: 128.92s   best: 27.3900
2023-11-17 20:08:14,220:INFO:  Epoch 229/500:  train Loss: 18.5455   val Loss: 23.7589   time: 426.29s   best: 22.3811
2023-11-17 20:08:52,722:INFO:  Epoch 170/500:  train Loss: 24.8546   val Loss: 28.0847   time: 128.56s   best: 27.3900
2023-11-17 20:11:00,991:INFO:  Epoch 171/500:  train Loss: 24.8079   val Loss: 28.2065   time: 128.26s   best: 27.3900
2023-11-17 20:13:09,007:INFO:  Epoch 172/500:  train Loss: 24.8973   val Loss: 28.6813   time: 128.00s   best: 27.3900
2023-11-17 20:15:17,975:INFO:  Epoch 173/500:  train Loss: 25.0764   val Loss: 28.0904   time: 128.97s   best: 27.3900
2023-11-17 20:15:21,306:INFO:  Epoch 230/500:  train Loss: 18.3154   val Loss: 23.6897   time: 427.06s   best: 22.3811
2023-11-17 20:17:26,156:INFO:  Epoch 174/500:  train Loss: 24.8644   val Loss: 27.4861   time: 128.18s   best: 27.3900
2023-11-17 20:19:34,289:INFO:  Epoch 175/500:  train Loss: 24.7638   val Loss: 28.0631   time: 128.10s   best: 27.3900
2023-11-17 20:21:43,217:INFO:  Epoch 176/500:  train Loss: 24.6927   val Loss: 28.1359   time: 128.93s   best: 27.3900
2023-11-17 20:22:28,179:INFO:  Epoch 231/500:  train Loss: 18.4689   val Loss: 25.3966   time: 426.87s   best: 22.3811
2023-11-17 20:23:51,563:INFO:  Epoch 177/500:  train Loss: 25.6861   val Loss: 28.2095   time: 128.34s   best: 27.3900
2023-11-17 20:25:59,749:INFO:  Epoch 178/500:  train Loss: 24.9734   val Loss: 27.4049   time: 128.17s   best: 27.3900
2023-11-17 20:28:08,565:INFO:  Epoch 179/500:  train Loss: 25.1036   val Loss: 27.5080   time: 128.80s   best: 27.3900
2023-11-17 20:29:32,682:INFO:  Epoch 232/500:  train Loss: 18.3656   val Loss: 23.0258   time: 424.46s   best: 22.3811
2023-11-17 20:30:16,653:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 20:30:16,677:INFO:  Epoch 180/500:  train Loss: 24.6130   val Loss: 27.2467   time: 128.08s   best: 27.2467
2023-11-17 20:32:24,607:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 20:32:24,627:INFO:  Epoch 181/500:  train Loss: 24.7740   val Loss: 26.9560   time: 127.93s   best: 26.9560
2023-11-17 20:34:32,675:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 20:34:32,695:INFO:  Epoch 182/500:  train Loss: 24.5341   val Loss: 26.9272   time: 128.04s   best: 26.9272
2023-11-17 20:36:38,185:INFO:  Epoch 233/500:  train Loss: 18.4529   val Loss: 22.6271   time: 425.48s   best: 22.3811
2023-11-17 20:36:41,467:INFO:  Epoch 183/500:  train Loss: 24.9529   val Loss: 28.1304   time: 128.76s   best: 26.9272
2023-11-17 20:38:49,213:INFO:  Epoch 184/500:  train Loss: 24.4984   val Loss: 27.8950   time: 127.73s   best: 26.9272
2023-11-17 20:40:57,237:INFO:  Epoch 185/500:  train Loss: 24.5615   val Loss: 27.2126   time: 128.02s   best: 26.9272
2023-11-17 20:43:04,895:INFO:  Epoch 186/500:  train Loss: 24.6608   val Loss: 28.0373   time: 127.66s   best: 26.9272
2023-11-17 20:43:41,672:INFO:  Epoch 234/500:  train Loss: 18.4281   val Loss: 23.1727   time: 423.46s   best: 22.3811
2023-11-17 20:45:13,558:INFO:  Epoch 187/500:  train Loss: 25.3658   val Loss: 27.4737   time: 128.65s   best: 26.9272
2023-11-17 20:47:21,857:INFO:  Epoch 188/500:  train Loss: 24.4136   val Loss: 27.5229   time: 128.26s   best: 26.9272
2023-11-17 20:49:30,978:INFO:  Epoch 189/500:  train Loss: 24.3255   val Loss: 27.3667   time: 129.12s   best: 26.9272
2023-11-17 20:50:47,542:INFO:  Epoch 235/500:  train Loss: 18.3703   val Loss: 22.5290   time: 425.86s   best: 22.3811
2023-11-17 20:51:39,227:INFO:  Epoch 190/500:  train Loss: 24.3746   val Loss: 27.4397   time: 128.25s   best: 26.9272
2023-11-17 20:53:47,459:INFO:  Epoch 191/500:  train Loss: 24.3752   val Loss: 27.7715   time: 128.22s   best: 26.9272
2023-11-17 20:55:55,833:INFO:  Epoch 192/500:  train Loss: 24.6340   val Loss: 26.9807   time: 128.36s   best: 26.9272
2023-11-17 20:57:55,304:INFO:  Epoch 236/500:  train Loss: 18.3584   val Loss: 23.1979   time: 427.75s   best: 22.3811
2023-11-17 20:58:04,498:INFO:  Epoch 193/500:  train Loss: 24.3990   val Loss: 27.1321   time: 128.66s   best: 26.9272
2023-11-17 21:00:12,144:INFO:  Epoch 194/500:  train Loss: 24.1669   val Loss: 27.2657   time: 127.64s   best: 26.9272
2023-11-17 21:02:19,885:INFO:  Epoch 195/500:  train Loss: 24.1515   val Loss: 27.6126   time: 127.73s   best: 26.9272
2023-11-17 21:04:27,414:INFO:  Epoch 196/500:  train Loss: 24.3258   val Loss: 29.8315   time: 127.53s   best: 26.9272
2023-11-17 21:05:02,450:INFO:  Epoch 237/500:  train Loss: 18.2845   val Loss: 23.4735   time: 427.13s   best: 22.3811
2023-11-17 21:06:35,045:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 21:06:35,085:INFO:  Epoch 197/500:  train Loss: 24.1263   val Loss: 26.8213   time: 127.61s   best: 26.8213
2023-11-17 21:08:42,810:INFO:  Epoch 198/500:  train Loss: 24.0823   val Loss: 27.1788   time: 127.71s   best: 26.8213
2023-11-17 21:10:51,567:INFO:  Epoch 199/500:  train Loss: 24.2109   val Loss: 27.1851   time: 128.76s   best: 26.8213
2023-11-17 21:12:09,572:INFO:  Epoch 238/500:  train Loss: 18.3424   val Loss: 22.6724   time: 427.11s   best: 22.3811
2023-11-17 21:12:59,315:INFO:  Epoch 200/500:  train Loss: 23.9318   val Loss: 27.1343   time: 127.75s   best: 26.8213
2023-11-17 21:15:07,430:INFO:  Epoch 201/500:  train Loss: 24.4022   val Loss: 27.1292   time: 128.10s   best: 26.8213
2023-11-17 21:17:15,193:INFO:  Epoch 202/500:  train Loss: 24.3378   val Loss: 26.8782   time: 127.75s   best: 26.8213
2023-11-17 21:19:14,097:INFO:  Epoch 239/500:  train Loss: 18.2962   val Loss: 23.2466   time: 424.52s   best: 22.3811
2023-11-17 21:19:23,219:INFO:  Epoch 203/500:  train Loss: 23.9106   val Loss: 29.3997   time: 128.01s   best: 26.8213
2023-11-17 21:21:30,959:INFO:  Epoch 204/500:  train Loss: 23.8772   val Loss: 27.3805   time: 127.74s   best: 26.8213
2023-11-17 21:23:39,007:INFO:  Epoch 205/500:  train Loss: 23.9933   val Loss: 28.6661   time: 128.04s   best: 26.8213
2023-11-17 21:25:46,692:INFO:  Epoch 206/500:  train Loss: 24.0969   val Loss: 27.0725   time: 127.67s   best: 26.8213
2023-11-17 21:26:19,875:INFO:  Epoch 240/500:  train Loss: 18.4868   val Loss: 23.1284   time: 425.75s   best: 22.3811
2023-11-17 21:27:54,672:INFO:  Epoch 207/500:  train Loss: 23.9416   val Loss: 28.9944   time: 127.97s   best: 26.8213
2023-11-17 21:30:02,498:INFO:  Epoch 208/500:  train Loss: 23.8187   val Loss: 26.8312   time: 127.80s   best: 26.8213
2023-11-17 21:32:10,491:INFO:  Epoch 209/500:  train Loss: 23.9238   val Loss: 27.2661   time: 127.99s   best: 26.8213
2023-11-17 21:33:26,299:INFO:  Epoch 241/500:  train Loss: 18.4223   val Loss: 23.5165   time: 426.41s   best: 22.3811
2023-11-17 21:34:18,318:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 21:34:18,341:INFO:  Epoch 210/500:  train Loss: 23.8855   val Loss: 26.7622   time: 127.79s   best: 26.7622
2023-11-17 21:36:26,091:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 21:36:26,110:INFO:  Epoch 211/500:  train Loss: 23.6701   val Loss: 26.6561   time: 127.74s   best: 26.6561
2023-11-17 21:38:33,935:INFO:  Epoch 212/500:  train Loss: 23.8345   val Loss: 27.0893   time: 127.82s   best: 26.6561
2023-11-17 21:40:33,638:INFO:  Epoch 242/500:  train Loss: 18.3591   val Loss: 22.9353   time: 427.31s   best: 22.3811
2023-11-17 21:40:42,420:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 21:40:42,441:INFO:  Epoch 213/500:  train Loss: 23.7805   val Loss: 26.5452   time: 128.47s   best: 26.5452
2023-11-17 21:42:50,266:INFO:  Epoch 214/500:  train Loss: 23.5669   val Loss: 27.4806   time: 127.82s   best: 26.5452
2023-11-17 21:45:00,186:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 21:45:00,205:INFO:  Epoch 215/500:  train Loss: 23.6347   val Loss: 26.4733   time: 129.91s   best: 26.4733
2023-11-17 21:47:08,333:INFO:  Epoch 216/500:  train Loss: 23.6244   val Loss: 26.8163   time: 128.13s   best: 26.4733
2023-11-17 21:47:41,811:INFO:  Epoch 243/500:  train Loss: 18.2728   val Loss: 23.2299   time: 428.14s   best: 22.3811
2023-11-17 21:49:16,862:INFO:  Epoch 217/500:  train Loss: 23.6602   val Loss: 26.9946   time: 128.53s   best: 26.4733
2023-11-17 21:51:24,717:INFO:  Epoch 218/500:  train Loss: 24.2582   val Loss: 26.5742   time: 127.84s   best: 26.4733
2023-11-17 21:53:32,493:INFO:  Epoch 219/500:  train Loss: 23.4587   val Loss: 26.9312   time: 127.76s   best: 26.4733
2023-11-17 21:54:50,775:INFO:  Epoch 244/500:  train Loss: 18.5868   val Loss: 25.2710   time: 428.96s   best: 22.3811
2023-11-17 21:55:40,053:INFO:  Epoch 220/500:  train Loss: 23.5697   val Loss: 26.8463   time: 127.55s   best: 26.4733
2023-11-17 21:57:48,924:INFO:  Epoch 221/500:  train Loss: 23.4422   val Loss: 26.8055   time: 128.86s   best: 26.4733
2023-11-17 21:59:57,197:INFO:  Epoch 222/500:  train Loss: 23.5878   val Loss: 26.8329   time: 128.27s   best: 26.4733
2023-11-17 22:01:58,085:INFO:  Epoch 245/500:  train Loss: 18.8473   val Loss: 23.3892   time: 427.30s   best: 22.3811
2023-11-17 22:02:05,855:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 22:02:05,886:INFO:  Epoch 223/500:  train Loss: 23.4711   val Loss: 26.4270   time: 128.64s   best: 26.4270
2023-11-17 22:04:13,531:INFO:  Epoch 224/500:  train Loss: 23.5583   val Loss: 26.8765   time: 127.64s   best: 26.4270
2023-11-17 22:06:22,034:INFO:  Epoch 225/500:  train Loss: 23.3500   val Loss: 26.4319   time: 128.50s   best: 26.4270
2023-11-17 22:08:30,422:INFO:  Epoch 226/500:  train Loss: 23.2376   val Loss: 26.7193   time: 128.39s   best: 26.4270
2023-11-17 22:09:06,198:INFO:  Epoch 246/500:  train Loss: 18.3392   val Loss: 23.0119   time: 428.09s   best: 22.3811
2023-11-17 22:10:38,667:INFO:  Epoch 227/500:  train Loss: 24.5228   val Loss: 32.7346   time: 128.23s   best: 26.4270
2023-11-17 22:12:46,618:INFO:  Epoch 228/500:  train Loss: 24.8929   val Loss: 27.2312   time: 127.92s   best: 26.4270
2023-11-17 22:14:54,580:INFO:  Epoch 229/500:  train Loss: 23.2358   val Loss: 27.0078   time: 127.95s   best: 26.4270
2023-11-17 22:16:09,962:INFO:  Epoch 247/500:  train Loss: 18.3590   val Loss: 23.1735   time: 423.76s   best: 22.3811
2023-11-17 22:17:02,806:INFO:  Epoch 230/500:  train Loss: 23.4835   val Loss: 27.5191   time: 128.22s   best: 26.4270
2023-11-17 22:19:11,489:INFO:  Epoch 231/500:  train Loss: 23.3401   val Loss: 26.7699   time: 128.66s   best: 26.4270
2023-11-17 22:21:19,519:INFO:  Epoch 232/500:  train Loss: 23.2666   val Loss: 27.9801   time: 128.02s   best: 26.4270
2023-11-17 22:23:15,256:INFO:  Epoch 248/500:  train Loss: 18.1984   val Loss: 23.2934   time: 425.28s   best: 22.3811
2023-11-17 22:23:27,848:INFO:  Epoch 233/500:  train Loss: 23.2170   val Loss: 27.8761   time: 128.32s   best: 26.4270
2023-11-17 22:25:36,193:INFO:  Epoch 234/500:  train Loss: 23.4710   val Loss: 26.5716   time: 128.33s   best: 26.4270
2023-11-17 22:27:45,185:INFO:  Epoch 235/500:  train Loss: 23.1569   val Loss: 26.7942   time: 128.98s   best: 26.4270
2023-11-17 22:29:53,507:INFO:  Epoch 236/500:  train Loss: 23.3842   val Loss: 26.9901   time: 128.32s   best: 26.4270
2023-11-17 22:30:21,296:INFO:  Epoch 249/500:  train Loss: 18.1187   val Loss: 23.6102   time: 426.01s   best: 22.3811
2023-11-17 22:32:02,329:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 22:32:02,361:INFO:  Epoch 237/500:  train Loss: 23.3087   val Loss: 26.3523   time: 128.82s   best: 26.3523
2023-11-17 22:34:11,423:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 22:34:11,443:INFO:  Epoch 238/500:  train Loss: 23.3186   val Loss: 26.2871   time: 129.06s   best: 26.2871
2023-11-17 22:36:20,547:INFO:  Epoch 239/500:  train Loss: 23.1399   val Loss: 26.4688   time: 129.10s   best: 26.2871
2023-11-17 22:37:27,162:INFO:  Epoch 250/500:  train Loss: 18.2378   val Loss: 23.1968   time: 425.86s   best: 22.3811
2023-11-17 22:38:29,039:INFO:  Epoch 240/500:  train Loss: 23.0074   val Loss: 26.8478   time: 128.48s   best: 26.2871
2023-11-17 22:40:37,082:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 22:40:37,102:INFO:  Epoch 241/500:  train Loss: 23.0896   val Loss: 26.0042   time: 128.02s   best: 26.0042
2023-11-17 22:42:46,000:INFO:  Epoch 242/500:  train Loss: 23.1371   val Loss: 26.9476   time: 128.90s   best: 26.0042
2023-11-17 22:44:33,223:INFO:  Epoch 251/500:  train Loss: 18.3519   val Loss: 24.6520   time: 426.04s   best: 22.3811
2023-11-17 22:44:54,174:INFO:  Epoch 243/500:  train Loss: 23.1975   val Loss: 27.7219   time: 128.17s   best: 26.0042
2023-11-17 22:47:02,152:INFO:  Epoch 244/500:  train Loss: 22.9708   val Loss: 26.3500   time: 127.95s   best: 26.0042
2023-11-17 22:49:10,207:INFO:  Epoch 245/500:  train Loss: 23.0224   val Loss: 26.4153   time: 128.05s   best: 26.0042
2023-11-17 22:51:18,186:INFO:  Epoch 246/500:  train Loss: 22.9567   val Loss: 27.1002   time: 127.96s   best: 26.0042
2023-11-17 22:51:40,663:INFO:  Epoch 252/500:  train Loss: 18.1638   val Loss: 26.0803   time: 427.40s   best: 22.3811
2023-11-17 22:53:26,510:INFO:  Epoch 247/500:  train Loss: 23.0098   val Loss: 27.4632   time: 128.31s   best: 26.0042
2023-11-17 22:55:34,546:INFO:  Epoch 248/500:  train Loss: 23.2841   val Loss: 26.6190   time: 128.03s   best: 26.0042
2023-11-17 22:57:42,731:INFO:  Epoch 249/500:  train Loss: 22.9095   val Loss: 26.7198   time: 128.18s   best: 26.0042
2023-11-17 22:58:45,610:INFO:  Epoch 253/500:  train Loss: 18.2011   val Loss: 22.8582   time: 424.94s   best: 22.3811
2023-11-17 22:59:50,878:INFO:  Epoch 250/500:  train Loss: 22.7510   val Loss: 26.6162   time: 128.15s   best: 26.0042
2023-11-17 23:01:59,390:INFO:  Epoch 251/500:  train Loss: 22.8561   val Loss: 28.2286   time: 128.50s   best: 26.0042
2023-11-17 23:04:08,681:INFO:  Epoch 252/500:  train Loss: 23.1829   val Loss: 26.3560   time: 129.28s   best: 26.0042
2023-11-17 23:05:49,214:INFO:  Epoch 254/500:  train Loss: 18.3760   val Loss: 23.2380   time: 423.59s   best: 22.3811
2023-11-17 23:06:17,248:INFO:  Epoch 253/500:  train Loss: 22.7456   val Loss: 26.4235   time: 128.56s   best: 26.0042
2023-11-17 23:08:25,694:INFO:  Epoch 254/500:  train Loss: 22.9259   val Loss: 26.5299   time: 128.42s   best: 26.0042
2023-11-17 23:10:34,749:INFO:  Epoch 255/500:  train Loss: 22.6902   val Loss: 27.1823   time: 129.04s   best: 26.0042
2023-11-17 23:12:43,471:INFO:  Epoch 256/500:  train Loss: 22.8667   val Loss: 27.2652   time: 128.71s   best: 26.0042
2023-11-17 23:12:56,662:INFO:  Epoch 255/500:  train Loss: 18.3176   val Loss: 23.3323   time: 427.41s   best: 22.3811
2023-11-17 23:14:53,139:INFO:  Epoch 257/500:  train Loss: 22.8826   val Loss: 26.2025   time: 129.66s   best: 26.0042
2023-11-17 23:17:01,455:INFO:  Epoch 258/500:  train Loss: 22.7690   val Loss: 29.2388   time: 128.29s   best: 26.0042
2023-11-17 23:19:10,139:INFO:  Epoch 259/500:  train Loss: 22.8094   val Loss: 26.4325   time: 128.67s   best: 26.0042
2023-11-17 23:20:00,393:INFO:  Epoch 256/500:  train Loss: 18.2061   val Loss: 22.6322   time: 423.73s   best: 22.3811
2023-11-17 23:21:19,193:INFO:  Epoch 260/500:  train Loss: 23.1211   val Loss: 26.5534   time: 129.05s   best: 26.0042
2023-11-17 23:23:28,108:INFO:  Epoch 261/500:  train Loss: 23.1375   val Loss: 26.4260   time: 128.87s   best: 26.0042
2023-11-17 23:25:36,766:INFO:  Epoch 262/500:  train Loss: 22.7865   val Loss: 26.1727   time: 128.65s   best: 26.0042
2023-11-17 23:27:04,502:INFO:  Epoch 257/500:  train Loss: 18.0802   val Loss: 23.4589   time: 424.11s   best: 22.3811
2023-11-17 23:27:44,827:INFO:  Epoch 263/500:  train Loss: 22.8857   val Loss: 27.1753   time: 128.06s   best: 26.0042
2023-11-17 23:29:53,181:INFO:  Epoch 264/500:  train Loss: 22.5042   val Loss: 26.9684   time: 128.35s   best: 26.0042
2023-11-17 23:32:01,813:INFO:  Epoch 265/500:  train Loss: 22.7512   val Loss: 27.0752   time: 128.62s   best: 26.0042
2023-11-17 23:34:10,032:INFO:  Epoch 266/500:  train Loss: 22.7085   val Loss: 26.1453   time: 128.21s   best: 26.0042
2023-11-17 23:34:11,895:INFO:  Epoch 258/500:  train Loss: 18.2872   val Loss: 23.0438   time: 427.37s   best: 22.3811
2023-11-17 23:36:18,332:INFO:  Epoch 267/500:  train Loss: 22.9550   val Loss: 26.2249   time: 128.30s   best: 26.0042
2023-11-17 23:38:26,301:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 23:38:26,320:INFO:  Epoch 268/500:  train Loss: 22.4534   val Loss: 25.9838   time: 127.95s   best: 25.9838
2023-11-17 23:40:34,752:INFO:  Epoch 269/500:  train Loss: 22.5992   val Loss: 27.1263   time: 128.43s   best: 25.9838
2023-11-17 23:41:17,732:INFO:  Epoch 259/500:  train Loss: 18.2386   val Loss: 23.0872   time: 425.77s   best: 22.3811
2023-11-17 23:42:42,766:INFO:  Epoch 270/500:  train Loss: 22.8389   val Loss: 26.5033   time: 128.01s   best: 25.9838
2023-11-17 23:44:50,894:INFO:  Epoch 271/500:  train Loss: 22.5916   val Loss: 26.1830   time: 128.12s   best: 25.9838
2023-11-17 23:46:58,891:INFO:  Epoch 272/500:  train Loss: 22.6769   val Loss: 26.5550   time: 128.00s   best: 25.9838
2023-11-17 23:48:22,893:INFO:  Epoch 260/500:  train Loss: 18.0600   val Loss: 22.4614   time: 425.16s   best: 22.3811
2023-11-17 23:49:07,478:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-17 23:49:07,499:INFO:  Epoch 273/500:  train Loss: 22.3843   val Loss: 25.9482   time: 128.58s   best: 25.9482
2023-11-17 23:51:16,801:INFO:  Epoch 274/500:  train Loss: 22.5226   val Loss: 26.7885   time: 129.30s   best: 25.9482
2023-11-17 23:53:26,097:INFO:  Epoch 275/500:  train Loss: 22.3236   val Loss: 26.5779   time: 129.28s   best: 25.9482
2023-11-17 23:55:30,351:INFO:  Epoch 261/500:  train Loss: 18.6638   val Loss: 22.5281   time: 427.44s   best: 22.3811
2023-11-17 23:55:34,985:INFO:  Epoch 276/500:  train Loss: 22.7269   val Loss: 27.7912   time: 128.89s   best: 25.9482
2023-11-17 23:57:43,227:INFO:  Epoch 277/500:  train Loss: 22.2894   val Loss: 26.1430   time: 128.23s   best: 25.9482
2023-11-17 23:59:52,082:INFO:  Epoch 278/500:  train Loss: 22.5544   val Loss: 27.1036   time: 128.84s   best: 25.9482
2023-11-18 00:02:00,522:INFO:  Epoch 279/500:  train Loss: 22.5458   val Loss: 26.4877   time: 128.43s   best: 25.9482
2023-11-18 00:02:35,895:INFO:  Epoch 262/500:  train Loss: 18.3240   val Loss: 22.8864   time: 425.52s   best: 22.3811
2023-11-18 00:04:09,507:INFO:  Epoch 280/500:  train Loss: 22.3878   val Loss: 26.3619   time: 128.98s   best: 25.9482
2023-11-18 00:06:17,820:INFO:  Epoch 281/500:  train Loss: 22.9925   val Loss: 26.6553   time: 128.29s   best: 25.9482
2023-11-18 00:08:26,001:INFO:  Epoch 282/500:  train Loss: 22.5199   val Loss: 26.9258   time: 128.17s   best: 25.9482
2023-11-18 00:09:40,043:INFO:  Epoch 263/500:  train Loss: 18.0774   val Loss: 22.7135   time: 424.15s   best: 22.3811
2023-11-18 00:10:34,206:INFO:  Epoch 283/500:  train Loss: 22.4704   val Loss: 26.5677   time: 128.19s   best: 25.9482
2023-11-18 00:12:43,102:INFO:  Epoch 284/500:  train Loss: 22.4230   val Loss: 26.5280   time: 128.89s   best: 25.9482
2023-11-18 00:14:52,348:INFO:  Epoch 285/500:  train Loss: 22.3954   val Loss: 29.6405   time: 129.24s   best: 25.9482
2023-11-18 00:16:44,541:INFO:  Epoch 264/500:  train Loss: 18.1115   val Loss: 23.6441   time: 424.50s   best: 22.3811
2023-11-18 00:17:00,724:INFO:  Epoch 286/500:  train Loss: 22.7073   val Loss: 26.8694   time: 128.38s   best: 25.9482
2023-11-18 00:19:09,214:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 00:19:09,234:INFO:  Epoch 287/500:  train Loss: 22.8464   val Loss: 25.9127   time: 128.48s   best: 25.9127
2023-11-18 00:21:17,107:INFO:  Epoch 288/500:  train Loss: 22.3091   val Loss: 26.0622   time: 127.86s   best: 25.9127
2023-11-18 00:23:25,639:INFO:  Epoch 289/500:  train Loss: 22.2803   val Loss: 26.0384   time: 128.53s   best: 25.9127
2023-11-18 00:23:52,608:INFO:  Epoch 265/500:  train Loss: 18.4004   val Loss: 28.1887   time: 428.04s   best: 22.3811
2023-11-18 00:25:33,834:INFO:  Epoch 290/500:  train Loss: 23.6980   val Loss: 26.3958   time: 128.19s   best: 25.9127
2023-11-18 00:27:42,431:INFO:  Epoch 291/500:  train Loss: 22.2601   val Loss: 26.7558   time: 128.55s   best: 25.9127
2023-11-18 00:29:50,420:INFO:  Epoch 292/500:  train Loss: 22.3484   val Loss: 25.9617   time: 127.98s   best: 25.9127
2023-11-18 00:31:00,951:INFO:  Epoch 266/500:  train Loss: 18.5217   val Loss: 24.8861   time: 428.33s   best: 22.3811
2023-11-18 00:31:58,914:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 00:31:58,934:INFO:  Epoch 293/500:  train Loss: 22.1063   val Loss: 25.8911   time: 128.48s   best: 25.8911
2023-11-18 00:34:07,240:INFO:  Epoch 294/500:  train Loss: 22.2604   val Loss: 26.1488   time: 128.30s   best: 25.8911
2023-11-18 00:36:16,404:INFO:  Epoch 295/500:  train Loss: 22.4469   val Loss: 26.3151   time: 129.15s   best: 25.8911
2023-11-18 00:38:09,390:INFO:  Epoch 267/500:  train Loss: 18.4134   val Loss: 23.1777   time: 428.43s   best: 22.3811
2023-11-18 00:38:24,871:INFO:  Epoch 296/500:  train Loss: 23.0792   val Loss: 26.0624   time: 128.47s   best: 25.8911
2023-11-18 00:40:34,147:INFO:  Epoch 297/500:  train Loss: 22.1994   val Loss: 25.9089   time: 129.26s   best: 25.8911
2023-11-18 00:42:43,006:INFO:  Epoch 298/500:  train Loss: 22.5107   val Loss: 26.3993   time: 128.85s   best: 25.8911
2023-11-18 00:44:52,080:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 00:44:52,099:INFO:  Epoch 299/500:  train Loss: 22.0049   val Loss: 25.8563   time: 129.06s   best: 25.8563
2023-11-18 00:45:14,278:INFO:  Epoch 268/500:  train Loss: 18.0480   val Loss: 22.6754   time: 424.86s   best: 22.3811
2023-11-18 00:47:00,375:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 00:47:00,395:INFO:  Epoch 300/500:  train Loss: 22.3949   val Loss: 25.6915   time: 128.26s   best: 25.6915
2023-11-18 00:49:09,101:INFO:  Epoch 301/500:  train Loss: 21.9704   val Loss: 26.4058   time: 128.69s   best: 25.6915
2023-11-18 00:51:17,558:INFO:  Epoch 302/500:  train Loss: 23.2353   val Loss: 26.8445   time: 128.46s   best: 25.6915
2023-11-18 00:52:21,357:INFO:  Epoch 269/500:  train Loss: 18.0289   val Loss: 22.9272   time: 427.08s   best: 22.3811
2023-11-18 00:53:26,605:INFO:  Epoch 303/500:  train Loss: 22.1671   val Loss: 27.1388   time: 129.05s   best: 25.6915
2023-11-18 00:55:35,335:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 00:55:35,354:INFO:  Epoch 304/500:  train Loss: 22.0846   val Loss: 25.5667   time: 128.71s   best: 25.5667
2023-11-18 00:57:44,415:INFO:  Epoch 305/500:  train Loss: 22.2755   val Loss: 26.1744   time: 129.06s   best: 25.5667
2023-11-18 00:59:26,544:INFO:  Epoch 270/500:  train Loss: 18.1729   val Loss: 23.2645   time: 425.18s   best: 22.3811
2023-11-18 00:59:52,899:INFO:  Epoch 306/500:  train Loss: 22.1329   val Loss: 26.1553   time: 128.47s   best: 25.5667
2023-11-18 01:02:01,931:INFO:  Epoch 307/500:  train Loss: 22.1735   val Loss: 27.2703   time: 129.01s   best: 25.5667
2023-11-18 01:04:10,884:INFO:  Epoch 308/500:  train Loss: 22.0066   val Loss: 26.6318   time: 128.95s   best: 25.5667
2023-11-18 01:06:18,889:INFO:  Epoch 309/500:  train Loss: 22.2687   val Loss: 26.4456   time: 127.99s   best: 25.5667
2023-11-18 01:06:33,612:INFO:  Epoch 271/500:  train Loss: 17.9691   val Loss: 23.2515   time: 427.07s   best: 22.3811
2023-11-18 01:08:26,620:INFO:  Epoch 310/500:  train Loss: 21.8958   val Loss: 26.7410   time: 127.73s   best: 25.5667
2023-11-18 01:10:34,616:INFO:  Epoch 311/500:  train Loss: 22.5770   val Loss: 26.2249   time: 127.98s   best: 25.5667
2023-11-18 01:12:43,009:INFO:  Epoch 312/500:  train Loss: 22.3967   val Loss: 26.6004   time: 128.39s   best: 25.5667
2023-11-18 01:13:38,087:INFO:  Epoch 272/500:  train Loss: 18.0255   val Loss: 24.5801   time: 424.47s   best: 22.3811
2023-11-18 01:14:51,450:INFO:  Epoch 313/500:  train Loss: 22.9204   val Loss: 26.7912   time: 128.44s   best: 25.5667
2023-11-18 01:16:59,154:INFO:  Epoch 314/500:  train Loss: 21.9405   val Loss: 26.7356   time: 127.70s   best: 25.5667
2023-11-18 01:19:06,623:INFO:  Epoch 315/500:  train Loss: 21.9702   val Loss: 26.6305   time: 127.47s   best: 25.5667
2023-11-18 01:20:41,738:INFO:  Epoch 273/500:  train Loss: 18.0719   val Loss: 23.0139   time: 423.64s   best: 22.3811
2023-11-18 01:21:14,017:INFO:  Epoch 316/500:  train Loss: 21.9670   val Loss: 26.4105   time: 127.39s   best: 25.5667
2023-11-18 01:23:21,625:INFO:  Epoch 317/500:  train Loss: 22.0400   val Loss: 26.5725   time: 127.59s   best: 25.5667
2023-11-18 01:25:29,579:INFO:  Epoch 318/500:  train Loss: 21.7779   val Loss: 26.0823   time: 127.94s   best: 25.5667
2023-11-18 01:27:37,739:INFO:  Epoch 319/500:  train Loss: 21.8376   val Loss: 28.1047   time: 128.15s   best: 25.5667
2023-11-18 01:27:47,347:INFO:  Epoch 274/500:  train Loss: 18.0396   val Loss: 22.6180   time: 425.57s   best: 22.3811
2023-11-18 01:29:45,022:INFO:  Epoch 320/500:  train Loss: 22.0388   val Loss: 26.1318   time: 127.27s   best: 25.5667
2023-11-18 01:31:52,571:INFO:  Epoch 321/500:  train Loss: 21.8923   val Loss: 26.0818   time: 127.51s   best: 25.5667
2023-11-18 01:33:59,793:INFO:  Epoch 322/500:  train Loss: 21.7536   val Loss: 26.2087   time: 127.21s   best: 25.5667
2023-11-18 01:34:56,021:INFO:  Epoch 275/500:  train Loss: 18.1545   val Loss: 23.1770   time: 428.67s   best: 22.3811
2023-11-18 01:36:07,133:INFO:  Epoch 323/500:  train Loss: 21.9634   val Loss: 26.2129   time: 127.33s   best: 25.5667
2023-11-18 01:38:14,326:INFO:  Epoch 324/500:  train Loss: 22.4440   val Loss: 26.4852   time: 127.17s   best: 25.5667
2023-11-18 01:40:21,523:INFO:  Epoch 325/500:  train Loss: 22.2205   val Loss: 26.1410   time: 127.19s   best: 25.5667
2023-11-18 01:42:01,685:INFO:  Epoch 276/500:  train Loss: 18.0563   val Loss: 27.1267   time: 425.66s   best: 22.3811
2023-11-18 01:42:29,959:INFO:  Epoch 326/500:  train Loss: 21.7746   val Loss: 26.5352   time: 128.44s   best: 25.5667
2023-11-18 01:44:37,608:INFO:  Epoch 327/500:  train Loss: 21.8219   val Loss: 26.1471   time: 127.65s   best: 25.5667
2023-11-18 01:46:46,172:INFO:  Epoch 328/500:  train Loss: 21.6429   val Loss: 26.1353   time: 128.55s   best: 25.5667
2023-11-18 01:48:53,998:INFO:  Epoch 329/500:  train Loss: 21.7445   val Loss: 26.4253   time: 127.81s   best: 25.5667
2023-11-18 01:49:08,985:INFO:  Epoch 277/500:  train Loss: 17.9476   val Loss: 22.8228   time: 427.27s   best: 22.3811
2023-11-18 01:51:01,599:INFO:  Epoch 330/500:  train Loss: 21.6559   val Loss: 25.8032   time: 127.60s   best: 25.5667
2023-11-18 01:53:09,121:INFO:  Epoch 331/500:  train Loss: 21.8027   val Loss: 31.2306   time: 127.50s   best: 25.5667
2023-11-18 01:55:16,479:INFO:  Epoch 332/500:  train Loss: 21.9458   val Loss: 26.1346   time: 127.35s   best: 25.5667
2023-11-18 01:56:14,910:INFO:  Epoch 278/500:  train Loss: 18.1375   val Loss: 22.6078   time: 425.91s   best: 22.3811
2023-11-18 01:57:23,580:INFO:  Epoch 333/500:  train Loss: 21.6800   val Loss: 25.9490   time: 127.10s   best: 25.5667
2023-11-18 01:59:30,484:INFO:  Epoch 334/500:  train Loss: 21.7473   val Loss: 26.4422   time: 126.89s   best: 25.5667
2023-11-18 02:01:37,544:INFO:  Epoch 335/500:  train Loss: 21.6139   val Loss: 26.5321   time: 127.06s   best: 25.5667
2023-11-18 02:03:23,025:INFO:  Epoch 279/500:  train Loss: 18.0776   val Loss: 23.3921   time: 428.08s   best: 22.3811
2023-11-18 02:03:44,526:INFO:  Epoch 336/500:  train Loss: 21.5950   val Loss: 26.4219   time: 126.96s   best: 25.5667
2023-11-18 02:05:51,991:INFO:  Epoch 337/500:  train Loss: 21.9993   val Loss: 26.3643   time: 127.46s   best: 25.5667
2023-11-18 02:07:58,989:INFO:  Epoch 338/500:  train Loss: 21.8437   val Loss: 26.6776   time: 126.99s   best: 25.5667
2023-11-18 02:10:06,495:INFO:  Epoch 339/500:  train Loss: 22.3951   val Loss: 30.5390   time: 127.49s   best: 25.5667
2023-11-18 02:10:30,899:INFO:  Epoch 280/500:  train Loss: 18.2628   val Loss: 22.6451   time: 427.85s   best: 22.3811
2023-11-18 02:12:13,775:INFO:  Epoch 340/500:  train Loss: 21.8851   val Loss: 26.1111   time: 127.27s   best: 25.5667
2023-11-18 02:14:21,295:INFO:  Epoch 341/500:  train Loss: 21.7555   val Loss: 26.0393   time: 127.52s   best: 25.5667
2023-11-18 02:16:28,588:INFO:  Epoch 342/500:  train Loss: 21.7817   val Loss: 26.1472   time: 127.28s   best: 25.5667
2023-11-18 02:17:34,772:INFO:  Epoch 281/500:  train Loss: 18.1497   val Loss: 23.1178   time: 423.87s   best: 22.3811
2023-11-18 02:18:36,280:INFO:  Epoch 343/500:  train Loss: 21.6820   val Loss: 27.1620   time: 127.68s   best: 25.5667
2023-11-18 02:20:44,710:INFO:  Epoch 344/500:  train Loss: 21.5755   val Loss: 26.3748   time: 128.42s   best: 25.5667
2023-11-18 02:22:52,834:INFO:  Epoch 345/500:  train Loss: 21.6323   val Loss: 26.1865   time: 128.12s   best: 25.5667
2023-11-18 02:24:43,787:INFO:  Epoch 282/500:  train Loss: 18.0691   val Loss: 23.1438   time: 429.01s   best: 22.3811
2023-11-18 02:25:00,450:INFO:  Epoch 346/500:  train Loss: 21.4787   val Loss: 26.2523   time: 127.60s   best: 25.5667
2023-11-18 02:27:08,482:INFO:  Epoch 347/500:  train Loss: 21.5219   val Loss: 26.0374   time: 128.02s   best: 25.5667
2023-11-18 02:29:16,151:INFO:  Epoch 348/500:  train Loss: 21.5152   val Loss: 26.3481   time: 127.67s   best: 25.5667
2023-11-18 02:31:24,632:INFO:  Epoch 349/500:  train Loss: 21.5403   val Loss: 26.0088   time: 128.48s   best: 25.5667
2023-11-18 02:31:47,589:INFO:  Epoch 283/500:  train Loss: 18.2016   val Loss: 22.8415   time: 423.79s   best: 22.3811
2023-11-18 02:33:32,338:INFO:  Epoch 350/500:  train Loss: 21.5942   val Loss: 26.3018   time: 127.71s   best: 25.5667
2023-11-18 02:35:40,439:INFO:  Epoch 351/500:  train Loss: 21.9135   val Loss: 26.5307   time: 128.09s   best: 25.5667
2023-11-18 02:37:48,495:INFO:  Epoch 352/500:  train Loss: 21.4384   val Loss: 26.5442   time: 128.06s   best: 25.5667
2023-11-18 02:38:56,357:INFO:  Epoch 284/500:  train Loss: 18.0235   val Loss: 22.9737   time: 428.77s   best: 22.3811
2023-11-18 02:39:55,897:INFO:  Epoch 353/500:  train Loss: 21.5059   val Loss: 25.8491   time: 127.39s   best: 25.5667
2023-11-18 02:42:03,228:INFO:  Epoch 354/500:  train Loss: 21.4825   val Loss: 32.7826   time: 127.30s   best: 25.5667
2023-11-18 02:44:10,601:INFO:  Epoch 355/500:  train Loss: 21.4635   val Loss: 25.6346   time: 127.36s   best: 25.5667
2023-11-18 02:46:04,534:INFO:  Epoch 285/500:  train Loss: 18.1048   val Loss: 23.0288   time: 428.16s   best: 22.3811
2023-11-18 02:46:18,527:INFO:  Epoch 356/500:  train Loss: 21.6464   val Loss: 25.8442   time: 127.91s   best: 25.5667
2023-11-18 02:48:26,967:INFO:  Epoch 357/500:  train Loss: 21.3415   val Loss: 25.8780   time: 128.43s   best: 25.5667
2023-11-18 02:50:34,390:INFO:  Epoch 358/500:  train Loss: 21.6250   val Loss: 26.1308   time: 127.41s   best: 25.5667
2023-11-18 02:52:41,828:INFO:  Epoch 359/500:  train Loss: 21.6766   val Loss: 26.3861   time: 127.44s   best: 25.5667
2023-11-18 02:53:11,754:INFO:  Epoch 286/500:  train Loss: 17.9956   val Loss: 22.5470   time: 427.20s   best: 22.3811
2023-11-18 02:54:49,273:INFO:  Epoch 360/500:  train Loss: 21.3219   val Loss: 26.6001   time: 127.44s   best: 25.5667
2023-11-18 02:56:57,645:INFO:  Epoch 361/500:  train Loss: 21.3759   val Loss: 26.3864   time: 128.35s   best: 25.5667
2023-11-18 02:59:06,117:INFO:  Epoch 362/500:  train Loss: 21.3539   val Loss: 26.2183   time: 128.47s   best: 25.5667
2023-11-18 03:00:18,992:INFO:  Epoch 287/500:  train Loss: 17.8883   val Loss: 25.2671   time: 427.20s   best: 22.3811
2023-11-18 03:01:13,916:INFO:  Epoch 363/500:  train Loss: 22.3818   val Loss: 25.9619   time: 127.79s   best: 25.5667
2023-11-18 03:03:22,657:INFO:  Epoch 364/500:  train Loss: 21.3349   val Loss: 25.9659   time: 128.72s   best: 25.5667
2023-11-18 03:05:30,222:INFO:  Epoch 365/500:  train Loss: 21.2827   val Loss: 27.0425   time: 127.56s   best: 25.5667
2023-11-18 03:07:26,716:INFO:  Epoch 288/500:  train Loss: 18.0044   val Loss: 22.8326   time: 427.70s   best: 22.3811
2023-11-18 03:07:38,832:INFO:  Epoch 366/500:  train Loss: 21.6565   val Loss: 26.3826   time: 128.60s   best: 25.5667
2023-11-18 03:09:46,900:INFO:  Epoch 367/500:  train Loss: 21.6604   val Loss: 25.8025   time: 128.04s   best: 25.5667
2023-11-18 03:11:55,523:INFO:  Epoch 368/500:  train Loss: 21.3719   val Loss: 25.7015   time: 128.62s   best: 25.5667
2023-11-18 03:14:04,299:INFO:  Epoch 369/500:  train Loss: 21.2904   val Loss: 25.8965   time: 128.76s   best: 25.5667
2023-11-18 03:14:30,219:INFO:  Epoch 289/500:  train Loss: 17.8360   val Loss: 25.4576   time: 423.48s   best: 22.3811
2023-11-18 03:16:12,218:INFO:  Epoch 370/500:  train Loss: 21.2692   val Loss: 26.7819   time: 127.91s   best: 25.5667
2023-11-18 03:18:19,701:INFO:  Epoch 371/500:  train Loss: 21.2475   val Loss: 26.0121   time: 127.45s   best: 25.5667
2023-11-18 03:20:27,288:INFO:  Epoch 372/500:  train Loss: 21.2584   val Loss: 26.4867   time: 127.59s   best: 25.5667
2023-11-18 03:21:34,532:INFO:  Epoch 290/500:  train Loss: 18.0615   val Loss: 22.8845   time: 424.30s   best: 22.3811
2023-11-18 03:22:34,618:INFO:  Epoch 373/500:  train Loss: 21.1382   val Loss: 25.9010   time: 127.32s   best: 25.5667
2023-11-18 03:24:41,833:INFO:  Epoch 374/500:  train Loss: 21.1419   val Loss: 25.9285   time: 127.19s   best: 25.5667
2023-11-18 03:26:49,566:INFO:  Epoch 375/500:  train Loss: 21.2492   val Loss: 25.8955   time: 127.72s   best: 25.5667
2023-11-18 03:28:40,775:INFO:  Epoch 291/500:  train Loss: 18.1944   val Loss: 22.7075   time: 426.24s   best: 22.3811
2023-11-18 03:28:58,285:INFO:  Epoch 376/500:  train Loss: 21.7527   val Loss: 25.7117   time: 128.71s   best: 25.5667
2023-11-18 03:31:05,586:INFO:  Epoch 377/500:  train Loss: 21.2398   val Loss: 25.8931   time: 127.30s   best: 25.5667
2023-11-18 03:33:12,737:INFO:  Epoch 378/500:  train Loss: 21.2128   val Loss: 26.0735   time: 127.15s   best: 25.5667
2023-11-18 03:35:20,394:INFO:  Epoch 379/500:  train Loss: 22.0268   val Loss: 26.6726   time: 127.64s   best: 25.5667
2023-11-18 03:35:44,409:INFO:  Epoch 292/500:  train Loss: 17.9385   val Loss: 23.1809   time: 423.62s   best: 22.3811
2023-11-18 03:37:28,629:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 03:37:28,659:INFO:  Epoch 380/500:  train Loss: 21.2917   val Loss: 25.5373   time: 128.23s   best: 25.5373
2023-11-18 03:39:36,714:INFO:  Epoch 381/500:  train Loss: 21.1323   val Loss: 25.7494   time: 128.05s   best: 25.5373
2023-11-18 03:41:44,278:INFO:  Epoch 382/500:  train Loss: 21.1648   val Loss: 27.0972   time: 127.55s   best: 25.5373
2023-11-18 03:42:52,687:INFO:  Epoch 293/500:  train Loss: 17.9364   val Loss: 22.9999   time: 428.26s   best: 22.3811
2023-11-18 03:43:52,590:INFO:  Epoch 383/500:  train Loss: 21.1897   val Loss: 25.9225   time: 128.31s   best: 25.5373
2023-11-18 03:46:01,412:INFO:  Epoch 384/500:  train Loss: 21.1108   val Loss: 26.2557   time: 128.78s   best: 25.5373
2023-11-18 03:48:09,254:INFO:  Epoch 385/500:  train Loss: 21.0556   val Loss: 25.8122   time: 127.84s   best: 25.5373
2023-11-18 03:49:57,241:INFO:  Epoch 294/500:  train Loss: 18.1453   val Loss: 23.2689   time: 424.51s   best: 22.3811
2023-11-18 03:50:17,874:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 03:50:18,051:INFO:  Epoch 386/500:  train Loss: 21.0023   val Loss: 25.4020   time: 128.62s   best: 25.4020
2023-11-18 03:52:26,581:INFO:  Epoch 387/500:  train Loss: 21.0857   val Loss: 25.9094   time: 128.52s   best: 25.4020
2023-11-18 03:54:35,023:INFO:  Epoch 388/500:  train Loss: 21.0409   val Loss: 25.6962   time: 128.43s   best: 25.4020
2023-11-18 03:56:43,649:INFO:  Epoch 389/500:  train Loss: 21.2017   val Loss: 28.0523   time: 128.61s   best: 25.4020
2023-11-18 03:57:05,682:INFO:  Epoch 295/500:  train Loss: 18.1248   val Loss: 23.3617   time: 428.39s   best: 22.3811
2023-11-18 03:58:52,590:INFO:  Epoch 390/500:  train Loss: 21.2775   val Loss: 25.7097   time: 128.94s   best: 25.4020
2023-11-18 04:01:00,603:INFO:  Epoch 391/500:  train Loss: 21.0315   val Loss: 26.0706   time: 128.00s   best: 25.4020
2023-11-18 04:03:08,778:INFO:  Epoch 392/500:  train Loss: 21.1979   val Loss: 26.3672   time: 128.16s   best: 25.4020
2023-11-18 04:04:10,281:INFO:  Epoch 296/500:  train Loss: 17.9801   val Loss: 22.7283   time: 424.58s   best: 22.3811
2023-11-18 04:05:17,281:INFO:  Epoch 393/500:  train Loss: 21.2133   val Loss: 25.6553   time: 128.49s   best: 25.4020
2023-11-18 04:07:25,465:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 04:07:25,484:INFO:  Epoch 394/500:  train Loss: 20.9418   val Loss: 25.3436   time: 128.17s   best: 25.3436
2023-11-18 04:09:33,245:INFO:  Epoch 395/500:  train Loss: 21.9021   val Loss: 25.7862   time: 127.75s   best: 25.3436
2023-11-18 04:11:19,237:INFO:  Epoch 297/500:  train Loss: 18.1025   val Loss: 23.4688   time: 428.93s   best: 22.3811
2023-11-18 04:11:40,822:INFO:  Epoch 396/500:  train Loss: 20.9927   val Loss: 26.4361   time: 127.57s   best: 25.3436
2023-11-18 04:13:48,397:INFO:  Epoch 397/500:  train Loss: 20.9830   val Loss: 25.8613   time: 127.55s   best: 25.3436
2023-11-18 04:15:56,070:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 04:15:56,090:INFO:  Epoch 398/500:  train Loss: 21.2606   val Loss: 25.0899   time: 127.66s   best: 25.0899
2023-11-18 04:18:03,458:INFO:  Epoch 399/500:  train Loss: 21.7805   val Loss: 26.6544   time: 127.36s   best: 25.0899
2023-11-18 04:18:26,699:INFO:  Epoch 298/500:  train Loss: 17.8103   val Loss: 23.4837   time: 427.45s   best: 22.3811
2023-11-18 04:20:11,010:INFO:  Epoch 400/500:  train Loss: 21.3472   val Loss: 25.9469   time: 127.54s   best: 25.0899
2023-11-18 04:22:18,810:INFO:  Epoch 401/500:  train Loss: 20.9374   val Loss: 25.5921   time: 127.78s   best: 25.0899
2023-11-18 04:24:26,610:INFO:  Epoch 402/500:  train Loss: 21.1720   val Loss: 25.7143   time: 127.79s   best: 25.0899
2023-11-18 04:25:30,363:INFO:  Epoch 299/500:  train Loss: 17.9455   val Loss: 22.5394   time: 423.66s   best: 22.3811
2023-11-18 04:26:34,393:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 04:26:34,415:INFO:  Epoch 403/500:  train Loss: 21.0244   val Loss: 25.0654   time: 127.78s   best: 25.0654
2023-11-18 04:28:42,679:INFO:  Epoch 404/500:  train Loss: 20.9133   val Loss: 25.5039   time: 128.26s   best: 25.0654
2023-11-18 04:30:51,068:INFO:  Epoch 405/500:  train Loss: 20.8803   val Loss: 25.5989   time: 128.39s   best: 25.0654
2023-11-18 04:32:38,086:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-18 04:32:38,106:INFO:  Epoch 300/500:  train Loss: 17.8856   val Loss: 22.2111   time: 427.68s   best: 22.2111
2023-11-18 04:32:58,541:INFO:  Epoch 406/500:  train Loss: 21.3305   val Loss: 26.4160   time: 127.46s   best: 25.0654
2023-11-18 04:35:06,241:INFO:  Epoch 407/500:  train Loss: 21.0910   val Loss: 25.7601   time: 127.69s   best: 25.0654
2023-11-18 04:37:14,400:INFO:  Epoch 408/500:  train Loss: 21.3338   val Loss: 25.8385   time: 128.16s   best: 25.0654
2023-11-18 04:39:23,240:INFO:  Epoch 409/500:  train Loss: 20.9271   val Loss: 28.3333   time: 128.84s   best: 25.0654
2023-11-18 04:39:42,291:INFO:  Epoch 301/500:  train Loss: 17.9743   val Loss: 23.1875   time: 424.17s   best: 22.2111
2023-11-18 04:41:31,251:INFO:  Epoch 410/500:  train Loss: 21.5413   val Loss: 25.9110   time: 128.00s   best: 25.0654
2023-11-18 04:43:38,855:INFO:  Epoch 411/500:  train Loss: 21.5756   val Loss: 27.1289   time: 127.59s   best: 25.0654
2023-11-18 04:45:46,230:INFO:  Epoch 412/500:  train Loss: 21.7679   val Loss: 25.2450   time: 127.37s   best: 25.0654
2023-11-18 04:46:51,771:INFO:  Epoch 302/500:  train Loss: 18.0209   val Loss: 22.7207   time: 429.45s   best: 22.2111
2023-11-18 04:47:53,695:INFO:  Epoch 413/500:  train Loss: 20.7948   val Loss: 25.8289   time: 127.45s   best: 25.0654
2023-11-18 04:50:01,603:INFO:  Epoch 414/500:  train Loss: 21.1857   val Loss: 25.9379   time: 127.88s   best: 25.0654
2023-11-18 04:52:09,070:INFO:  Epoch 415/500:  train Loss: 21.4381   val Loss: 25.5640   time: 127.45s   best: 25.0654
2023-11-18 04:53:55,272:INFO:  Epoch 303/500:  train Loss: 17.8521   val Loss: 23.0910   time: 423.50s   best: 22.2111
2023-11-18 04:54:16,896:INFO:  Epoch 416/500:  train Loss: 20.8448   val Loss: 25.5090   time: 127.83s   best: 25.0654
2023-11-18 04:56:24,840:INFO:  Epoch 417/500:  train Loss: 20.9484   val Loss: 26.0498   time: 127.94s   best: 25.0654
2023-11-18 04:58:32,125:INFO:  Epoch 418/500:  train Loss: 21.6269   val Loss: 25.7695   time: 127.27s   best: 25.0654
2023-11-18 05:00:39,659:INFO:  Epoch 419/500:  train Loss: 20.9338   val Loss: 25.5888   time: 127.52s   best: 25.0654
2023-11-18 05:01:00,190:INFO:  Epoch 304/500:  train Loss: 17.8691   val Loss: 22.9476   time: 424.89s   best: 22.2111
2023-11-18 05:02:47,385:INFO:  Epoch 420/500:  train Loss: 20.8790   val Loss: 25.9800   time: 127.71s   best: 25.0654
2023-11-18 05:04:55,110:INFO:  Epoch 421/500:  train Loss: 20.8011   val Loss: 26.4943   time: 127.69s   best: 25.0654
2023-11-18 05:07:03,186:INFO:  Epoch 422/500:  train Loss: 21.0540   val Loss: 25.7680   time: 128.07s   best: 25.0654
2023-11-18 05:08:04,447:INFO:  Epoch 305/500:  train Loss: 17.8055   val Loss: 23.5375   time: 424.26s   best: 22.2111
2023-11-18 05:09:10,906:INFO:  Epoch 423/500:  train Loss: 21.3109   val Loss: 26.3501   time: 127.71s   best: 25.0654
2023-11-18 05:11:18,631:INFO:  Epoch 424/500:  train Loss: 20.8483   val Loss: 26.1049   time: 127.71s   best: 25.0654
2023-11-18 05:13:26,766:INFO:  Epoch 425/500:  train Loss: 20.8255   val Loss: 26.0330   time: 128.12s   best: 25.0654
2023-11-18 05:15:13,914:INFO:  Epoch 306/500:  train Loss: 17.8080   val Loss: 28.6953   time: 429.46s   best: 22.2111
2023-11-18 05:15:35,045:INFO:  Epoch 426/500:  train Loss: 20.6957   val Loss: 25.9001   time: 128.28s   best: 25.0654
2023-11-18 05:17:43,039:INFO:  Epoch 427/500:  train Loss: 20.7594   val Loss: 25.3856   time: 127.98s   best: 25.0654
2023-11-18 05:19:51,305:INFO:  Epoch 428/500:  train Loss: 22.5604   val Loss: 25.8419   time: 128.26s   best: 25.0654
2023-11-18 05:21:59,124:INFO:  Epoch 429/500:  train Loss: 21.2345   val Loss: 26.1304   time: 127.81s   best: 25.0654
2023-11-18 05:22:17,960:INFO:  Epoch 307/500:  train Loss: 18.1711   val Loss: 22.9410   time: 424.04s   best: 22.2111
2023-11-18 05:24:07,079:INFO:  Epoch 430/500:  train Loss: 21.1279   val Loss: 25.2837   time: 127.95s   best: 25.0654
2023-11-18 05:26:14,751:INFO:  Epoch 431/500:  train Loss: 20.7691   val Loss: 25.4750   time: 127.66s   best: 25.0654
2023-11-18 05:28:22,921:INFO:  Epoch 432/500:  train Loss: 20.7653   val Loss: 25.2503   time: 128.17s   best: 25.0654
2023-11-18 05:29:26,046:INFO:  Epoch 308/500:  train Loss: 17.8704   val Loss: 23.5170   time: 428.07s   best: 22.2111
2023-11-18 05:30:31,227:INFO:  Epoch 433/500:  train Loss: 20.6707   val Loss: 26.4388   time: 128.30s   best: 25.0654
2023-11-18 05:32:38,661:INFO:  Epoch 434/500:  train Loss: 21.2574   val Loss: 25.4376   time: 127.43s   best: 25.0654
2023-11-18 05:34:46,313:INFO:  Epoch 435/500:  train Loss: 20.7395   val Loss: 25.6319   time: 127.65s   best: 25.0654
2023-11-18 05:36:29,788:INFO:  Epoch 309/500:  train Loss: 17.7692   val Loss: 23.1342   time: 423.73s   best: 22.2111
2023-11-18 05:36:54,167:INFO:  Epoch 436/500:  train Loss: 20.9422   val Loss: 25.9387   time: 127.84s   best: 25.0654
2023-11-18 05:39:01,864:INFO:  Epoch 437/500:  train Loss: 20.7409   val Loss: 26.2116   time: 127.68s   best: 25.0654
2023-11-18 05:41:09,028:INFO:  Epoch 438/500:  train Loss: 20.6980   val Loss: 25.6115   time: 127.15s   best: 25.0654
2023-11-18 05:43:16,751:INFO:  Epoch 439/500:  train Loss: 20.8178   val Loss: 26.4977   time: 127.71s   best: 25.0654
2023-11-18 05:43:34,041:INFO:  Epoch 310/500:  train Loss: 17.9716   val Loss: 24.0314   time: 424.24s   best: 22.2111
2023-11-18 05:45:24,950:INFO:  Epoch 440/500:  train Loss: 20.6590   val Loss: 26.1719   time: 128.19s   best: 25.0654
2023-11-18 05:47:32,703:INFO:  Epoch 441/500:  train Loss: 21.0309   val Loss: 25.7940   time: 127.72s   best: 25.0654
2023-11-18 05:49:40,719:INFO:  Epoch 442/500:  train Loss: 20.6303   val Loss: 25.4236   time: 128.02s   best: 25.0654
2023-11-18 05:50:38,610:INFO:  Epoch 311/500:  train Loss: 17.7208   val Loss: 27.7834   time: 424.54s   best: 22.2111
2023-11-18 05:51:48,410:INFO:  Epoch 443/500:  train Loss: 20.7175   val Loss: 25.5495   time: 127.69s   best: 25.0654
2023-11-18 05:53:56,418:INFO:  Epoch 444/500:  train Loss: 20.6590   val Loss: 25.4715   time: 127.98s   best: 25.0654
2023-11-18 05:56:03,949:INFO:  Epoch 445/500:  train Loss: 20.6951   val Loss: 25.9580   time: 127.53s   best: 25.0654
2023-11-18 05:57:47,189:INFO:  Epoch 312/500:  train Loss: 18.2360   val Loss: 23.0364   time: 428.57s   best: 22.2111
2023-11-18 05:58:11,542:INFO:  Epoch 446/500:  train Loss: 21.1799   val Loss: 25.9374   time: 127.59s   best: 25.0654
2023-11-18 06:00:19,070:INFO:  Epoch 447/500:  train Loss: 20.5716   val Loss: 25.7965   time: 127.51s   best: 25.0654
2023-11-18 06:02:26,437:INFO:  Epoch 448/500:  train Loss: 20.8184   val Loss: 25.6494   time: 127.37s   best: 25.0654
2023-11-18 06:04:34,036:INFO:  Epoch 449/500:  train Loss: 20.5661   val Loss: 26.2994   time: 127.60s   best: 25.0654
2023-11-18 06:04:51,822:INFO:  Epoch 313/500:  train Loss: 17.8472   val Loss: 23.4611   time: 424.62s   best: 22.2111
2023-11-18 06:06:41,450:INFO:  Epoch 450/500:  train Loss: 20.6607   val Loss: 25.7976   time: 127.41s   best: 25.0654
2023-11-18 06:08:50,034:INFO:  Epoch 451/500:  train Loss: 20.6404   val Loss: 25.1256   time: 128.55s   best: 25.0654
2023-11-18 06:10:57,851:INFO:  Epoch 452/500:  train Loss: 20.5683   val Loss: 25.7169   time: 127.82s   best: 25.0654
2023-11-18 06:11:59,861:INFO:  Epoch 314/500:  train Loss: 17.8254   val Loss: 22.8029   time: 428.04s   best: 22.2111
2023-11-18 06:13:05,691:INFO:  Epoch 453/500:  train Loss: 20.5968   val Loss: 25.1670   time: 127.83s   best: 25.0654
2023-11-18 06:15:13,948:INFO:  Epoch 454/500:  train Loss: 20.4805   val Loss: 25.4229   time: 128.25s   best: 25.0654
2023-11-18 06:17:21,535:INFO:  Epoch 455/500:  train Loss: 20.4721   val Loss: 25.1706   time: 127.57s   best: 25.0654
2023-11-18 06:19:07,377:INFO:  Epoch 315/500:  train Loss: 18.8757   val Loss: 25.5729   time: 427.49s   best: 22.2111
2023-11-18 06:19:28,983:INFO:  Epoch 456/500:  train Loss: 20.5101   val Loss: 26.4862   time: 127.44s   best: 25.0654
2023-11-18 06:21:37,844:INFO:  Epoch 457/500:  train Loss: 20.6940   val Loss: 25.4853   time: 128.81s   best: 25.0654
2023-11-18 06:23:45,698:INFO:  Epoch 458/500:  train Loss: 20.6478   val Loss: 25.9305   time: 127.84s   best: 25.0654
2023-11-18 06:25:53,596:INFO:  Epoch 459/500:  train Loss: 20.5278   val Loss: 26.0061   time: 127.89s   best: 25.0654
2023-11-18 06:26:11,512:INFO:  Epoch 316/500:  train Loss: 18.1237   val Loss: 22.5217   time: 424.13s   best: 22.2111
2023-11-18 06:28:01,216:INFO:  Epoch 460/500:  train Loss: 20.5695   val Loss: 27.7259   time: 127.62s   best: 25.0654
2023-11-18 06:30:09,443:INFO:  Epoch 461/500:  train Loss: 20.8596   val Loss: 25.3454   time: 128.21s   best: 25.0654
2023-11-18 06:32:17,173:INFO:  Epoch 462/500:  train Loss: 20.4888   val Loss: 26.2501   time: 127.72s   best: 25.0654
2023-11-18 06:33:18,829:INFO:  Epoch 317/500:  train Loss: 17.7853   val Loss: 23.0465   time: 427.32s   best: 22.2111
2023-11-18 06:34:24,508:INFO:  Epoch 463/500:  train Loss: 20.4946   val Loss: 25.7462   time: 127.33s   best: 25.0654
2023-11-18 06:36:31,989:INFO:  Epoch 464/500:  train Loss: 20.6498   val Loss: 25.7631   time: 127.47s   best: 25.0654
2023-11-18 06:38:40,086:INFO:  Epoch 465/500:  train Loss: 20.4673   val Loss: 25.4233   time: 128.10s   best: 25.0654
2023-11-18 06:40:23,684:INFO:  Epoch 318/500:  train Loss: 17.8815   val Loss: 24.5042   time: 424.84s   best: 22.2111
2023-11-18 06:40:47,863:INFO:  Epoch 466/500:  train Loss: 21.5423   val Loss: 25.6965   time: 127.78s   best: 25.0654
2023-11-18 06:42:56,611:INFO:  Epoch 467/500:  train Loss: 20.3270   val Loss: 25.5225   time: 128.74s   best: 25.0654
2023-11-18 06:45:04,335:INFO:  Epoch 468/500:  train Loss: 20.3821   val Loss: 25.4230   time: 127.71s   best: 25.0654
2023-11-18 06:47:12,955:INFO:  Epoch 469/500:  train Loss: 20.5726   val Loss: 25.3416   time: 128.61s   best: 25.0654
2023-11-18 06:47:32,493:INFO:  Epoch 319/500:  train Loss: 17.8568   val Loss: 22.6295   time: 428.79s   best: 22.2111
2023-11-18 06:49:21,710:INFO:  Epoch 470/500:  train Loss: 20.5825   val Loss: 25.1087   time: 128.75s   best: 25.0654
2023-11-18 06:51:29,700:INFO:  Epoch 471/500:  train Loss: 20.3543   val Loss: 25.9088   time: 127.95s   best: 25.0654
2023-11-18 06:53:37,494:INFO:  Epoch 472/500:  train Loss: 20.5159   val Loss: 25.2087   time: 127.78s   best: 25.0654
2023-11-18 06:54:41,102:INFO:  Epoch 320/500:  train Loss: 18.0230   val Loss: 23.1481   time: 428.60s   best: 22.2111
2023-11-18 06:55:45,702:INFO:  Epoch 473/500:  train Loss: 20.3126   val Loss: 25.9004   time: 128.20s   best: 25.0654
2023-11-18 06:57:53,260:INFO:  Epoch 474/500:  train Loss: 20.4235   val Loss: 25.6045   time: 127.56s   best: 25.0654
2023-11-18 07:00:01,406:INFO:  Epoch 475/500:  train Loss: 20.4225   val Loss: 26.6585   time: 128.13s   best: 25.0654
2023-11-18 07:01:48,972:INFO:  Epoch 321/500:  train Loss: 17.8817   val Loss: 22.6374   time: 427.87s   best: 22.2111
2023-11-18 07:02:08,793:INFO:  Epoch 476/500:  train Loss: 20.4304   val Loss: 26.3408   time: 127.38s   best: 25.0654
2023-11-18 07:04:16,231:INFO:  Epoch 477/500:  train Loss: 20.5005   val Loss: 25.6168   time: 127.44s   best: 25.0654
2023-11-18 07:06:24,075:INFO:  Epoch 478/500:  train Loss: 20.4214   val Loss: 25.6825   time: 127.83s   best: 25.0654
2023-11-18 07:08:31,494:INFO:  Epoch 479/500:  train Loss: 20.4713   val Loss: 25.7510   time: 127.42s   best: 25.0654
2023-11-18 07:08:57,626:INFO:  Epoch 322/500:  train Loss: 17.8653   val Loss: 22.3764   time: 428.62s   best: 22.2111
2023-11-18 07:10:38,886:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_f0e7.pt
2023-11-18 07:10:38,917:INFO:  Epoch 480/500:  train Loss: 20.4200   val Loss: 24.9695   time: 127.38s   best: 24.9695
2023-11-18 07:12:46,753:INFO:  Epoch 481/500:  train Loss: 20.4315   val Loss: 25.9580   time: 127.82s   best: 24.9695
2023-11-18 07:14:54,240:INFO:  Epoch 482/500:  train Loss: 20.4404   val Loss: 25.3616   time: 127.48s   best: 24.9695
2023-11-18 07:16:01,829:INFO:  Epoch 323/500:  train Loss: 17.6975   val Loss: 22.4591   time: 424.19s   best: 22.2111
2023-11-18 07:17:02,426:INFO:  Epoch 483/500:  train Loss: 20.7263   val Loss: 27.6783   time: 128.18s   best: 24.9695
2023-11-18 07:19:09,984:INFO:  Epoch 484/500:  train Loss: 21.3511   val Loss: 25.3961   time: 127.55s   best: 24.9695
2023-11-18 07:21:17,611:INFO:  Epoch 485/500:  train Loss: 20.2626   val Loss: 26.1220   time: 127.62s   best: 24.9695
2023-11-18 07:23:07,461:INFO:  Epoch 324/500:  train Loss: 17.9464   val Loss: 23.0777   time: 425.61s   best: 22.2111
2023-11-18 07:23:25,051:INFO:  Epoch 486/500:  train Loss: 20.5847   val Loss: 25.3502   time: 127.44s   best: 24.9695
2023-11-18 07:25:33,543:INFO:  Epoch 487/500:  train Loss: 20.4856   val Loss: 25.1597   time: 128.48s   best: 24.9695
2023-11-18 07:27:41,264:INFO:  Epoch 488/500:  train Loss: 20.5249   val Loss: 30.3588   time: 127.71s   best: 24.9695
2023-11-18 07:29:49,718:INFO:  Epoch 489/500:  train Loss: 20.2900   val Loss: 25.6275   time: 128.44s   best: 24.9695
2023-11-18 07:30:12,456:INFO:  Epoch 325/500:  train Loss: 17.8782   val Loss: 22.9304   time: 424.98s   best: 22.2111
2023-11-18 07:31:57,693:INFO:  Epoch 490/500:  train Loss: 20.2923   val Loss: 25.1534   time: 127.97s   best: 24.9695
2023-11-18 07:34:05,203:INFO:  Epoch 491/500:  train Loss: 20.1951   val Loss: 25.2151   time: 127.49s   best: 24.9695
2023-11-18 07:36:12,562:INFO:  Epoch 492/500:  train Loss: 20.2869   val Loss: 25.1900   time: 127.36s   best: 24.9695
2023-11-18 07:37:17,587:INFO:  Epoch 326/500:  train Loss: 17.9147   val Loss: 22.5407   time: 425.12s   best: 22.2111
2023-11-18 07:38:20,090:INFO:  Epoch 493/500:  train Loss: 20.2294   val Loss: 25.0522   time: 127.53s   best: 24.9695
2023-11-18 07:40:28,304:INFO:  Epoch 494/500:  train Loss: 20.3346   val Loss: 25.5075   time: 128.20s   best: 24.9695
2023-11-18 07:42:36,399:INFO:  Epoch 495/500:  train Loss: 20.2717   val Loss: 25.7550   time: 128.09s   best: 24.9695
2023-11-18 07:44:22,365:INFO:  Epoch 327/500:  train Loss: 17.9557   val Loss: 22.4924   time: 424.78s   best: 22.2111
2023-11-18 07:44:44,476:INFO:  Epoch 496/500:  train Loss: 20.4445   val Loss: 24.9771   time: 128.08s   best: 24.9695
2023-11-18 07:46:52,725:INFO:  Epoch 497/500:  train Loss: 20.2599   val Loss: 25.0637   time: 128.25s   best: 24.9695
2023-11-18 07:49:00,126:INFO:  Epoch 498/500:  train Loss: 20.7054   val Loss: 25.0095   time: 127.39s   best: 24.9695
2023-11-18 07:51:08,583:INFO:  Epoch 499/500:  train Loss: 20.2952   val Loss: 25.5473   time: 128.46s   best: 24.9695
2023-11-18 07:51:30,281:INFO:  Epoch 328/500:  train Loss: 17.8763   val Loss: 22.7896   time: 427.90s   best: 22.2111
2023-11-18 07:53:16,344:INFO:  Epoch 500/500:  train Loss: 20.2081   val Loss: 25.4605   time: 127.76s   best: 24.9695
2023-11-18 07:53:16,370:INFO:  -----> Training complete in 1068m 34s   best validation loss: 24.9695
 
2023-11-18 07:58:36,550:INFO:  Epoch 329/500:  train Loss: 17.7779   val Loss: 22.7478   time: 426.27s   best: 22.2111
2023-11-18 08:05:42,097:INFO:  Epoch 330/500:  train Loss: 17.6550   val Loss: 23.8158   time: 425.52s   best: 22.2111
2023-11-18 08:12:50,650:INFO:  Epoch 331/500:  train Loss: 17.8085   val Loss: 22.5254   time: 428.54s   best: 22.2111
2023-11-18 08:19:55,974:INFO:  Epoch 332/500:  train Loss: 17.6506   val Loss: 22.4728   time: 425.31s   best: 22.2111
2023-11-18 08:27:05,658:INFO:  Epoch 333/500:  train Loss: 17.7242   val Loss: 22.3583   time: 429.66s   best: 22.2111
2023-11-18 08:34:11,912:INFO:  Epoch 334/500:  train Loss: 17.6956   val Loss: 23.4386   time: 426.24s   best: 22.2111
2023-11-18 08:41:18,440:INFO:  Epoch 335/500:  train Loss: 17.7244   val Loss: 22.7341   time: 426.50s   best: 22.2111
2023-11-18 08:48:23,626:INFO:  Epoch 336/500:  train Loss: 17.6161   val Loss: 26.3795   time: 425.16s   best: 22.2111
2023-11-18 08:55:31,400:INFO:  Epoch 337/500:  train Loss: 17.7366   val Loss: 23.2218   time: 427.75s   best: 22.2111
2023-11-18 09:02:39,152:INFO:  Epoch 338/500:  train Loss: 17.6276   val Loss: 22.6638   time: 427.73s   best: 22.2111
2023-11-18 09:09:45,074:INFO:  Epoch 339/500:  train Loss: 17.6653   val Loss: 22.7722   time: 425.90s   best: 22.2111
2023-11-18 09:16:54,029:INFO:  Epoch 340/500:  train Loss: 17.8691   val Loss: 23.3629   time: 428.95s   best: 22.2111
2023-11-18 09:23:59,528:INFO:  Epoch 341/500:  train Loss: 17.9231   val Loss: 23.9891   time: 425.11s   best: 22.2111
2023-11-18 09:31:05,990:INFO:  Epoch 342/500:  train Loss: 17.9062   val Loss: 22.9364   time: 426.45s   best: 22.2111
2023-11-18 09:38:13,662:INFO:  Epoch 343/500:  train Loss: 17.5465   val Loss: 23.0483   time: 427.66s   best: 22.2111
2023-11-18 09:45:18,468:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-18 09:45:18,527:INFO:  Epoch 344/500:  train Loss: 17.6865   val Loss: 22.1769   time: 424.78s   best: 22.1769
2023-11-18 09:52:27,450:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-18 09:52:27,489:INFO:  Epoch 345/500:  train Loss: 17.9400   val Loss: 22.0564   time: 428.91s   best: 22.0564
2023-11-18 09:59:35,440:INFO:  Epoch 346/500:  train Loss: 17.7229   val Loss: 23.2944   time: 427.94s   best: 22.0564
2023-11-18 10:06:39,874:INFO:  Epoch 347/500:  train Loss: 17.6203   val Loss: 22.8744   time: 424.41s   best: 22.0564
2023-11-18 10:13:47,270:INFO:  Epoch 348/500:  train Loss: 17.8977   val Loss: 23.8588   time: 427.38s   best: 22.0564
2023-11-18 10:20:55,971:INFO:  Epoch 349/500:  train Loss: 17.5456   val Loss: 22.6562   time: 428.69s   best: 22.0564
2023-11-18 10:28:05,469:INFO:  Epoch 350/500:  train Loss: 17.6873   val Loss: 23.0984   time: 429.47s   best: 22.0564
2023-11-18 10:31:44,509:INFO:  Starting experiment lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)
2023-11-18 10:31:44,539:INFO:  Defining the model
2023-11-18 10:31:44,641:INFO:  Reading the dataset
2023-11-18 10:35:10,253:INFO:  Epoch 351/500:  train Loss: 17.8827   val Loss: 23.6915   time: 424.77s   best: 22.0564
2023-11-18 10:42:18,399:INFO:  Epoch 352/500:  train Loss: 17.7048   val Loss: 22.5204   time: 428.12s   best: 22.0564
2023-11-18 10:49:27,331:INFO:  Epoch 353/500:  train Loss: 17.7421   val Loss: 22.5648   time: 428.92s   best: 22.0564
2023-11-18 10:56:35,675:INFO:  Epoch 354/500:  train Loss: 17.6831   val Loss: 22.4169   time: 428.32s   best: 22.0564
2023-11-18 10:57:27,883:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 10:57:27,913:INFO:  Epoch 1/500:  train Loss: 87.8607   val Loss: 84.4206   time: 129.77s   best: 84.4206
2023-11-18 10:59:36,054:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 10:59:36,100:INFO:  Epoch 2/500:  train Loss: 79.8381   val Loss: 76.7167   time: 128.12s   best: 76.7167
2023-11-18 11:01:44,480:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:01:44,499:INFO:  Epoch 3/500:  train Loss: 74.5622   val Loss: 75.1686   time: 128.37s   best: 75.1686
2023-11-18 11:03:40,193:INFO:  Epoch 355/500:  train Loss: 17.6526   val Loss: 23.5476   time: 424.50s   best: 22.0564
2023-11-18 11:03:52,762:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:03:52,791:INFO:  Epoch 4/500:  train Loss: 72.5482   val Loss: 73.5001   time: 128.25s   best: 73.5001
2023-11-18 11:06:01,942:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:06:01,962:INFO:  Epoch 5/500:  train Loss: 69.7988   val Loss: 68.5442   time: 129.14s   best: 68.5442
2023-11-18 11:08:10,893:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:08:10,912:INFO:  Epoch 6/500:  train Loss: 67.9908   val Loss: 67.9195   time: 128.92s   best: 67.9195
2023-11-18 11:10:20,656:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:10:20,675:INFO:  Epoch 7/500:  train Loss: 66.4995   val Loss: 65.7409   time: 129.73s   best: 65.7409
2023-11-18 11:10:49,142:INFO:  Epoch 356/500:  train Loss: 17.5199   val Loss: 23.7945   time: 428.92s   best: 22.0564
2023-11-18 11:12:29,697:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:12:29,730:INFO:  Epoch 8/500:  train Loss: 65.0756   val Loss: 63.8179   time: 129.01s   best: 63.8179
2023-11-18 11:14:39,091:INFO:  Epoch 9/500:  train Loss: 63.7433   val Loss: 64.1545   time: 129.36s   best: 63.8179
2023-11-18 11:16:47,688:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:16:47,873:INFO:  Epoch 10/500:  train Loss: 62.5425   val Loss: 62.4810   time: 128.59s   best: 62.4810
2023-11-18 11:17:56,850:INFO:  Epoch 357/500:  train Loss: 17.5364   val Loss: 25.8304   time: 427.67s   best: 22.0564
2023-11-18 11:18:56,611:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:18:56,640:INFO:  Epoch 11/500:  train Loss: 61.6607   val Loss: 61.2321   time: 128.73s   best: 61.2321
2023-11-18 11:21:05,565:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:21:05,883:INFO:  Epoch 12/500:  train Loss: 60.5518   val Loss: 60.7114   time: 128.39s   best: 60.7114
2023-11-18 11:23:15,596:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:23:15,615:INFO:  Epoch 13/500:  train Loss: 59.6110   val Loss: 59.2918   time: 129.71s   best: 59.2918
2023-11-18 11:25:01,870:INFO:  Epoch 358/500:  train Loss: 17.6434   val Loss: 22.8785   time: 425.01s   best: 22.0564
2023-11-18 11:25:24,144:INFO:  Epoch 14/500:  train Loss: 58.3838   val Loss: 59.4245   time: 128.52s   best: 59.2918
2023-11-18 11:27:33,214:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:27:33,233:INFO:  Epoch 15/500:  train Loss: 57.3995   val Loss: 56.8902   time: 129.04s   best: 56.8902
2023-11-18 11:29:42,368:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:29:42,387:INFO:  Epoch 16/500:  train Loss: 55.9632   val Loss: 55.0860   time: 129.12s   best: 55.0860
2023-11-18 11:31:51,326:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:31:51,711:INFO:  Epoch 17/500:  train Loss: 54.8316   val Loss: 54.7616   time: 128.92s   best: 54.7616
2023-11-18 11:32:08,997:INFO:  Epoch 359/500:  train Loss: 17.6892   val Loss: 23.5410   time: 427.10s   best: 22.0564
2023-11-18 11:34:00,230:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:34:00,264:INFO:  Epoch 18/500:  train Loss: 53.6626   val Loss: 53.9243   time: 128.51s   best: 53.9243
2023-11-18 11:36:08,773:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:36:08,792:INFO:  Epoch 19/500:  train Loss: 52.5774   val Loss: 51.8533   time: 128.50s   best: 51.8533
2023-11-18 11:38:17,784:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:38:17,817:INFO:  Epoch 20/500:  train Loss: 51.6607   val Loss: 50.6688   time: 128.99s   best: 50.6688
2023-11-18 11:39:16,438:INFO:  Epoch 360/500:  train Loss: 17.7109   val Loss: 22.7138   time: 427.43s   best: 22.0564
2023-11-18 11:40:27,022:INFO:  Epoch 21/500:  train Loss: 50.6241   val Loss: 50.7939   time: 129.20s   best: 50.6688
2023-11-18 11:42:36,103:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:42:36,123:INFO:  Epoch 22/500:  train Loss: 49.6063   val Loss: 49.8964   time: 129.07s   best: 49.8964
2023-11-18 11:44:44,407:INFO:  Epoch 23/500:  train Loss: 49.0184   val Loss: 50.1094   time: 128.28s   best: 49.8964
2023-11-18 11:46:23,043:INFO:  Epoch 361/500:  train Loss: 18.1891   val Loss: 23.0483   time: 426.59s   best: 22.0564
2023-11-18 11:46:53,096:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:46:53,125:INFO:  Epoch 24/500:  train Loss: 47.9514   val Loss: 49.1063   time: 128.68s   best: 49.1063
2023-11-18 11:49:01,781:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:49:01,800:INFO:  Epoch 25/500:  train Loss: 47.2385   val Loss: 47.6630   time: 128.62s   best: 47.6630
2023-11-18 11:51:10,926:INFO:  Epoch 26/500:  train Loss: 46.5256   val Loss: 48.7725   time: 129.11s   best: 47.6630
2023-11-18 11:53:19,874:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:53:19,894:INFO:  Epoch 27/500:  train Loss: 46.0461   val Loss: 46.1506   time: 128.94s   best: 46.1506
2023-11-18 11:53:29,181:INFO:  Epoch 362/500:  train Loss: 17.5865   val Loss: 22.9492   time: 426.13s   best: 22.0564
2023-11-18 11:55:28,294:INFO:  Epoch 28/500:  train Loss: 45.0690   val Loss: 47.2259   time: 128.39s   best: 46.1506
2023-11-18 11:57:37,859:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:57:37,879:INFO:  Epoch 29/500:  train Loss: 44.4048   val Loss: 45.3619   time: 129.55s   best: 45.3619
2023-11-18 11:59:46,365:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 11:59:46,390:INFO:  Epoch 30/500:  train Loss: 43.6676   val Loss: 44.5594   time: 128.48s   best: 44.5594
2023-11-18 12:00:32,366:INFO:  Epoch 363/500:  train Loss: 17.7421   val Loss: 22.9888   time: 423.18s   best: 22.0564
2023-11-18 12:01:54,942:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:01:54,984:INFO:  Epoch 31/500:  train Loss: 43.0168   val Loss: 43.8558   time: 128.54s   best: 43.8558
2023-11-18 12:04:03,340:INFO:  Epoch 32/500:  train Loss: 42.9104   val Loss: 45.5072   time: 128.34s   best: 43.8558
2023-11-18 12:06:12,358:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:06:12,377:INFO:  Epoch 33/500:  train Loss: 42.1452   val Loss: 43.5768   time: 129.01s   best: 43.5768
2023-11-18 12:07:40,518:INFO:  Epoch 364/500:  train Loss: 17.7666   val Loss: 22.8866   time: 428.14s   best: 22.0564
2023-11-18 12:08:20,779:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:08:20,830:INFO:  Epoch 34/500:  train Loss: 41.3426   val Loss: 42.5301   time: 128.38s   best: 42.5301
2023-11-18 12:10:29,627:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:10:29,647:INFO:  Epoch 35/500:  train Loss: 40.8169   val Loss: 41.9388   time: 128.78s   best: 41.9388
2023-11-18 12:12:38,517:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:12:38,536:INFO:  Epoch 36/500:  train Loss: 40.7709   val Loss: 41.1676   time: 128.85s   best: 41.1676
2023-11-18 12:14:45,002:INFO:  Epoch 365/500:  train Loss: 17.6779   val Loss: 22.8569   time: 424.47s   best: 22.0564
2023-11-18 12:14:47,369:INFO:  Epoch 37/500:  train Loss: 40.1124   val Loss: 41.1902   time: 128.83s   best: 41.1676
2023-11-18 12:16:56,864:INFO:  Epoch 38/500:  train Loss: 39.5278   val Loss: 41.3577   time: 129.47s   best: 41.1676
2023-11-18 12:19:05,484:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:19:05,520:INFO:  Epoch 39/500:  train Loss: 39.1793   val Loss: 41.0132   time: 128.60s   best: 41.0132
2023-11-18 12:21:13,825:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:21:13,847:INFO:  Epoch 40/500:  train Loss: 38.8618   val Loss: 40.2793   time: 128.29s   best: 40.2793
2023-11-18 12:21:52,732:INFO:  Epoch 366/500:  train Loss: 17.5743   val Loss: 22.9595   time: 427.72s   best: 22.0564
2023-11-18 12:23:22,346:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:23:22,377:INFO:  Epoch 41/500:  train Loss: 38.3746   val Loss: 40.2128   time: 128.49s   best: 40.2128
2023-11-18 12:25:30,786:INFO:  Epoch 42/500:  train Loss: 38.0114   val Loss: 41.7356   time: 128.40s   best: 40.2128
2023-11-18 12:27:39,108:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:27:39,128:INFO:  Epoch 43/500:  train Loss: 37.6679   val Loss: 39.0368   time: 128.30s   best: 39.0368
2023-11-18 12:29:00,755:INFO:  Epoch 367/500:  train Loss: 17.5993   val Loss: 23.3577   time: 428.00s   best: 22.0564
2023-11-18 12:29:47,417:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:29:47,445:INFO:  Epoch 44/500:  train Loss: 37.3618   val Loss: 38.2826   time: 128.28s   best: 38.2826
2023-11-18 12:31:55,979:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:31:55,999:INFO:  Epoch 45/500:  train Loss: 36.9711   val Loss: 38.0029   time: 128.52s   best: 38.0029
2023-11-18 12:34:04,320:INFO:  Epoch 46/500:  train Loss: 36.6204   val Loss: 39.2773   time: 128.32s   best: 38.0029
2023-11-18 12:36:08,746:INFO:  Epoch 368/500:  train Loss: 17.4932   val Loss: 24.0899   time: 427.98s   best: 22.0564
2023-11-18 12:36:12,967:INFO:  Epoch 47/500:  train Loss: 36.4582   val Loss: 38.1820   time: 128.65s   best: 38.0029
2023-11-18 12:38:21,858:INFO:  Epoch 48/500:  train Loss: 36.0283   val Loss: 39.8847   time: 128.87s   best: 38.0029
2023-11-18 12:40:31,359:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:40:31,378:INFO:  Epoch 49/500:  train Loss: 35.7773   val Loss: 37.6259   time: 129.50s   best: 37.6259
2023-11-18 12:42:40,067:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:42:40,087:INFO:  Epoch 50/500:  train Loss: 35.3606   val Loss: 37.0889   time: 128.67s   best: 37.0889
2023-11-18 12:43:12,688:INFO:  Epoch 369/500:  train Loss: 17.6032   val Loss: 22.5893   time: 423.92s   best: 22.0564
2023-11-18 12:44:49,712:INFO:  Epoch 51/500:  train Loss: 35.1190   val Loss: 39.3348   time: 129.61s   best: 37.0889
2023-11-18 12:46:58,591:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:46:58,617:INFO:  Epoch 52/500:  train Loss: 34.9483   val Loss: 36.8289   time: 128.84s   best: 36.8289
2023-11-18 12:49:07,549:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:49:07,568:INFO:  Epoch 53/500:  train Loss: 34.4715   val Loss: 36.4900   time: 128.93s   best: 36.4900
2023-11-18 12:50:21,136:INFO:  Epoch 370/500:  train Loss: 17.4989   val Loss: 22.4366   time: 428.43s   best: 22.0564
2023-11-18 12:51:16,090:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:51:16,122:INFO:  Epoch 54/500:  train Loss: 34.4226   val Loss: 36.2485   time: 128.52s   best: 36.2485
2023-11-18 12:53:25,369:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:53:25,388:INFO:  Epoch 55/500:  train Loss: 34.0479   val Loss: 35.7132   time: 129.23s   best: 35.7132
2023-11-18 12:55:34,745:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:55:34,764:INFO:  Epoch 56/500:  train Loss: 34.0300   val Loss: 35.0735   time: 129.34s   best: 35.0735
2023-11-18 12:57:25,530:INFO:  Epoch 371/500:  train Loss: 17.5516   val Loss: 23.4423   time: 424.37s   best: 22.0564
2023-11-18 12:57:43,595:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 12:57:43,630:INFO:  Epoch 57/500:  train Loss: 33.9939   val Loss: 34.9362   time: 128.82s   best: 34.9362
2023-11-18 12:59:52,881:INFO:  Epoch 58/500:  train Loss: 33.5077   val Loss: 36.5054   time: 129.24s   best: 34.9362
2023-11-18 13:02:01,371:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 13:02:01,390:INFO:  Epoch 59/500:  train Loss: 33.6561   val Loss: 34.4667   time: 128.49s   best: 34.4667
2023-11-18 13:04:10,747:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 13:04:10,766:INFO:  Epoch 60/500:  train Loss: 33.0507   val Loss: 34.1695   time: 129.34s   best: 34.1695
2023-11-18 13:04:33,335:INFO:  Epoch 372/500:  train Loss: 17.5516   val Loss: 23.1197   time: 427.80s   best: 22.0564
2023-11-18 13:06:20,241:INFO:  Epoch 61/500:  train Loss: 32.9872   val Loss: 35.0040   time: 129.46s   best: 34.1695
2023-11-18 13:08:28,851:INFO:  Epoch 62/500:  train Loss: 32.7667   val Loss: 36.1794   time: 128.60s   best: 34.1695
2023-11-18 13:10:38,452:INFO:  Epoch 63/500:  train Loss: 32.6460   val Loss: 34.5539   time: 129.59s   best: 34.1695
2023-11-18 13:11:37,297:INFO:  Epoch 373/500:  train Loss: 17.5431   val Loss: 23.0528   time: 423.94s   best: 22.0564
2023-11-18 13:12:48,039:INFO:  Epoch 64/500:  train Loss: 32.3929   val Loss: 34.5504   time: 129.59s   best: 34.1695
2023-11-18 13:14:57,286:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 13:14:57,306:INFO:  Epoch 65/500:  train Loss: 32.1505   val Loss: 33.1897   time: 129.22s   best: 33.1897
2023-11-18 13:17:05,760:INFO:  Epoch 66/500:  train Loss: 32.0130   val Loss: 36.1686   time: 128.45s   best: 33.1897
2023-11-18 13:18:46,830:INFO:  Epoch 374/500:  train Loss: 17.7520   val Loss: 22.8357   time: 429.51s   best: 22.0564
2023-11-18 13:19:14,419:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 13:19:14,448:INFO:  Epoch 67/500:  train Loss: 32.0522   val Loss: 32.8023   time: 128.64s   best: 32.8023
2023-11-18 13:21:23,890:INFO:  Epoch 68/500:  train Loss: 31.5757   val Loss: 33.3898   time: 129.43s   best: 32.8023
2023-11-18 13:23:32,748:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 13:23:32,777:INFO:  Epoch 69/500:  train Loss: 31.5974   val Loss: 32.7901   time: 128.85s   best: 32.7901
2023-11-18 13:25:41,377:INFO:  Epoch 70/500:  train Loss: 31.4810   val Loss: 32.9902   time: 128.59s   best: 32.7901
2023-11-18 13:25:54,218:INFO:  Epoch 375/500:  train Loss: 17.4396   val Loss: 22.6458   time: 427.36s   best: 22.0564
2023-11-18 13:27:50,550:INFO:  Epoch 71/500:  train Loss: 31.1535   val Loss: 33.4908   time: 129.16s   best: 32.7901
2023-11-18 13:29:59,079:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 13:29:59,122:INFO:  Epoch 72/500:  train Loss: 31.1097   val Loss: 32.4991   time: 128.50s   best: 32.4991
2023-11-18 13:32:08,662:INFO:  Epoch 73/500:  train Loss: 31.2313   val Loss: 34.7651   time: 129.54s   best: 32.4991
2023-11-18 13:33:03,664:INFO:  Epoch 376/500:  train Loss: 17.4742   val Loss: 23.4335   time: 429.43s   best: 22.0564
2023-11-18 13:34:17,709:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 13:34:17,751:INFO:  Epoch 74/500:  train Loss: 30.9230   val Loss: 32.1651   time: 129.04s   best: 32.1651
2023-11-18 13:36:26,538:INFO:  Epoch 75/500:  train Loss: 30.6548   val Loss: 32.2224   time: 128.77s   best: 32.1651
2023-11-18 13:38:35,303:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 13:38:35,323:INFO:  Epoch 76/500:  train Loss: 30.6480   val Loss: 32.0354   time: 128.75s   best: 32.0354
2023-11-18 13:40:11,599:INFO:  Epoch 377/500:  train Loss: 17.5810   val Loss: 22.9326   time: 427.93s   best: 22.0564
2023-11-18 13:40:44,297:INFO:  Epoch 77/500:  train Loss: 30.5908   val Loss: 32.4632   time: 128.97s   best: 32.0354
2023-11-18 13:42:53,790:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 13:42:53,809:INFO:  Epoch 78/500:  train Loss: 30.3860   val Loss: 31.2691   time: 129.47s   best: 31.2691
2023-11-18 13:45:03,122:INFO:  Epoch 79/500:  train Loss: 30.2153   val Loss: 31.2857   time: 129.31s   best: 31.2691
2023-11-18 13:47:11,872:INFO:  Epoch 80/500:  train Loss: 30.0952   val Loss: 32.4321   time: 128.75s   best: 31.2691
2023-11-18 13:47:19,323:INFO:  Epoch 378/500:  train Loss: 17.7988   val Loss: 24.8781   time: 427.71s   best: 22.0564
2023-11-18 13:49:21,136:INFO:  Epoch 81/500:  train Loss: 30.1607   val Loss: 34.6561   time: 129.26s   best: 31.2691
2023-11-18 13:51:30,983:INFO:  Epoch 82/500:  train Loss: 29.8779   val Loss: 31.4485   time: 129.84s   best: 31.2691
2023-11-18 13:53:39,925:INFO:  Epoch 83/500:  train Loss: 29.9896   val Loss: 33.2086   time: 128.93s   best: 31.2691
2023-11-18 13:54:22,692:INFO:  Epoch 379/500:  train Loss: 17.4914   val Loss: 22.8549   time: 423.35s   best: 22.0564
2023-11-18 13:55:49,493:INFO:  Epoch 84/500:  train Loss: 29.9294   val Loss: 31.5023   time: 129.54s   best: 31.2691
2023-11-18 13:57:58,772:INFO:  Epoch 85/500:  train Loss: 29.5658   val Loss: 31.3266   time: 129.25s   best: 31.2691
2023-11-18 14:00:07,793:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 14:00:07,813:INFO:  Epoch 86/500:  train Loss: 29.6527   val Loss: 30.9281   time: 129.02s   best: 30.9281
2023-11-18 14:01:31,916:INFO:  Epoch 380/500:  train Loss: 17.3912   val Loss: 23.0093   time: 429.21s   best: 22.0564
2023-11-18 14:02:16,994:INFO:  Epoch 87/500:  train Loss: 29.3697   val Loss: 31.2486   time: 129.18s   best: 30.9281
2023-11-18 14:04:25,978:INFO:  Epoch 88/500:  train Loss: 29.3491   val Loss: 31.4364   time: 128.97s   best: 30.9281
2023-11-18 14:06:35,533:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 14:06:35,564:INFO:  Epoch 89/500:  train Loss: 29.3110   val Loss: 30.4562   time: 129.53s   best: 30.4562
2023-11-18 14:08:35,253:INFO:  Epoch 381/500:  train Loss: 17.6050   val Loss: 24.8843   time: 423.31s   best: 22.0564
2023-11-18 14:08:45,188:INFO:  Epoch 90/500:  train Loss: 29.0878   val Loss: 30.5481   time: 129.61s   best: 30.4562
2023-11-18 14:10:54,413:INFO:  Epoch 91/500:  train Loss: 29.2256   val Loss: 31.0344   time: 129.21s   best: 30.4562
2023-11-18 14:13:04,455:INFO:  Epoch 92/500:  train Loss: 28.9401   val Loss: 30.9488   time: 130.03s   best: 30.4562
2023-11-18 14:15:13,573:INFO:  Epoch 93/500:  train Loss: 29.1062   val Loss: 31.4724   time: 129.12s   best: 30.4562
2023-11-18 14:15:39,401:INFO:  Epoch 382/500:  train Loss: 17.5366   val Loss: 22.4608   time: 424.12s   best: 22.0564
2023-11-18 14:17:22,793:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 14:17:22,824:INFO:  Epoch 94/500:  train Loss: 28.6854   val Loss: 30.3846   time: 129.20s   best: 30.3846
2023-11-18 14:19:31,522:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 14:19:31,542:INFO:  Epoch 95/500:  train Loss: 28.7441   val Loss: 30.1882   time: 128.68s   best: 30.1882
2023-11-18 14:21:40,629:INFO:  Epoch 96/500:  train Loss: 28.5556   val Loss: 30.3156   time: 129.09s   best: 30.1882
2023-11-18 14:22:46,644:INFO:  Epoch 383/500:  train Loss: 17.4705   val Loss: 22.7822   time: 427.22s   best: 22.0564
2023-11-18 14:23:49,703:INFO:  Epoch 97/500:  train Loss: 28.6346   val Loss: 31.2039   time: 129.07s   best: 30.1882
2023-11-18 14:25:58,540:INFO:  Epoch 98/500:  train Loss: 28.4954   val Loss: 30.8831   time: 128.82s   best: 30.1882
2023-11-18 14:28:07,340:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 14:28:07,360:INFO:  Epoch 99/500:  train Loss: 28.3735   val Loss: 30.1705   time: 128.78s   best: 30.1705
2023-11-18 14:29:50,397:INFO:  Epoch 384/500:  train Loss: 17.6014   val Loss: 23.0832   time: 423.73s   best: 22.0564
2023-11-18 14:30:16,680:INFO:  Epoch 100/500:  train Loss: 28.2257   val Loss: 30.3557   time: 129.31s   best: 30.1705
2023-11-18 14:32:25,888:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 14:32:25,918:INFO:  Epoch 101/500:  train Loss: 28.1934   val Loss: 29.8891   time: 129.18s   best: 29.8891
2023-11-18 14:34:35,311:INFO:  Epoch 102/500:  train Loss: 28.0244   val Loss: 31.2527   time: 129.39s   best: 29.8891
2023-11-18 14:36:44,366:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 14:36:44,386:INFO:  Epoch 103/500:  train Loss: 28.1468   val Loss: 29.8152   time: 129.05s   best: 29.8152
2023-11-18 14:36:58,054:INFO:  Epoch 385/500:  train Loss: 17.7608   val Loss: 22.9950   time: 427.63s   best: 22.0564
2023-11-18 14:38:54,252:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 14:38:54,283:INFO:  Epoch 104/500:  train Loss: 28.2424   val Loss: 29.4190   time: 129.86s   best: 29.4190
2023-11-18 14:41:03,352:INFO:  Epoch 105/500:  train Loss: 28.0797   val Loss: 29.6318   time: 129.06s   best: 29.4190
2023-11-18 14:43:12,329:INFO:  Epoch 106/500:  train Loss: 27.8904   val Loss: 29.7572   time: 128.97s   best: 29.4190
2023-11-18 14:44:02,437:INFO:  Epoch 386/500:  train Loss: 17.9255   val Loss: 22.9856   time: 424.36s   best: 22.0564
2023-11-18 14:45:21,063:INFO:  Epoch 107/500:  train Loss: 27.6911   val Loss: 29.5030   time: 128.73s   best: 29.4190
2023-11-18 14:47:29,887:INFO:  Epoch 108/500:  train Loss: 27.5297   val Loss: 29.4550   time: 128.80s   best: 29.4190
2023-11-18 14:49:38,667:INFO:  Epoch 109/500:  train Loss: 27.6657   val Loss: 30.6652   time: 128.78s   best: 29.4190
2023-11-18 14:51:09,121:INFO:  Epoch 387/500:  train Loss: 17.8897   val Loss: 23.3149   time: 426.66s   best: 22.0564
2023-11-18 14:51:47,413:INFO:  Epoch 110/500:  train Loss: 27.8929   val Loss: 29.6079   time: 128.75s   best: 29.4190
2023-11-18 14:53:56,461:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 14:53:56,481:INFO:  Epoch 111/500:  train Loss: 27.3289   val Loss: 29.0506   time: 129.02s   best: 29.0506
2023-11-18 14:56:06,114:INFO:  Epoch 112/500:  train Loss: 27.3981   val Loss: 29.2413   time: 129.63s   best: 29.0506
2023-11-18 14:58:13,872:INFO:  Epoch 388/500:  train Loss: 17.4753   val Loss: 22.8567   time: 424.73s   best: 22.0564
2023-11-18 14:58:15,120:INFO:  Epoch 113/500:  train Loss: 27.2595   val Loss: 29.5076   time: 129.01s   best: 29.0506
2023-11-18 15:00:24,481:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 15:00:24,501:INFO:  Epoch 114/500:  train Loss: 27.1912   val Loss: 29.0243   time: 128.96s   best: 29.0243
2023-11-18 15:02:33,748:INFO:  Epoch 115/500:  train Loss: 28.1376   val Loss: 29.2229   time: 129.25s   best: 29.0243
2023-11-18 15:04:42,470:INFO:  Epoch 116/500:  train Loss: 27.0998   val Loss: 29.3957   time: 128.71s   best: 29.0243
2023-11-18 15:05:18,175:INFO:  Epoch 389/500:  train Loss: 17.7240   val Loss: 22.4303   time: 424.29s   best: 22.0564
2023-11-18 15:06:52,177:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 15:06:52,244:INFO:  Epoch 117/500:  train Loss: 27.1630   val Loss: 28.7726   time: 129.70s   best: 28.7726
2023-11-18 15:09:00,848:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 15:09:00,868:INFO:  Epoch 118/500:  train Loss: 26.9795   val Loss: 28.7381   time: 128.59s   best: 28.7381
2023-11-18 15:11:09,655:INFO:  Epoch 119/500:  train Loss: 26.9498   val Loss: 29.2725   time: 128.79s   best: 28.7381
2023-11-18 15:12:25,841:INFO:  Epoch 390/500:  train Loss: 17.5766   val Loss: 23.4103   time: 427.64s   best: 22.0564
2023-11-18 15:13:18,699:INFO:  Epoch 120/500:  train Loss: 26.7174   val Loss: 28.7609   time: 129.04s   best: 28.7381
2023-11-18 15:15:28,389:INFO:  Epoch 121/500:  train Loss: 26.9221   val Loss: 34.3443   time: 129.68s   best: 28.7381
2023-11-18 15:17:37,268:INFO:  Epoch 122/500:  train Loss: 26.6242   val Loss: 29.0680   time: 128.87s   best: 28.7381
2023-11-18 15:19:29,970:INFO:  Epoch 391/500:  train Loss: 17.5260   val Loss: 22.3542   time: 424.11s   best: 22.0564
2023-11-18 15:19:47,099:INFO:  Epoch 123/500:  train Loss: 27.1291   val Loss: 28.9710   time: 129.83s   best: 28.7381
2023-11-18 15:21:57,121:INFO:  Epoch 124/500:  train Loss: 26.5908   val Loss: 29.0209   time: 130.00s   best: 28.7381
2023-11-18 15:24:05,760:INFO:  Epoch 125/500:  train Loss: 27.1087   val Loss: 29.0054   time: 128.64s   best: 28.7381
2023-11-18 15:26:14,318:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 15:26:14,340:INFO:  Epoch 126/500:  train Loss: 26.3288   val Loss: 28.5319   time: 128.55s   best: 28.5319
2023-11-18 15:26:36,760:INFO:  Epoch 392/500:  train Loss: 17.4127   val Loss: 22.5210   time: 426.76s   best: 22.0564
2023-11-18 15:28:23,087:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 15:28:23,128:INFO:  Epoch 127/500:  train Loss: 26.5004   val Loss: 28.3572   time: 128.73s   best: 28.3572
2023-11-18 15:30:31,747:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 15:30:31,767:INFO:  Epoch 128/500:  train Loss: 26.3684   val Loss: 28.0224   time: 128.62s   best: 28.0224
2023-11-18 15:32:40,712:INFO:  Epoch 129/500:  train Loss: 26.4546   val Loss: 29.7638   time: 128.93s   best: 28.0224
2023-11-18 15:33:44,089:INFO:  Epoch 393/500:  train Loss: 17.3891   val Loss: 22.8411   time: 427.30s   best: 22.0564
2023-11-18 15:34:49,319:INFO:  Epoch 130/500:  train Loss: 26.1976   val Loss: 28.6149   time: 128.61s   best: 28.0224
2023-11-18 15:36:58,665:INFO:  Epoch 131/500:  train Loss: 26.3001   val Loss: 28.4328   time: 129.32s   best: 28.0224
2023-11-18 15:39:07,593:INFO:  Epoch 132/500:  train Loss: 26.0654   val Loss: 28.5130   time: 128.93s   best: 28.0224
2023-11-18 15:40:50,999:INFO:  Epoch 394/500:  train Loss: 17.5320   val Loss: 22.5619   time: 426.88s   best: 22.0564
2023-11-18 15:41:16,343:INFO:  Epoch 133/500:  train Loss: 26.0361   val Loss: 28.2954   time: 128.74s   best: 28.0224
2023-11-18 15:43:25,806:INFO:  Epoch 134/500:  train Loss: 26.0034   val Loss: 28.6316   time: 129.44s   best: 28.0224
2023-11-18 15:45:35,562:INFO:  Epoch 135/500:  train Loss: 25.8647   val Loss: 28.4923   time: 129.74s   best: 28.0224
2023-11-18 15:47:44,210:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 15:47:44,237:INFO:  Epoch 136/500:  train Loss: 25.9609   val Loss: 27.8549   time: 128.64s   best: 27.8549
2023-11-18 15:47:54,777:INFO:  Epoch 395/500:  train Loss: 17.4711   val Loss: 22.9051   time: 423.75s   best: 22.0564
2023-11-18 15:49:53,125:INFO:  Epoch 137/500:  train Loss: 25.9396   val Loss: 28.4313   time: 128.89s   best: 27.8549
2023-11-18 15:52:01,876:INFO:  Epoch 138/500:  train Loss: 25.8467   val Loss: 28.3564   time: 128.73s   best: 27.8549
2023-11-18 15:54:11,164:INFO:  Epoch 139/500:  train Loss: 25.9872   val Loss: 28.3874   time: 129.29s   best: 27.8549
2023-11-18 15:55:02,311:INFO:  Epoch 396/500:  train Loss: 17.6461   val Loss: 23.0690   time: 427.53s   best: 22.0564
2023-11-18 15:56:19,868:INFO:  Epoch 140/500:  train Loss: 25.8473   val Loss: 28.1809   time: 128.70s   best: 27.8549
2023-11-18 15:58:28,773:INFO:  Epoch 141/500:  train Loss: 25.8342   val Loss: 28.3572   time: 128.89s   best: 27.8549
2023-11-18 16:00:37,485:INFO:  Epoch 142/500:  train Loss: 25.5284   val Loss: 27.9887   time: 128.70s   best: 27.8549
2023-11-18 16:02:06,883:INFO:  Epoch 397/500:  train Loss: 17.3806   val Loss: 23.2226   time: 424.56s   best: 22.0564
2023-11-18 16:02:46,390:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 16:02:46,414:INFO:  Epoch 143/500:  train Loss: 25.5516   val Loss: 27.5084   time: 128.90s   best: 27.5084
2023-11-18 16:04:55,297:INFO:  Epoch 144/500:  train Loss: 26.6573   val Loss: 37.2636   time: 128.88s   best: 27.5084
2023-11-18 16:07:04,918:INFO:  Epoch 145/500:  train Loss: 26.2369   val Loss: 27.6611   time: 129.61s   best: 27.5084
2023-11-18 16:09:13,432:INFO:  Epoch 146/500:  train Loss: 25.4029   val Loss: 27.7912   time: 128.51s   best: 27.5084
2023-11-18 16:09:14,529:INFO:  Epoch 398/500:  train Loss: 17.5127   val Loss: 22.8238   time: 427.64s   best: 22.0564
2023-11-18 16:11:22,149:INFO:  Epoch 147/500:  train Loss: 25.3531   val Loss: 28.3701   time: 128.71s   best: 27.5084
2023-11-18 16:13:30,990:INFO:  Epoch 148/500:  train Loss: 25.4319   val Loss: 27.7165   time: 128.82s   best: 27.5084
2023-11-18 16:15:40,112:INFO:  Epoch 149/500:  train Loss: 25.3765   val Loss: 27.6765   time: 129.12s   best: 27.5084
2023-11-18 16:16:20,261:INFO:  Epoch 399/500:  train Loss: 17.4140   val Loss: 24.3188   time: 425.69s   best: 22.0564
2023-11-18 16:17:49,486:INFO:  Epoch 150/500:  train Loss: 25.2542   val Loss: 27.7646   time: 129.36s   best: 27.5084
2023-11-18 16:19:58,422:INFO:  Epoch 151/500:  train Loss: 25.1913   val Loss: 28.2639   time: 128.93s   best: 27.5084
2023-11-18 16:22:07,454:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 16:22:07,485:INFO:  Epoch 152/500:  train Loss: 25.5027   val Loss: 27.4800   time: 129.02s   best: 27.4800
2023-11-18 16:23:25,381:INFO:  Epoch 400/500:  train Loss: 17.6622   val Loss: 22.5737   time: 425.09s   best: 22.0564
2023-11-18 16:24:17,311:INFO:  Epoch 153/500:  train Loss: 25.0023   val Loss: 31.5230   time: 129.81s   best: 27.4800
2023-11-18 16:26:26,422:INFO:  Epoch 154/500:  train Loss: 25.1080   val Loss: 27.6802   time: 129.10s   best: 27.4800
2023-11-18 16:28:36,230:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 16:28:36,257:INFO:  Epoch 155/500:  train Loss: 25.0815   val Loss: 27.4578   time: 129.78s   best: 27.4578
2023-11-18 16:30:31,039:INFO:  Epoch 401/500:  train Loss: 17.5790   val Loss: 22.8609   time: 425.65s   best: 22.0564
2023-11-18 16:30:46,265:INFO:  Epoch 156/500:  train Loss: 25.0020   val Loss: 27.5598   time: 130.00s   best: 27.4578
2023-11-18 16:32:55,294:INFO:  Epoch 157/500:  train Loss: 24.8662   val Loss: 27.9348   time: 129.00s   best: 27.4578
2023-11-18 16:35:03,992:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 16:35:04,012:INFO:  Epoch 158/500:  train Loss: 25.1590   val Loss: 27.1848   time: 128.69s   best: 27.1848
2023-11-18 16:37:12,685:INFO:  Epoch 159/500:  train Loss: 24.7983   val Loss: 27.8559   time: 128.66s   best: 27.1848
2023-11-18 16:37:38,161:INFO:  Epoch 402/500:  train Loss: 17.4781   val Loss: 23.0942   time: 427.11s   best: 22.0564
2023-11-18 16:39:21,302:INFO:  Epoch 160/500:  train Loss: 24.7872   val Loss: 28.1047   time: 128.61s   best: 27.1848
2023-11-18 16:41:30,202:INFO:  Epoch 161/500:  train Loss: 25.1176   val Loss: 27.4053   time: 128.89s   best: 27.1848
2023-11-18 16:43:39,427:INFO:  Epoch 162/500:  train Loss: 24.6524   val Loss: 27.3424   time: 129.22s   best: 27.1848
2023-11-18 16:44:42,234:INFO:  Epoch 403/500:  train Loss: 17.4821   val Loss: 22.8809   time: 424.06s   best: 22.0564
2023-11-18 16:45:48,266:INFO:  Epoch 163/500:  train Loss: 24.8144   val Loss: 27.9903   time: 128.84s   best: 27.1848
2023-11-18 16:47:57,291:INFO:  Epoch 164/500:  train Loss: 24.5575   val Loss: 27.3371   time: 128.99s   best: 27.1848
2023-11-18 16:50:07,169:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 16:50:07,188:INFO:  Epoch 165/500:  train Loss: 24.6089   val Loss: 27.1469   time: 129.87s   best: 27.1469
2023-11-18 16:51:48,296:INFO:  Epoch 404/500:  train Loss: 17.5810   val Loss: 22.8592   time: 426.01s   best: 22.0564
2023-11-18 16:52:17,021:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 16:52:17,051:INFO:  Epoch 166/500:  train Loss: 24.7763   val Loss: 27.0708   time: 129.83s   best: 27.0708
2023-11-18 16:54:26,082:INFO:  Epoch 167/500:  train Loss: 24.5397   val Loss: 27.2386   time: 129.02s   best: 27.0708
2023-11-18 16:56:35,244:INFO:  Epoch 168/500:  train Loss: 24.6374   val Loss: 27.2063   time: 129.15s   best: 27.0708
2023-11-18 16:58:44,916:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 16:58:44,936:INFO:  Epoch 169/500:  train Loss: 24.4076   val Loss: 27.0694   time: 129.66s   best: 27.0694
2023-11-18 16:58:55,443:INFO:  Epoch 405/500:  train Loss: 17.5014   val Loss: 24.3388   time: 427.12s   best: 22.0564
2023-11-18 17:00:54,083:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 17:00:54,115:INFO:  Epoch 170/500:  train Loss: 24.4928   val Loss: 26.8373   time: 129.14s   best: 26.8373
2023-11-18 17:03:03,197:INFO:  Epoch 171/500:  train Loss: 24.3206   val Loss: 27.1616   time: 129.08s   best: 26.8373
2023-11-18 17:05:12,069:INFO:  Epoch 172/500:  train Loss: 24.2839   val Loss: 27.2948   time: 128.87s   best: 26.8373
2023-11-18 17:06:03,778:INFO:  Epoch 406/500:  train Loss: 17.5756   val Loss: 23.5301   time: 428.33s   best: 22.0564
2023-11-18 17:07:21,043:INFO:  Epoch 173/500:  train Loss: 24.3178   val Loss: 27.2060   time: 128.96s   best: 26.8373
2023-11-18 17:09:30,471:INFO:  Epoch 174/500:  train Loss: 24.2957   val Loss: 27.5035   time: 129.41s   best: 26.8373
2023-11-18 17:11:39,060:INFO:  Epoch 175/500:  train Loss: 24.1710   val Loss: 27.6761   time: 128.58s   best: 26.8373
2023-11-18 17:13:07,718:INFO:  Epoch 407/500:  train Loss: 17.8607   val Loss: 23.3530   time: 423.92s   best: 22.0564
2023-11-18 17:13:47,823:INFO:  Epoch 176/500:  train Loss: 24.0938   val Loss: 26.8668   time: 128.75s   best: 26.8373
2023-11-18 17:15:56,982:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 17:15:57,002:INFO:  Epoch 177/500:  train Loss: 24.0538   val Loss: 26.7507   time: 129.14s   best: 26.7507
2023-11-18 17:18:06,165:INFO:  Epoch 178/500:  train Loss: 24.1587   val Loss: 27.1435   time: 129.16s   best: 26.7507
2023-11-18 17:20:13,406:INFO:  Epoch 408/500:  train Loss: 17.6200   val Loss: 22.4498   time: 425.66s   best: 22.0564
2023-11-18 17:20:14,945:INFO:  Epoch 179/500:  train Loss: 24.3725   val Loss: 29.2189   time: 128.77s   best: 26.7507
2023-11-18 17:22:23,891:INFO:  Epoch 180/500:  train Loss: 24.0772   val Loss: 26.8133   time: 128.91s   best: 26.7507
2023-11-18 17:24:32,752:INFO:  Epoch 181/500:  train Loss: 24.1432   val Loss: 30.9596   time: 128.85s   best: 26.7507
2023-11-18 17:26:41,512:INFO:  Epoch 182/500:  train Loss: 24.0646   val Loss: 26.8290   time: 128.75s   best: 26.7507
2023-11-18 17:27:19,485:INFO:  Epoch 409/500:  train Loss: 17.6459   val Loss: 23.0418   time: 426.05s   best: 22.0564
2023-11-18 17:28:51,155:INFO:  Epoch 183/500:  train Loss: 24.3771   val Loss: 27.1260   time: 129.64s   best: 26.7507
2023-11-18 17:31:00,727:INFO:  Epoch 184/500:  train Loss: 24.1905   val Loss: 27.3820   time: 129.56s   best: 26.7507
2023-11-18 17:33:10,103:INFO:  Epoch 185/500:  train Loss: 23.9222   val Loss: 31.5858   time: 129.37s   best: 26.7507
2023-11-18 17:34:27,019:INFO:  Epoch 410/500:  train Loss: 17.4765   val Loss: 22.4289   time: 427.50s   best: 22.0564
2023-11-18 17:35:19,326:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 17:35:19,368:INFO:  Epoch 186/500:  train Loss: 23.8639   val Loss: 26.5290   time: 129.21s   best: 26.5290
2023-11-18 17:37:28,881:INFO:  Epoch 187/500:  train Loss: 24.3101   val Loss: 26.9858   time: 129.51s   best: 26.5290
2023-11-18 17:39:37,620:INFO:  Epoch 188/500:  train Loss: 23.8897   val Loss: 26.7210   time: 128.74s   best: 26.5290
2023-11-18 17:41:30,184:INFO:  Epoch 411/500:  train Loss: 17.3126   val Loss: 22.5284   time: 423.14s   best: 22.0564
2023-11-18 17:41:46,546:INFO:  Epoch 189/500:  train Loss: 23.9067   val Loss: 26.8224   time: 128.91s   best: 26.5290
2023-11-18 17:43:56,127:INFO:  Epoch 190/500:  train Loss: 23.6691   val Loss: 27.3335   time: 129.56s   best: 26.5290
2023-11-18 17:46:04,917:INFO:  Epoch 191/500:  train Loss: 23.8076   val Loss: 26.9509   time: 128.79s   best: 26.5290
2023-11-18 17:48:13,892:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 17:48:13,912:INFO:  Epoch 192/500:  train Loss: 23.7635   val Loss: 26.5186   time: 128.93s   best: 26.5186
2023-11-18 17:48:33,600:INFO:  Epoch 412/500:  train Loss: 17.4264   val Loss: 23.2760   time: 423.40s   best: 22.0564
2023-11-18 17:50:23,068:INFO:  Epoch 193/500:  train Loss: 23.6188   val Loss: 26.5258   time: 129.14s   best: 26.5186
2023-11-18 17:52:31,850:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 17:52:31,870:INFO:  Epoch 194/500:  train Loss: 23.6337   val Loss: 26.4641   time: 128.74s   best: 26.4641
2023-11-18 17:54:40,560:INFO:  Epoch 195/500:  train Loss: 23.7634   val Loss: 27.5144   time: 128.69s   best: 26.4641
2023-11-18 17:55:41,384:INFO:  Epoch 413/500:  train Loss: 17.3915   val Loss: 22.5258   time: 427.77s   best: 22.0564
2023-11-18 17:56:49,574:INFO:  Epoch 196/500:  train Loss: 23.6211   val Loss: 26.6412   time: 129.01s   best: 26.4641
2023-11-18 17:58:59,119:INFO:  Epoch 197/500:  train Loss: 23.6056   val Loss: 26.5329   time: 129.53s   best: 26.4641
2023-11-18 18:01:07,808:INFO:  Epoch 198/500:  train Loss: 23.8685   val Loss: 28.1455   time: 128.69s   best: 26.4641
2023-11-18 18:02:49,582:INFO:  Epoch 414/500:  train Loss: 17.4093   val Loss: 23.4024   time: 428.18s   best: 22.0564
2023-11-18 18:03:16,779:INFO:  Epoch 199/500:  train Loss: 23.7050   val Loss: 27.4015   time: 128.97s   best: 26.4641
2023-11-18 18:05:25,456:INFO:  Epoch 200/500:  train Loss: 23.7400   val Loss: 27.1331   time: 128.65s   best: 26.4641
2023-11-18 18:07:33,907:INFO:  Epoch 201/500:  train Loss: 23.5261   val Loss: 26.4735   time: 128.45s   best: 26.4641
2023-11-18 18:09:42,857:INFO:  Epoch 202/500:  train Loss: 23.5163   val Loss: 26.8379   time: 128.95s   best: 26.4641
2023-11-18 18:09:54,604:INFO:  Epoch 415/500:  train Loss: 17.5748   val Loss: 22.4813   time: 424.99s   best: 22.0564
2023-11-18 18:11:52,155:INFO:  Epoch 203/500:  train Loss: 23.3416   val Loss: 27.4313   time: 129.30s   best: 26.4641
2023-11-18 18:14:01,161:INFO:  Epoch 204/500:  train Loss: 23.4401   val Loss: 26.4723   time: 128.99s   best: 26.4641
2023-11-18 18:16:10,521:INFO:  Epoch 205/500:  train Loss: 23.3606   val Loss: 27.5323   time: 129.36s   best: 26.4641
2023-11-18 18:17:02,848:INFO:  Epoch 416/500:  train Loss: 17.3898   val Loss: 22.7393   time: 428.23s   best: 22.0564
2023-11-18 18:18:19,013:INFO:  Epoch 206/500:  train Loss: 23.5912   val Loss: 26.5244   time: 128.49s   best: 26.4641
2023-11-18 18:20:27,763:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 18:20:27,804:INFO:  Epoch 207/500:  train Loss: 23.2470   val Loss: 26.2789   time: 128.72s   best: 26.2789
2023-11-18 18:22:37,308:INFO:  Epoch 208/500:  train Loss: 23.3266   val Loss: 27.4065   time: 129.50s   best: 26.2789
2023-11-18 18:24:10,419:INFO:  Epoch 417/500:  train Loss: 17.6017   val Loss: 22.9304   time: 427.56s   best: 22.0564
2023-11-18 18:24:46,313:INFO:  Epoch 209/500:  train Loss: 23.4152   val Loss: 26.5396   time: 128.99s   best: 26.2789
2023-11-18 18:26:55,522:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 18:26:55,542:INFO:  Epoch 210/500:  train Loss: 23.1226   val Loss: 26.0176   time: 129.18s   best: 26.0176
2023-11-18 18:29:04,011:INFO:  Epoch 211/500:  train Loss: 23.1751   val Loss: 27.1633   time: 128.46s   best: 26.0176
2023-11-18 18:31:12,415:INFO:  Epoch 212/500:  train Loss: 23.2496   val Loss: 26.0768   time: 128.40s   best: 26.0176
2023-11-18 18:31:15,131:INFO:  Epoch 418/500:  train Loss: 17.5714   val Loss: 25.1391   time: 424.70s   best: 22.0564
2023-11-18 18:33:20,895:INFO:  Epoch 213/500:  train Loss: 23.4036   val Loss: 26.2906   time: 128.48s   best: 26.0176
2023-11-18 18:35:29,880:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 18:35:29,900:INFO:  Epoch 214/500:  train Loss: 22.9940   val Loss: 25.7128   time: 128.97s   best: 25.7128
2023-11-18 18:37:38,757:INFO:  Epoch 215/500:  train Loss: 23.1578   val Loss: 26.2206   time: 128.86s   best: 25.7128
2023-11-18 18:38:19,472:INFO:  Epoch 419/500:  train Loss: 17.5789   val Loss: 22.1819   time: 424.28s   best: 22.0564
2023-11-18 18:39:47,138:INFO:  Epoch 216/500:  train Loss: 23.0305   val Loss: 26.5323   time: 128.38s   best: 25.7128
2023-11-18 18:41:56,161:INFO:  Epoch 217/500:  train Loss: 22.9893   val Loss: 26.8783   time: 129.00s   best: 25.7128
2023-11-18 18:44:04,497:INFO:  Epoch 218/500:  train Loss: 23.1528   val Loss: 25.9643   time: 128.32s   best: 25.7128
2023-11-18 18:45:25,963:INFO:  Epoch 420/500:  train Loss: 17.4748   val Loss: 22.5159   time: 426.48s   best: 22.0564
2023-11-18 18:46:12,940:INFO:  Epoch 219/500:  train Loss: 22.8902   val Loss: 26.8818   time: 128.43s   best: 25.7128
2023-11-18 18:48:21,427:INFO:  Epoch 220/500:  train Loss: 22.9835   val Loss: 26.3739   time: 128.44s   best: 25.7128
2023-11-18 18:50:29,790:INFO:  Epoch 221/500:  train Loss: 24.2116   val Loss: 27.1661   time: 128.35s   best: 25.7128
2023-11-18 18:52:29,327:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-18 18:52:29,421:INFO:  Epoch 421/500:  train Loss: 17.2697   val Loss: 22.0396   time: 423.34s   best: 22.0396
2023-11-18 18:52:38,710:INFO:  Epoch 222/500:  train Loss: 23.3342   val Loss: 26.3932   time: 128.91s   best: 25.7128
2023-11-18 18:54:47,615:INFO:  Epoch 223/500:  train Loss: 22.9545   val Loss: 28.4482   time: 128.88s   best: 25.7128
2023-11-18 18:56:56,674:INFO:  Epoch 224/500:  train Loss: 22.9309   val Loss: 27.1943   time: 129.05s   best: 25.7128
2023-11-18 18:59:06,121:INFO:  Epoch 225/500:  train Loss: 22.8711   val Loss: 26.2368   time: 129.44s   best: 25.7128
2023-11-18 18:59:33,230:INFO:  Epoch 422/500:  train Loss: 17.3753   val Loss: 22.4064   time: 423.79s   best: 22.0396
2023-11-18 19:01:15,487:INFO:  Epoch 226/500:  train Loss: 22.9624   val Loss: 25.9164   time: 129.36s   best: 25.7128
2023-11-18 19:03:24,646:INFO:  Epoch 227/500:  train Loss: 22.7507   val Loss: 26.5850   time: 129.13s   best: 25.7128
2023-11-18 19:05:33,920:INFO:  Epoch 228/500:  train Loss: 22.9914   val Loss: 25.7821   time: 129.26s   best: 25.7128
2023-11-18 19:06:40,821:INFO:  Epoch 423/500:  train Loss: 17.4405   val Loss: 22.3919   time: 427.57s   best: 22.0396
2023-11-18 19:07:42,897:INFO:  Epoch 229/500:  train Loss: 22.6865   val Loss: 26.3026   time: 128.97s   best: 25.7128
2023-11-18 19:09:51,371:INFO:  Epoch 230/500:  train Loss: 22.7222   val Loss: 26.4327   time: 128.47s   best: 25.7128
2023-11-18 19:11:59,728:INFO:  Epoch 231/500:  train Loss: 22.9292   val Loss: 26.0518   time: 128.36s   best: 25.7128
2023-11-18 19:13:49,995:INFO:  Epoch 424/500:  train Loss: 17.3733   val Loss: 22.6541   time: 429.15s   best: 22.0396
2023-11-18 19:14:09,125:INFO:  Epoch 232/500:  train Loss: 22.6546   val Loss: 26.6553   time: 129.40s   best: 25.7128
2023-11-18 19:16:17,871:INFO:  Epoch 233/500:  train Loss: 22.6489   val Loss: 26.1883   time: 128.72s   best: 25.7128
2023-11-18 19:18:26,523:INFO:  Epoch 234/500:  train Loss: 22.7812   val Loss: 26.3113   time: 128.65s   best: 25.7128
2023-11-18 19:20:35,338:INFO:  Epoch 235/500:  train Loss: 22.5927   val Loss: 26.5688   time: 128.80s   best: 25.7128
2023-11-18 19:20:58,737:INFO:  Epoch 425/500:  train Loss: 17.3719   val Loss: 28.8125   time: 428.70s   best: 22.0396
2023-11-18 19:22:43,897:INFO:  Epoch 236/500:  train Loss: 22.6289   val Loss: 26.3075   time: 128.56s   best: 25.7128
2023-11-18 19:24:52,370:INFO:  Epoch 237/500:  train Loss: 22.6237   val Loss: 26.4224   time: 128.45s   best: 25.7128
2023-11-18 19:27:00,872:INFO:  Epoch 238/500:  train Loss: 22.5143   val Loss: 26.1364   time: 128.49s   best: 25.7128
2023-11-18 19:28:03,269:INFO:  Epoch 426/500:  train Loss: 17.5647   val Loss: 24.2330   time: 424.52s   best: 22.0396
2023-11-18 19:29:09,426:INFO:  Epoch 239/500:  train Loss: 22.8241   val Loss: 26.3288   time: 128.54s   best: 25.7128
2023-11-18 19:31:18,610:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 19:31:18,629:INFO:  Epoch 240/500:  train Loss: 22.5948   val Loss: 25.6938   time: 129.15s   best: 25.6938
2023-11-18 19:33:27,448:INFO:  Epoch 241/500:  train Loss: 22.6726   val Loss: 25.9152   time: 128.82s   best: 25.6938
2023-11-18 19:35:08,043:INFO:  Epoch 427/500:  train Loss: 17.4639   val Loss: 22.4522   time: 424.76s   best: 22.0396
2023-11-18 19:35:36,356:INFO:  Epoch 242/500:  train Loss: 22.4278   val Loss: 26.2118   time: 128.90s   best: 25.6938
2023-11-18 19:37:45,865:INFO:  Epoch 243/500:  train Loss: 22.7585   val Loss: 26.0405   time: 129.48s   best: 25.6938
2023-11-18 19:39:54,714:INFO:  Epoch 244/500:  train Loss: 22.5037   val Loss: 26.0997   time: 128.84s   best: 25.6938
2023-11-18 19:42:03,624:INFO:  Epoch 245/500:  train Loss: 22.4285   val Loss: 27.4512   time: 128.90s   best: 25.6938
2023-11-18 19:42:12,527:INFO:  Epoch 428/500:  train Loss: 17.3427   val Loss: 22.3426   time: 424.46s   best: 22.0396
2023-11-18 19:44:12,433:INFO:  Epoch 246/500:  train Loss: 22.4798   val Loss: 26.0171   time: 128.80s   best: 25.6938
2023-11-18 19:46:21,029:INFO:  Epoch 247/500:  train Loss: 22.3937   val Loss: 25.7591   time: 128.59s   best: 25.6938
2023-11-18 19:48:29,790:INFO:  Epoch 248/500:  train Loss: 22.4461   val Loss: 27.1602   time: 128.75s   best: 25.6938
2023-11-18 19:49:20,669:INFO:  Epoch 429/500:  train Loss: 17.6190   val Loss: 23.2068   time: 428.14s   best: 22.0396
2023-11-18 19:50:38,714:INFO:  Epoch 249/500:  train Loss: 22.3752   val Loss: 26.2453   time: 128.92s   best: 25.6938
2023-11-18 19:52:46,744:INFO:  Epoch 250/500:  train Loss: 22.3457   val Loss: 26.7030   time: 128.00s   best: 25.6938
2023-11-18 19:54:56,218:INFO:  Epoch 251/500:  train Loss: 22.5368   val Loss: 26.1986   time: 129.46s   best: 25.6938
2023-11-18 19:56:27,697:INFO:  Epoch 430/500:  train Loss: 17.6407   val Loss: 22.9128   time: 427.02s   best: 22.0396
2023-11-18 19:57:05,256:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 19:57:05,285:INFO:  Epoch 252/500:  train Loss: 22.4057   val Loss: 25.6499   time: 129.02s   best: 25.6499
2023-11-18 19:59:14,737:INFO:  Epoch 253/500:  train Loss: 22.2453   val Loss: 26.1262   time: 129.45s   best: 25.6499
2023-11-18 20:01:23,590:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 20:01:23,609:INFO:  Epoch 254/500:  train Loss: 22.4385   val Loss: 25.5791   time: 128.84s   best: 25.5791
2023-11-18 20:03:32,277:INFO:  Epoch 255/500:  train Loss: 22.2474   val Loss: 26.0234   time: 128.66s   best: 25.5791
2023-11-18 20:03:36,293:INFO:  Epoch 431/500:  train Loss: 17.5800   val Loss: 22.4215   time: 428.57s   best: 22.0396
2023-11-18 20:05:40,727:INFO:  Epoch 256/500:  train Loss: 22.1276   val Loss: 25.6319   time: 128.44s   best: 25.5791
2023-11-18 20:07:49,338:INFO:  Epoch 257/500:  train Loss: 22.7771   val Loss: 26.0376   time: 128.57s   best: 25.5791
2023-11-18 20:09:57,534:INFO:  Epoch 258/500:  train Loss: 22.6734   val Loss: 26.0480   time: 128.19s   best: 25.5791
2023-11-18 20:10:44,853:INFO:  Epoch 432/500:  train Loss: 17.4427   val Loss: 22.6112   time: 428.55s   best: 22.0396
2023-11-18 20:12:06,570:INFO:  Epoch 259/500:  train Loss: 22.1509   val Loss: 29.3310   time: 129.04s   best: 25.5791
2023-11-18 20:14:15,408:INFO:  Epoch 260/500:  train Loss: 22.7520   val Loss: 26.8489   time: 128.80s   best: 25.5791
2023-11-18 20:16:24,166:INFO:  Epoch 261/500:  train Loss: 22.3168   val Loss: 26.1176   time: 128.75s   best: 25.5791
2023-11-18 20:17:53,629:INFO:  Epoch 433/500:  train Loss: 17.2504   val Loss: 22.0736   time: 428.77s   best: 22.0396
2023-11-18 20:18:32,955:INFO:  Epoch 262/500:  train Loss: 22.1928   val Loss: 25.7681   time: 128.79s   best: 25.5791
2023-11-18 20:20:41,936:INFO:  Epoch 263/500:  train Loss: 22.0551   val Loss: 26.0801   time: 128.94s   best: 25.5791
2023-11-18 20:22:50,544:INFO:  Epoch 264/500:  train Loss: 22.1256   val Loss: 25.6118   time: 128.61s   best: 25.5791
2023-11-18 20:24:59,624:INFO:  Epoch 434/500:  train Loss: 17.3051   val Loss: 27.4837   time: 425.97s   best: 22.0396
2023-11-18 20:24:59,815:INFO:  Epoch 265/500:  train Loss: 22.7437   val Loss: 25.7393   time: 129.27s   best: 25.5791
2023-11-18 20:27:08,472:INFO:  Epoch 266/500:  train Loss: 22.0570   val Loss: 25.6980   time: 128.64s   best: 25.5791
2023-11-18 20:29:16,920:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 20:29:16,939:INFO:  Epoch 267/500:  train Loss: 21.9959   val Loss: 25.3956   time: 128.44s   best: 25.3956
2023-11-18 20:31:26,025:INFO:  Epoch 268/500:  train Loss: 22.1272   val Loss: 25.7648   time: 129.07s   best: 25.3956
2023-11-18 20:32:03,485:INFO:  Epoch 435/500:  train Loss: 17.2945   val Loss: 22.7581   time: 423.84s   best: 22.0396
2023-11-18 20:33:34,470:INFO:  Epoch 269/500:  train Loss: 22.3846   val Loss: 26.1363   time: 128.43s   best: 25.3956
2023-11-18 20:35:43,192:INFO:  Epoch 270/500:  train Loss: 21.9240   val Loss: 25.6649   time: 128.70s   best: 25.3956
2023-11-18 20:37:51,812:INFO:  Epoch 271/500:  train Loss: 21.9840   val Loss: 25.5977   time: 128.62s   best: 25.3956
2023-11-18 20:39:10,578:INFO:  Epoch 436/500:  train Loss: 17.3209   val Loss: 23.7644   time: 427.07s   best: 22.0396
2023-11-18 20:40:00,611:INFO:  Epoch 272/500:  train Loss: 21.9310   val Loss: 26.5106   time: 128.79s   best: 25.3956
2023-11-18 20:42:09,559:INFO:  Epoch 273/500:  train Loss: 22.2964   val Loss: 29.0748   time: 128.92s   best: 25.3956
2023-11-18 20:44:18,105:INFO:  Epoch 274/500:  train Loss: 22.3284   val Loss: 25.5663   time: 128.53s   best: 25.3956
2023-11-18 20:46:18,938:INFO:  Epoch 437/500:  train Loss: 17.2977   val Loss: 22.8915   time: 428.34s   best: 22.0396
2023-11-18 20:46:26,736:INFO:  Epoch 275/500:  train Loss: 21.9114   val Loss: 25.4129   time: 128.62s   best: 25.3956
2023-11-18 20:48:35,705:INFO:  Epoch 276/500:  train Loss: 22.0185   val Loss: 25.6026   time: 128.96s   best: 25.3956
2023-11-18 20:50:44,555:INFO:  Epoch 277/500:  train Loss: 22.2689   val Loss: 34.7952   time: 128.85s   best: 25.3956
2023-11-18 20:52:53,335:INFO:  Epoch 278/500:  train Loss: 22.3631   val Loss: 25.9362   time: 128.77s   best: 25.3956
2023-11-18 20:53:24,144:INFO:  Epoch 438/500:  train Loss: 17.2530   val Loss: 23.3219   time: 425.18s   best: 22.0396
2023-11-18 20:55:02,446:INFO:  Epoch 279/500:  train Loss: 22.2768   val Loss: 25.6874   time: 129.10s   best: 25.3956
2023-11-18 20:57:11,191:INFO:  Epoch 280/500:  train Loss: 21.7498   val Loss: 25.7308   time: 128.74s   best: 25.3956
2023-11-18 20:59:20,012:INFO:  Epoch 281/500:  train Loss: 21.9451   val Loss: 25.5771   time: 128.82s   best: 25.3956
2023-11-18 21:00:33,195:INFO:  Epoch 439/500:  train Loss: 17.5820   val Loss: 22.5790   time: 429.03s   best: 22.0396
2023-11-18 21:01:29,138:INFO:  Epoch 282/500:  train Loss: 21.9381   val Loss: 26.1529   time: 129.11s   best: 25.3956
2023-11-18 21:03:37,643:INFO:  Epoch 283/500:  train Loss: 21.7728   val Loss: 25.4190   time: 128.48s   best: 25.3956
2023-11-18 21:05:46,313:INFO:  Epoch 284/500:  train Loss: 22.1224   val Loss: 26.2766   time: 128.66s   best: 25.3956
2023-11-18 21:07:39,682:INFO:  Epoch 440/500:  train Loss: 17.4914   val Loss: 22.3922   time: 426.48s   best: 22.0396
2023-11-18 21:07:54,796:INFO:  Epoch 285/500:  train Loss: 21.8283   val Loss: 25.6996   time: 128.47s   best: 25.3956
2023-11-18 21:10:03,583:INFO:  Epoch 286/500:  train Loss: 21.7494   val Loss: 25.4786   time: 128.77s   best: 25.3956
2023-11-18 21:12:12,772:INFO:  Epoch 287/500:  train Loss: 21.8589   val Loss: 25.4782   time: 129.18s   best: 25.3956
2023-11-18 21:14:22,247:INFO:  Epoch 288/500:  train Loss: 21.6557   val Loss: 25.5262   time: 129.46s   best: 25.3956
2023-11-18 21:14:46,969:INFO:  Epoch 441/500:  train Loss: 17.4192   val Loss: 22.5042   time: 427.26s   best: 22.0396
2023-11-18 21:16:30,817:INFO:  Epoch 289/500:  train Loss: 21.7764   val Loss: 27.3255   time: 128.57s   best: 25.3956
2023-11-18 21:18:39,164:INFO:  Epoch 290/500:  train Loss: 21.7619   val Loss: 25.6366   time: 128.32s   best: 25.3956
2023-11-18 21:20:48,822:INFO:  Epoch 291/500:  train Loss: 21.7420   val Loss: 25.9925   time: 129.66s   best: 25.3956
2023-11-18 21:21:55,851:INFO:  Epoch 442/500:  train Loss: 17.4313   val Loss: 22.6353   time: 428.86s   best: 22.0396
2023-11-18 21:22:57,633:INFO:  Epoch 292/500:  train Loss: 22.0168   val Loss: 27.3288   time: 128.81s   best: 25.3956
2023-11-18 21:25:06,367:INFO:  Epoch 293/500:  train Loss: 22.5280   val Loss: 28.5940   time: 128.71s   best: 25.3956
2023-11-18 21:27:15,752:INFO:  Epoch 294/500:  train Loss: 22.6144   val Loss: 25.4206   time: 129.37s   best: 25.3956
2023-11-18 21:29:04,242:INFO:  Epoch 443/500:  train Loss: 17.2870   val Loss: 22.8242   time: 428.38s   best: 22.0396
2023-11-18 21:29:25,049:INFO:  Epoch 295/500:  train Loss: 21.6722   val Loss: 25.4952   time: 129.28s   best: 25.3956
2023-11-18 21:31:33,369:INFO:  Epoch 296/500:  train Loss: 21.6420   val Loss: 25.4612   time: 128.31s   best: 25.3956
2023-11-18 21:33:42,213:INFO:  Epoch 297/500:  train Loss: 21.8621   val Loss: 25.8065   time: 128.84s   best: 25.3956
2023-11-18 21:35:50,704:INFO:  Epoch 298/500:  train Loss: 21.6252   val Loss: 27.2066   time: 128.48s   best: 25.3956
2023-11-18 21:36:12,397:INFO:  Epoch 444/500:  train Loss: 17.3392   val Loss: 22.1154   time: 428.13s   best: 22.0396
2023-11-18 21:37:59,501:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 21:37:59,554:INFO:  Epoch 299/500:  train Loss: 21.6105   val Loss: 25.3756   time: 128.78s   best: 25.3756
2023-11-18 21:40:07,959:INFO:  Epoch 300/500:  train Loss: 21.6074   val Loss: 26.1557   time: 128.39s   best: 25.3756
2023-11-18 21:42:16,437:INFO:  Epoch 301/500:  train Loss: 21.9378   val Loss: 27.6714   time: 128.47s   best: 25.3756
2023-11-18 21:43:19,722:INFO:  Epoch 445/500:  train Loss: 17.4295   val Loss: 22.6168   time: 427.32s   best: 22.0396
2023-11-18 21:44:25,130:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-18 21:44:25,159:INFO:  Epoch 302/500:  train Loss: 21.5977   val Loss: 24.9662   time: 128.69s   best: 24.9662
2023-11-18 21:46:33,696:INFO:  Epoch 303/500:  train Loss: 21.5582   val Loss: 25.7720   time: 128.53s   best: 24.9662
2023-11-18 21:48:41,795:INFO:  Epoch 304/500:  train Loss: 21.5507   val Loss: 25.5398   time: 128.10s   best: 24.9662
2023-11-18 21:50:23,053:INFO:  Epoch 446/500:  train Loss: 17.2385   val Loss: 22.9258   time: 423.29s   best: 22.0396
2023-11-18 21:50:51,110:INFO:  Epoch 305/500:  train Loss: 21.6849   val Loss: 25.5668   time: 129.30s   best: 24.9662
2023-11-18 21:53:00,412:INFO:  Epoch 306/500:  train Loss: 21.4797   val Loss: 25.4027   time: 129.27s   best: 24.9662
2023-11-18 21:55:09,810:INFO:  Epoch 307/500:  train Loss: 21.4294   val Loss: 25.6433   time: 129.37s   best: 24.9662
2023-11-18 21:57:18,780:INFO:  Epoch 308/500:  train Loss: 21.5775   val Loss: 25.6329   time: 128.97s   best: 24.9662
2023-11-18 21:57:30,327:INFO:  Epoch 447/500:  train Loss: 17.4341   val Loss: 22.5012   time: 427.25s   best: 22.0396
2023-11-18 21:59:28,054:INFO:  Epoch 309/500:  train Loss: 22.1069   val Loss: 26.0478   time: 129.26s   best: 24.9662
2023-11-18 22:01:36,748:INFO:  Epoch 310/500:  train Loss: 21.4532   val Loss: 25.5450   time: 128.67s   best: 24.9662
2023-11-18 22:03:45,686:INFO:  Epoch 311/500:  train Loss: 21.5055   val Loss: 25.3730   time: 128.92s   best: 24.9662
2023-11-18 22:04:38,429:INFO:  Epoch 448/500:  train Loss: 17.4379   val Loss: 23.4710   time: 428.08s   best: 22.0396
2023-11-18 22:05:54,739:INFO:  Epoch 312/500:  train Loss: 21.5324   val Loss: 25.5504   time: 129.05s   best: 24.9662
2023-11-18 22:08:03,454:INFO:  Epoch 313/500:  train Loss: 21.4763   val Loss: 25.5184   time: 128.71s   best: 24.9662
2023-11-18 22:10:11,730:INFO:  Epoch 314/500:  train Loss: 21.4290   val Loss: 27.9521   time: 128.26s   best: 24.9662
2023-11-18 22:11:45,761:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 (0.05 dropout)_ 4ee.pt
2023-11-18 22:11:45,789:INFO:  Epoch 449/500:  train Loss: 17.3006   val Loss: 21.4467   time: 427.31s   best: 21.4467
2023-11-18 22:12:20,798:INFO:  Epoch 315/500:  train Loss: 21.3363   val Loss: 25.2674   time: 129.06s   best: 24.9662
2023-11-18 22:14:29,220:INFO:  Epoch 316/500:  train Loss: 21.3576   val Loss: 28.7090   time: 128.39s   best: 24.9662
2023-11-18 22:16:38,465:INFO:  Epoch 317/500:  train Loss: 21.3771   val Loss: 25.7079   time: 129.23s   best: 24.9662
2023-11-18 22:18:46,913:INFO:  Epoch 318/500:  train Loss: 21.3824   val Loss: 25.3847   time: 128.44s   best: 24.9662
2023-11-18 22:18:52,111:INFO:  Epoch 450/500:  train Loss: 17.3086   val Loss: 21.9155   time: 426.30s   best: 21.4467
2023-11-18 22:20:56,405:INFO:  Epoch 319/500:  train Loss: 21.2531   val Loss: 25.5673   time: 129.48s   best: 24.9662
2023-11-18 22:23:05,125:INFO:  Epoch 320/500:  train Loss: 21.3801   val Loss: 25.8625   time: 128.70s   best: 24.9662
2023-11-18 22:25:13,406:INFO:  Epoch 321/500:  train Loss: 21.4632   val Loss: 25.5241   time: 128.28s   best: 24.9662
2023-11-18 22:26:01,902:INFO:  Epoch 451/500:  train Loss: 17.3718   val Loss: 23.3826   time: 429.78s   best: 21.4467
2023-11-18 22:27:21,774:INFO:  Epoch 322/500:  train Loss: 21.2592   val Loss: 25.2231   time: 128.36s   best: 24.9662
2023-11-18 22:29:30,297:INFO:  Epoch 323/500:  train Loss: 21.5124   val Loss: 25.7312   time: 128.51s   best: 24.9662
2023-11-18 22:31:38,900:INFO:  Epoch 324/500:  train Loss: 21.2681   val Loss: 25.2570   time: 128.60s   best: 24.9662
2023-11-18 22:33:11,036:INFO:  Epoch 452/500:  train Loss: 17.2924   val Loss: 23.0069   time: 429.11s   best: 21.4467
2023-11-18 22:33:47,695:INFO:  Epoch 325/500:  train Loss: 21.2111   val Loss: 25.8197   time: 128.79s   best: 24.9662
2023-11-18 22:35:56,198:INFO:  Epoch 326/500:  train Loss: 21.6351   val Loss: 25.9810   time: 128.48s   best: 24.9662
2023-11-18 22:38:04,491:INFO:  Epoch 327/500:  train Loss: 21.2292   val Loss: 25.4412   time: 128.29s   best: 24.9662
2023-11-18 22:40:13,763:INFO:  Epoch 328/500:  train Loss: 21.5226   val Loss: 26.5630   time: 129.27s   best: 24.9662
2023-11-18 22:40:18,169:INFO:  Epoch 453/500:  train Loss: 17.2434   val Loss: 22.9313   time: 427.13s   best: 21.4467
2023-11-18 22:42:22,276:INFO:  Epoch 329/500:  train Loss: 21.1307   val Loss: 25.4282   time: 128.50s   best: 24.9662
2023-11-18 22:44:30,442:INFO:  Epoch 330/500:  train Loss: 21.2046   val Loss: 25.6342   time: 128.15s   best: 24.9662
2023-11-18 22:46:38,585:INFO:  Epoch 331/500:  train Loss: 21.1997   val Loss: 25.5972   time: 128.13s   best: 24.9662
2023-11-18 22:47:25,152:INFO:  Epoch 454/500:  train Loss: 17.3063   val Loss: 24.1058   time: 426.98s   best: 21.4467
2023-11-18 22:48:47,249:INFO:  Epoch 332/500:  train Loss: 21.7027   val Loss: 26.0373   time: 128.65s   best: 24.9662
2023-11-18 22:50:55,405:INFO:  Epoch 333/500:  train Loss: 21.1418   val Loss: 25.9162   time: 128.13s   best: 24.9662
2023-11-18 22:53:03,344:INFO:  Epoch 334/500:  train Loss: 21.0966   val Loss: 25.3891   time: 127.93s   best: 24.9662
2023-11-18 22:54:30,277:INFO:  Epoch 455/500:  train Loss: 17.2445   val Loss: 22.2546   time: 425.10s   best: 21.4467
2023-11-18 22:55:12,177:INFO:  Epoch 335/500:  train Loss: 21.4379   val Loss: 25.2772   time: 128.83s   best: 24.9662
2023-11-18 22:57:20,185:INFO:  Epoch 336/500:  train Loss: 21.2388   val Loss: 25.2897   time: 128.00s   best: 24.9662
2023-11-18 22:59:28,228:INFO:  Epoch 337/500:  train Loss: 21.1209   val Loss: 30.3160   time: 128.04s   best: 24.9662
2023-11-18 23:01:36,971:INFO:  Epoch 338/500:  train Loss: 21.1386   val Loss: 25.6074   time: 128.73s   best: 24.9662
2023-11-18 23:01:37,194:INFO:  Epoch 456/500:  train Loss: 17.1772   val Loss: 22.6401   time: 426.89s   best: 21.4467
2023-11-18 23:03:45,681:INFO:  Epoch 339/500:  train Loss: 21.2250   val Loss: 25.1155   time: 128.70s   best: 24.9662
2023-11-18 23:05:53,924:INFO:  Epoch 340/500:  train Loss: 21.3786   val Loss: 25.2228   time: 128.23s   best: 24.9662
2023-11-18 23:08:01,905:INFO:  Epoch 341/500:  train Loss: 21.0176   val Loss: 25.2381   time: 127.97s   best: 24.9662
2023-11-18 23:08:41,327:INFO:  Epoch 457/500:  train Loss: 17.1891   val Loss: 22.7271   time: 424.09s   best: 21.4467
2023-11-18 23:10:09,635:INFO:  Epoch 342/500:  train Loss: 21.2144   val Loss: 28.1662   time: 127.72s   best: 24.9662
2023-11-18 23:12:17,606:INFO:  Epoch 343/500:  train Loss: 21.1306   val Loss: 25.1570   time: 127.95s   best: 24.9662
2023-11-18 23:14:25,481:INFO:  Epoch 344/500:  train Loss: 21.0350   val Loss: 24.9919   time: 127.87s   best: 24.9662
2023-11-18 23:15:47,191:INFO:  Epoch 458/500:  train Loss: 17.3767   val Loss: 22.7123   time: 425.85s   best: 21.4467
2023-11-18 23:16:33,314:INFO:  Epoch 345/500:  train Loss: 21.0247   val Loss: 25.8971   time: 127.83s   best: 24.9662
2023-11-18 23:18:41,672:INFO:  Epoch 346/500:  train Loss: 20.9759   val Loss: 25.0413   time: 128.33s   best: 24.9662
2023-11-18 23:20:50,325:INFO:  Epoch 347/500:  train Loss: 21.1185   val Loss: 25.0142   time: 128.65s   best: 24.9662
2023-11-18 23:22:52,860:INFO:  Epoch 459/500:  train Loss: 17.2435   val Loss: 22.2881   time: 425.65s   best: 21.4467
2023-11-18 23:22:58,453:INFO:  Epoch 348/500:  train Loss: 20.9038   val Loss: 25.2963   time: 128.13s   best: 24.9662
2023-11-18 23:25:07,189:INFO:  Epoch 349/500:  train Loss: 20.9847   val Loss: 25.4610   time: 128.73s   best: 24.9662
2023-11-18 23:27:15,029:INFO:  Epoch 350/500:  train Loss: 20.9593   val Loss: 25.2187   time: 127.83s   best: 24.9662
2023-11-18 23:29:22,968:INFO:  Epoch 351/500:  train Loss: 21.1676   val Loss: 25.6530   time: 127.94s   best: 24.9662
2023-11-18 23:30:01,917:INFO:  Epoch 460/500:  train Loss: 17.2165   val Loss: 22.4560   time: 429.03s   best: 21.4467
2023-11-18 23:31:31,046:INFO:  Epoch 352/500:  train Loss: 20.8596   val Loss: 26.1959   time: 128.08s   best: 24.9662
2023-11-18 23:33:39,267:INFO:  Epoch 353/500:  train Loss: 21.0139   val Loss: 25.2572   time: 128.21s   best: 24.9662
2023-11-18 23:35:47,674:INFO:  Epoch 354/500:  train Loss: 20.9691   val Loss: 25.2906   time: 128.39s   best: 24.9662
2023-11-18 23:37:10,689:INFO:  Epoch 461/500:  train Loss: 17.3486   val Loss: 22.5663   time: 428.76s   best: 21.4467
2023-11-18 23:37:55,934:INFO:  Epoch 355/500:  train Loss: 20.8736   val Loss: 25.6284   time: 128.25s   best: 24.9662
2023-11-18 23:40:03,628:INFO:  Epoch 356/500:  train Loss: 20.8499   val Loss: 25.9974   time: 127.67s   best: 24.9662
2023-11-18 23:42:11,783:INFO:  Epoch 357/500:  train Loss: 21.1715   val Loss: 25.6623   time: 128.15s   best: 24.9662
2023-11-18 23:44:19,907:INFO:  Epoch 462/500:  train Loss: 17.3056   val Loss: 22.3076   time: 429.21s   best: 21.4467
2023-11-18 23:44:20,485:INFO:  Epoch 358/500:  train Loss: 21.2902   val Loss: 26.3137   time: 128.70s   best: 24.9662
2023-11-18 23:46:28,462:INFO:  Epoch 359/500:  train Loss: 20.9938   val Loss: 25.3594   time: 127.95s   best: 24.9662
2023-11-18 23:48:36,331:INFO:  Epoch 360/500:  train Loss: 21.2461   val Loss: 25.3290   time: 127.86s   best: 24.9662
2023-11-18 23:50:45,247:INFO:  Epoch 361/500:  train Loss: 20.7981   val Loss: 25.1533   time: 128.90s   best: 24.9662
2023-11-18 23:51:27,244:INFO:  Epoch 463/500:  train Loss: 17.2665   val Loss: 22.3134   time: 427.30s   best: 21.4467
2023-11-18 23:52:52,947:INFO:  Epoch 362/500:  train Loss: 20.8178   val Loss: 25.8994   time: 127.70s   best: 24.9662
2023-11-18 23:55:00,963:INFO:  Epoch 363/500:  train Loss: 20.8076   val Loss: 24.9818   time: 127.99s   best: 24.9662
2023-11-18 23:57:08,963:INFO:  Epoch 364/500:  train Loss: 21.0242   val Loss: 25.1855   time: 127.99s   best: 24.9662
2023-11-18 23:58:33,519:INFO:  Epoch 464/500:  train Loss: 17.4249   val Loss: 23.4005   time: 426.27s   best: 21.4467
2023-11-18 23:59:16,919:INFO:  Epoch 365/500:  train Loss: 20.8659   val Loss: 25.3276   time: 127.95s   best: 24.9662
2023-11-19 00:01:24,961:INFO:  Epoch 366/500:  train Loss: 20.9337   val Loss: 25.1108   time: 128.03s   best: 24.9662
2023-11-19 00:03:33,818:INFO:  Epoch 367/500:  train Loss: 20.8769   val Loss: 25.0324   time: 128.84s   best: 24.9662
2023-11-19 00:05:35,369:INFO:  Epoch 465/500:  train Loss: 17.2645   val Loss: 23.7217   time: 421.84s   best: 21.4467
2023-11-19 00:05:41,780:INFO:  Epoch 368/500:  train Loss: 20.7374   val Loss: 25.0953   time: 127.96s   best: 24.9662
2023-11-19 00:07:50,624:INFO:  Epoch 369/500:  train Loss: 21.0960   val Loss: 25.5251   time: 128.83s   best: 24.9662
2023-11-19 00:09:58,834:INFO:  Epoch 370/500:  train Loss: 20.6901   val Loss: 25.5801   time: 128.21s   best: 24.9662
2023-11-19 00:12:07,782:INFO:  Epoch 371/500:  train Loss: 20.8806   val Loss: 26.1989   time: 128.94s   best: 24.9662
2023-11-19 00:12:40,872:INFO:  Epoch 466/500:  train Loss: 17.3907   val Loss: 22.3891   time: 425.49s   best: 21.4467
2023-11-19 00:14:15,873:INFO:  Epoch 372/500:  train Loss: 20.7147   val Loss: 25.3650   time: 128.08s   best: 24.9662
2023-11-19 00:16:24,603:INFO:  Epoch 373/500:  train Loss: 20.7247   val Loss: 25.0399   time: 128.71s   best: 24.9662
2023-11-19 00:18:32,110:INFO:  Epoch 374/500:  train Loss: 20.7943   val Loss: 25.1136   time: 127.49s   best: 24.9662
2023-11-19 00:19:42,875:INFO:  Epoch 467/500:  train Loss: 17.1957   val Loss: 22.8160   time: 421.99s   best: 21.4467
2023-11-19 00:20:40,554:INFO:  Epoch 375/500:  train Loss: 20.9540   val Loss: 25.1214   time: 128.43s   best: 24.9662
2023-11-19 00:22:48,676:INFO:  Epoch 376/500:  train Loss: 20.8964   val Loss: 25.7575   time: 128.09s   best: 24.9662
2023-11-19 00:24:56,960:INFO:  Epoch 377/500:  train Loss: 20.7071   val Loss: 25.4926   time: 128.28s   best: 24.9662
2023-11-19 00:26:45,472:INFO:  Epoch 468/500:  train Loss: 17.1513   val Loss: 23.4026   time: 422.57s   best: 21.4467
2023-11-19 00:27:05,305:INFO:  Epoch 378/500:  train Loss: 20.6738   val Loss: 25.6131   time: 128.34s   best: 24.9662
2023-11-19 00:29:13,462:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.2 dataset (0.05 dropout)_c2bc.pt
2023-11-19 00:29:13,485:INFO:  Epoch 379/500:  train Loss: 20.7219   val Loss: 24.4882   time: 128.13s   best: 24.4882
2023-11-19 00:31:21,576:INFO:  Epoch 380/500:  train Loss: 20.6051   val Loss: 24.6882   time: 128.09s   best: 24.4882
2023-11-19 00:33:30,447:INFO:  Epoch 381/500:  train Loss: 20.5805   val Loss: 25.0289   time: 128.86s   best: 24.4882
2023-11-19 00:33:48,799:INFO:  Epoch 469/500:  train Loss: 17.2050   val Loss: 22.4745   time: 423.32s   best: 21.4467
2023-11-19 00:35:38,347:INFO:  Epoch 382/500:  train Loss: 20.6224   val Loss: 25.2808   time: 127.90s   best: 24.4882
2023-11-19 00:37:46,579:INFO:  Epoch 383/500:  train Loss: 20.6015   val Loss: 25.0434   time: 128.21s   best: 24.4882
2023-11-19 00:39:54,768:INFO:  Epoch 384/500:  train Loss: 20.6199   val Loss: 25.3056   time: 128.19s   best: 24.4882
2023-11-19 00:40:55,607:INFO:  Epoch 470/500:  train Loss: 17.1719   val Loss: 22.6922   time: 426.78s   best: 21.4467
2023-11-19 00:42:03,670:INFO:  Epoch 385/500:  train Loss: 20.6183   val Loss: 25.2073   time: 128.89s   best: 24.4882
2023-11-19 00:44:12,720:INFO:  Epoch 386/500:  train Loss: 20.5678   val Loss: 25.2589   time: 129.04s   best: 24.4882
2023-11-19 00:46:21,245:INFO:  Epoch 387/500:  train Loss: 21.0289   val Loss: 25.4340   time: 128.52s   best: 24.4882
2023-11-19 00:48:01,079:INFO:  Epoch 471/500:  train Loss: 17.2002   val Loss: 22.6541   time: 425.46s   best: 21.4467
2023-11-19 00:48:29,222:INFO:  Epoch 388/500:  train Loss: 20.4965   val Loss: 25.4435   time: 127.96s   best: 24.4882
2023-11-19 00:50:37,323:INFO:  Epoch 389/500:  train Loss: 20.9162   val Loss: 25.1523   time: 128.09s   best: 24.4882
2023-11-19 00:52:45,122:INFO:  Epoch 390/500:  train Loss: 20.6065   val Loss: 25.9465   time: 127.79s   best: 24.4882
2023-11-19 00:54:53,159:INFO:  Epoch 391/500:  train Loss: 20.5341   val Loss: 24.8083   time: 128.03s   best: 24.4882
2023-11-19 00:55:08,599:INFO:  Epoch 472/500:  train Loss: 17.1283   val Loss: 23.0466   time: 427.49s   best: 21.4467
2023-11-19 00:57:01,102:INFO:  Epoch 392/500:  train Loss: 20.5410   val Loss: 25.9843   time: 127.93s   best: 24.4882
2023-11-19 00:59:09,151:INFO:  Epoch 393/500:  train Loss: 20.5663   val Loss: 24.9645   time: 128.02s   best: 24.4882
2023-11-19 01:01:17,833:INFO:  Epoch 394/500:  train Loss: 20.5041   val Loss: 25.0911   time: 128.67s   best: 24.4882
2023-11-19 01:02:12,741:INFO:  Epoch 473/500:  train Loss: 17.3820   val Loss: 22.1918   time: 424.12s   best: 21.4467
2023-11-19 01:03:25,702:INFO:  Epoch 395/500:  train Loss: 20.5960   val Loss: 24.9082   time: 127.86s   best: 24.4882
2023-11-19 01:05:34,373:INFO:  Epoch 396/500:  train Loss: 20.3973   val Loss: 25.1718   time: 128.64s   best: 24.4882
2023-11-19 01:07:42,228:INFO:  Epoch 397/500:  train Loss: 20.4639   val Loss: 25.6204   time: 127.85s   best: 24.4882
2023-11-19 01:09:19,927:INFO:  Epoch 474/500:  train Loss: 17.1627   val Loss: 22.2458   time: 427.17s   best: 21.4467
2023-11-19 01:09:50,282:INFO:  Epoch 398/500:  train Loss: 20.7866   val Loss: 25.3364   time: 128.04s   best: 24.4882
2023-11-19 01:11:58,198:INFO:  Epoch 399/500:  train Loss: 20.7626   val Loss: 25.3121   time: 127.89s   best: 24.4882
2023-11-19 01:14:06,137:INFO:  Epoch 400/500:  train Loss: 20.3957   val Loss: 26.5137   time: 127.93s   best: 24.4882
2023-11-19 01:16:13,712:INFO:  Epoch 401/500:  train Loss: 20.7690   val Loss: 24.7034   time: 127.56s   best: 24.4882
2023-11-19 01:16:27,286:INFO:  Epoch 475/500:  train Loss: 17.4873   val Loss: 22.2462   time: 427.32s   best: 21.4467
2023-11-19 01:18:21,327:INFO:  Epoch 402/500:  train Loss: 20.5276   val Loss: 25.4320   time: 127.61s   best: 24.4882
2023-11-19 01:20:28,952:INFO:  Epoch 403/500:  train Loss: 20.6143   val Loss: 25.5677   time: 127.61s   best: 24.4882
2023-11-19 01:22:36,832:INFO:  Epoch 404/500:  train Loss: 20.5149   val Loss: 27.7711   time: 127.87s   best: 24.4882
2023-11-19 01:23:34,416:INFO:  Epoch 476/500:  train Loss: 17.1496   val Loss: 22.2292   time: 427.10s   best: 21.4467
2023-11-19 01:24:45,236:INFO:  Epoch 405/500:  train Loss: 20.6690   val Loss: 25.0052   time: 128.40s   best: 24.4882
2023-11-19 01:26:52,976:INFO:  Epoch 406/500:  train Loss: 20.5650   val Loss: 25.3605   time: 127.73s   best: 24.4882
2023-11-19 01:29:01,419:INFO:  Epoch 407/500:  train Loss: 20.4405   val Loss: 28.1353   time: 128.43s   best: 24.4882
2023-11-19 01:30:40,844:INFO:  Epoch 477/500:  train Loss: 17.1947   val Loss: 22.8843   time: 426.42s   best: 21.4467
2023-11-19 01:31:09,712:INFO:  Epoch 408/500:  train Loss: 20.5976   val Loss: 26.6294   time: 128.28s   best: 24.4882
2023-11-19 01:33:17,518:INFO:  Epoch 409/500:  train Loss: 20.6505   val Loss: 24.8512   time: 127.80s   best: 24.4882
2023-11-19 01:35:25,163:INFO:  Epoch 410/500:  train Loss: 20.6516   val Loss: 26.8200   time: 127.64s   best: 24.4882
2023-11-19 01:37:33,016:INFO:  Epoch 411/500:  train Loss: 20.8010   val Loss: 25.0406   time: 127.85s   best: 24.4882
2023-11-19 01:37:45,235:INFO:  Epoch 478/500:  train Loss: 17.2043   val Loss: 22.2482   time: 424.38s   best: 21.4467
2023-11-19 01:39:40,795:INFO:  Epoch 412/500:  train Loss: 20.5244   val Loss: 24.8627   time: 127.78s   best: 24.4882
2023-11-19 01:41:49,373:INFO:  Epoch 413/500:  train Loss: 20.5809   val Loss: 27.9868   time: 128.55s   best: 24.4882
2023-11-19 01:43:57,063:INFO:  Epoch 414/500:  train Loss: 20.4294   val Loss: 24.9636   time: 127.68s   best: 24.4882
2023-11-19 01:44:55,491:INFO:  Epoch 479/500:  train Loss: 17.4663   val Loss: 23.4662   time: 430.19s   best: 21.4467
2023-11-19 01:46:05,575:INFO:  Epoch 415/500:  train Loss: 20.2992   val Loss: 24.7466   time: 128.50s   best: 24.4882
2023-11-19 01:48:13,419:INFO:  Epoch 416/500:  train Loss: 20.6806   val Loss: 24.7814   time: 127.82s   best: 24.4882
2023-11-19 01:50:21,620:INFO:  Epoch 417/500:  train Loss: 20.6425   val Loss: 25.0230   time: 128.18s   best: 24.4882
2023-11-19 01:52:04,065:INFO:  Epoch 480/500:  train Loss: 17.2654   val Loss: 22.7229   time: 428.55s   best: 21.4467
2023-11-19 01:52:29,844:INFO:  Epoch 418/500:  train Loss: 20.3368   val Loss: 25.1272   time: 128.22s   best: 24.4882
2023-11-19 01:54:37,998:INFO:  Epoch 419/500:  train Loss: 20.3040   val Loss: 25.0457   time: 128.14s   best: 24.4882
2023-11-19 01:56:46,338:INFO:  Epoch 420/500:  train Loss: 20.3391   val Loss: 25.0138   time: 128.33s   best: 24.4882
2023-11-19 01:58:54,092:INFO:  Epoch 421/500:  train Loss: 20.2809   val Loss: 25.0175   time: 127.75s   best: 24.4882
2023-11-19 01:59:09,031:INFO:  Epoch 481/500:  train Loss: 17.2302   val Loss: 22.4931   time: 424.95s   best: 21.4467
2023-11-19 02:01:02,000:INFO:  Epoch 422/500:  train Loss: 20.4023   val Loss: 25.2033   time: 127.89s   best: 24.4882
2023-11-19 02:03:10,121:INFO:  Epoch 423/500:  train Loss: 20.4253   val Loss: 24.9527   time: 128.10s   best: 24.4882
2023-11-19 02:05:17,940:INFO:  Epoch 424/500:  train Loss: 20.1837   val Loss: 25.4464   time: 127.81s   best: 24.4882
2023-11-19 02:06:17,693:INFO:  Epoch 482/500:  train Loss: 17.5572   val Loss: 28.1747   time: 428.58s   best: 21.4467
2023-11-19 02:07:25,993:INFO:  Epoch 425/500:  train Loss: 20.2145   val Loss: 25.0890   time: 128.05s   best: 24.4882
2023-11-19 02:09:34,325:INFO:  Epoch 426/500:  train Loss: 20.4123   val Loss: 24.8990   time: 128.30s   best: 24.4882
2023-11-19 02:11:42,259:INFO:  Epoch 427/500:  train Loss: 20.2069   val Loss: 24.6694   time: 127.92s   best: 24.4882
2023-11-19 02:13:23,158:INFO:  Epoch 483/500:  train Loss: 17.1648   val Loss: 22.7922   time: 425.44s   best: 21.4467
2023-11-19 02:13:50,011:INFO:  Epoch 428/500:  train Loss: 20.2755   val Loss: 25.0145   time: 127.75s   best: 24.4882
2023-11-19 02:15:57,896:INFO:  Epoch 429/500:  train Loss: 20.1529   val Loss: 25.1669   time: 127.85s   best: 24.4882
2023-11-19 02:18:05,495:INFO:  Epoch 430/500:  train Loss: 20.5885   val Loss: 28.3190   time: 127.59s   best: 24.4882
2023-11-19 02:20:13,164:INFO:  Epoch 431/500:  train Loss: 20.3155   val Loss: 25.6715   time: 127.66s   best: 24.4882
2023-11-19 02:20:28,196:INFO:  Epoch 484/500:  train Loss: 17.1631   val Loss: 22.1781   time: 425.00s   best: 21.4467
2023-11-19 02:22:21,727:INFO:  Epoch 432/500:  train Loss: 20.1494   val Loss: 24.7991   time: 128.55s   best: 24.4882
2023-11-19 02:24:29,601:INFO:  Epoch 433/500:  train Loss: 20.3287   val Loss: 25.3224   time: 127.85s   best: 24.4882
2023-11-19 02:26:37,017:INFO:  Epoch 434/500:  train Loss: 20.2073   val Loss: 25.0866   time: 127.42s   best: 24.4882
2023-11-19 02:27:34,137:INFO:  Epoch 485/500:  train Loss: 17.2831   val Loss: 22.4659   time: 425.93s   best: 21.4467
2023-11-19 02:28:44,780:INFO:  Epoch 435/500:  train Loss: 20.1812   val Loss: 24.9937   time: 127.75s   best: 24.4882
2023-11-19 02:30:53,185:INFO:  Epoch 436/500:  train Loss: 20.1956   val Loss: 25.9684   time: 128.38s   best: 24.4882
2023-11-19 02:33:01,269:INFO:  Epoch 437/500:  train Loss: 20.1947   val Loss: 25.8616   time: 128.08s   best: 24.4882
2023-11-19 02:34:44,504:INFO:  Epoch 486/500:  train Loss: 17.1224   val Loss: 22.4092   time: 430.35s   best: 21.4467
2023-11-19 02:35:08,922:INFO:  Epoch 438/500:  train Loss: 21.0176   val Loss: 25.6242   time: 127.65s   best: 24.4882
2023-11-19 02:37:17,188:INFO:  Epoch 439/500:  train Loss: 20.2291   val Loss: 24.6093   time: 128.25s   best: 24.4882
2023-11-19 02:39:24,807:INFO:  Epoch 440/500:  train Loss: 20.1819   val Loss: 25.0368   time: 127.62s   best: 24.4882
2023-11-19 02:41:32,504:INFO:  Epoch 441/500:  train Loss: 20.2700   val Loss: 25.0727   time: 127.68s   best: 24.4882
2023-11-19 02:41:50,975:INFO:  Epoch 487/500:  train Loss: 17.2372   val Loss: 22.5171   time: 426.45s   best: 21.4467
2023-11-19 02:43:40,303:INFO:  Epoch 442/500:  train Loss: 20.2028   val Loss: 24.8708   time: 127.80s   best: 24.4882
2023-11-19 02:45:48,732:INFO:  Epoch 443/500:  train Loss: 21.6309   val Loss: 25.5265   time: 128.41s   best: 24.4882
2023-11-19 02:47:57,607:INFO:  Epoch 444/500:  train Loss: 20.3315   val Loss: 25.4385   time: 128.87s   best: 24.4882
2023-11-19 02:48:59,957:INFO:  Epoch 488/500:  train Loss: 17.3915   val Loss: 22.1906   time: 428.96s   best: 21.4467
2023-11-19 02:50:05,692:INFO:  Epoch 445/500:  train Loss: 20.1435   val Loss: 25.0022   time: 128.08s   best: 24.4882
2023-11-19 02:52:13,384:INFO:  Epoch 446/500:  train Loss: 20.1017   val Loss: 24.9387   time: 127.67s   best: 24.4882
2023-11-19 02:54:21,357:INFO:  Epoch 447/500:  train Loss: 20.1282   val Loss: 25.1450   time: 127.96s   best: 24.4882
2023-11-19 02:56:08,594:INFO:  Epoch 489/500:  train Loss: 17.1755   val Loss: 22.6474   time: 428.61s   best: 21.4467
2023-11-19 02:56:30,341:INFO:  Epoch 448/500:  train Loss: 20.0458   val Loss: 25.5132   time: 128.97s   best: 24.4882
2023-11-19 02:58:38,335:INFO:  Epoch 449/500:  train Loss: 20.0459   val Loss: 25.1111   time: 127.98s   best: 24.4882
2023-11-19 03:00:47,125:INFO:  Epoch 450/500:  train Loss: 20.1454   val Loss: 25.3347   time: 128.78s   best: 24.4882
2023-11-19 03:02:56,236:INFO:  Epoch 451/500:  train Loss: 20.2644   val Loss: 25.1521   time: 129.11s   best: 24.4882
2023-11-19 03:03:17,450:INFO:  Epoch 490/500:  train Loss: 17.2711   val Loss: 22.1141   time: 428.83s   best: 21.4467
2023-11-19 03:05:04,496:INFO:  Epoch 452/500:  train Loss: 20.3055   val Loss: 25.0437   time: 128.26s   best: 24.4882
2023-11-19 03:07:12,150:INFO:  Epoch 453/500:  train Loss: 20.0263   val Loss: 25.2516   time: 127.63s   best: 24.4882
2023-11-19 03:09:19,753:INFO:  Epoch 454/500:  train Loss: 20.2157   val Loss: 28.3472   time: 127.60s   best: 24.4882
2023-11-19 03:10:21,806:INFO:  Epoch 491/500:  train Loss: 17.2057   val Loss: 22.0506   time: 424.32s   best: 21.4467
2023-11-19 03:11:28,116:INFO:  Epoch 455/500:  train Loss: 20.1873   val Loss: 24.8212   time: 128.35s   best: 24.4882
2023-11-19 03:13:35,829:INFO:  Epoch 456/500:  train Loss: 20.1005   val Loss: 25.3057   time: 127.68s   best: 24.4882
2023-11-19 03:15:43,397:INFO:  Epoch 457/500:  train Loss: 19.9351   val Loss: 24.8923   time: 127.55s   best: 24.4882
2023-11-19 03:17:26,503:INFO:  Epoch 492/500:  train Loss: 17.3102   val Loss: 23.2785   time: 424.67s   best: 21.4467
2023-11-19 03:17:51,372:INFO:  Epoch 458/500:  train Loss: 19.9811   val Loss: 25.1335   time: 127.97s   best: 24.4882
2023-11-19 03:19:59,508:INFO:  Epoch 459/500:  train Loss: 20.5184   val Loss: 25.3968   time: 128.13s   best: 24.4882
2023-11-19 03:22:07,368:INFO:  Epoch 460/500:  train Loss: 20.1597   val Loss: 25.3126   time: 127.86s   best: 24.4882
2023-11-19 03:24:15,483:INFO:  Epoch 461/500:  train Loss: 19.9837   val Loss: 25.5567   time: 128.10s   best: 24.4882
2023-11-19 03:24:35,326:INFO:  Epoch 493/500:  train Loss: 17.0568   val Loss: 22.6009   time: 428.82s   best: 21.4467
2023-11-19 03:26:23,698:INFO:  Epoch 462/500:  train Loss: 19.9992   val Loss: 25.2200   time: 128.21s   best: 24.4882
2023-11-19 03:28:31,597:INFO:  Epoch 463/500:  train Loss: 20.3090   val Loss: 25.3287   time: 127.87s   best: 24.4882
2023-11-19 03:30:39,658:INFO:  Epoch 464/500:  train Loss: 19.9546   val Loss: 25.2565   time: 128.05s   best: 24.4882
2023-11-19 03:31:44,248:INFO:  Epoch 494/500:  train Loss: 17.2751   val Loss: 25.4593   time: 428.92s   best: 21.4467
2023-11-19 03:32:48,681:INFO:  Epoch 465/500:  train Loss: 19.9930   val Loss: 25.4286   time: 129.02s   best: 24.4882
2023-11-19 03:34:56,545:INFO:  Epoch 466/500:  train Loss: 19.9727   val Loss: 25.2690   time: 127.85s   best: 24.4882
2023-11-19 03:37:04,559:INFO:  Epoch 467/500:  train Loss: 20.1651   val Loss: 28.0224   time: 128.01s   best: 24.4882
2023-11-19 03:38:48,723:INFO:  Epoch 495/500:  train Loss: 17.3273   val Loss: 22.8238   time: 424.46s   best: 21.4467
2023-11-19 03:39:12,743:INFO:  Epoch 468/500:  train Loss: 20.3320   val Loss: 24.8343   time: 128.18s   best: 24.4882
2023-11-19 03:41:20,574:INFO:  Epoch 469/500:  train Loss: 19.8088   val Loss: 25.1746   time: 127.80s   best: 24.4882
2023-11-19 03:43:28,227:INFO:  Epoch 470/500:  train Loss: 20.2354   val Loss: 25.1008   time: 127.64s   best: 24.4882
2023-11-19 03:45:36,178:INFO:  Epoch 471/500:  train Loss: 19.8306   val Loss: 25.3744   time: 127.94s   best: 24.4882
2023-11-19 03:45:58,048:INFO:  Epoch 496/500:  train Loss: 17.0783   val Loss: 22.6163   time: 429.30s   best: 21.4467
2023-11-19 03:47:44,220:INFO:  Epoch 472/500:  train Loss: 20.4432   val Loss: 25.5611   time: 128.04s   best: 24.4882
2023-11-19 03:49:52,125:INFO:  Epoch 473/500:  train Loss: 19.9219   val Loss: 25.6184   time: 127.89s   best: 24.4882
2023-11-19 03:51:59,848:INFO:  Epoch 474/500:  train Loss: 20.4316   val Loss: 25.9216   time: 127.72s   best: 24.4882
2023-11-19 03:53:05,915:INFO:  Epoch 497/500:  train Loss: 17.0538   val Loss: 22.2347   time: 427.80s   best: 21.4467
2023-11-19 03:54:08,672:INFO:  Epoch 475/500:  train Loss: 19.9858   val Loss: 24.7689   time: 128.82s   best: 24.4882
2023-11-19 03:56:17,008:INFO:  Epoch 476/500:  train Loss: 19.8927   val Loss: 24.7922   time: 128.32s   best: 24.4882
2023-11-19 03:58:24,755:INFO:  Epoch 477/500:  train Loss: 19.9112   val Loss: 24.8630   time: 127.74s   best: 24.4882
2023-11-19 04:00:13,173:INFO:  Epoch 498/500:  train Loss: 17.0578   val Loss: 22.3742   time: 427.23s   best: 21.4467
2023-11-19 04:00:32,806:INFO:  Epoch 478/500:  train Loss: 19.8432   val Loss: 25.2872   time: 128.04s   best: 24.4882
2023-11-19 04:02:41,039:INFO:  Epoch 479/500:  train Loss: 19.8512   val Loss: 25.0939   time: 128.22s   best: 24.4882
2023-11-19 04:04:48,603:INFO:  Epoch 480/500:  train Loss: 20.4298   val Loss: 24.8979   time: 127.55s   best: 24.4882
2023-11-19 04:06:56,390:INFO:  Epoch 481/500:  train Loss: 19.8178   val Loss: 25.3522   time: 127.77s   best: 24.4882
2023-11-19 04:07:17,458:INFO:  Epoch 499/500:  train Loss: 17.1459   val Loss: 22.6507   time: 424.26s   best: 21.4467
2023-11-19 04:09:04,300:INFO:  Epoch 482/500:  train Loss: 19.9715   val Loss: 27.9123   time: 127.91s   best: 24.4882
2023-11-19 04:11:12,834:INFO:  Epoch 483/500:  train Loss: 19.9832   val Loss: 25.2208   time: 128.51s   best: 24.4882
2023-11-19 04:13:20,623:INFO:  Epoch 484/500:  train Loss: 19.7912   val Loss: 25.3128   time: 127.79s   best: 24.4882
2023-11-19 04:14:24,073:INFO:  Epoch 500/500:  train Loss: 17.2649   val Loss: 24.9848   time: 426.61s   best: 21.4467
2023-11-19 04:14:24,079:INFO:  -----> Training complete in 3552m 36s   best validation loss: 21.4467
 
2023-11-19 04:15:28,538:INFO:  Epoch 485/500:  train Loss: 19.8880   val Loss: 25.3915   time: 127.91s   best: 24.4882
2023-11-19 04:17:36,981:INFO:  Epoch 486/500:  train Loss: 19.7603   val Loss: 25.6447   time: 128.42s   best: 24.4882
2023-11-19 04:19:44,785:INFO:  Epoch 487/500:  train Loss: 19.9790   val Loss: 24.9321   time: 127.79s   best: 24.4882
2023-11-19 04:21:52,666:INFO:  Epoch 488/500:  train Loss: 19.7940   val Loss: 24.9531   time: 127.88s   best: 24.4882
2023-11-19 04:24:00,603:INFO:  Epoch 489/500:  train Loss: 20.1691   val Loss: 25.0068   time: 127.92s   best: 24.4882
2023-11-19 04:26:09,102:INFO:  Epoch 490/500:  train Loss: 19.8115   val Loss: 24.6182   time: 128.49s   best: 24.4882
2023-11-19 04:28:17,481:INFO:  Epoch 491/500:  train Loss: 19.9240   val Loss: 25.6551   time: 128.38s   best: 24.4882
2023-11-19 04:30:25,380:INFO:  Epoch 492/500:  train Loss: 19.9950   val Loss: 25.1553   time: 127.90s   best: 24.4882
2023-11-19 04:32:33,561:INFO:  Epoch 493/500:  train Loss: 19.6990   val Loss: 25.2050   time: 128.18s   best: 24.4882
2023-11-19 04:34:41,972:INFO:  Epoch 494/500:  train Loss: 20.1738   val Loss: 24.9564   time: 128.41s   best: 24.4882
2023-11-19 04:36:50,251:INFO:  Epoch 495/500:  train Loss: 19.7533   val Loss: 25.2559   time: 128.27s   best: 24.4882
2023-11-19 04:38:57,841:INFO:  Epoch 496/500:  train Loss: 19.9353   val Loss: 25.6056   time: 127.59s   best: 24.4882
2023-11-19 04:41:05,431:INFO:  Epoch 497/500:  train Loss: 19.6437   val Loss: 24.9159   time: 127.58s   best: 24.4882
2023-11-19 04:43:13,044:INFO:  Epoch 498/500:  train Loss: 19.9788   val Loss: 25.0240   time: 127.60s   best: 24.4882
2023-11-19 04:45:21,012:INFO:  Epoch 499/500:  train Loss: 19.9402   val Loss: 25.0769   time: 127.96s   best: 24.4882
2023-11-19 04:47:28,712:INFO:  Epoch 500/500:  train Loss: 19.7659   val Loss: 25.4397   time: 127.70s   best: 24.4882
2023-11-19 04:47:28,724:INFO:  -----> Training complete in 1072m 11s   best validation loss: 24.4882
 
2023-11-20 20:01:54,330:INFO:  Starting experiment lstm autoencoder perm100 (0.05 dropout)
2023-11-20 20:01:54,332:INFO:  Defining the model
2023-11-20 20:01:54,441:INFO:  Reading the dataset
2023-11-20 20:13:17,429:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_f28b.pt
2023-11-20 20:13:17,450:INFO:  Epoch 1/500:  train Loss: 74.7909   val Loss: 70.2982   time: 432.88s   best: 70.2982
2023-11-20 20:19:31,672:INFO:  Starting experiment lstm autoencoder perm100 (0.05 dropout)
2023-11-20 20:19:31,673:INFO:  Defining the model
2023-11-20 20:19:31,780:INFO:  Reading the dataset
2023-11-20 20:20:27,109:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_f28b.pt
2023-11-20 20:20:27,128:INFO:  Epoch 2/500:  train Loss: 62.3220   val Loss: 58.6674   time: 429.64s   best: 58.6674
2023-11-20 20:25:27,729:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:25:27,758:INFO:  Epoch 1/500:  train Loss: 88.3184   val Loss: 85.2846   time: 129.10s   best: 85.2846
2023-11-20 20:27:35,647:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:27:35,665:INFO:  Epoch 2/500:  train Loss: 80.2254   val Loss: 77.0907   time: 127.88s   best: 77.0907
2023-11-20 20:27:35,989:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_f28b.pt
2023-11-20 20:27:36,008:INFO:  Epoch 3/500:  train Loss: 55.4122   val Loss: 54.8431   time: 428.85s   best: 54.8431
2023-11-20 20:29:43,519:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:29:43,539:INFO:  Epoch 3/500:  train Loss: 74.9893   val Loss: 74.6064   time: 127.84s   best: 74.6064
2023-11-20 20:31:51,729:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:31:51,768:INFO:  Epoch 4/500:  train Loss: 72.1826   val Loss: 70.3348   time: 128.18s   best: 70.3348
2023-11-20 20:34:00,001:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:34:00,020:INFO:  Epoch 5/500:  train Loss: 69.0612   val Loss: 67.3220   time: 128.22s   best: 67.3220
2023-11-20 20:34:44,795:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_f28b.pt
2023-11-20 20:34:44,813:INFO:  Epoch 4/500:  train Loss: 50.1626   val Loss: 53.3470   time: 428.77s   best: 53.3470
2023-11-20 20:36:08,140:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:36:08,161:INFO:  Epoch 6/500:  train Loss: 67.1724   val Loss: 66.0107   time: 128.12s   best: 66.0107
2023-11-20 20:38:15,009:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:38:15,028:INFO:  Epoch 7/500:  train Loss: 65.8375   val Loss: 64.3691   time: 126.84s   best: 64.3691
2023-11-20 20:40:22,744:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:40:22,763:INFO:  Epoch 8/500:  train Loss: 64.7929   val Loss: 63.7191   time: 127.70s   best: 63.7191
2023-11-20 20:41:55,149:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_f28b.pt
2023-11-20 20:41:55,168:INFO:  Epoch 5/500:  train Loss: 45.8694   val Loss: 46.1595   time: 430.33s   best: 46.1595
2023-11-20 20:42:30,987:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:42:31,007:INFO:  Epoch 9/500:  train Loss: 63.7113   val Loss: 62.3171   time: 128.22s   best: 62.3171
2023-11-20 20:44:38,360:INFO:  Epoch 10/500:  train Loss: 62.6966   val Loss: 63.0459   time: 127.35s   best: 62.3171
2023-11-20 20:46:45,833:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:46:45,852:INFO:  Epoch 11/500:  train Loss: 61.6264   val Loss: 60.8284   time: 127.46s   best: 60.8284
2023-11-20 20:48:54,378:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:48:54,397:INFO:  Epoch 12/500:  train Loss: 60.8226   val Loss: 59.6737   time: 128.51s   best: 59.6737
2023-11-20 20:49:06,610:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_f28b.pt
2023-11-20 20:49:06,629:INFO:  Epoch 6/500:  train Loss: 42.4183   val Loss: 41.8196   time: 431.43s   best: 41.8196
2023-11-20 20:51:02,970:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:51:02,992:INFO:  Epoch 13/500:  train Loss: 59.9316   val Loss: 59.4452   time: 128.57s   best: 59.4452
2023-11-20 20:53:10,545:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_a6d3.pt
2023-11-20 20:53:10,564:INFO:  Epoch 14/500:  train Loss: 59.0772   val Loss: 57.9224   time: 127.55s   best: 57.9224
2023-11-20 21:10:47,911:INFO:  Starting experiment lstm autoencoder perm100 (0.05 dropout)
2023-11-20 21:10:47,929:INFO:  Defining the model
2023-11-20 21:10:48,006:INFO:  Reading the dataset
2023-11-20 21:12:26,260:INFO:  Starting experiment lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)
2023-11-20 21:12:26,261:INFO:  Defining the model
2023-11-20 21:12:26,356:INFO:  Reading the dataset
2023-11-20 21:18:41,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:18:42,015:INFO:  Epoch 1/500:  train Loss: 88.8201   val Loss: 84.8790   time: 129.23s   best: 84.8790
2023-11-20 21:20:49,490:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:20:49,508:INFO:  Epoch 2/500:  train Loss: 80.6709   val Loss: 77.5941   time: 127.45s   best: 77.5941
2023-11-20 21:22:33,994:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 21:22:34,018:INFO:  Epoch 1/500:  train Loss: 74.1384   val Loss: 67.0940   time: 434.75s   best: 67.0940
2023-11-20 21:22:57,084:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:22:57,105:INFO:  Epoch 3/500:  train Loss: 73.5337   val Loss: 72.2071   time: 127.56s   best: 72.2071
2023-11-20 21:25:04,313:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:25:04,331:INFO:  Epoch 4/500:  train Loss: 70.4218   val Loss: 69.9663   time: 127.20s   best: 69.9663
2023-11-20 21:27:12,163:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:27:12,181:INFO:  Epoch 5/500:  train Loss: 68.5021   val Loss: 69.4106   time: 127.82s   best: 69.4106
2023-11-20 21:29:19,687:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:29:19,705:INFO:  Epoch 6/500:  train Loss: 66.7672   val Loss: 66.1749   time: 127.49s   best: 66.1749
2023-11-20 21:29:44,451:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 21:29:44,470:INFO:  Epoch 2/500:  train Loss: 62.4129   val Loss: 57.9708   time: 430.42s   best: 57.9708
2023-11-20 21:31:27,590:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:31:27,610:INFO:  Epoch 7/500:  train Loss: 65.5781   val Loss: 64.7950   time: 127.87s   best: 64.7950
2023-11-20 21:33:35,311:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:33:35,331:INFO:  Epoch 8/500:  train Loss: 64.6655   val Loss: 64.2476   time: 127.70s   best: 64.2476
2023-11-20 21:35:42,950:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:35:42,968:INFO:  Epoch 9/500:  train Loss: 63.4325   val Loss: 63.9455   time: 127.60s   best: 63.9455
2023-11-20 21:36:52,798:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 21:36:52,818:INFO:  Epoch 3/500:  train Loss: 54.9330   val Loss: 51.7382   time: 428.32s   best: 51.7382
2023-11-20 21:37:50,701:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:37:50,722:INFO:  Epoch 10/500:  train Loss: 62.5290   val Loss: 62.1668   time: 127.73s   best: 62.1668
2023-11-20 21:39:58,184:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:39:58,202:INFO:  Epoch 11/500:  train Loss: 61.3388   val Loss: 61.4455   time: 127.45s   best: 61.4455
2023-11-20 21:42:05,623:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:42:05,641:INFO:  Epoch 12/500:  train Loss: 60.6949   val Loss: 59.7949   time: 127.42s   best: 59.7949
2023-11-20 21:44:02,526:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 21:44:02,545:INFO:  Epoch 4/500:  train Loss: 48.9904   val Loss: 48.2542   time: 429.69s   best: 48.2542
2023-11-20 21:44:13,266:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:44:13,286:INFO:  Epoch 13/500:  train Loss: 59.5910   val Loss: 59.2001   time: 127.61s   best: 59.2001
2023-11-20 21:46:21,151:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:46:21,169:INFO:  Epoch 14/500:  train Loss: 58.8368   val Loss: 58.3039   time: 127.86s   best: 58.3039
2023-11-20 21:48:29,586:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:48:29,604:INFO:  Epoch 15/500:  train Loss: 57.8683   val Loss: 57.0976   time: 128.40s   best: 57.0976
2023-11-20 21:50:37,307:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:50:37,325:INFO:  Epoch 16/500:  train Loss: 56.8299   val Loss: 56.4816   time: 127.70s   best: 56.4816
2023-11-20 21:51:15,473:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 21:51:15,492:INFO:  Epoch 5/500:  train Loss: 44.8662   val Loss: 44.2726   time: 432.91s   best: 44.2726
2023-11-20 21:52:45,001:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:52:45,021:INFO:  Epoch 17/500:  train Loss: 55.8976   val Loss: 55.3696   time: 127.67s   best: 55.3696
2023-11-20 21:54:52,901:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:54:52,919:INFO:  Epoch 18/500:  train Loss: 54.8612   val Loss: 54.5980   time: 127.87s   best: 54.5980
2023-11-20 21:57:00,638:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:57:00,656:INFO:  Epoch 19/500:  train Loss: 53.9783   val Loss: 53.3325   time: 127.70s   best: 53.3325
2023-11-20 21:58:25,551:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 21:58:25,571:INFO:  Epoch 6/500:  train Loss: 41.5828   val Loss: 43.9537   time: 430.05s   best: 43.9537
2023-11-20 21:59:08,137:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 21:59:08,158:INFO:  Epoch 20/500:  train Loss: 52.9820   val Loss: 53.1942   time: 127.48s   best: 53.1942
2023-11-20 22:01:15,659:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:01:15,677:INFO:  Epoch 21/500:  train Loss: 52.1633   val Loss: 51.6767   time: 127.49s   best: 51.6767
2023-11-20 22:03:23,207:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:03:23,225:INFO:  Epoch 22/500:  train Loss: 51.2405   val Loss: 51.2041   time: 127.51s   best: 51.2041
2023-11-20 22:05:30,682:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:05:30,701:INFO:  Epoch 23/500:  train Loss: 50.3855   val Loss: 49.7733   time: 127.45s   best: 49.7733
2023-11-20 22:05:39,459:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 22:05:39,479:INFO:  Epoch 7/500:  train Loss: 39.1674   val Loss: 39.4041   time: 433.87s   best: 39.4041
2023-11-20 22:07:38,298:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:07:38,318:INFO:  Epoch 24/500:  train Loss: 49.5567   val Loss: 49.2735   time: 127.58s   best: 49.2735
2023-11-20 22:09:46,039:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:09:46,057:INFO:  Epoch 25/500:  train Loss: 48.7901   val Loss: 47.9532   time: 127.72s   best: 47.9532
2023-11-20 22:11:53,485:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:11:53,503:INFO:  Epoch 26/500:  train Loss: 47.7390   val Loss: 47.4817   time: 127.42s   best: 47.4817
2023-11-20 22:12:51,565:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 22:12:51,585:INFO:  Epoch 8/500:  train Loss: 37.1717   val Loss: 37.8448   time: 432.08s   best: 37.8448
2023-11-20 22:14:01,466:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:14:01,488:INFO:  Epoch 27/500:  train Loss: 46.9834   val Loss: 46.7401   time: 127.95s   best: 46.7401
2023-11-20 22:16:08,983:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:16:09,001:INFO:  Epoch 28/500:  train Loss: 46.3733   val Loss: 46.6754   time: 127.49s   best: 46.6754
2023-11-20 22:18:16,838:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:18:16,856:INFO:  Epoch 29/500:  train Loss: 45.5431   val Loss: 46.4919   time: 127.82s   best: 46.4919
2023-11-20 22:20:05,765:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 22:20:05,785:INFO:  Epoch 9/500:  train Loss: 35.7258   val Loss: 35.2366   time: 434.18s   best: 35.2366
2023-11-20 22:20:25,331:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:20:25,352:INFO:  Epoch 30/500:  train Loss: 44.9754   val Loss: 45.3265   time: 128.47s   best: 45.3265
2023-11-20 22:22:33,025:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:22:33,054:INFO:  Epoch 31/500:  train Loss: 44.2781   val Loss: 44.8076   time: 127.66s   best: 44.8076
2023-11-20 22:24:40,661:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:24:40,680:INFO:  Epoch 32/500:  train Loss: 43.6813   val Loss: 44.5624   time: 127.59s   best: 44.5624
2023-11-20 22:26:49,521:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:26:49,539:INFO:  Epoch 33/500:  train Loss: 43.0566   val Loss: 44.2712   time: 128.84s   best: 44.2712
2023-11-20 22:27:18,218:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 22:27:18,237:INFO:  Epoch 10/500:  train Loss: 34.2990   val Loss: 34.4401   time: 432.42s   best: 34.4401
2023-11-20 22:28:57,116:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:28:57,136:INFO:  Epoch 34/500:  train Loss: 42.7243   val Loss: 43.0759   time: 127.57s   best: 43.0759
2023-11-20 22:31:04,811:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:31:04,829:INFO:  Epoch 35/500:  train Loss: 42.2016   val Loss: 42.5843   time: 127.67s   best: 42.5843
2023-11-20 22:33:13,112:INFO:  Epoch 36/500:  train Loss: 41.6347   val Loss: 42.6645   time: 128.27s   best: 42.5843
2023-11-20 22:34:28,580:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 22:34:28,600:INFO:  Epoch 11/500:  train Loss: 33.2900   val Loss: 33.1468   time: 430.33s   best: 33.1468
2023-11-20 22:35:20,908:INFO:  Epoch 37/500:  train Loss: 41.2203   val Loss: 43.6775   time: 127.78s   best: 42.5843
2023-11-20 22:37:28,828:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:37:28,847:INFO:  Epoch 38/500:  train Loss: 40.6819   val Loss: 41.4504   time: 127.91s   best: 41.4504
2023-11-20 22:39:36,579:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:39:36,598:INFO:  Epoch 39/500:  train Loss: 40.9961   val Loss: 41.0476   time: 127.73s   best: 41.0476
2023-11-20 22:41:40,758:INFO:  Epoch 12/500:  train Loss: 32.5280   val Loss: 33.1966   time: 432.16s   best: 33.1468
2023-11-20 22:41:45,091:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:41:45,111:INFO:  Epoch 40/500:  train Loss: 40.0625   val Loss: 40.5086   time: 128.49s   best: 40.5086
2023-11-20 22:43:53,023:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:43:53,042:INFO:  Epoch 41/500:  train Loss: 39.6157   val Loss: 40.1697   time: 127.90s   best: 40.1697
2023-11-20 22:46:00,740:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:46:00,759:INFO:  Epoch 42/500:  train Loss: 39.0704   val Loss: 39.7653   time: 127.69s   best: 39.7653
2023-11-20 22:48:09,725:INFO:  Epoch 43/500:  train Loss: 38.8735   val Loss: 39.9092   time: 128.95s   best: 39.7653
2023-11-20 22:48:52,930:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 22:48:52,949:INFO:  Epoch 13/500:  train Loss: 31.7677   val Loss: 31.8222   time: 432.16s   best: 31.8222
2023-11-20 22:50:17,840:INFO:  Epoch 44/500:  train Loss: 38.9206   val Loss: 40.1189   time: 128.10s   best: 39.7653
2023-11-20 22:52:26,022:INFO:  Epoch 45/500:  train Loss: 38.3589   val Loss: 40.2933   time: 128.17s   best: 39.7653
2023-11-20 22:54:34,194:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:54:34,213:INFO:  Epoch 46/500:  train Loss: 37.8968   val Loss: 39.1262   time: 128.17s   best: 39.1262
2023-11-20 22:56:06,429:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 22:56:06,449:INFO:  Epoch 14/500:  train Loss: 31.0402   val Loss: 31.2060   time: 433.48s   best: 31.2060
2023-11-20 22:56:42,505:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:56:42,526:INFO:  Epoch 47/500:  train Loss: 37.4062   val Loss: 38.8635   time: 128.29s   best: 38.8635
2023-11-20 22:58:50,785:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 22:58:50,805:INFO:  Epoch 48/500:  train Loss: 37.2975   val Loss: 38.0629   time: 128.24s   best: 38.0629
2023-11-20 23:00:59,084:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:00:59,103:INFO:  Epoch 49/500:  train Loss: 37.0019   val Loss: 38.0132   time: 128.26s   best: 38.0132
2023-11-20 23:03:07,274:INFO:  Epoch 50/500:  train Loss: 36.5549   val Loss: 38.0317   time: 128.16s   best: 38.0132
2023-11-20 23:03:15,820:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 23:03:15,841:INFO:  Epoch 15/500:  train Loss: 30.5904   val Loss: 30.5783   time: 429.37s   best: 30.5783
2023-11-20 23:05:15,702:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:05:15,723:INFO:  Epoch 51/500:  train Loss: 36.4310   val Loss: 37.3604   time: 128.41s   best: 37.3604
2023-11-20 23:07:24,090:INFO:  Epoch 52/500:  train Loss: 36.6442   val Loss: 38.4137   time: 128.36s   best: 37.3604
2023-11-20 23:09:32,606:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:09:32,624:INFO:  Epoch 53/500:  train Loss: 36.0600   val Loss: 37.2584   time: 128.51s   best: 37.2584
2023-11-20 23:10:30,520:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 23:10:30,540:INFO:  Epoch 16/500:  train Loss: 30.0127   val Loss: 30.0427   time: 434.67s   best: 30.0427
2023-11-20 23:11:41,417:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:11:41,437:INFO:  Epoch 54/500:  train Loss: 35.6234   val Loss: 36.7100   time: 128.78s   best: 36.7100
2023-11-20 23:13:50,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:13:50,106:INFO:  Epoch 55/500:  train Loss: 35.4271   val Loss: 36.5385   time: 128.65s   best: 36.5385
2023-11-20 23:15:58,117:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:15:58,135:INFO:  Epoch 56/500:  train Loss: 35.1677   val Loss: 35.9571   time: 128.01s   best: 35.9571
2023-11-20 23:17:39,918:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 23:17:39,938:INFO:  Epoch 17/500:  train Loss: 29.5999   val Loss: 29.6143   time: 429.37s   best: 29.6143
2023-11-20 23:18:06,311:INFO:  Epoch 57/500:  train Loss: 34.9838   val Loss: 36.1049   time: 128.18s   best: 35.9571
2023-11-20 23:20:14,488:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:20:14,507:INFO:  Epoch 58/500:  train Loss: 34.6213   val Loss: 35.4643   time: 128.17s   best: 35.4643
2023-11-20 23:22:22,567:INFO:  Epoch 59/500:  train Loss: 34.4426   val Loss: 35.5926   time: 128.06s   best: 35.4643
2023-11-20 23:24:31,544:INFO:  Epoch 60/500:  train Loss: 34.2518   val Loss: 35.4884   time: 128.98s   best: 35.4643
2023-11-20 23:24:49,723:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 23:24:49,743:INFO:  Epoch 18/500:  train Loss: 29.0884   val Loss: 29.2378   time: 429.76s   best: 29.2378
2023-11-20 23:26:40,721:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:26:40,742:INFO:  Epoch 61/500:  train Loss: 33.9858   val Loss: 35.0537   time: 129.17s   best: 35.0537
2023-11-20 23:28:49,620:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:28:49,639:INFO:  Epoch 62/500:  train Loss: 34.0329   val Loss: 34.5923   time: 128.86s   best: 34.5923
2023-11-20 23:30:57,751:INFO:  Epoch 63/500:  train Loss: 33.8713   val Loss: 34.8022   time: 128.10s   best: 34.5923
2023-11-20 23:31:59,549:INFO:  Epoch 19/500:  train Loss: 28.6438   val Loss: 29.9803   time: 429.80s   best: 29.2378
2023-11-20 23:33:06,501:INFO:  Epoch 64/500:  train Loss: 33.4085   val Loss: 34.8034   time: 128.75s   best: 34.5923
2023-11-20 23:35:14,960:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:35:14,979:INFO:  Epoch 65/500:  train Loss: 33.2748   val Loss: 34.2509   time: 128.44s   best: 34.2509
2023-11-20 23:37:23,168:INFO:  Epoch 66/500:  train Loss: 33.0571   val Loss: 34.3880   time: 128.19s   best: 34.2509
2023-11-20 23:39:11,643:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 23:39:11,662:INFO:  Epoch 20/500:  train Loss: 28.5140   val Loss: 29.2032   time: 432.07s   best: 29.2032
2023-11-20 23:39:31,061:INFO:  Epoch 67/500:  train Loss: 32.8805   val Loss: 34.4058   time: 127.89s   best: 34.2509
2023-11-20 23:41:39,134:INFO:  Epoch 68/500:  train Loss: 32.5017   val Loss: 34.3376   time: 128.07s   best: 34.2509
2023-11-20 23:43:47,199:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:43:47,218:INFO:  Epoch 69/500:  train Loss: 32.7647   val Loss: 34.0012   time: 128.06s   best: 34.0012
2023-11-20 23:45:55,298:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:45:55,317:INFO:  Epoch 70/500:  train Loss: 32.4161   val Loss: 33.8367   time: 128.06s   best: 33.8367
2023-11-20 23:46:21,080:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-20 23:46:21,099:INFO:  Epoch 21/500:  train Loss: 28.0973   val Loss: 28.8943   time: 429.40s   best: 28.8943
2023-11-20 23:48:03,181:INFO:  Epoch 71/500:  train Loss: 32.4214   val Loss: 34.1429   time: 127.85s   best: 33.8367
2023-11-20 23:50:11,061:INFO:  Epoch 72/500:  train Loss: 32.4625   val Loss: 34.0003   time: 127.87s   best: 33.8367
2023-11-20 23:52:19,380:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:52:19,399:INFO:  Epoch 73/500:  train Loss: 31.9447   val Loss: 33.6376   time: 128.32s   best: 33.6376
2023-11-20 23:53:33,767:INFO:  Epoch 22/500:  train Loss: 27.5803   val Loss: 29.8662   time: 432.66s   best: 28.8943
2023-11-20 23:54:27,323:INFO:  Epoch 74/500:  train Loss: 31.9859   val Loss: 33.6863   time: 127.91s   best: 33.6376
2023-11-20 23:56:34,969:INFO:  Epoch 75/500:  train Loss: 31.9027   val Loss: 34.0828   time: 127.63s   best: 33.6376
2023-11-20 23:58:43,371:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-20 23:58:43,402:INFO:  Epoch 76/500:  train Loss: 31.5153   val Loss: 33.1260   time: 128.40s   best: 33.1260
2023-11-21 00:00:43,571:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 00:00:43,591:INFO:  Epoch 23/500:  train Loss: 27.2875   val Loss: 28.1272   time: 429.79s   best: 28.1272
2023-11-21 00:00:51,618:INFO:  Epoch 77/500:  train Loss: 31.6611   val Loss: 33.2142   time: 128.20s   best: 33.1260
2023-11-21 00:03:00,124:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:03:00,142:INFO:  Epoch 78/500:  train Loss: 31.3446   val Loss: 32.6998   time: 128.50s   best: 32.6998
2023-11-21 00:05:08,234:INFO:  Epoch 79/500:  train Loss: 31.1953   val Loss: 32.9000   time: 128.09s   best: 32.6998
2023-11-21 00:07:16,519:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:07:16,537:INFO:  Epoch 80/500:  train Loss: 31.1246   val Loss: 32.5628   time: 128.28s   best: 32.5628
2023-11-21 00:07:52,139:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 00:07:52,176:INFO:  Epoch 24/500:  train Loss: 27.0105   val Loss: 27.5799   time: 428.53s   best: 27.5799
2023-11-21 00:09:24,219:INFO:  Epoch 81/500:  train Loss: 31.6839   val Loss: 34.1274   time: 127.67s   best: 32.5628
2023-11-21 00:11:32,546:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:11:32,565:INFO:  Epoch 82/500:  train Loss: 30.8189   val Loss: 32.3684   time: 128.32s   best: 32.3684
2023-11-21 00:13:40,642:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:13:40,661:INFO:  Epoch 83/500:  train Loss: 30.6980   val Loss: 32.2520   time: 128.07s   best: 32.2520
2023-11-21 00:15:01,225:INFO:  Epoch 25/500:  train Loss: 26.9163   val Loss: 27.6138   time: 429.05s   best: 27.5799
2023-11-21 00:15:49,942:INFO:  Epoch 84/500:  train Loss: 31.0614   val Loss: 32.6737   time: 129.27s   best: 32.2520
2023-11-21 00:17:58,590:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:17:58,609:INFO:  Epoch 85/500:  train Loss: 30.6395   val Loss: 31.6184   time: 128.63s   best: 31.6184
2023-11-21 00:20:06,893:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:20:06,911:INFO:  Epoch 86/500:  train Loss: 30.4242   val Loss: 31.4462   time: 128.27s   best: 31.4462
2023-11-21 00:22:09,941:INFO:  Epoch 26/500:  train Loss: 26.5675   val Loss: 28.3928   time: 428.70s   best: 27.5799
2023-11-21 00:22:15,737:INFO:  Epoch 87/500:  train Loss: 30.3548   val Loss: 32.3683   time: 128.82s   best: 31.4462
2023-11-21 00:24:24,878:INFO:  Epoch 88/500:  train Loss: 30.2403   val Loss: 31.6690   time: 129.14s   best: 31.4462
2023-11-21 00:26:34,314:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:26:34,333:INFO:  Epoch 89/500:  train Loss: 30.0539   val Loss: 31.3554   time: 129.43s   best: 31.3554
2023-11-21 00:28:42,613:INFO:  Epoch 90/500:  train Loss: 30.3188   val Loss: 31.5518   time: 128.27s   best: 31.3554
2023-11-21 00:29:19,122:INFO:  Epoch 27/500:  train Loss: 26.3828   val Loss: 28.0808   time: 429.17s   best: 27.5799
2023-11-21 00:30:51,514:INFO:  Epoch 91/500:  train Loss: 30.0923   val Loss: 31.4251   time: 128.89s   best: 31.3554
2023-11-21 00:33:00,796:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:33:00,815:INFO:  Epoch 92/500:  train Loss: 29.6871   val Loss: 31.3210   time: 129.28s   best: 31.3210
2023-11-21 00:35:09,801:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:35:09,820:INFO:  Epoch 93/500:  train Loss: 29.9855   val Loss: 31.0602   time: 128.97s   best: 31.0602
2023-11-21 00:36:27,618:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 00:36:27,638:INFO:  Epoch 28/500:  train Loss: 26.3216   val Loss: 27.2363   time: 428.48s   best: 27.2363
2023-11-21 00:37:17,972:INFO:  Epoch 94/500:  train Loss: 29.7172   val Loss: 38.7536   time: 128.14s   best: 31.0602
2023-11-21 00:39:26,930:INFO:  Epoch 95/500:  train Loss: 30.3543   val Loss: 31.6570   time: 128.95s   best: 31.0602
2023-11-21 00:41:34,953:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:41:34,972:INFO:  Epoch 96/500:  train Loss: 29.3705   val Loss: 30.9169   time: 128.01s   best: 30.9169
2023-11-21 00:43:40,564:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 00:43:40,583:INFO:  Epoch 29/500:  train Loss: 25.9237   val Loss: 26.9062   time: 432.92s   best: 26.9062
2023-11-21 00:43:43,393:INFO:  Epoch 97/500:  train Loss: 29.5531   val Loss: 31.1190   time: 128.42s   best: 30.9169
2023-11-21 00:45:51,779:INFO:  Epoch 98/500:  train Loss: 29.8049   val Loss: 33.8886   time: 128.37s   best: 30.9169
2023-11-21 00:48:00,636:INFO:  Epoch 99/500:  train Loss: 29.4749   val Loss: 31.2091   time: 128.85s   best: 30.9169
2023-11-21 00:50:08,850:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:50:08,868:INFO:  Epoch 100/500:  train Loss: 29.0862   val Loss: 30.7335   time: 128.21s   best: 30.7335
2023-11-21 00:50:49,282:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 00:50:49,302:INFO:  Epoch 30/500:  train Loss: 25.6184   val Loss: 26.7815   time: 428.69s   best: 26.7815
2023-11-21 00:52:17,460:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:52:17,481:INFO:  Epoch 101/500:  train Loss: 29.1989   val Loss: 30.1626   time: 128.59s   best: 30.1626
2023-11-21 00:54:25,493:INFO:  Epoch 102/500:  train Loss: 29.2340   val Loss: 30.3192   time: 128.01s   best: 30.1626
2023-11-21 00:56:33,402:INFO:  Epoch 103/500:  train Loss: 28.8279   val Loss: 30.4050   time: 127.90s   best: 30.1626
2023-11-21 00:58:01,622:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 00:58:01,642:INFO:  Epoch 31/500:  train Loss: 25.6827   val Loss: 26.3837   time: 432.30s   best: 26.3837
2023-11-21 00:58:40,982:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 00:58:41,003:INFO:  Epoch 104/500:  train Loss: 28.9839   val Loss: 29.8287   time: 127.57s   best: 29.8287
2023-11-21 01:00:50,308:INFO:  Epoch 105/500:  train Loss: 28.6269   val Loss: 30.7140   time: 129.30s   best: 29.8287
2023-11-21 01:02:58,574:INFO:  Epoch 106/500:  train Loss: 28.7664   val Loss: 30.8275   time: 128.27s   best: 29.8287
2023-11-21 01:05:06,624:INFO:  Epoch 107/500:  train Loss: 28.7590   val Loss: 31.0249   time: 128.05s   best: 29.8287
2023-11-21 01:05:14,418:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 01:05:14,437:INFO:  Epoch 32/500:  train Loss: 25.3388   val Loss: 26.3678   time: 432.77s   best: 26.3678
2023-11-21 01:07:14,606:INFO:  Epoch 108/500:  train Loss: 28.3807   val Loss: 30.3805   time: 127.98s   best: 29.8287
2023-11-21 01:09:23,161:INFO:  Epoch 109/500:  train Loss: 28.3609   val Loss: 30.2943   time: 128.55s   best: 29.8287
2023-11-21 01:11:31,448:INFO:  Epoch 110/500:  train Loss: 28.2603   val Loss: 30.3373   time: 128.28s   best: 29.8287
2023-11-21 01:12:27,526:INFO:  Epoch 33/500:  train Loss: 25.1728   val Loss: 26.8880   time: 433.09s   best: 26.3678
2023-11-21 01:13:40,439:INFO:  Epoch 111/500:  train Loss: 28.3763   val Loss: 30.0402   time: 128.99s   best: 29.8287
2023-11-21 01:15:48,280:INFO:  Epoch 112/500:  train Loss: 28.1956   val Loss: 32.5158   time: 127.84s   best: 29.8287
2023-11-21 01:17:56,428:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 01:17:56,447:INFO:  Epoch 113/500:  train Loss: 28.0971   val Loss: 29.8226   time: 128.13s   best: 29.8226
2023-11-21 01:19:36,467:INFO:  Epoch 34/500:  train Loss: 25.0447   val Loss: 26.9063   time: 428.94s   best: 26.3678
2023-11-21 01:20:03,608:INFO:  Epoch 114/500:  train Loss: 28.0958   val Loss: 29.8784   time: 127.15s   best: 29.8226
2023-11-21 01:22:11,204:INFO:  Epoch 115/500:  train Loss: 28.0515   val Loss: 30.0293   time: 127.59s   best: 29.8226
2023-11-21 01:24:19,613:INFO:  Epoch 116/500:  train Loss: 27.8776   val Loss: 31.7899   time: 128.40s   best: 29.8226
2023-11-21 01:26:27,154:INFO:  Epoch 117/500:  train Loss: 27.8571   val Loss: 29.8413   time: 127.54s   best: 29.8226
2023-11-21 01:26:47,339:INFO:  Epoch 35/500:  train Loss: 24.8030   val Loss: 26.7205   time: 430.87s   best: 26.3678
2023-11-21 01:28:35,020:INFO:  Epoch 118/500:  train Loss: 27.8314   val Loss: 30.1572   time: 127.85s   best: 29.8226
2023-11-21 01:30:43,160:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 01:30:43,178:INFO:  Epoch 119/500:  train Loss: 27.6081   val Loss: 29.5306   time: 128.12s   best: 29.5306
2023-11-21 01:32:51,077:INFO:  Epoch 120/500:  train Loss: 27.8199   val Loss: 29.9587   time: 127.89s   best: 29.5306
2023-11-21 01:33:57,215:INFO:  Epoch 36/500:  train Loss: 24.9795   val Loss: 26.4560   time: 429.87s   best: 26.3678
2023-11-21 01:34:59,527:INFO:  Epoch 121/500:  train Loss: 27.6770   val Loss: 29.7891   time: 128.45s   best: 29.5306
2023-11-21 01:37:07,354:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 01:37:07,373:INFO:  Epoch 122/500:  train Loss: 27.5953   val Loss: 29.4016   time: 127.81s   best: 29.4016
2023-11-21 01:39:16,050:INFO:  Epoch 123/500:  train Loss: 27.3630   val Loss: 29.8183   time: 128.68s   best: 29.4016
2023-11-21 01:41:06,231:INFO:  Epoch 37/500:  train Loss: 24.5521   val Loss: 27.3283   time: 429.00s   best: 26.3678
2023-11-21 01:41:24,406:INFO:  Epoch 124/500:  train Loss: 27.4031   val Loss: 29.5923   time: 128.35s   best: 29.4016
2023-11-21 01:43:32,837:INFO:  Epoch 125/500:  train Loss: 27.5409   val Loss: 32.2490   time: 128.42s   best: 29.4016
2023-11-21 01:45:41,038:INFO:  Epoch 126/500:  train Loss: 28.1259   val Loss: 29.5016   time: 128.19s   best: 29.4016
2023-11-21 01:47:48,757:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 01:47:48,775:INFO:  Epoch 127/500:  train Loss: 27.4633   val Loss: 28.9405   time: 127.70s   best: 28.9405
2023-11-21 01:48:14,866:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 01:48:14,898:INFO:  Epoch 38/500:  train Loss: 24.4965   val Loss: 26.3039   time: 428.62s   best: 26.3039
2023-11-21 01:49:57,184:INFO:  Epoch 128/500:  train Loss: 27.1210   val Loss: 28.9791   time: 128.41s   best: 28.9405
2023-11-21 01:52:06,156:INFO:  Epoch 129/500:  train Loss: 27.2407   val Loss: 29.3111   time: 128.97s   best: 28.9405
2023-11-21 01:54:14,618:INFO:  Epoch 130/500:  train Loss: 27.1022   val Loss: 29.0656   time: 128.45s   best: 28.9405
2023-11-21 01:55:23,931:INFO:  Epoch 39/500:  train Loss: 24.3342   val Loss: 26.3039   time: 429.03s   best: 26.3039
2023-11-21 01:56:23,301:INFO:  Epoch 131/500:  train Loss: 26.8380   val Loss: 29.1168   time: 128.67s   best: 28.9405
2023-11-21 01:58:32,131:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 01:58:32,149:INFO:  Epoch 132/500:  train Loss: 27.6430   val Loss: 28.9334   time: 128.80s   best: 28.9334
2023-11-21 02:00:40,366:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 02:00:40,385:INFO:  Epoch 133/500:  train Loss: 27.2237   val Loss: 28.7700   time: 128.21s   best: 28.7700
2023-11-21 02:02:32,386:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 02:02:32,406:INFO:  Epoch 40/500:  train Loss: 24.1611   val Loss: 25.6860   time: 428.45s   best: 25.6860
2023-11-21 02:02:48,180:INFO:  Epoch 134/500:  train Loss: 26.6569   val Loss: 28.9584   time: 127.78s   best: 28.7700
2023-11-21 02:04:56,342:INFO:  Epoch 135/500:  train Loss: 27.4044   val Loss: 29.6490   time: 128.16s   best: 28.7700
2023-11-21 02:07:03,820:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 02:07:03,839:INFO:  Epoch 136/500:  train Loss: 26.8973   val Loss: 28.7013   time: 127.46s   best: 28.7013
2023-11-21 02:09:11,531:INFO:  Epoch 137/500:  train Loss: 26.8184   val Loss: 29.0565   time: 127.69s   best: 28.7013
2023-11-21 02:09:41,827:INFO:  Epoch 41/500:  train Loss: 23.9927   val Loss: 25.8916   time: 429.41s   best: 25.6860
2023-11-21 02:11:18,998:INFO:  Epoch 138/500:  train Loss: 27.0321   val Loss: 28.8216   time: 127.45s   best: 28.7013
2023-11-21 02:13:26,742:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 02:13:26,761:INFO:  Epoch 139/500:  train Loss: 26.5632   val Loss: 28.6326   time: 127.73s   best: 28.6326
2023-11-21 02:15:34,385:INFO:  Epoch 140/500:  train Loss: 26.5999   val Loss: 29.2603   time: 127.62s   best: 28.6326
2023-11-21 02:16:53,804:INFO:  Epoch 42/500:  train Loss: 23.9369   val Loss: 25.6896   time: 431.95s   best: 25.6860
2023-11-21 02:17:41,759:INFO:  Epoch 141/500:  train Loss: 26.4618   val Loss: 28.6711   time: 127.37s   best: 28.6326
2023-11-21 02:19:50,269:INFO:  Epoch 142/500:  train Loss: 26.6846   val Loss: 28.9401   time: 128.50s   best: 28.6326
2023-11-21 02:21:58,495:INFO:  Epoch 143/500:  train Loss: 26.2693   val Loss: 29.1406   time: 128.23s   best: 28.6326
2023-11-21 02:24:04,665:INFO:  Epoch 43/500:  train Loss: 23.7727   val Loss: 25.8386   time: 430.85s   best: 25.6860
2023-11-21 02:24:06,131:INFO:  Epoch 144/500:  train Loss: 26.5738   val Loss: 33.0571   time: 127.63s   best: 28.6326
2023-11-21 02:26:14,856:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 02:26:14,875:INFO:  Epoch 145/500:  train Loss: 26.3609   val Loss: 28.0860   time: 128.71s   best: 28.0860
2023-11-21 02:28:22,685:INFO:  Epoch 146/500:  train Loss: 26.2410   val Loss: 28.8536   time: 127.81s   best: 28.0860
2023-11-21 02:30:30,663:INFO:  Epoch 147/500:  train Loss: 26.7454   val Loss: 28.7567   time: 127.98s   best: 28.0860
2023-11-21 02:31:17,753:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 02:31:17,773:INFO:  Epoch 44/500:  train Loss: 23.8520   val Loss: 25.4138   time: 433.08s   best: 25.4138
2023-11-21 02:32:38,899:INFO:  Epoch 148/500:  train Loss: 26.1072   val Loss: 30.1694   time: 128.22s   best: 28.0860
2023-11-21 02:34:46,558:INFO:  Epoch 149/500:  train Loss: 26.1417   val Loss: 29.0128   time: 127.66s   best: 28.0860
2023-11-21 02:36:54,402:INFO:  Epoch 150/500:  train Loss: 26.0134   val Loss: 28.4625   time: 127.83s   best: 28.0860
2023-11-21 02:38:26,887:INFO:  Epoch 45/500:  train Loss: 23.6013   val Loss: 25.9710   time: 429.11s   best: 25.4138
2023-11-21 02:39:02,761:INFO:  Epoch 151/500:  train Loss: 26.2040   val Loss: 28.6308   time: 128.36s   best: 28.0860
2023-11-21 02:41:11,297:INFO:  Epoch 152/500:  train Loss: 25.7889   val Loss: 28.1641   time: 128.53s   best: 28.0860
2023-11-21 02:43:19,140:INFO:  Epoch 153/500:  train Loss: 26.0438   val Loss: 28.1251   time: 127.84s   best: 28.0860
2023-11-21 02:45:27,086:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 02:45:27,105:INFO:  Epoch 154/500:  train Loss: 25.7482   val Loss: 27.7897   time: 127.93s   best: 27.7897
2023-11-21 02:45:39,731:INFO:  Epoch 46/500:  train Loss: 23.4500   val Loss: 25.6706   time: 432.82s   best: 25.4138
2023-11-21 02:47:35,878:INFO:  Epoch 155/500:  train Loss: 25.9703   val Loss: 27.9532   time: 128.77s   best: 27.7897
2023-11-21 02:49:43,536:INFO:  Epoch 156/500:  train Loss: 25.7739   val Loss: 27.8504   time: 127.64s   best: 27.7897
2023-11-21 02:51:52,166:INFO:  Epoch 157/500:  train Loss: 25.5234   val Loss: 27.9188   time: 128.63s   best: 27.7897
2023-11-21 02:52:51,651:INFO:  Epoch 47/500:  train Loss: 23.4559   val Loss: 26.4731   time: 431.90s   best: 25.4138
2023-11-21 02:53:59,692:INFO:  Epoch 158/500:  train Loss: 25.7140   val Loss: 27.9956   time: 127.53s   best: 27.7897
2023-11-21 02:56:08,021:INFO:  Epoch 159/500:  train Loss: 26.0807   val Loss: 28.0014   time: 128.32s   best: 27.7897
2023-11-21 02:58:16,366:INFO:  Epoch 160/500:  train Loss: 25.3123   val Loss: 27.9141   time: 128.33s   best: 27.7897
2023-11-21 03:00:02,940:INFO:  Epoch 48/500:  train Loss: 23.3082   val Loss: 25.4863   time: 431.29s   best: 25.4138
2023-11-21 03:00:24,326:INFO:  Epoch 161/500:  train Loss: 25.9110   val Loss: 28.4535   time: 127.95s   best: 27.7897
2023-11-21 03:02:32,723:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 03:02:32,741:INFO:  Epoch 162/500:  train Loss: 25.6997   val Loss: 27.7121   time: 128.38s   best: 27.7121
2023-11-21 03:04:40,540:INFO:  Epoch 163/500:  train Loss: 25.3812   val Loss: 28.1832   time: 127.80s   best: 27.7121
2023-11-21 03:06:48,973:INFO:  Epoch 164/500:  train Loss: 25.3043   val Loss: 27.7891   time: 128.42s   best: 27.7121
2023-11-21 03:07:16,125:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 03:07:16,146:INFO:  Epoch 49/500:  train Loss: 23.2094   val Loss: 25.3396   time: 433.17s   best: 25.3396
2023-11-21 03:08:56,898:INFO:  Epoch 165/500:  train Loss: 25.5822   val Loss: 27.7706   time: 127.92s   best: 27.7121
2023-11-21 03:11:05,052:INFO:  Epoch 166/500:  train Loss: 25.1577   val Loss: 28.4554   time: 128.15s   best: 27.7121
2023-11-21 03:13:13,583:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 03:13:13,611:INFO:  Epoch 167/500:  train Loss: 25.3400   val Loss: 27.5642   time: 128.53s   best: 27.5642
2023-11-21 03:14:29,716:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 03:14:29,736:INFO:  Epoch 50/500:  train Loss: 23.0780   val Loss: 25.2560   time: 433.57s   best: 25.2560
2023-11-21 03:15:21,111:INFO:  Epoch 168/500:  train Loss: 25.3278   val Loss: 28.1786   time: 127.50s   best: 27.5642
2023-11-21 03:17:29,395:INFO:  Epoch 169/500:  train Loss: 25.0933   val Loss: 28.0018   time: 128.27s   best: 27.5642
2023-11-21 03:19:37,201:INFO:  Epoch 170/500:  train Loss: 25.1275   val Loss: 29.0851   time: 127.81s   best: 27.5642
2023-11-21 03:21:43,003:INFO:  Epoch 51/500:  train Loss: 23.2342   val Loss: 25.3514   time: 433.27s   best: 25.2560
2023-11-21 03:21:45,479:INFO:  Epoch 171/500:  train Loss: 25.5273   val Loss: 28.8853   time: 128.28s   best: 27.5642
2023-11-21 03:23:53,087:INFO:  Epoch 172/500:  train Loss: 25.3640   val Loss: 28.7325   time: 127.61s   best: 27.5642
2023-11-21 03:26:01,959:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 03:26:01,978:INFO:  Epoch 173/500:  train Loss: 24.9608   val Loss: 27.4581   time: 128.86s   best: 27.4581
2023-11-21 03:28:10,455:INFO:  Epoch 174/500:  train Loss: 25.0579   val Loss: 27.9284   time: 128.48s   best: 27.4581
2023-11-21 03:28:55,536:INFO:  Epoch 52/500:  train Loss: 22.9101   val Loss: 25.3267   time: 432.53s   best: 25.2560
2023-11-21 03:30:18,909:INFO:  Epoch 175/500:  train Loss: 24.9095   val Loss: 27.5187   time: 128.44s   best: 27.4581
2023-11-21 03:32:26,468:INFO:  Epoch 176/500:  train Loss: 25.0957   val Loss: 28.5768   time: 127.54s   best: 27.4581
2023-11-21 03:34:35,198:INFO:  Epoch 177/500:  train Loss: 24.8824   val Loss: 27.6666   time: 128.72s   best: 27.4581
2023-11-21 03:36:07,039:INFO:  Epoch 53/500:  train Loss: 22.7679   val Loss: 25.2591   time: 431.50s   best: 25.2560
2023-11-21 03:36:43,211:INFO:  Epoch 178/500:  train Loss: 25.0476   val Loss: 27.7142   time: 128.01s   best: 27.4581
2023-11-21 03:38:51,302:INFO:  Epoch 179/500:  train Loss: 24.6618   val Loss: 27.7758   time: 128.08s   best: 27.4581
2023-11-21 03:40:59,340:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 03:40:59,359:INFO:  Epoch 180/500:  train Loss: 24.7257   val Loss: 27.4407   time: 128.02s   best: 27.4407
2023-11-21 03:43:07,578:INFO:  Epoch 181/500:  train Loss: 24.6772   val Loss: 27.4852   time: 128.22s   best: 27.4407
2023-11-21 03:43:17,309:INFO:  Epoch 54/500:  train Loss: 22.8093   val Loss: 25.5092   time: 430.27s   best: 25.2560
2023-11-21 03:45:15,147:INFO:  Epoch 182/500:  train Loss: 24.6432   val Loss: 27.7429   time: 127.56s   best: 27.4407
2023-11-21 03:47:22,828:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 03:47:22,847:INFO:  Epoch 183/500:  train Loss: 25.2286   val Loss: 27.4059   time: 127.66s   best: 27.4059
2023-11-21 03:49:31,393:INFO:  Epoch 184/500:  train Loss: 24.5053   val Loss: 27.4707   time: 128.55s   best: 27.4059
2023-11-21 03:50:27,073:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 03:50:27,093:INFO:  Epoch 55/500:  train Loss: 22.6210   val Loss: 24.8986   time: 429.75s   best: 24.8986
2023-11-21 03:51:39,964:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 03:51:39,985:INFO:  Epoch 185/500:  train Loss: 24.8710   val Loss: 27.3318   time: 128.56s   best: 27.3318
2023-11-21 03:53:47,838:INFO:  Epoch 186/500:  train Loss: 24.5693   val Loss: 27.4394   time: 127.85s   best: 27.3318
2023-11-21 03:55:55,945:INFO:  Epoch 187/500:  train Loss: 24.4369   val Loss: 27.3493   time: 128.10s   best: 27.3318
2023-11-21 03:57:37,861:INFO:  Epoch 56/500:  train Loss: 22.6115   val Loss: 24.9912   time: 430.77s   best: 24.8986
2023-11-21 03:58:04,250:INFO:  Epoch 188/500:  train Loss: 24.8939   val Loss: 27.9454   time: 128.29s   best: 27.3318
2023-11-21 04:00:12,591:INFO:  Epoch 189/500:  train Loss: 24.5201   val Loss: 27.5206   time: 128.33s   best: 27.3318
2023-11-21 04:02:20,875:INFO:  Epoch 190/500:  train Loss: 24.7982   val Loss: 30.6970   time: 128.27s   best: 27.3318
2023-11-21 04:04:28,519:INFO:  Epoch 191/500:  train Loss: 24.4528   val Loss: 27.4336   time: 127.62s   best: 27.3318
2023-11-21 04:04:49,660:INFO:  Epoch 57/500:  train Loss: 22.5273   val Loss: 24.9706   time: 431.79s   best: 24.8986
2023-11-21 04:06:35,871:INFO:  Epoch 192/500:  train Loss: 24.5233   val Loss: 27.4664   time: 127.35s   best: 27.3318
2023-11-21 04:08:44,118:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 04:08:44,136:INFO:  Epoch 193/500:  train Loss: 24.4644   val Loss: 27.2520   time: 128.23s   best: 27.2520
2023-11-21 04:10:52,112:INFO:  Epoch 194/500:  train Loss: 24.1742   val Loss: 28.0562   time: 127.97s   best: 27.2520
2023-11-21 04:12:02,343:INFO:  Epoch 58/500:  train Loss: 22.5462   val Loss: 25.2519   time: 432.68s   best: 24.8986
2023-11-21 04:13:00,017:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 04:13:00,037:INFO:  Epoch 195/500:  train Loss: 24.2536   val Loss: 27.1627   time: 127.89s   best: 27.1627
2023-11-21 04:15:07,291:INFO:  Epoch 196/500:  train Loss: 24.2111   val Loss: 27.1810   time: 127.25s   best: 27.1627
2023-11-21 04:17:14,595:INFO:  Epoch 197/500:  train Loss: 24.1559   val Loss: 27.4837   time: 127.30s   best: 27.1627
2023-11-21 04:19:10,084:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 04:19:10,104:INFO:  Epoch 59/500:  train Loss: 22.3304   val Loss: 24.7068   time: 427.72s   best: 24.7068
2023-11-21 04:19:21,947:INFO:  Epoch 198/500:  train Loss: 24.1417   val Loss: 27.5278   time: 127.35s   best: 27.1627
2023-11-21 04:21:30,335:INFO:  Epoch 199/500:  train Loss: 24.1756   val Loss: 27.3810   time: 128.37s   best: 27.1627
2023-11-21 04:23:38,630:INFO:  Epoch 200/500:  train Loss: 24.3440   val Loss: 27.1729   time: 128.29s   best: 27.1627
2023-11-21 04:25:46,910:INFO:  Epoch 201/500:  train Loss: 24.4434   val Loss: 27.8769   time: 128.27s   best: 27.1627
2023-11-21 04:26:18,186:INFO:  Epoch 60/500:  train Loss: 22.6265   val Loss: 25.2529   time: 428.07s   best: 24.7068
2023-11-21 04:27:55,295:INFO:  Epoch 202/500:  train Loss: 24.0143   val Loss: 27.7166   time: 128.38s   best: 27.1627
2023-11-21 04:30:02,961:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 04:30:02,980:INFO:  Epoch 203/500:  train Loss: 24.1687   val Loss: 27.1380   time: 127.66s   best: 27.1380
2023-11-21 04:32:10,693:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 04:32:10,712:INFO:  Epoch 204/500:  train Loss: 23.9757   val Loss: 27.0138   time: 127.70s   best: 27.0138
2023-11-21 04:33:31,743:INFO:  Epoch 61/500:  train Loss: 22.2432   val Loss: 24.9488   time: 433.54s   best: 24.7068
2023-11-21 04:34:18,164:INFO:  Epoch 205/500:  train Loss: 23.9655   val Loss: 27.1831   time: 127.45s   best: 27.0138
2023-11-21 04:36:26,502:INFO:  Epoch 206/500:  train Loss: 23.8613   val Loss: 27.1957   time: 128.32s   best: 27.0138
2023-11-21 04:38:34,253:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 04:38:34,272:INFO:  Epoch 207/500:  train Loss: 24.0155   val Loss: 26.8351   time: 127.75s   best: 26.8351
2023-11-21 04:40:41,876:INFO:  Epoch 208/500:  train Loss: 23.9359   val Loss: 27.3185   time: 127.60s   best: 26.8351
2023-11-21 04:40:44,383:INFO:  Epoch 62/500:  train Loss: 22.1826   val Loss: 25.2211   time: 432.64s   best: 24.7068
2023-11-21 04:42:49,459:INFO:  Epoch 209/500:  train Loss: 23.8310   val Loss: 27.0619   time: 127.57s   best: 26.8351
2023-11-21 04:44:57,318:INFO:  Epoch 210/500:  train Loss: 23.7672   val Loss: 26.9546   time: 127.86s   best: 26.8351
2023-11-21 04:47:05,317:INFO:  Epoch 211/500:  train Loss: 24.0034   val Loss: 27.1616   time: 127.99s   best: 26.8351
2023-11-21 04:47:53,449:INFO:  Epoch 63/500:  train Loss: 22.2184   val Loss: 24.8773   time: 429.05s   best: 24.7068
2023-11-21 04:49:13,289:INFO:  Epoch 212/500:  train Loss: 23.5939   val Loss: 27.1352   time: 127.96s   best: 26.8351
2023-11-21 04:51:21,708:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 04:51:21,727:INFO:  Epoch 213/500:  train Loss: 23.7120   val Loss: 26.6966   time: 128.40s   best: 26.6966
2023-11-21 04:53:28,739:INFO:  Epoch 214/500:  train Loss: 23.8051   val Loss: 26.7094   time: 127.01s   best: 26.6966
2023-11-21 04:55:06,902:INFO:  Epoch 64/500:  train Loss: 22.0538   val Loss: 25.2140   time: 433.45s   best: 24.7068
2023-11-21 04:55:36,159:INFO:  Epoch 215/500:  train Loss: 23.6396   val Loss: 27.0234   time: 127.41s   best: 26.6966
2023-11-21 04:57:44,138:INFO:  Epoch 216/500:  train Loss: 23.6168   val Loss: 26.8274   time: 127.97s   best: 26.6966
2023-11-21 04:59:52,119:INFO:  Epoch 217/500:  train Loss: 23.8878   val Loss: 29.0418   time: 127.98s   best: 26.6966
2023-11-21 05:01:59,422:INFO:  Epoch 218/500:  train Loss: 23.5212   val Loss: 26.9216   time: 127.29s   best: 26.6966
2023-11-21 05:02:15,866:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 05:02:15,886:INFO:  Epoch 65/500:  train Loss: 21.9600   val Loss: 24.5927   time: 428.94s   best: 24.5927
2023-11-21 05:04:07,010:INFO:  Epoch 219/500:  train Loss: 23.7290   val Loss: 27.0526   time: 127.57s   best: 26.6966
2023-11-21 05:06:14,376:INFO:  Epoch 220/500:  train Loss: 23.4256   val Loss: 27.2482   time: 127.35s   best: 26.6966
2023-11-21 05:08:21,826:INFO:  Epoch 221/500:  train Loss: 24.2187   val Loss: 27.1808   time: 127.45s   best: 26.6966
2023-11-21 05:09:28,912:INFO:  Epoch 66/500:  train Loss: 21.8736   val Loss: 24.9213   time: 433.01s   best: 24.5927
2023-11-21 05:10:29,214:INFO:  Epoch 222/500:  train Loss: 23.9530   val Loss: 27.2882   time: 127.38s   best: 26.6966
2023-11-21 05:12:37,074:INFO:  Epoch 223/500:  train Loss: 23.3843   val Loss: 26.9718   time: 127.85s   best: 26.6966
2023-11-21 05:14:44,497:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 05:14:44,516:INFO:  Epoch 224/500:  train Loss: 23.7159   val Loss: 26.4677   time: 127.42s   best: 26.4677
2023-11-21 05:16:38,011:INFO:  Epoch 67/500:  train Loss: 22.0320   val Loss: 25.1362   time: 429.08s   best: 24.5927
2023-11-21 05:16:52,941:INFO:  Epoch 225/500:  train Loss: 23.2936   val Loss: 26.7670   time: 128.42s   best: 26.4677
2023-11-21 05:19:01,327:INFO:  Epoch 226/500:  train Loss: 23.7926   val Loss: 26.6261   time: 128.37s   best: 26.4677
2023-11-21 05:21:08,982:INFO:  Epoch 227/500:  train Loss: 23.2302   val Loss: 27.3315   time: 127.64s   best: 26.4677
2023-11-21 05:23:17,076:INFO:  Epoch 228/500:  train Loss: 23.4265   val Loss: 26.4714   time: 128.08s   best: 26.4677
2023-11-21 05:23:46,857:INFO:  Epoch 68/500:  train Loss: 21.7893   val Loss: 25.2525   time: 428.83s   best: 24.5927
2023-11-21 05:25:25,346:INFO:  Epoch 229/500:  train Loss: 23.2603   val Loss: 26.7593   time: 128.26s   best: 26.4677
2023-11-21 05:27:32,986:INFO:  Epoch 230/500:  train Loss: 23.3268   val Loss: 26.7250   time: 127.61s   best: 26.4677
2023-11-21 05:29:40,157:INFO:  Epoch 231/500:  train Loss: 23.4674   val Loss: 26.6670   time: 127.17s   best: 26.4677
2023-11-21 05:30:55,529:INFO:  Epoch 69/500:  train Loss: 21.6533   val Loss: 24.6072   time: 428.65s   best: 24.5927
2023-11-21 05:31:48,364:INFO:  Epoch 232/500:  train Loss: 23.4240   val Loss: 26.6740   time: 128.21s   best: 26.4677
2023-11-21 05:33:56,558:INFO:  Epoch 233/500:  train Loss: 23.2741   val Loss: 26.5194   time: 128.18s   best: 26.4677
2023-11-21 05:36:03,617:INFO:  Epoch 234/500:  train Loss: 24.1348   val Loss: 27.5391   time: 127.05s   best: 26.4677
2023-11-21 05:38:10,004:INFO:  Epoch 70/500:  train Loss: 21.8074   val Loss: 25.0294   time: 434.47s   best: 24.5927
2023-11-21 05:38:12,071:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 05:38:12,092:INFO:  Epoch 235/500:  train Loss: 24.3102   val Loss: 26.4412   time: 128.44s   best: 26.4412
2023-11-21 05:40:19,981:INFO:  Epoch 236/500:  train Loss: 23.1671   val Loss: 26.5598   time: 127.89s   best: 26.4412
2023-11-21 05:42:27,762:INFO:  Epoch 237/500:  train Loss: 23.1080   val Loss: 26.9881   time: 127.78s   best: 26.4412
2023-11-21 05:44:35,871:INFO:  Epoch 238/500:  train Loss: 23.1245   val Loss: 26.6915   time: 128.10s   best: 26.4412
2023-11-21 05:45:22,012:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 05:45:22,031:INFO:  Epoch 71/500:  train Loss: 21.7677   val Loss: 24.4155   time: 431.99s   best: 24.4155
2023-11-21 05:46:43,684:INFO:  Epoch 239/500:  train Loss: 23.2951   val Loss: 26.5895   time: 127.80s   best: 26.4412
2023-11-21 05:48:51,346:INFO:  Epoch 240/500:  train Loss: 23.1185   val Loss: 26.5099   time: 127.65s   best: 26.4412
2023-11-21 05:50:58,946:INFO:  Epoch 241/500:  train Loss: 23.3741   val Loss: 27.0306   time: 127.59s   best: 26.4412
2023-11-21 05:52:34,408:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 05:52:34,428:INFO:  Epoch 72/500:  train Loss: 21.5697   val Loss: 24.3237   time: 432.36s   best: 24.3237
2023-11-21 05:53:06,909:INFO:  Epoch 242/500:  train Loss: 23.3068   val Loss: 26.5405   time: 127.95s   best: 26.4412
2023-11-21 05:55:14,243:INFO:  Epoch 243/500:  train Loss: 22.8904   val Loss: 26.4718   time: 127.33s   best: 26.4412
2023-11-21 05:57:21,241:INFO:  Epoch 244/500:  train Loss: 22.9791   val Loss: 26.5127   time: 126.99s   best: 26.4412
2023-11-21 05:59:28,962:INFO:  Epoch 245/500:  train Loss: 22.9232   val Loss: 26.6714   time: 127.71s   best: 26.4412
2023-11-21 05:59:46,172:INFO:  Epoch 73/500:  train Loss: 21.5764   val Loss: 27.0326   time: 431.73s   best: 24.3237
2023-11-21 06:01:36,274:INFO:  Epoch 246/500:  train Loss: 23.1085   val Loss: 27.3168   time: 127.31s   best: 26.4412
2023-11-21 06:03:44,041:INFO:  Epoch 247/500:  train Loss: 23.4451   val Loss: 26.6888   time: 127.76s   best: 26.4412
2023-11-21 06:05:52,002:INFO:  Epoch 248/500:  train Loss: 22.8162   val Loss: 26.8721   time: 127.96s   best: 26.4412
2023-11-21 06:06:59,544:INFO:  Epoch 74/500:  train Loss: 21.4871   val Loss: 24.7405   time: 433.37s   best: 24.3237
2023-11-21 06:07:59,066:INFO:  Epoch 249/500:  train Loss: 22.8704   val Loss: 26.8775   time: 127.05s   best: 26.4412
2023-11-21 06:10:06,429:INFO:  Epoch 250/500:  train Loss: 22.7511   val Loss: 26.5937   time: 127.35s   best: 26.4412
2023-11-21 06:12:14,414:INFO:  Epoch 251/500:  train Loss: 22.7802   val Loss: 27.7637   time: 127.97s   best: 26.4412
2023-11-21 06:14:08,369:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 06:14:08,388:INFO:  Epoch 75/500:  train Loss: 21.3628   val Loss: 24.1039   time: 428.81s   best: 24.1039
2023-11-21 06:14:22,404:INFO:  Epoch 252/500:  train Loss: 22.7862   val Loss: 26.6796   time: 127.99s   best: 26.4412
2023-11-21 06:16:30,340:INFO:  Epoch 253/500:  train Loss: 23.2931   val Loss: 26.5744   time: 127.92s   best: 26.4412
2023-11-21 06:18:38,235:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 06:18:38,254:INFO:  Epoch 254/500:  train Loss: 22.6335   val Loss: 26.3343   time: 127.88s   best: 26.3343
2023-11-21 06:20:45,875:INFO:  Epoch 255/500:  train Loss: 22.7011   val Loss: 26.5363   time: 127.61s   best: 26.3343
2023-11-21 06:21:17,771:INFO:  Epoch 76/500:  train Loss: 21.3286   val Loss: 24.6251   time: 429.37s   best: 24.1039
2023-11-21 06:22:53,227:INFO:  Epoch 256/500:  train Loss: 22.8685   val Loss: 30.7200   time: 127.35s   best: 26.3343
2023-11-21 06:25:00,950:INFO:  Epoch 257/500:  train Loss: 22.7731   val Loss: 26.5824   time: 127.72s   best: 26.3343
2023-11-21 06:27:09,212:INFO:  Epoch 258/500:  train Loss: 22.6626   val Loss: 26.4169   time: 128.25s   best: 26.3343
2023-11-21 06:28:31,719:INFO:  Epoch 77/500:  train Loss: 21.4192   val Loss: 24.1282   time: 433.93s   best: 24.1039
2023-11-21 06:29:17,305:INFO:  Epoch 259/500:  train Loss: 23.9234   val Loss: 26.5704   time: 128.09s   best: 26.3343
2023-11-21 06:31:24,870:INFO:  Epoch 260/500:  train Loss: 22.6065   val Loss: 26.7097   time: 127.56s   best: 26.3343
2023-11-21 06:33:32,570:INFO:  Epoch 261/500:  train Loss: 22.7018   val Loss: 26.5623   time: 127.70s   best: 26.3343
2023-11-21 06:35:40,770:INFO:  Epoch 262/500:  train Loss: 22.5822   val Loss: 26.3390   time: 128.20s   best: 26.3343
2023-11-21 06:35:42,588:INFO:  Epoch 78/500:  train Loss: 21.3096   val Loss: 24.6081   time: 430.87s   best: 24.1039
2023-11-21 06:37:48,426:INFO:  Epoch 263/500:  train Loss: 22.7460   val Loss: 26.5773   time: 127.64s   best: 26.3343
2023-11-21 06:39:56,627:INFO:  Epoch 264/500:  train Loss: 22.5996   val Loss: 26.6743   time: 128.19s   best: 26.3343
2023-11-21 06:42:04,665:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 06:42:04,683:INFO:  Epoch 265/500:  train Loss: 22.4686   val Loss: 25.6843   time: 128.02s   best: 25.6843
2023-11-21 06:42:56,498:INFO:  Epoch 79/500:  train Loss: 21.2525   val Loss: 24.3635   time: 433.91s   best: 24.1039
2023-11-21 06:44:11,926:INFO:  Epoch 266/500:  train Loss: 22.6094   val Loss: 26.4640   time: 127.24s   best: 25.6843
2023-11-21 06:46:19,373:INFO:  Epoch 267/500:  train Loss: 22.4836   val Loss: 26.4171   time: 127.44s   best: 25.6843
2023-11-21 06:48:27,737:INFO:  Epoch 268/500:  train Loss: 22.5502   val Loss: 26.6236   time: 128.35s   best: 25.6843
2023-11-21 06:50:08,954:INFO:  Epoch 80/500:  train Loss: 21.2393   val Loss: 24.2901   time: 432.45s   best: 24.1039
2023-11-21 06:50:35,053:INFO:  Epoch 269/500:  train Loss: 22.4713   val Loss: 26.6064   time: 127.32s   best: 25.6843
2023-11-21 06:52:43,118:INFO:  Epoch 270/500:  train Loss: 22.3833   val Loss: 26.5838   time: 128.05s   best: 25.6843
2023-11-21 06:54:50,514:INFO:  Epoch 271/500:  train Loss: 22.5100   val Loss: 26.3004   time: 127.38s   best: 25.6843
2023-11-21 06:56:57,592:INFO:  Epoch 272/500:  train Loss: 22.2449   val Loss: 26.4434   time: 127.06s   best: 25.6843
2023-11-21 06:57:19,898:INFO:  Epoch 81/500:  train Loss: 21.1941   val Loss: 24.3567   time: 430.94s   best: 24.1039
2023-11-21 06:59:04,859:INFO:  Epoch 273/500:  train Loss: 22.7531   val Loss: 26.6725   time: 127.27s   best: 25.6843
2023-11-21 07:01:13,090:INFO:  Epoch 274/500:  train Loss: 22.3309   val Loss: 27.4062   time: 128.22s   best: 25.6843
2023-11-21 07:03:21,748:INFO:  Epoch 275/500:  train Loss: 22.3690   val Loss: 27.7002   time: 128.65s   best: 25.6843
2023-11-21 07:04:28,955:INFO:  Epoch 82/500:  train Loss: 21.1385   val Loss: 24.8911   time: 429.04s   best: 24.1039
2023-11-21 07:05:30,054:INFO:  Epoch 276/500:  train Loss: 22.3675   val Loss: 26.3198   time: 128.29s   best: 25.6843
2023-11-21 07:07:38,233:INFO:  Epoch 277/500:  train Loss: 22.2534   val Loss: 26.6609   time: 128.16s   best: 25.6843
2023-11-21 07:09:45,982:INFO:  Epoch 278/500:  train Loss: 22.9930   val Loss: 26.1004   time: 127.75s   best: 25.6843
2023-11-21 07:11:42,468:INFO:  Epoch 83/500:  train Loss: 21.1628   val Loss: 24.7171   time: 433.50s   best: 24.1039
2023-11-21 07:11:54,134:INFO:  Epoch 279/500:  train Loss: 22.4277   val Loss: 26.3661   time: 128.15s   best: 25.6843
2023-11-21 07:14:01,999:INFO:  Epoch 280/500:  train Loss: 22.1976   val Loss: 25.8452   time: 127.86s   best: 25.6843
2023-11-21 07:16:10,122:INFO:  Epoch 281/500:  train Loss: 22.9201   val Loss: 26.2291   time: 128.12s   best: 25.6843
2023-11-21 07:18:18,295:INFO:  Epoch 282/500:  train Loss: 22.5904   val Loss: 26.4129   time: 128.17s   best: 25.6843
2023-11-21 07:18:51,314:INFO:  Epoch 84/500:  train Loss: 20.9669   val Loss: 24.2610   time: 428.83s   best: 24.1039
2023-11-21 07:20:25,701:INFO:  Epoch 283/500:  train Loss: 22.3661   val Loss: 26.6729   time: 127.39s   best: 25.6843
2023-11-21 07:22:34,024:INFO:  Epoch 284/500:  train Loss: 22.0950   val Loss: 26.6104   time: 128.30s   best: 25.6843
2023-11-21 07:24:42,030:INFO:  Epoch 285/500:  train Loss: 22.2564   val Loss: 26.2016   time: 128.01s   best: 25.6843
2023-11-21 07:26:05,425:INFO:  Epoch 85/500:  train Loss: 21.0799   val Loss: 24.5717   time: 434.11s   best: 24.1039
2023-11-21 07:26:50,418:INFO:  Epoch 286/500:  train Loss: 22.0469   val Loss: 26.5461   time: 128.39s   best: 25.6843
2023-11-21 07:28:58,320:INFO:  Epoch 287/500:  train Loss: 22.1417   val Loss: 26.5791   time: 127.89s   best: 25.6843
2023-11-21 07:31:06,248:INFO:  Epoch 288/500:  train Loss: 22.5963   val Loss: 26.4981   time: 127.92s   best: 25.6843
2023-11-21 07:33:14,266:INFO:  Epoch 289/500:  train Loss: 22.2389   val Loss: 26.1822   time: 128.01s   best: 25.6843
2023-11-21 07:33:19,226:INFO:  Epoch 86/500:  train Loss: 20.9600   val Loss: 24.6167   time: 433.80s   best: 24.1039
2023-11-21 07:35:21,676:INFO:  Epoch 290/500:  train Loss: 22.2186   val Loss: 25.9409   time: 127.40s   best: 25.6843
2023-11-21 07:37:29,087:INFO:  Epoch 291/500:  train Loss: 22.2820   val Loss: 26.3353   time: 127.41s   best: 25.6843
2023-11-21 07:39:36,546:INFO:  Epoch 292/500:  train Loss: 22.0296   val Loss: 26.3448   time: 127.46s   best: 25.6843
2023-11-21 07:40:29,568:INFO:  Epoch 87/500:  train Loss: 20.9240   val Loss: 24.3953   time: 430.31s   best: 24.1039
2023-11-21 07:41:44,753:INFO:  Epoch 293/500:  train Loss: 22.8347   val Loss: 26.0259   time: 128.20s   best: 25.6843
2023-11-21 07:43:52,676:INFO:  Epoch 294/500:  train Loss: 21.9685   val Loss: 26.0104   time: 127.92s   best: 25.6843
2023-11-21 07:46:00,697:INFO:  Epoch 295/500:  train Loss: 22.4061   val Loss: 26.0251   time: 128.00s   best: 25.6843
2023-11-21 07:47:38,811:INFO:  Epoch 88/500:  train Loss: 20.9095   val Loss: 24.5622   time: 429.24s   best: 24.1039
2023-11-21 07:48:08,051:INFO:  Epoch 296/500:  train Loss: 21.9380   val Loss: 26.1886   time: 127.35s   best: 25.6843
2023-11-21 07:50:15,753:INFO:  Epoch 297/500:  train Loss: 21.9971   val Loss: 25.9101   time: 127.70s   best: 25.6843
2023-11-21 07:52:22,908:INFO:  Epoch 298/500:  train Loss: 22.2306   val Loss: 27.7764   time: 127.15s   best: 25.6843
2023-11-21 07:54:30,876:INFO:  Epoch 299/500:  train Loss: 21.9976   val Loss: 26.5006   time: 127.97s   best: 25.6843
2023-11-21 07:54:52,886:INFO:  Epoch 89/500:  train Loss: 20.8747   val Loss: 24.5217   time: 434.05s   best: 24.1039
2023-11-21 07:56:38,828:INFO:  Epoch 300/500:  train Loss: 21.9271   val Loss: 26.0628   time: 127.95s   best: 25.6843
2023-11-21 07:58:46,042:INFO:  Epoch 301/500:  train Loss: 22.0153   val Loss: 26.1439   time: 127.20s   best: 25.6843
2023-11-21 08:00:53,520:INFO:  Epoch 302/500:  train Loss: 21.8671   val Loss: 26.1325   time: 127.48s   best: 25.6843
2023-11-21 08:02:03,619:INFO:  Epoch 90/500:  train Loss: 20.7483   val Loss: 24.8886   time: 430.73s   best: 24.1039
2023-11-21 08:03:02,261:INFO:  Epoch 303/500:  train Loss: 21.9364   val Loss: 26.1206   time: 128.74s   best: 25.6843
2023-11-21 08:05:10,053:INFO:  Epoch 304/500:  train Loss: 21.9072   val Loss: 26.0194   time: 127.78s   best: 25.6843
2023-11-21 08:07:17,653:INFO:  Epoch 305/500:  train Loss: 21.8573   val Loss: 26.2409   time: 127.60s   best: 25.6843
2023-11-21 08:09:13,400:INFO:  Epoch 91/500:  train Loss: 20.8442   val Loss: 24.5279   time: 429.78s   best: 24.1039
2023-11-21 08:09:25,399:INFO:  Epoch 306/500:  train Loss: 21.7885   val Loss: 26.2906   time: 127.73s   best: 25.6843
2023-11-21 08:11:33,463:INFO:  Epoch 307/500:  train Loss: 22.4108   val Loss: 26.1019   time: 128.06s   best: 25.6843
2023-11-21 08:13:40,931:INFO:  Epoch 308/500:  train Loss: 22.3755   val Loss: 26.1362   time: 127.46s   best: 25.6843
2023-11-21 08:15:48,625:INFO:  Epoch 309/500:  train Loss: 21.6643   val Loss: 25.9593   time: 127.68s   best: 25.6843
2023-11-21 08:16:26,511:INFO:  Epoch 92/500:  train Loss: 20.7855   val Loss: 24.6509   time: 433.11s   best: 24.1039
2023-11-21 08:17:56,765:INFO:  Epoch 310/500:  train Loss: 21.7645   val Loss: 26.0596   time: 128.14s   best: 25.6843
2023-11-21 08:20:04,917:INFO:  Epoch 311/500:  train Loss: 21.8921   val Loss: 25.9931   time: 128.15s   best: 25.6843
2023-11-21 08:22:12,202:INFO:  Epoch 312/500:  train Loss: 21.6608   val Loss: 25.8880   time: 127.27s   best: 25.6843
2023-11-21 08:23:41,129:INFO:  Epoch 93/500:  train Loss: 20.9326   val Loss: 24.4957   time: 434.60s   best: 24.1039
2023-11-21 08:24:19,825:INFO:  Epoch 313/500:  train Loss: 22.1345   val Loss: 26.2153   time: 127.62s   best: 25.6843
2023-11-21 08:26:27,843:INFO:  Epoch 314/500:  train Loss: 21.8925   val Loss: 26.1064   time: 128.00s   best: 25.6843
2023-11-21 08:28:35,794:INFO:  Epoch 315/500:  train Loss: 21.6460   val Loss: 25.7983   time: 127.95s   best: 25.6843
2023-11-21 08:30:43,355:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 08:30:43,373:INFO:  Epoch 316/500:  train Loss: 21.7751   val Loss: 25.6835   time: 127.56s   best: 25.6835
2023-11-21 08:30:50,235:INFO:  Epoch 94/500:  train Loss: 20.7855   val Loss: 24.9912   time: 429.10s   best: 24.1039
2023-11-21 08:32:50,900:INFO:  Epoch 317/500:  train Loss: 21.6582   val Loss: 25.8911   time: 127.53s   best: 25.6835
2023-11-21 08:34:59,054:INFO:  Epoch 318/500:  train Loss: 23.1766   val Loss: 26.0754   time: 128.14s   best: 25.6835
2023-11-21 08:37:07,112:INFO:  Epoch 319/500:  train Loss: 21.6551   val Loss: 25.7534   time: 128.06s   best: 25.6835
2023-11-21 08:38:01,295:INFO:  Epoch 95/500:  train Loss: 20.6938   val Loss: 24.5898   time: 431.04s   best: 24.1039
2023-11-21 08:39:14,175:INFO:  Epoch 320/500:  train Loss: 21.7757   val Loss: 32.5421   time: 127.06s   best: 25.6835
2023-11-21 08:41:21,384:INFO:  Epoch 321/500:  train Loss: 21.9888   val Loss: 25.8728   time: 127.20s   best: 25.6835
2023-11-21 08:43:28,606:INFO:  Epoch 322/500:  train Loss: 21.7541   val Loss: 25.9293   time: 127.21s   best: 25.6835
2023-11-21 08:45:11,516:INFO:  Epoch 96/500:  train Loss: 20.5789   val Loss: 24.8149   time: 430.21s   best: 24.1039
2023-11-21 08:45:36,132:INFO:  Epoch 323/500:  train Loss: 21.5802   val Loss: 26.1161   time: 127.53s   best: 25.6835
2023-11-21 08:47:43,085:INFO:  Epoch 324/500:  train Loss: 21.5913   val Loss: 26.3677   time: 126.93s   best: 25.6835
2023-11-21 08:49:50,200:INFO:  Epoch 325/500:  train Loss: 21.4966   val Loss: 26.0500   time: 127.11s   best: 25.6835
2023-11-21 08:51:57,002:INFO:  Epoch 326/500:  train Loss: 21.7566   val Loss: 26.0045   time: 126.79s   best: 25.6835
2023-11-21 08:52:24,915:INFO:  Epoch 97/500:  train Loss: 20.4560   val Loss: 25.2608   time: 433.38s   best: 24.1039
2023-11-21 08:54:04,092:INFO:  Epoch 327/500:  train Loss: 21.4610   val Loss: 25.8845   time: 127.07s   best: 25.6835
2023-11-21 08:56:11,338:INFO:  Epoch 328/500:  train Loss: 21.6651   val Loss: 26.2808   time: 127.24s   best: 25.6835
2023-11-21 08:58:18,398:INFO:  Epoch 329/500:  train Loss: 21.5334   val Loss: 26.0595   time: 127.06s   best: 25.6835
2023-11-21 08:59:37,767:INFO:  Epoch 98/500:  train Loss: 20.5945   val Loss: 24.5246   time: 432.85s   best: 24.1039
2023-11-21 09:00:25,444:INFO:  Epoch 330/500:  train Loss: 21.6685   val Loss: 26.1776   time: 127.05s   best: 25.6835
2023-11-21 09:02:33,027:INFO:  Epoch 331/500:  train Loss: 21.8285   val Loss: 26.5727   time: 127.58s   best: 25.6835
2023-11-21 09:04:39,923:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 09:04:39,941:INFO:  Epoch 332/500:  train Loss: 21.4331   val Loss: 25.4779   time: 126.88s   best: 25.4779
2023-11-21 09:06:46,991:INFO:  Epoch 333/500:  train Loss: 21.4595   val Loss: 25.8169   time: 127.05s   best: 25.4779
2023-11-21 09:06:47,573:INFO:  Epoch 99/500:  train Loss: 20.3778   val Loss: 24.7496   time: 429.80s   best: 24.1039
2023-11-21 09:08:54,024:INFO:  Epoch 334/500:  train Loss: 21.5604   val Loss: 26.0044   time: 127.03s   best: 25.4779
2023-11-21 09:11:01,033:INFO:  Epoch 335/500:  train Loss: 21.4864   val Loss: 26.1752   time: 126.99s   best: 25.4779
2023-11-21 09:13:08,246:INFO:  Epoch 336/500:  train Loss: 21.6139   val Loss: 25.9943   time: 127.20s   best: 25.4779
2023-11-21 09:13:57,501:INFO:  Epoch 100/500:  train Loss: 20.4192   val Loss: 24.5768   time: 429.90s   best: 24.1039
2023-11-21 09:15:15,521:INFO:  Epoch 337/500:  train Loss: 21.5199   val Loss: 25.8669   time: 127.27s   best: 25.4779
2023-11-21 09:17:23,007:INFO:  Epoch 338/500:  train Loss: 21.4853   val Loss: 26.0491   time: 127.47s   best: 25.4779
2023-11-21 09:19:30,103:INFO:  Epoch 339/500:  train Loss: 21.8034   val Loss: 26.1724   time: 127.10s   best: 25.4779
2023-11-21 09:21:11,648:INFO:  Epoch 101/500:  train Loss: 20.4443   val Loss: 26.0669   time: 434.13s   best: 24.1039
2023-11-21 09:21:37,871:INFO:  Epoch 340/500:  train Loss: 21.3754   val Loss: 25.9097   time: 127.76s   best: 25.4779
2023-11-21 09:23:44,817:INFO:  Epoch 341/500:  train Loss: 21.5631   val Loss: 25.8673   time: 126.94s   best: 25.4779
2023-11-21 09:25:51,951:INFO:  Epoch 342/500:  train Loss: 21.4404   val Loss: 26.1932   time: 127.12s   best: 25.4779
2023-11-21 09:27:58,759:INFO:  Epoch 343/500:  train Loss: 21.2796   val Loss: 26.0154   time: 126.81s   best: 25.4779
2023-11-21 09:28:24,125:INFO:  Epoch 102/500:  train Loss: 20.4668   val Loss: 24.4487   time: 432.47s   best: 24.1039
2023-11-21 09:30:05,850:INFO:  Epoch 344/500:  train Loss: 22.1340   val Loss: 25.9734   time: 127.09s   best: 25.4779
2023-11-21 09:32:12,694:INFO:  Epoch 345/500:  train Loss: 21.3601   val Loss: 25.9799   time: 126.83s   best: 25.4779
2023-11-21 09:34:20,054:INFO:  Epoch 346/500:  train Loss: 21.4477   val Loss: 26.7091   time: 127.35s   best: 25.4779
2023-11-21 09:35:35,933:INFO:  Epoch 103/500:  train Loss: 20.3012   val Loss: 24.5812   time: 431.79s   best: 24.1039
2023-11-21 09:36:26,791:INFO:  Epoch 347/500:  train Loss: 21.5489   val Loss: 26.4037   time: 126.73s   best: 25.4779
2023-11-21 09:38:33,556:INFO:  Epoch 348/500:  train Loss: 21.8510   val Loss: 25.8522   time: 126.75s   best: 25.4779
2023-11-21 09:40:41,377:INFO:  Epoch 349/500:  train Loss: 21.3433   val Loss: 26.0641   time: 127.82s   best: 25.4779
2023-11-21 09:42:48,203:INFO:  Epoch 350/500:  train Loss: 21.3809   val Loss: 25.9599   time: 126.83s   best: 25.4779
2023-11-21 09:42:48,722:INFO:  Epoch 104/500:  train Loss: 20.3328   val Loss: 24.2737   time: 432.78s   best: 24.1039
2023-11-21 09:44:55,480:INFO:  Epoch 351/500:  train Loss: 21.5157   val Loss: 25.9433   time: 127.27s   best: 25.4779
2023-11-21 09:47:03,117:INFO:  Epoch 352/500:  train Loss: 21.5252   val Loss: 26.2615   time: 127.63s   best: 25.4779
2023-11-21 09:49:10,587:INFO:  Epoch 353/500:  train Loss: 22.4001   val Loss: 26.2735   time: 127.47s   best: 25.4779
2023-11-21 09:49:57,683:INFO:  Epoch 105/500:  train Loss: 20.4722   val Loss: 24.2914   time: 428.96s   best: 24.1039
2023-11-21 09:51:17,340:INFO:  Epoch 354/500:  train Loss: 21.1971   val Loss: 25.9197   time: 126.75s   best: 25.4779
2023-11-21 09:53:24,961:INFO:  Epoch 355/500:  train Loss: 21.7748   val Loss: 26.3020   time: 127.60s   best: 25.4779
2023-11-21 09:55:31,765:INFO:  Epoch 356/500:  train Loss: 21.1670   val Loss: 25.5540   time: 126.80s   best: 25.4779
2023-11-21 09:57:05,935:INFO:  Epoch 106/500:  train Loss: 20.3410   val Loss: 24.4955   time: 428.24s   best: 24.1039
2023-11-21 09:57:38,765:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 09:57:38,785:INFO:  Epoch 357/500:  train Loss: 21.6864   val Loss: 25.4449   time: 126.98s   best: 25.4449
2023-11-21 09:59:46,169:INFO:  Epoch 358/500:  train Loss: 21.2456   val Loss: 26.4022   time: 127.38s   best: 25.4449
2023-11-21 10:01:52,717:INFO:  Epoch 359/500:  train Loss: 21.2155   val Loss: 25.7256   time: 126.54s   best: 25.4449
2023-11-21 10:03:59,891:INFO:  Epoch 360/500:  train Loss: 21.4182   val Loss: 25.5944   time: 127.17s   best: 25.4449
2023-11-21 10:04:17,937:INFO:  Epoch 107/500:  train Loss: 20.5195   val Loss: 24.5818   time: 432.00s   best: 24.1039
2023-11-21 10:06:06,410:INFO:  Epoch 361/500:  train Loss: 21.2062   val Loss: 25.8658   time: 126.52s   best: 25.4449
2023-11-21 10:08:13,134:INFO:  Epoch 362/500:  train Loss: 21.1485   val Loss: 26.0388   time: 126.72s   best: 25.4449
2023-11-21 10:10:21,176:INFO:  Epoch 363/500:  train Loss: 21.4576   val Loss: 25.7347   time: 128.04s   best: 25.4449
2023-11-21 10:11:30,399:INFO:  Epoch 108/500:  train Loss: 20.4704   val Loss: 24.4155   time: 432.45s   best: 24.1039
2023-11-21 10:12:28,337:INFO:  Epoch 364/500:  train Loss: 21.0520   val Loss: 25.7720   time: 127.15s   best: 25.4449
2023-11-21 10:14:35,482:INFO:  Epoch 365/500:  train Loss: 21.9133   val Loss: 25.5795   time: 127.14s   best: 25.4449
2023-11-21 10:16:42,571:INFO:  Epoch 366/500:  train Loss: 21.4466   val Loss: 25.9997   time: 127.07s   best: 25.4449
2023-11-21 10:18:42,949:INFO:  Epoch 109/500:  train Loss: 20.3507   val Loss: 26.0723   time: 432.55s   best: 24.1039
2023-11-21 10:18:49,438:INFO:  Epoch 367/500:  train Loss: 21.0858   val Loss: 25.7635   time: 126.85s   best: 25.4449
2023-11-21 10:20:56,343:INFO:  Epoch 368/500:  train Loss: 21.3036   val Loss: 26.3406   time: 126.90s   best: 25.4449
2023-11-21 10:23:03,567:INFO:  Epoch 369/500:  train Loss: 21.0411   val Loss: 26.2675   time: 127.22s   best: 25.4449
2023-11-21 10:25:10,285:INFO:  Epoch 370/500:  train Loss: 21.1289   val Loss: 25.7846   time: 126.71s   best: 25.4449
2023-11-21 10:25:55,721:INFO:  Epoch 110/500:  train Loss: 20.3855   val Loss: 24.1844   time: 432.77s   best: 24.1039
2023-11-21 10:27:17,163:INFO:  Epoch 371/500:  train Loss: 21.0787   val Loss: 25.9444   time: 126.88s   best: 25.4449
2023-11-21 10:29:23,688:INFO:  Epoch 372/500:  train Loss: 22.4987   val Loss: 26.6506   time: 126.52s   best: 25.4449
2023-11-21 10:31:31,631:INFO:  Epoch 373/500:  train Loss: 21.1944   val Loss: 26.0139   time: 127.94s   best: 25.4449
2023-11-21 10:33:10,018:INFO:  Epoch 111/500:  train Loss: 20.1433   val Loss: 24.4622   time: 434.28s   best: 24.1039
2023-11-21 10:33:39,233:INFO:  Epoch 374/500:  train Loss: 21.9841   val Loss: 27.7237   time: 127.60s   best: 25.4449
2023-11-21 10:35:46,592:INFO:  Epoch 375/500:  train Loss: 21.7137   val Loss: 25.9177   time: 127.35s   best: 25.4449
2023-11-21 10:37:53,160:INFO:  Epoch 376/500:  train Loss: 21.0985   val Loss: 25.7998   time: 126.56s   best: 25.4449
2023-11-21 10:39:59,678:INFO:  Epoch 377/500:  train Loss: 21.0178   val Loss: 25.7895   time: 126.52s   best: 25.4449
2023-11-21 10:40:18,945:INFO:  Epoch 112/500:  train Loss: 20.1342   val Loss: 24.1972   time: 428.92s   best: 24.1039
2023-11-21 10:42:06,262:INFO:  Epoch 378/500:  train Loss: 21.1970   val Loss: 25.9847   time: 126.58s   best: 25.4449
2023-11-21 10:44:12,895:INFO:  Epoch 379/500:  train Loss: 21.0912   val Loss: 25.6342   time: 126.63s   best: 25.4449
2023-11-21 10:46:20,478:INFO:  Epoch 380/500:  train Loss: 20.8434   val Loss: 25.9422   time: 127.58s   best: 25.4449
2023-11-21 10:47:31,884:INFO:  Epoch 113/500:  train Loss: 20.2059   val Loss: 24.7655   time: 432.94s   best: 24.1039
2023-11-21 10:48:27,301:INFO:  Epoch 381/500:  train Loss: 21.6737   val Loss: 26.2623   time: 126.81s   best: 25.4449
2023-11-21 10:50:34,007:INFO:  Epoch 382/500:  train Loss: 21.4661   val Loss: 25.9351   time: 126.70s   best: 25.4449
2023-11-21 10:52:40,788:INFO:  Epoch 383/500:  train Loss: 20.9050   val Loss: 26.1992   time: 126.78s   best: 25.4449
2023-11-21 10:54:43,794:INFO:  Epoch 114/500:  train Loss: 20.1082   val Loss: 24.3303   time: 431.90s   best: 24.1039
2023-11-21 10:54:47,633:INFO:  Epoch 384/500:  train Loss: 20.9641   val Loss: 26.6273   time: 126.83s   best: 25.4449
2023-11-21 10:56:55,013:INFO:  Epoch 385/500:  train Loss: 21.6357   val Loss: 25.9781   time: 127.38s   best: 25.4449
2023-11-21 10:59:02,144:INFO:  Epoch 386/500:  train Loss: 21.3907   val Loss: 26.2882   time: 127.13s   best: 25.4449
2023-11-21 11:01:08,635:INFO:  Epoch 387/500:  train Loss: 20.9664   val Loss: 27.0549   time: 126.49s   best: 25.4449
2023-11-21 11:01:57,513:INFO:  Epoch 115/500:  train Loss: 20.1625   val Loss: 24.2056   time: 433.72s   best: 24.1039
2023-11-21 11:03:16,078:INFO:  Epoch 388/500:  train Loss: 21.1130   val Loss: 27.5041   time: 127.44s   best: 25.4449
2023-11-21 11:05:22,512:INFO:  Epoch 389/500:  train Loss: 20.8957   val Loss: 26.6525   time: 126.43s   best: 25.4449
2023-11-21 11:07:29,693:INFO:  Epoch 390/500:  train Loss: 20.8888   val Loss: 25.6249   time: 127.17s   best: 25.4449
2023-11-21 11:09:08,807:INFO:  Epoch 116/500:  train Loss: 20.1122   val Loss: 24.4105   time: 431.28s   best: 24.1039
2023-11-21 11:09:37,075:INFO:  Epoch 391/500:  train Loss: 20.9808   val Loss: 25.8714   time: 127.37s   best: 25.4449
2023-11-21 11:11:43,696:INFO:  Epoch 392/500:  train Loss: 20.8350   val Loss: 26.2268   time: 126.61s   best: 25.4449
2023-11-21 11:13:50,457:INFO:  Epoch 393/500:  train Loss: 20.8309   val Loss: 25.8223   time: 126.75s   best: 25.4449
2023-11-21 11:15:57,419:INFO:  Epoch 394/500:  train Loss: 20.7815   val Loss: 25.9159   time: 126.95s   best: 25.4449
2023-11-21 11:16:22,077:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 11:16:22,097:INFO:  Epoch 117/500:  train Loss: 20.4201   val Loss: 23.9911   time: 433.25s   best: 23.9911
2023-11-21 11:18:04,173:INFO:  Epoch 395/500:  train Loss: 20.9758   val Loss: 25.9343   time: 126.74s   best: 25.4449
2023-11-21 11:20:10,896:INFO:  Epoch 396/500:  train Loss: 21.0738   val Loss: 25.9324   time: 126.71s   best: 25.4449
2023-11-21 11:22:18,266:INFO:  Epoch 397/500:  train Loss: 21.0068   val Loss: 28.2043   time: 127.37s   best: 25.4449
2023-11-21 11:23:37,283:INFO:  Epoch 118/500:  train Loss: 20.0443   val Loss: 24.6463   time: 435.17s   best: 23.9911
2023-11-21 11:24:24,774:INFO:  Epoch 398/500:  train Loss: 21.0568   val Loss: 25.7962   time: 126.51s   best: 25.4449
2023-11-21 11:26:31,368:INFO:  Epoch 399/500:  train Loss: 20.8293   val Loss: 25.9354   time: 126.59s   best: 25.4449
2023-11-21 11:28:38,977:INFO:  Epoch 400/500:  train Loss: 20.9276   val Loss: 26.0111   time: 127.61s   best: 25.4449
2023-11-21 11:30:46,672:INFO:  Epoch 401/500:  train Loss: 20.8492   val Loss: 25.9156   time: 127.69s   best: 25.4449
2023-11-21 11:30:47,756:INFO:  Epoch 119/500:  train Loss: 20.1094   val Loss: 24.1856   time: 430.47s   best: 23.9911
2023-11-21 11:32:53,090:INFO:  Epoch 402/500:  train Loss: 20.7549   val Loss: 25.6363   time: 126.41s   best: 25.4449
2023-11-21 11:34:59,635:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 11:34:59,652:INFO:  Epoch 403/500:  train Loss: 20.7498   val Loss: 25.4302   time: 126.54s   best: 25.4302
2023-11-21 11:37:06,608:INFO:  Epoch 404/500:  train Loss: 21.0621   val Loss: 25.6454   time: 126.96s   best: 25.4302
2023-11-21 11:38:01,223:INFO:  Epoch 120/500:  train Loss: 20.5328   val Loss: 24.6355   time: 433.45s   best: 23.9911
2023-11-21 11:39:13,637:INFO:  Epoch 405/500:  train Loss: 21.1581   val Loss: 26.2800   time: 127.02s   best: 25.4302
2023-11-21 11:41:21,504:INFO:  Epoch 406/500:  train Loss: 21.0355   val Loss: 25.8609   time: 127.86s   best: 25.4302
2023-11-21 11:43:28,818:INFO:  Epoch 407/500:  train Loss: 20.6394   val Loss: 25.7874   time: 127.31s   best: 25.4302
2023-11-21 11:45:14,847:INFO:  Epoch 121/500:  train Loss: 19.9206   val Loss: 24.6396   time: 433.61s   best: 23.9911
2023-11-21 11:45:36,039:INFO:  Epoch 408/500:  train Loss: 20.7241   val Loss: 26.0483   time: 127.22s   best: 25.4302
2023-11-21 11:47:43,483:INFO:  Epoch 409/500:  train Loss: 20.9644   val Loss: 26.3337   time: 127.43s   best: 25.4302
2023-11-21 11:49:49,765:INFO:  Epoch 410/500:  train Loss: 24.0220   val Loss: 27.1737   time: 126.28s   best: 25.4302
2023-11-21 11:51:56,426:INFO:  Epoch 411/500:  train Loss: 21.4409   val Loss: 25.9135   time: 126.66s   best: 25.4302
2023-11-21 11:52:24,742:INFO:  Epoch 122/500:  train Loss: 20.0464   val Loss: 24.1133   time: 429.88s   best: 23.9911
2023-11-21 11:54:03,466:INFO:  Epoch 412/500:  train Loss: 21.5693   val Loss: 26.0412   time: 127.03s   best: 25.4302
2023-11-21 11:56:10,348:INFO:  Epoch 413/500:  train Loss: 20.8201   val Loss: 25.6854   time: 126.88s   best: 25.4302
2023-11-21 11:58:16,371:INFO:  Epoch 414/500:  train Loss: 22.5553   val Loss: 26.9029   time: 126.02s   best: 25.4302
2023-11-21 11:59:35,234:INFO:  Epoch 123/500:  train Loss: 19.9680   val Loss: 24.7938   time: 430.49s   best: 23.9911
2023-11-21 12:00:22,667:INFO:  Epoch 415/500:  train Loss: 21.1199   val Loss: 25.9027   time: 126.30s   best: 25.4302
2023-11-21 12:02:29,013:INFO:  Epoch 416/500:  train Loss: 20.6196   val Loss: 25.9586   time: 126.34s   best: 25.4302
2023-11-21 12:04:36,100:INFO:  Epoch 417/500:  train Loss: 20.6382   val Loss: 26.2006   time: 127.09s   best: 25.4302
2023-11-21 12:06:42,163:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 12:06:42,181:INFO:  Epoch 418/500:  train Loss: 21.0498   val Loss: 25.3731   time: 126.05s   best: 25.3731
2023-11-21 12:06:46,383:INFO:  Epoch 124/500:  train Loss: 19.9639   val Loss: 24.1139   time: 431.15s   best: 23.9911
2023-11-21 12:08:48,628:INFO:  Epoch 419/500:  train Loss: 20.6018   val Loss: 25.6755   time: 126.45s   best: 25.3731
2023-11-21 12:10:54,894:INFO:  Epoch 420/500:  train Loss: 22.3516   val Loss: 30.1621   time: 126.25s   best: 25.3731
2023-11-21 12:13:01,202:INFO:  Epoch 421/500:  train Loss: 21.7176   val Loss: 25.5848   time: 126.31s   best: 25.3731
2023-11-21 12:14:00,153:INFO:  Epoch 125/500:  train Loss: 20.0212   val Loss: 24.6837   time: 433.74s   best: 23.9911
2023-11-21 12:15:08,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 12:15:08,466:INFO:  Epoch 422/500:  train Loss: 20.5452   val Loss: 25.3707   time: 127.24s   best: 25.3707
2023-11-21 12:17:15,511:INFO:  Epoch 423/500:  train Loss: 20.5771   val Loss: 25.8532   time: 127.04s   best: 25.3707
2023-11-21 12:19:21,826:INFO:  Epoch 424/500:  train Loss: 20.5886   val Loss: 25.9845   time: 126.30s   best: 25.3707
2023-11-21 12:21:13,066:INFO:  Epoch 126/500:  train Loss: 19.8692   val Loss: 24.2212   time: 432.91s   best: 23.9911
2023-11-21 12:21:27,966:INFO:  Epoch 425/500:  train Loss: 20.5730   val Loss: 25.4385   time: 126.14s   best: 25.3707
2023-11-21 12:23:34,667:INFO:  Epoch 426/500:  train Loss: 21.2392   val Loss: 25.5463   time: 126.70s   best: 25.3707
2023-11-21 12:25:41,826:INFO:  Epoch 427/500:  train Loss: 20.5102   val Loss: 25.9757   time: 127.15s   best: 25.3707
2023-11-21 12:27:48,647:INFO:  Epoch 428/500:  train Loss: 20.6345   val Loss: 25.9122   time: 126.82s   best: 25.3707
2023-11-21 12:28:22,381:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 12:28:22,402:INFO:  Epoch 127/500:  train Loss: 19.7915   val Loss: 23.9367   time: 429.31s   best: 23.9367
2023-11-21 12:29:55,149:INFO:  Epoch 429/500:  train Loss: 20.4403   val Loss: 26.0126   time: 126.50s   best: 25.3707
2023-11-21 12:32:01,979:INFO:  Epoch 430/500:  train Loss: 20.6949   val Loss: 25.6860   time: 126.82s   best: 25.3707
2023-11-21 12:34:08,526:INFO:  Epoch 431/500:  train Loss: 20.5260   val Loss: 25.5058   time: 126.53s   best: 25.3707
2023-11-21 12:35:33,220:INFO:  Epoch 128/500:  train Loss: 19.7866   val Loss: 24.1590   time: 430.81s   best: 23.9367
2023-11-21 12:36:15,368:INFO:  Epoch 432/500:  train Loss: 20.6051   val Loss: 25.7456   time: 126.83s   best: 25.3707
2023-11-21 12:38:22,588:INFO:  Epoch 433/500:  train Loss: 20.7715   val Loss: 25.4686   time: 127.21s   best: 25.3707
2023-11-21 12:40:28,613:INFO:  Epoch 434/500:  train Loss: 20.7528   val Loss: 28.7523   time: 126.02s   best: 25.3707
2023-11-21 12:42:34,992:INFO:  Epoch 435/500:  train Loss: 21.0208   val Loss: 25.7189   time: 126.38s   best: 25.3707
2023-11-21 12:42:44,107:INFO:  Epoch 129/500:  train Loss: 19.8610   val Loss: 24.4656   time: 430.87s   best: 23.9367
2023-11-21 12:44:41,171:INFO:  Epoch 436/500:  train Loss: 20.3837   val Loss: 25.3961   time: 126.17s   best: 25.3707
2023-11-21 12:46:48,294:INFO:  Epoch 437/500:  train Loss: 20.5347   val Loss: 25.9322   time: 127.11s   best: 25.3707
2023-11-21 12:48:54,551:INFO:  Epoch 438/500:  train Loss: 20.4401   val Loss: 25.5894   time: 126.25s   best: 25.3707
2023-11-21 12:49:54,384:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 12:49:54,403:INFO:  Epoch 130/500:  train Loss: 19.6932   val Loss: 23.8633   time: 430.27s   best: 23.8633
2023-11-21 12:51:01,838:INFO:  Epoch 439/500:  train Loss: 20.5829   val Loss: 25.3775   time: 127.27s   best: 25.3707
2023-11-21 12:53:08,252:INFO:  Epoch 440/500:  train Loss: 20.4238   val Loss: 25.4212   time: 126.41s   best: 25.3707
2023-11-21 12:55:15,497:INFO:  Epoch 441/500:  train Loss: 20.3588   val Loss: 25.5951   time: 127.24s   best: 25.3707
2023-11-21 12:57:06,804:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 12:57:06,823:INFO:  Epoch 131/500:  train Loss: 19.8863   val Loss: 23.7011   time: 432.38s   best: 23.7011
2023-11-21 12:57:22,788:INFO:  Epoch 442/500:  train Loss: 20.5551   val Loss: 25.4041   time: 127.29s   best: 25.3707
2023-11-21 12:59:29,062:INFO:  Epoch 443/500:  train Loss: 20.5226   val Loss: 25.7505   time: 126.27s   best: 25.3707
2023-11-21 13:01:35,165:INFO:  Epoch 444/500:  train Loss: 20.3905   val Loss: 26.0264   time: 126.09s   best: 25.3707
2023-11-21 13:03:42,071:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 13:03:42,103:INFO:  Epoch 445/500:  train Loss: 20.6925   val Loss: 25.3230   time: 126.90s   best: 25.3230
2023-11-21 13:04:19,839:INFO:  Epoch 132/500:  train Loss: 19.8546   val Loss: 23.9026   time: 433.02s   best: 23.7011
2023-11-21 13:05:48,556:INFO:  Epoch 446/500:  train Loss: 20.7784   val Loss: 25.4398   time: 126.44s   best: 25.3230
2023-11-21 13:07:54,820:INFO:  Epoch 447/500:  train Loss: 20.4692   val Loss: 25.5402   time: 126.24s   best: 25.3230
2023-11-21 13:10:01,769:INFO:  Epoch 448/500:  train Loss: 20.4755   val Loss: 25.4921   time: 126.95s   best: 25.3230
2023-11-21 13:11:33,725:INFO:  Epoch 133/500:  train Loss: 19.6491   val Loss: 24.0645   time: 433.88s   best: 23.7011
2023-11-21 13:12:08,674:INFO:  Epoch 449/500:  train Loss: 20.5412   val Loss: 25.3421   time: 126.90s   best: 25.3230
2023-11-21 13:14:14,943:INFO:  Epoch 450/500:  train Loss: 20.3560   val Loss: 25.4692   time: 126.27s   best: 25.3230
2023-11-21 13:16:21,722:INFO:  Epoch 451/500:  train Loss: 20.5334   val Loss: 25.3817   time: 126.77s   best: 25.3230
2023-11-21 13:18:28,003:INFO:  Epoch 452/500:  train Loss: 20.5773   val Loss: 26.0118   time: 126.27s   best: 25.3230
2023-11-21 13:18:47,468:INFO:  Epoch 134/500:  train Loss: 19.6421   val Loss: 24.0941   time: 433.74s   best: 23.7011
2023-11-21 13:20:35,000:INFO:  Epoch 453/500:  train Loss: 20.3342   val Loss: 25.9184   time: 127.00s   best: 25.3230
2023-11-21 13:22:42,211:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 13:22:42,229:INFO:  Epoch 454/500:  train Loss: 20.4252   val Loss: 25.2045   time: 127.20s   best: 25.2045
2023-11-21 13:24:49,276:INFO:  Epoch 455/500:  train Loss: 20.2685   val Loss: 25.6472   time: 127.05s   best: 25.2045
2023-11-21 13:25:59,260:INFO:  Epoch 135/500:  train Loss: 19.8600   val Loss: 24.0045   time: 431.79s   best: 23.7011
2023-11-21 13:26:55,686:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_cc16.pt
2023-11-21 13:26:55,706:INFO:  Epoch 456/500:  train Loss: 20.9633   val Loss: 25.0932   time: 126.39s   best: 25.0932
2023-11-21 13:29:02,087:INFO:  Epoch 457/500:  train Loss: 20.3101   val Loss: 25.2192   time: 126.37s   best: 25.0932
2023-11-21 13:31:08,391:INFO:  Epoch 458/500:  train Loss: 20.3968   val Loss: 25.4238   time: 126.29s   best: 25.0932
2023-11-21 13:33:11,859:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 13:33:11,879:INFO:  Epoch 136/500:  train Loss: 19.6594   val Loss: 23.5912   time: 432.59s   best: 23.5912
2023-11-21 13:33:14,713:INFO:  Epoch 459/500:  train Loss: 20.4082   val Loss: 25.7670   time: 126.31s   best: 25.0932
2023-11-21 13:35:21,888:INFO:  Epoch 460/500:  train Loss: 20.2467   val Loss: 25.9976   time: 127.16s   best: 25.0932
2023-11-21 13:37:28,582:INFO:  Epoch 461/500:  train Loss: 20.4662   val Loss: 25.5935   time: 126.69s   best: 25.0932
2023-11-21 13:39:35,029:INFO:  Epoch 462/500:  train Loss: 20.5874   val Loss: 25.1861   time: 126.43s   best: 25.0932
2023-11-21 13:40:22,251:INFO:  Epoch 137/500:  train Loss: 19.7124   val Loss: 23.9207   time: 430.36s   best: 23.5912
2023-11-21 13:41:41,654:INFO:  Epoch 463/500:  train Loss: 20.2276   val Loss: 25.5540   time: 126.62s   best: 25.0932
2023-11-21 13:43:49,377:INFO:  Epoch 464/500:  train Loss: 20.3431   val Loss: 25.4795   time: 127.71s   best: 25.0932
2023-11-21 13:45:57,109:INFO:  Epoch 465/500:  train Loss: 21.7029   val Loss: 28.6202   time: 127.72s   best: 25.0932
2023-11-21 13:47:31,391:INFO:  Epoch 138/500:  train Loss: 19.5393   val Loss: 23.9942   time: 429.13s   best: 23.5912
2023-11-21 13:48:04,057:INFO:  Epoch 466/500:  train Loss: 20.7619   val Loss: 26.2970   time: 126.94s   best: 25.0932
2023-11-21 13:50:10,910:INFO:  Epoch 467/500:  train Loss: 20.2881   val Loss: 25.9827   time: 126.84s   best: 25.0932
2023-11-21 13:52:18,114:INFO:  Epoch 468/500:  train Loss: 20.2601   val Loss: 26.8082   time: 127.19s   best: 25.0932
2023-11-21 13:54:25,095:INFO:  Epoch 469/500:  train Loss: 20.2276   val Loss: 26.0367   time: 126.97s   best: 25.0932
2023-11-21 13:54:40,406:INFO:  Epoch 139/500:  train Loss: 19.6529   val Loss: 24.5910   time: 429.01s   best: 23.5912
2023-11-21 13:56:32,265:INFO:  Epoch 470/500:  train Loss: 20.2121   val Loss: 26.0121   time: 127.16s   best: 25.0932
2023-11-21 13:58:38,870:INFO:  Epoch 471/500:  train Loss: 20.9742   val Loss: 25.5079   time: 126.60s   best: 25.0932
2023-11-21 14:00:46,424:INFO:  Epoch 472/500:  train Loss: 20.3722   val Loss: 25.9383   time: 127.55s   best: 25.0932
2023-11-21 14:01:49,500:INFO:  Epoch 140/500:  train Loss: 19.5932   val Loss: 24.4112   time: 429.08s   best: 23.5912
2023-11-21 14:02:54,000:INFO:  Epoch 473/500:  train Loss: 20.2247   val Loss: 25.8315   time: 127.56s   best: 25.0932
2023-11-21 14:05:00,621:INFO:  Epoch 474/500:  train Loss: 20.3850   val Loss: 25.2146   time: 126.61s   best: 25.0932
2023-11-21 14:07:06,898:INFO:  Epoch 475/500:  train Loss: 20.2807   val Loss: 25.4331   time: 126.27s   best: 25.0932
2023-11-21 14:09:03,226:INFO:  Epoch 141/500:  train Loss: 19.5217   val Loss: 23.9527   time: 433.72s   best: 23.5912
2023-11-21 14:09:13,662:INFO:  Epoch 476/500:  train Loss: 20.2627   val Loss: 25.7195   time: 126.75s   best: 25.0932
2023-11-21 14:11:20,945:INFO:  Epoch 477/500:  train Loss: 20.1473   val Loss: 25.8927   time: 127.28s   best: 25.0932
2023-11-21 14:13:28,355:INFO:  Epoch 478/500:  train Loss: 20.1111   val Loss: 25.7486   time: 127.40s   best: 25.0932
2023-11-21 14:15:35,412:INFO:  Epoch 479/500:  train Loss: 21.1312   val Loss: 25.7962   time: 127.05s   best: 25.0932
2023-11-21 14:16:15,790:INFO:  Epoch 142/500:  train Loss: 19.7115   val Loss: 23.7752   time: 432.54s   best: 23.5912
2023-11-21 14:17:42,699:INFO:  Epoch 480/500:  train Loss: 20.3201   val Loss: 25.4318   time: 127.28s   best: 25.0932
2023-11-21 14:19:49,821:INFO:  Epoch 481/500:  train Loss: 20.3146   val Loss: 26.3972   time: 127.12s   best: 25.0932
2023-11-21 14:21:56,207:INFO:  Epoch 482/500:  train Loss: 20.2691   val Loss: 25.5995   time: 126.39s   best: 25.0932
2023-11-21 14:23:27,125:INFO:  Epoch 143/500:  train Loss: 19.8896   val Loss: 24.8852   time: 431.33s   best: 23.5912
2023-11-21 14:24:02,971:INFO:  Epoch 483/500:  train Loss: 20.2086   val Loss: 25.2476   time: 126.75s   best: 25.0932
2023-11-21 14:26:09,844:INFO:  Epoch 484/500:  train Loss: 20.0159   val Loss: 25.1624   time: 126.86s   best: 25.0932
2023-11-21 14:28:17,043:INFO:  Epoch 485/500:  train Loss: 21.3549   val Loss: 25.6252   time: 127.19s   best: 25.0932
2023-11-21 14:30:23,842:INFO:  Epoch 486/500:  train Loss: 20.1218   val Loss: 25.6207   time: 126.79s   best: 25.0932
2023-11-21 14:30:39,816:INFO:  Epoch 144/500:  train Loss: 19.7358   val Loss: 23.9660   time: 432.69s   best: 23.5912
2023-11-21 14:32:31,106:INFO:  Epoch 487/500:  train Loss: 20.1758   val Loss: 25.7710   time: 127.25s   best: 25.0932
2023-11-21 14:34:37,931:INFO:  Epoch 488/500:  train Loss: 21.0315   val Loss: 26.7292   time: 126.81s   best: 25.0932
2023-11-21 14:36:45,533:INFO:  Epoch 489/500:  train Loss: 20.6328   val Loss: 25.6227   time: 127.60s   best: 25.0932
2023-11-21 14:37:52,482:INFO:  Epoch 145/500:  train Loss: 19.4510   val Loss: 24.0302   time: 432.65s   best: 23.5912
2023-11-21 14:38:51,986:INFO:  Epoch 490/500:  train Loss: 20.5368   val Loss: 25.7917   time: 126.44s   best: 25.0932
2023-11-21 14:40:59,109:INFO:  Epoch 491/500:  train Loss: 20.0265   val Loss: 25.4483   time: 127.12s   best: 25.0932
2023-11-21 14:43:05,640:INFO:  Epoch 492/500:  train Loss: 20.3064   val Loss: 25.4250   time: 126.53s   best: 25.0932
2023-11-21 14:45:06,540:INFO:  Epoch 146/500:  train Loss: 19.4379   val Loss: 24.1068   time: 434.05s   best: 23.5912
2023-11-21 14:45:12,038:INFO:  Epoch 493/500:  train Loss: 20.0114   val Loss: 25.3747   time: 126.40s   best: 25.0932
2023-11-21 14:47:18,216:INFO:  Epoch 494/500:  train Loss: 20.4645   val Loss: 25.4987   time: 126.18s   best: 25.0932
2023-11-21 14:49:25,777:INFO:  Epoch 495/500:  train Loss: 20.1218   val Loss: 25.3150   time: 127.55s   best: 25.0932
2023-11-21 14:51:32,730:INFO:  Epoch 496/500:  train Loss: 20.8281   val Loss: 25.4589   time: 126.93s   best: 25.0932
2023-11-21 14:52:20,148:INFO:  Epoch 147/500:  train Loss: 19.4293   val Loss: 23.9443   time: 433.60s   best: 23.5912
2023-11-21 14:53:39,902:INFO:  Epoch 497/500:  train Loss: 19.9576   val Loss: 26.0281   time: 127.17s   best: 25.0932
2023-11-21 14:55:46,265:INFO:  Epoch 498/500:  train Loss: 20.6777   val Loss: 25.4771   time: 126.36s   best: 25.0932
2023-11-21 14:57:52,814:INFO:  Epoch 499/500:  train Loss: 20.0069   val Loss: 25.5626   time: 126.54s   best: 25.0932
2023-11-21 14:59:32,142:INFO:  Epoch 148/500:  train Loss: 19.5596   val Loss: 23.7054   time: 431.99s   best: 23.5912
2023-11-21 14:59:59,529:INFO:  Epoch 500/500:  train Loss: 20.3424   val Loss: 25.4598   time: 126.71s   best: 25.0932
2023-11-21 14:59:59,531:INFO:  -----> Training complete in 1063m 27s   best validation loss: 25.0932
 
2023-11-21 15:06:40,974:INFO:  Epoch 149/500:  train Loss: 19.5884   val Loss: 23.7780   time: 428.82s   best: 23.5912
2023-11-21 15:13:53,681:INFO:  Epoch 150/500:  train Loss: 19.3678   val Loss: 23.7486   time: 432.69s   best: 23.5912
2023-11-21 15:21:02,704:INFO:  Epoch 151/500:  train Loss: 19.4697   val Loss: 24.0903   time: 429.02s   best: 23.5912
2023-11-21 15:28:16,117:INFO:  Epoch 152/500:  train Loss: 19.3982   val Loss: 23.7642   time: 433.40s   best: 23.5912
2023-11-21 15:35:25,922:INFO:  Epoch 153/500:  train Loss: 19.4949   val Loss: 25.9055   time: 429.79s   best: 23.5912
2023-11-21 15:42:38,528:INFO:  Epoch 154/500:  train Loss: 19.8403   val Loss: 23.6725   time: 432.60s   best: 23.5912
2023-11-21 15:49:51,013:INFO:  Epoch 155/500:  train Loss: 19.4373   val Loss: 23.9925   time: 432.48s   best: 23.5912
2023-11-21 15:57:01,718:INFO:  Epoch 156/500:  train Loss: 19.3505   val Loss: 24.1078   time: 430.69s   best: 23.5912
2023-11-21 16:04:13,204:INFO:  Epoch 157/500:  train Loss: 19.2273   val Loss: 23.8504   time: 431.48s   best: 23.5912
2023-11-21 16:11:22,306:INFO:  Epoch 158/500:  train Loss: 19.1851   val Loss: 23.9657   time: 429.10s   best: 23.5912
2023-11-21 16:18:35,941:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 16:18:35,960:INFO:  Epoch 159/500:  train Loss: 19.5091   val Loss: 23.5579   time: 433.63s   best: 23.5579
2023-11-21 16:25:48,802:INFO:  Epoch 160/500:  train Loss: 19.2512   val Loss: 24.4360   time: 432.84s   best: 23.5579
2023-11-21 16:33:01,233:INFO:  Epoch 161/500:  train Loss: 19.2215   val Loss: 23.8620   time: 432.42s   best: 23.5579
2023-11-21 16:40:13,369:INFO:  Epoch 162/500:  train Loss: 19.2530   val Loss: 23.8983   time: 432.13s   best: 23.5579
2023-11-21 16:47:24,497:INFO:  Epoch 163/500:  train Loss: 19.3996   val Loss: 24.3208   time: 431.12s   best: 23.5579
2023-11-21 16:54:37,737:INFO:  Epoch 164/500:  train Loss: 19.5204   val Loss: 23.6001   time: 433.24s   best: 23.5579
2023-11-21 17:01:46,939:INFO:  Epoch 165/500:  train Loss: 19.1368   val Loss: 23.8042   time: 429.20s   best: 23.5579
2023-11-21 17:08:57,511:INFO:  Epoch 166/500:  train Loss: 19.1845   val Loss: 23.8535   time: 430.57s   best: 23.5579
2023-11-21 17:16:09,986:INFO:  Epoch 167/500:  train Loss: 19.8651   val Loss: 24.3487   time: 432.46s   best: 23.5579
2023-11-21 17:23:23,411:INFO:  Epoch 168/500:  train Loss: 19.4105   val Loss: 23.8383   time: 433.42s   best: 23.5579
2023-11-21 17:30:36,132:INFO:  Epoch 169/500:  train Loss: 19.0302   val Loss: 24.1957   time: 432.72s   best: 23.5579
2023-11-21 17:37:45,239:INFO:  Epoch 170/500:  train Loss: 19.2795   val Loss: 23.8582   time: 429.09s   best: 23.5579
2023-11-21 17:44:57,557:INFO:  Epoch 171/500:  train Loss: 19.1230   val Loss: 24.2131   time: 432.31s   best: 23.5579
2023-11-21 17:52:10,517:INFO:  Epoch 172/500:  train Loss: 19.2113   val Loss: 23.7880   time: 432.95s   best: 23.5579
2023-11-21 17:59:24,341:INFO:  Epoch 173/500:  train Loss: 19.4031   val Loss: 23.8960   time: 433.82s   best: 23.5579
2023-11-21 18:06:35,688:INFO:  Epoch 174/500:  train Loss: 19.1924   val Loss: 24.6483   time: 431.34s   best: 23.5579
2023-11-21 18:13:46,724:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 18:13:46,742:INFO:  Epoch 175/500:  train Loss: 19.0943   val Loss: 23.4361   time: 431.02s   best: 23.4361
2023-11-21 18:20:56,471:INFO:  Epoch 176/500:  train Loss: 19.1972   val Loss: 24.4706   time: 429.73s   best: 23.4361
2023-11-21 18:28:05,459:INFO:  Epoch 177/500:  train Loss: 19.6075   val Loss: 25.0973   time: 428.99s   best: 23.4361
2023-11-21 18:35:14,391:INFO:  Epoch 178/500:  train Loss: 20.3385   val Loss: 24.2899   time: 428.93s   best: 23.4361
2023-11-21 18:42:25,811:INFO:  Epoch 179/500:  train Loss: 19.6267   val Loss: 25.4531   time: 431.40s   best: 23.4361
2023-11-21 18:49:36,840:INFO:  Epoch 180/500:  train Loss: 19.5490   val Loss: 23.6016   time: 431.02s   best: 23.4361
2023-11-21 18:56:46,020:INFO:  Epoch 181/500:  train Loss: 19.2503   val Loss: 23.9097   time: 429.18s   best: 23.4361
2023-11-21 19:03:56,832:INFO:  Epoch 182/500:  train Loss: 19.2811   val Loss: 23.8672   time: 430.81s   best: 23.4361
2023-11-21 19:11:05,566:INFO:  Epoch 183/500:  train Loss: 19.2969   val Loss: 23.5328   time: 428.73s   best: 23.4361
2023-11-21 19:18:16,963:INFO:  Epoch 184/500:  train Loss: 19.1513   val Loss: 23.4725   time: 431.38s   best: 23.4361
2023-11-21 19:25:26,983:INFO:  Epoch 185/500:  train Loss: 19.0651   val Loss: 23.6575   time: 430.01s   best: 23.4361
2023-11-21 19:32:36,115:INFO:  Epoch 186/500:  train Loss: 19.1227   val Loss: 23.8200   time: 429.13s   best: 23.4361
2023-11-21 19:39:45,092:INFO:  Epoch 187/500:  train Loss: 19.0002   val Loss: 23.6836   time: 428.96s   best: 23.4361
2023-11-21 19:46:54,402:INFO:  Epoch 188/500:  train Loss: 19.6353   val Loss: 23.9318   time: 429.30s   best: 23.4361
2023-11-21 19:54:05,458:INFO:  Epoch 189/500:  train Loss: 19.0102   val Loss: 23.7264   time: 431.04s   best: 23.4361
2023-11-21 20:01:18,902:INFO:  Epoch 190/500:  train Loss: 19.0338   val Loss: 23.6308   time: 433.43s   best: 23.4361
2023-11-21 20:08:31,862:INFO:  Epoch 191/500:  train Loss: 18.9012   val Loss: 24.7091   time: 432.94s   best: 23.4361
2023-11-21 20:15:45,042:INFO:  Epoch 192/500:  train Loss: 18.8901   val Loss: 23.7741   time: 433.18s   best: 23.4361
2023-11-21 20:22:55,059:INFO:  Epoch 193/500:  train Loss: 18.9310   val Loss: 24.4654   time: 430.02s   best: 23.4361
2023-11-21 20:30:07,133:INFO:  Epoch 194/500:  train Loss: 18.8752   val Loss: 23.6894   time: 432.06s   best: 23.4361
2023-11-21 20:37:20,236:INFO:  Epoch 195/500:  train Loss: 18.9182   val Loss: 23.6574   time: 433.10s   best: 23.4361
2023-11-21 20:44:30,449:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-21 20:44:30,468:INFO:  Epoch 196/500:  train Loss: 18.8792   val Loss: 23.3945   time: 430.20s   best: 23.3945
2023-11-21 20:51:45,242:INFO:  Epoch 197/500:  train Loss: 18.9526   val Loss: 23.6337   time: 434.77s   best: 23.3945
2023-11-21 20:58:55,608:INFO:  Epoch 198/500:  train Loss: 20.1896   val Loss: 23.7971   time: 430.36s   best: 23.3945
2023-11-21 21:06:08,870:INFO:  Epoch 199/500:  train Loss: 19.2169   val Loss: 24.0262   time: 433.25s   best: 23.3945
2023-11-21 21:13:21,733:INFO:  Epoch 200/500:  train Loss: 18.9829   val Loss: 23.5243   time: 432.86s   best: 23.3945
2023-11-21 21:20:31,883:INFO:  Epoch 201/500:  train Loss: 18.8783   val Loss: 24.0009   time: 430.14s   best: 23.3945
2023-11-21 21:27:41,715:INFO:  Epoch 202/500:  train Loss: 18.7451   val Loss: 24.3139   time: 429.81s   best: 23.3945
2023-11-21 21:34:54,189:INFO:  Epoch 203/500:  train Loss: 18.7378   val Loss: 23.9079   time: 432.46s   best: 23.3945
2023-11-21 21:42:06,262:INFO:  Epoch 204/500:  train Loss: 18.8389   val Loss: 23.6517   time: 432.06s   best: 23.3945
2023-11-21 21:49:17,451:INFO:  Epoch 205/500:  train Loss: 18.6307   val Loss: 25.5040   time: 431.19s   best: 23.3945
2023-11-21 21:56:27,985:INFO:  Epoch 206/500:  train Loss: 19.3706   val Loss: 24.4227   time: 430.52s   best: 23.3945
2023-11-21 22:03:41,084:INFO:  Epoch 207/500:  train Loss: 18.8436   val Loss: 23.7020   time: 433.10s   best: 23.3945
2023-11-21 22:10:51,940:INFO:  Epoch 208/500:  train Loss: 18.9026   val Loss: 23.9908   time: 430.84s   best: 23.3945
2023-11-21 22:18:05,229:INFO:  Epoch 209/500:  train Loss: 18.7408   val Loss: 23.8461   time: 433.28s   best: 23.3945
2023-11-21 22:25:14,741:INFO:  Epoch 210/500:  train Loss: 18.8262   val Loss: 24.4602   time: 429.50s   best: 23.3945
2023-11-21 22:32:23,556:INFO:  Epoch 211/500:  train Loss: 18.7104   val Loss: 24.0333   time: 428.81s   best: 23.3945
2023-11-21 22:39:36,416:INFO:  Epoch 212/500:  train Loss: 18.6132   val Loss: 23.6290   time: 432.86s   best: 23.3945
2023-11-21 22:46:49,747:INFO:  Epoch 213/500:  train Loss: 18.6523   val Loss: 24.1356   time: 433.33s   best: 23.3945
2023-11-21 22:54:02,173:INFO:  Epoch 214/500:  train Loss: 18.9565   val Loss: 23.9088   time: 432.43s   best: 23.3945
2023-11-21 23:01:14,481:INFO:  Epoch 215/500:  train Loss: 18.6966   val Loss: 23.5925   time: 432.31s   best: 23.3945
2023-11-21 23:08:29,504:INFO:  Epoch 216/500:  train Loss: 18.5712   val Loss: 23.5188   time: 435.01s   best: 23.3945
2023-11-21 23:15:38,312:INFO:  Epoch 217/500:  train Loss: 18.7987   val Loss: 23.6318   time: 428.79s   best: 23.3945
2023-11-21 23:22:51,567:INFO:  Epoch 218/500:  train Loss: 19.4864   val Loss: 23.5287   time: 433.24s   best: 23.3945
2023-11-21 23:30:01,525:INFO:  Epoch 219/500:  train Loss: 18.7495   val Loss: 23.5461   time: 429.96s   best: 23.3945
2023-11-21 23:37:15,878:INFO:  Epoch 220/500:  train Loss: 18.5553   val Loss: 23.5682   time: 434.35s   best: 23.3945
2023-11-21 23:44:29,431:INFO:  Epoch 221/500:  train Loss: 18.6184   val Loss: 23.4847   time: 433.55s   best: 23.3945
2023-11-21 23:51:42,692:INFO:  Epoch 222/500:  train Loss: 18.5130   val Loss: 24.2916   time: 433.25s   best: 23.3945
2023-11-21 23:58:56,514:INFO:  Epoch 223/500:  train Loss: 18.5882   val Loss: 23.6780   time: 433.81s   best: 23.3945
2023-11-22 00:06:05,766:INFO:  Epoch 224/500:  train Loss: 18.7791   val Loss: 23.9776   time: 429.25s   best: 23.3945
2023-11-22 00:13:19,054:INFO:  Epoch 225/500:  train Loss: 18.8328   val Loss: 24.3522   time: 433.29s   best: 23.3945
2023-11-22 00:20:32,716:INFO:  Epoch 226/500:  train Loss: 18.6249   val Loss: 23.9740   time: 433.66s   best: 23.3945
2023-11-22 00:27:45,153:INFO:  Epoch 227/500:  train Loss: 18.8144   val Loss: 31.3774   time: 432.43s   best: 23.3945
2023-11-22 00:34:59,117:INFO:  Epoch 228/500:  train Loss: 19.2762   val Loss: 24.2319   time: 433.95s   best: 23.3945
2023-11-22 00:42:09,073:INFO:  Epoch 229/500:  train Loss: 18.5070   val Loss: 23.5990   time: 429.94s   best: 23.3945
2023-11-22 00:49:22,205:INFO:  Epoch 230/500:  train Loss: 18.7955   val Loss: 23.9895   time: 433.12s   best: 23.3945
2023-11-22 00:56:35,305:INFO:  Epoch 231/500:  train Loss: 18.4990   val Loss: 23.7235   time: 433.08s   best: 23.3945
2023-11-22 01:03:49,305:INFO:  Epoch 232/500:  train Loss: 18.8020   val Loss: 23.8137   time: 434.00s   best: 23.3945
2023-11-22 01:11:02,881:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-22 01:11:02,949:INFO:  Epoch 233/500:  train Loss: 18.4955   val Loss: 23.2268   time: 433.55s   best: 23.2268
2023-11-22 01:18:16,521:INFO:  Epoch 234/500:  train Loss: 18.6798   val Loss: 23.7539   time: 433.55s   best: 23.2268
2023-11-22 01:25:30,192:INFO:  Epoch 235/500:  train Loss: 18.7281   val Loss: 23.6096   time: 433.66s   best: 23.2268
2023-11-22 01:32:44,083:INFO:  Epoch 236/500:  train Loss: 18.3935   val Loss: 24.2740   time: 433.88s   best: 23.2268
2023-11-22 01:39:57,357:INFO:  Epoch 237/500:  train Loss: 18.4101   val Loss: 23.5349   time: 433.26s   best: 23.2268
2023-11-22 01:47:11,280:INFO:  Epoch 238/500:  train Loss: 18.4911   val Loss: 23.2382   time: 433.90s   best: 23.2268
2023-11-22 01:54:21,027:INFO:  Epoch 239/500:  train Loss: 18.5268   val Loss: 24.2155   time: 429.75s   best: 23.2268
2023-11-22 02:01:33,484:INFO:  Epoch 240/500:  train Loss: 18.4674   val Loss: 23.2485   time: 432.46s   best: 23.2268
2023-11-22 02:08:42,887:INFO:  Epoch 241/500:  train Loss: 18.5798   val Loss: 23.8899   time: 429.39s   best: 23.2268
2023-11-22 02:15:54,370:INFO:  Epoch 242/500:  train Loss: 18.3407   val Loss: 23.7317   time: 431.48s   best: 23.2268
2023-11-22 02:23:07,903:INFO:  Epoch 243/500:  train Loss: 18.3065   val Loss: 23.7374   time: 433.52s   best: 23.2268
2023-11-22 02:30:21,460:INFO:  Epoch 244/500:  train Loss: 18.9739   val Loss: 23.9588   time: 433.53s   best: 23.2268
2023-11-22 02:37:34,769:INFO:  Epoch 245/500:  train Loss: 18.5337   val Loss: 23.9384   time: 433.30s   best: 23.2268
2023-11-22 02:44:45,297:INFO:  Epoch 246/500:  train Loss: 18.4006   val Loss: 24.0138   time: 430.53s   best: 23.2268
2023-11-22 02:51:59,173:INFO:  Epoch 247/500:  train Loss: 18.3365   val Loss: 24.1218   time: 433.87s   best: 23.2268
2023-11-22 02:59:12,580:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-22 02:59:12,598:INFO:  Epoch 248/500:  train Loss: 18.3961   val Loss: 23.1706   time: 433.40s   best: 23.1706
2023-11-22 03:06:22,465:INFO:  Epoch 249/500:  train Loss: 18.6273   val Loss: 23.9237   time: 429.87s   best: 23.1706
2023-11-22 03:13:35,342:INFO:  Epoch 250/500:  train Loss: 18.2592   val Loss: 23.8588   time: 432.88s   best: 23.1706
2023-11-22 03:20:49,782:INFO:  Epoch 251/500:  train Loss: 18.5742   val Loss: 23.6177   time: 434.43s   best: 23.1706
2023-11-22 03:28:01,345:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-22 03:28:01,363:INFO:  Epoch 252/500:  train Loss: 18.2955   val Loss: 23.0429   time: 431.55s   best: 23.0429
2023-11-22 03:35:15,752:INFO:  Epoch 253/500:  train Loss: 18.4485   val Loss: 23.7217   time: 434.38s   best: 23.0429
2023-11-22 03:42:30,189:INFO:  Epoch 254/500:  train Loss: 18.6074   val Loss: 23.0544   time: 434.42s   best: 23.0429
2023-11-22 03:49:42,945:INFO:  Epoch 255/500:  train Loss: 18.3537   val Loss: 23.9855   time: 432.75s   best: 23.0429
2023-11-22 03:56:58,004:INFO:  Epoch 256/500:  train Loss: 18.3214   val Loss: 23.5283   time: 435.06s   best: 23.0429
2023-11-22 04:04:13,420:INFO:  Epoch 257/500:  train Loss: 18.3447   val Loss: 23.5760   time: 435.40s   best: 23.0429
2023-11-22 04:11:29,074:INFO:  Epoch 258/500:  train Loss: 18.9153   val Loss: 25.3340   time: 435.64s   best: 23.0429
2023-11-22 04:18:42,051:INFO:  Epoch 259/500:  train Loss: 18.3541   val Loss: 23.6414   time: 432.98s   best: 23.0429
2023-11-22 04:25:52,076:INFO:  Epoch 260/500:  train Loss: 18.4235   val Loss: 23.7307   time: 430.01s   best: 23.0429
2023-11-22 04:33:05,476:INFO:  Epoch 261/500:  train Loss: 18.2808   val Loss: 23.5783   time: 433.38s   best: 23.0429
2023-11-22 04:40:15,535:INFO:  Epoch 262/500:  train Loss: 18.6274   val Loss: 23.7408   time: 430.06s   best: 23.0429
2023-11-22 04:47:29,334:INFO:  Epoch 263/500:  train Loss: 18.3452   val Loss: 25.3621   time: 433.79s   best: 23.0429
2023-11-22 04:54:43,041:INFO:  Epoch 264/500:  train Loss: 18.5678   val Loss: 23.5699   time: 433.69s   best: 23.0429
2023-11-22 05:01:55,686:INFO:  Epoch 265/500:  train Loss: 18.2946   val Loss: 23.7874   time: 432.63s   best: 23.0429
2023-11-22 05:09:05,071:INFO:  Epoch 266/500:  train Loss: 18.4415   val Loss: 24.1947   time: 429.37s   best: 23.0429
2023-11-22 05:16:19,130:INFO:  Epoch 267/500:  train Loss: 18.3356   val Loss: 23.8146   time: 434.06s   best: 23.0429
2023-11-22 05:23:28,756:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-22 05:23:28,774:INFO:  Epoch 268/500:  train Loss: 18.1763   val Loss: 22.9362   time: 429.62s   best: 22.9362
2023-11-22 05:30:42,096:INFO:  Epoch 269/500:  train Loss: 18.3796   val Loss: 24.2806   time: 433.32s   best: 22.9362
2023-11-22 05:37:51,926:INFO:  Epoch 270/500:  train Loss: 18.2583   val Loss: 23.6402   time: 429.81s   best: 22.9362
2023-11-22 05:45:05,176:INFO:  Epoch 271/500:  train Loss: 18.1517   val Loss: 23.0771   time: 433.25s   best: 22.9362
2023-11-22 05:52:15,370:INFO:  Epoch 272/500:  train Loss: 18.2851   val Loss: 23.0507   time: 430.19s   best: 22.9362
2023-11-22 05:59:29,275:INFO:  Epoch 273/500:  train Loss: 18.1431   val Loss: 24.0883   time: 433.89s   best: 22.9362
2023-11-22 06:06:43,036:INFO:  Epoch 274/500:  train Loss: 18.5091   val Loss: 23.8404   time: 433.75s   best: 22.9362
2023-11-22 06:13:52,927:INFO:  Epoch 275/500:  train Loss: 18.2701   val Loss: 24.0007   time: 429.89s   best: 22.9362
2023-11-22 06:21:02,188:INFO:  Epoch 276/500:  train Loss: 18.1622   val Loss: 24.0018   time: 429.24s   best: 22.9362
2023-11-22 06:28:11,345:INFO:  Epoch 277/500:  train Loss: 18.5189   val Loss: 23.4276   time: 429.16s   best: 22.9362
2023-11-22 06:35:24,487:INFO:  Epoch 278/500:  train Loss: 18.2753   val Loss: 24.1135   time: 433.14s   best: 22.9362
2023-11-22 06:42:37,287:INFO:  Epoch 279/500:  train Loss: 18.1344   val Loss: 23.9280   time: 432.80s   best: 22.9362
2023-11-22 06:49:46,137:INFO:  Epoch 280/500:  train Loss: 18.3123   val Loss: 24.1557   time: 428.84s   best: 22.9362
2023-11-22 06:56:59,313:INFO:  Epoch 281/500:  train Loss: 18.1300   val Loss: 24.4479   time: 433.16s   best: 22.9362
2023-11-22 07:04:13,477:INFO:  Epoch 282/500:  train Loss: 18.1554   val Loss: 23.3794   time: 434.16s   best: 22.9362
2023-11-22 07:11:25,796:INFO:  Epoch 283/500:  train Loss: 18.2029   val Loss: 23.7695   time: 432.31s   best: 22.9362
2023-11-22 07:18:38,847:INFO:  Epoch 284/500:  train Loss: 18.3662   val Loss: 23.1878   time: 433.04s   best: 22.9362
2023-11-22 07:25:52,466:INFO:  Epoch 285/500:  train Loss: 18.0991   val Loss: 23.6232   time: 433.62s   best: 22.9362
2023-11-22 07:33:04,768:INFO:  Epoch 286/500:  train Loss: 18.2592   val Loss: 23.4013   time: 432.30s   best: 22.9362
2023-11-22 07:40:14,695:INFO:  Epoch 287/500:  train Loss: 18.0856   val Loss: 24.0355   time: 429.91s   best: 22.9362
2023-11-22 07:47:24,356:INFO:  Epoch 288/500:  train Loss: 18.1085   val Loss: 23.6051   time: 429.66s   best: 22.9362
2023-11-22 07:54:38,188:INFO:  Epoch 289/500:  train Loss: 18.1339   val Loss: 23.8475   time: 433.83s   best: 22.9362
2023-11-22 08:01:52,045:INFO:  Epoch 290/500:  train Loss: 18.3836   val Loss: 23.8158   time: 433.86s   best: 22.9362
2023-11-22 08:09:01,931:INFO:  Epoch 291/500:  train Loss: 18.2409   val Loss: 23.4927   time: 429.88s   best: 22.9362
2023-11-22 08:16:16,017:INFO:  Epoch 292/500:  train Loss: 18.1082   val Loss: 23.6769   time: 434.08s   best: 22.9362
2023-11-22 08:23:28,588:INFO:  Epoch 293/500:  train Loss: 18.4749   val Loss: 24.0336   time: 432.57s   best: 22.9362
2023-11-22 08:30:42,946:INFO:  Epoch 294/500:  train Loss: 18.3796   val Loss: 23.4188   time: 434.34s   best: 22.9362
2023-11-22 08:37:56,427:INFO:  Epoch 295/500:  train Loss: 18.6148   val Loss: 22.9516   time: 433.47s   best: 22.9362
2023-11-22 08:45:07,350:INFO:  Epoch 296/500:  train Loss: 18.1543   val Loss: 24.1119   time: 430.91s   best: 22.9362
2023-11-22 08:52:17,975:INFO:  Epoch 297/500:  train Loss: 18.0105   val Loss: 23.9825   time: 430.62s   best: 22.9362
2023-11-22 08:59:30,249:INFO:  Epoch 298/500:  train Loss: 18.0833   val Loss: 23.6277   time: 432.27s   best: 22.9362
2023-11-22 09:06:40,811:INFO:  Epoch 299/500:  train Loss: 18.3888   val Loss: 23.5504   time: 430.55s   best: 22.9362
2023-11-22 09:13:50,505:INFO:  Epoch 300/500:  train Loss: 18.1696   val Loss: 25.7134   time: 429.69s   best: 22.9362
2023-11-22 09:21:00,076:INFO:  Epoch 301/500:  train Loss: 18.2839   val Loss: 23.7353   time: 429.56s   best: 22.9362
2023-11-22 09:23:37,171:INFO:  Starting experiment lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)
2023-11-22 09:23:37,173:INFO:  Defining the model
2023-11-22 09:23:37,258:INFO:  Reading the dataset
2023-11-22 09:28:13,901:INFO:  Epoch 302/500:  train Loss: 18.3366   val Loss: 24.0575   time: 433.81s   best: 22.9362
2023-11-22 09:35:24,045:INFO:  Epoch 303/500:  train Loss: 18.0846   val Loss: 23.6018   time: 430.14s   best: 22.9362
2023-11-22 09:35:26,324:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 09:35:26,345:INFO:  Epoch 1/500:  train Loss: 80.8758   val Loss: 71.6473   time: 433.81s   best: 71.6473
2023-11-22 09:42:32,473:INFO:  Epoch 304/500:  train Loss: 19.0886   val Loss: 25.0194   time: 428.42s   best: 22.9362
2023-11-22 09:42:36,309:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 09:42:36,357:INFO:  Epoch 2/500:  train Loss: 69.6000   val Loss: 67.7839   time: 429.95s   best: 67.7839
2023-11-22 09:49:44,792:INFO:  Epoch 305/500:  train Loss: 18.1903   val Loss: 24.0003   time: 432.32s   best: 22.9362
2023-11-22 09:49:45,678:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 09:49:45,699:INFO:  Epoch 3/500:  train Loss: 62.9020   val Loss: 59.9069   time: 429.31s   best: 59.9069
2023-11-22 09:56:57,538:INFO:  Epoch 306/500:  train Loss: 17.9873   val Loss: 24.0023   time: 432.74s   best: 22.9362
2023-11-22 09:56:57,785:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 09:56:57,803:INFO:  Epoch 4/500:  train Loss: 57.5156   val Loss: 55.8320   time: 432.07s   best: 55.8320
2023-11-22 10:04:07,221:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 10:04:07,240:INFO:  Epoch 5/500:  train Loss: 52.4526   val Loss: 49.8814   time: 429.41s   best: 49.8814
2023-11-22 10:04:10,401:INFO:  Epoch 307/500:  train Loss: 18.0004   val Loss: 23.9625   time: 432.85s   best: 22.9362
2023-11-22 10:11:15,910:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 10:11:15,930:INFO:  Epoch 6/500:  train Loss: 47.8597   val Loss: 46.4536   time: 428.67s   best: 46.4536
2023-11-22 10:11:24,842:INFO:  Epoch 308/500:  train Loss: 18.0066   val Loss: 23.7608   time: 434.44s   best: 22.9362
2023-11-22 10:18:26,664:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 10:18:26,699:INFO:  Epoch 7/500:  train Loss: 44.5455   val Loss: 44.1978   time: 430.73s   best: 44.1978
2023-11-22 10:18:37,424:INFO:  Epoch 309/500:  train Loss: 18.1466   val Loss: 24.2540   time: 432.58s   best: 22.9362
2023-11-22 10:25:34,452:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 10:25:34,471:INFO:  Epoch 8/500:  train Loss: 41.9465   val Loss: 41.2864   time: 427.75s   best: 41.2864
2023-11-22 10:25:50,287:INFO:  Epoch 310/500:  train Loss: 18.0458   val Loss: 24.0369   time: 432.86s   best: 22.9362
2023-11-22 10:32:43,016:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 10:32:43,037:INFO:  Epoch 9/500:  train Loss: 39.9441   val Loss: 41.2244   time: 428.53s   best: 41.2244
2023-11-22 10:33:00,388:INFO:  Epoch 311/500:  train Loss: 17.9916   val Loss: 23.4487   time: 430.08s   best: 22.9362
2023-11-22 10:39:54,848:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 10:39:54,867:INFO:  Epoch 10/500:  train Loss: 38.3084   val Loss: 39.0244   time: 431.80s   best: 39.0244
2023-11-22 10:40:12,162:INFO:  Epoch 312/500:  train Loss: 17.9808   val Loss: 23.5018   time: 431.76s   best: 22.9362
2023-11-22 10:47:07,302:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 10:47:07,322:INFO:  Epoch 11/500:  train Loss: 37.2042   val Loss: 37.6010   time: 432.42s   best: 37.6010
2023-11-22 10:47:25,664:INFO:  Epoch 313/500:  train Loss: 17.9684   val Loss: 23.0783   time: 433.50s   best: 22.9362
2023-11-22 10:54:15,643:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 10:54:15,663:INFO:  Epoch 12/500:  train Loss: 36.0361   val Loss: 36.6363   time: 428.30s   best: 36.6363
2023-11-22 10:54:36,998:INFO:  Epoch 314/500:  train Loss: 18.4909   val Loss: 23.1823   time: 431.32s   best: 22.9362
2023-11-22 11:01:23,894:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 11:01:23,914:INFO:  Epoch 13/500:  train Loss: 35.0903   val Loss: 35.4292   time: 428.23s   best: 35.4292
2023-11-22 11:01:50,841:INFO:  Epoch 315/500:  train Loss: 18.4703   val Loss: 23.6172   time: 433.83s   best: 22.9362
2023-11-22 11:08:32,243:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 11:08:32,273:INFO:  Epoch 14/500:  train Loss: 34.2876   val Loss: 35.3838   time: 428.31s   best: 35.3838
2023-11-22 11:09:04,717:INFO:  Epoch 316/500:  train Loss: 17.9676   val Loss: 23.6018   time: 433.86s   best: 22.9362
2023-11-22 11:15:44,261:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 11:15:44,290:INFO:  Epoch 15/500:  train Loss: 33.7140   val Loss: 34.7748   time: 431.97s   best: 34.7748
2023-11-22 11:16:16,220:INFO:  Epoch 317/500:  train Loss: 17.8791   val Loss: 23.7903   time: 431.50s   best: 22.9362
2023-11-22 11:22:53,380:INFO:  Epoch 16/500:  train Loss: 33.1262   val Loss: 34.8475   time: 429.08s   best: 34.7748
2023-11-22 11:23:27,553:INFO:  Epoch 318/500:  train Loss: 17.9192   val Loss: 24.3196   time: 431.33s   best: 22.9362
2023-11-22 11:30:04,855:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 11:30:04,888:INFO:  Epoch 17/500:  train Loss: 32.6920   val Loss: 33.4002   time: 431.46s   best: 33.4002
2023-11-22 11:30:36,890:INFO:  Epoch 319/500:  train Loss: 18.0881   val Loss: 23.2830   time: 429.33s   best: 22.9362
2023-11-22 11:37:12,621:INFO:  Epoch 18/500:  train Loss: 32.1507   val Loss: 33.9596   time: 427.73s   best: 33.4002
2023-11-22 11:37:50,524:INFO:  Epoch 320/500:  train Loss: 18.0956   val Loss: 23.7078   time: 433.62s   best: 22.9362
2023-11-22 11:44:25,912:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 11:44:25,932:INFO:  Epoch 19/500:  train Loss: 31.6303   val Loss: 32.8178   time: 433.27s   best: 32.8178
2023-11-22 11:45:03,563:INFO:  Epoch 321/500:  train Loss: 18.0078   val Loss: 23.8391   time: 433.04s   best: 22.9362
2023-11-22 11:51:33,906:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 11:51:33,937:INFO:  Epoch 20/500:  train Loss: 31.2272   val Loss: 32.1440   time: 427.96s   best: 32.1440
2023-11-22 11:52:16,613:INFO:  Epoch 322/500:  train Loss: 18.1173   val Loss: 23.4837   time: 433.04s   best: 22.9362
2023-11-22 11:58:41,491:INFO:  Epoch 21/500:  train Loss: 30.9846   val Loss: 32.3053   time: 427.55s   best: 32.1440
2023-11-22 11:59:30,997:INFO:  Epoch 323/500:  train Loss: 18.1497   val Loss: 23.6019   time: 434.38s   best: 22.9362
2023-11-22 12:05:53,916:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 12:05:53,937:INFO:  Epoch 22/500:  train Loss: 30.5322   val Loss: 31.5790   time: 432.41s   best: 31.5790
2023-11-22 12:06:40,979:INFO:  Epoch 324/500:  train Loss: 17.8957   val Loss: 23.8824   time: 429.98s   best: 22.9362
2023-11-22 12:13:00,751:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 12:13:00,771:INFO:  Epoch 23/500:  train Loss: 30.3267   val Loss: 31.5230   time: 426.80s   best: 31.5230
2023-11-22 12:13:54,250:INFO:  Epoch 325/500:  train Loss: 18.0691   val Loss: 23.1409   time: 433.26s   best: 22.9362
2023-11-22 12:20:07,571:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 12:20:07,590:INFO:  Epoch 24/500:  train Loss: 30.0342   val Loss: 31.2489   time: 426.78s   best: 31.2489
2023-11-22 12:21:03,571:INFO:  Epoch 326/500:  train Loss: 17.9356   val Loss: 23.9073   time: 429.31s   best: 22.9362
2023-11-22 12:27:19,650:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 12:27:19,681:INFO:  Epoch 25/500:  train Loss: 29.7111   val Loss: 30.6217   time: 432.06s   best: 30.6217
2023-11-22 12:28:12,860:INFO:  Epoch 327/500:  train Loss: 18.0244   val Loss: 34.9715   time: 429.27s   best: 22.9362
2023-11-22 12:34:31,790:INFO:  Epoch 26/500:  train Loss: 29.4656   val Loss: 30.7666   time: 432.10s   best: 30.6217
2023-11-22 12:35:25,322:INFO:  Epoch 328/500:  train Loss: 17.9411   val Loss: 26.1593   time: 432.46s   best: 22.9362
2023-11-22 12:41:44,540:INFO:  Epoch 27/500:  train Loss: 29.1398   val Loss: 30.9212   time: 432.75s   best: 30.6217
2023-11-22 12:42:38,727:INFO:  Epoch 329/500:  train Loss: 17.9510   val Loss: 23.4306   time: 433.39s   best: 22.9362
2023-11-22 12:48:51,566:INFO:  Epoch 28/500:  train Loss: 28.9366   val Loss: 30.9724   time: 427.02s   best: 30.6217
2023-11-22 12:49:49,023:INFO:  Epoch 330/500:  train Loss: 18.8016   val Loss: 24.0115   time: 430.29s   best: 22.9362
2023-11-22 12:56:03,231:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 12:56:03,251:INFO:  Epoch 29/500:  train Loss: 28.8020   val Loss: 30.3754   time: 431.65s   best: 30.3754
2023-11-22 12:56:58,183:INFO:  Epoch 331/500:  train Loss: 17.9270   val Loss: 24.5020   time: 429.16s   best: 22.9362
2023-11-22 13:03:15,821:INFO:  Epoch 30/500:  train Loss: 28.6285   val Loss: 31.7037   time: 432.56s   best: 30.3754
2023-11-22 13:04:11,404:INFO:  Epoch 332/500:  train Loss: 18.1351   val Loss: 24.0294   time: 433.22s   best: 22.9362
2023-11-22 13:10:27,127:INFO:  Epoch 31/500:  train Loss: 28.5794   val Loss: 31.1316   time: 431.30s   best: 30.3754
2023-11-22 13:11:21,384:INFO:  Epoch 333/500:  train Loss: 18.0859   val Loss: 24.0308   time: 429.97s   best: 22.9362
2023-11-22 13:17:34,798:INFO:  Epoch 32/500:  train Loss: 28.5137   val Loss: 33.1920   time: 427.67s   best: 30.3754
2023-11-22 13:18:34,975:INFO:  Epoch 334/500:  train Loss: 17.9245   val Loss: 23.4291   time: 433.57s   best: 22.9362
2023-11-22 13:24:45,540:INFO:  Epoch 33/500:  train Loss: 28.1395   val Loss: 30.9309   time: 430.74s   best: 30.3754
2023-11-22 13:25:47,609:INFO:  Epoch 335/500:  train Loss: 17.9815   val Loss: 23.0848   time: 432.62s   best: 22.9362
2023-11-22 13:31:57,138:INFO:  Epoch 34/500:  train Loss: 27.8714   val Loss: 30.7039   time: 431.58s   best: 30.3754
2023-11-22 13:33:01,347:INFO:  Epoch 336/500:  train Loss: 17.8308   val Loss: 24.3659   time: 433.72s   best: 22.9362
2023-11-22 13:39:09,058:INFO:  Epoch 35/500:  train Loss: 27.7509   val Loss: 30.4361   time: 431.91s   best: 30.3754
2023-11-22 13:40:14,697:INFO:  Epoch 337/500:  train Loss: 18.0742   val Loss: 24.0270   time: 433.35s   best: 22.9362
2023-11-22 13:46:21,037:INFO:  Epoch 36/500:  train Loss: 27.5270   val Loss: 31.2893   time: 431.96s   best: 30.3754
2023-11-22 13:47:28,785:INFO:  Epoch 338/500:  train Loss: 18.1404   val Loss: 24.0035   time: 434.07s   best: 22.9362
2023-11-22 13:53:30,041:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 13:53:30,062:INFO:  Epoch 37/500:  train Loss: 27.4135   val Loss: 30.2494   time: 429.00s   best: 30.2494
2023-11-22 13:54:42,820:INFO:  Epoch 339/500:  train Loss: 18.0690   val Loss: 23.3511   time: 434.03s   best: 22.9362
2023-11-22 14:00:42,473:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 14:00:42,493:INFO:  Epoch 38/500:  train Loss: 27.2860   val Loss: 30.0051   time: 432.40s   best: 30.0051
2023-11-22 14:01:53,080:INFO:  Epoch 340/500:  train Loss: 18.0907   val Loss: 23.8618   time: 430.26s   best: 22.9362
2023-11-22 14:07:54,278:INFO:  Epoch 39/500:  train Loss: 27.2661   val Loss: 32.4091   time: 431.78s   best: 30.0051
2023-11-22 14:09:06,591:INFO:  Epoch 341/500:  train Loss: 17.7430   val Loss: 23.4306   time: 433.50s   best: 22.9362
2023-11-22 14:15:05,563:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 14:15:05,583:INFO:  Epoch 40/500:  train Loss: 27.1756   val Loss: 29.6444   time: 431.25s   best: 29.6444
2023-11-22 14:16:19,797:INFO:  Epoch 342/500:  train Loss: 17.8639   val Loss: 23.6429   time: 433.19s   best: 22.9362
2023-11-22 14:22:16,233:INFO:  Epoch 41/500:  train Loss: 26.8905   val Loss: 30.6619   time: 430.64s   best: 29.6444
2023-11-22 14:23:28,352:INFO:  Epoch 343/500:  train Loss: 17.8964   val Loss: 23.8110   time: 428.54s   best: 22.9362
2023-11-22 14:29:23,791:INFO:  Epoch 42/500:  train Loss: 26.7191   val Loss: 29.9384   time: 427.56s   best: 29.6444
2023-11-22 14:30:39,291:INFO:  Epoch 344/500:  train Loss: 17.8870   val Loss: 23.0954   time: 430.94s   best: 22.9362
2023-11-22 14:36:34,835:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 14:36:34,855:INFO:  Epoch 43/500:  train Loss: 26.9160   val Loss: 29.2788   time: 431.02s   best: 29.2788
2023-11-22 14:37:52,663:INFO:  Epoch 345/500:  train Loss: 17.9430   val Loss: 23.1632   time: 433.36s   best: 22.9362
2023-11-22 14:43:47,042:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 14:43:47,061:INFO:  Epoch 44/500:  train Loss: 26.5846   val Loss: 29.2551   time: 432.18s   best: 29.2551
2023-11-22 14:45:02,846:INFO:  Epoch 346/500:  train Loss: 18.6336   val Loss: 23.7941   time: 430.17s   best: 22.9362
2023-11-22 14:50:58,992:INFO:  Epoch 45/500:  train Loss: 26.3949   val Loss: 29.4589   time: 431.93s   best: 29.2551
2023-11-22 14:52:11,981:INFO:  Epoch 347/500:  train Loss: 17.7335   val Loss: 23.1013   time: 429.13s   best: 22.9362
2023-11-22 14:58:07,032:INFO:  Epoch 46/500:  train Loss: 26.6949   val Loss: 29.4689   time: 428.01s   best: 29.2551
2023-11-22 14:59:26,048:INFO:  Epoch 348/500:  train Loss: 17.8938   val Loss: 24.4910   time: 434.05s   best: 22.9362
2023-11-22 15:05:14,836:INFO:  Epoch 47/500:  train Loss: 26.2564   val Loss: 30.8302   time: 427.80s   best: 29.2551
2023-11-22 15:06:36,642:INFO:  Epoch 349/500:  train Loss: 17.7543   val Loss: 23.6940   time: 430.58s   best: 22.9362
2023-11-22 15:12:27,212:INFO:  Epoch 48/500:  train Loss: 26.2419   val Loss: 29.9859   time: 432.37s   best: 29.2551
2023-11-22 15:13:46,751:INFO:  Epoch 350/500:  train Loss: 17.9152   val Loss: 24.3557   time: 430.11s   best: 22.9362
2023-11-22 15:19:38,973:INFO:  Epoch 49/500:  train Loss: 26.0699   val Loss: 29.2872   time: 431.76s   best: 29.2551
2023-11-22 15:20:59,807:INFO:  Epoch 351/500:  train Loss: 17.7106   val Loss: 23.8204   time: 433.05s   best: 22.9362
2023-11-22 15:26:47,048:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 15:26:47,068:INFO:  Epoch 50/500:  train Loss: 26.3983   val Loss: 28.7379   time: 428.06s   best: 28.7379
2023-11-22 15:28:09,996:INFO:  Epoch 352/500:  train Loss: 17.8225   val Loss: 23.8335   time: 430.19s   best: 22.9362
2023-11-22 15:33:57,790:INFO:  Epoch 51/500:  train Loss: 26.1256   val Loss: 28.9178   time: 430.72s   best: 28.7379
2023-11-22 15:35:19,856:INFO:  Epoch 353/500:  train Loss: 17.7356   val Loss: 23.5431   time: 429.86s   best: 22.9362
2023-11-22 15:41:09,892:INFO:  Epoch 52/500:  train Loss: 25.8990   val Loss: 29.3842   time: 432.09s   best: 28.7379
2023-11-22 15:42:29,740:INFO:  Epoch 354/500:  train Loss: 17.7844   val Loss: 23.0246   time: 429.88s   best: 22.9362
2023-11-22 15:48:19,714:INFO:  Epoch 53/500:  train Loss: 25.8300   val Loss: 29.0176   time: 429.81s   best: 28.7379
2023-11-22 15:49:38,370:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-22 15:49:38,390:INFO:  Epoch 355/500:  train Loss: 17.8120   val Loss: 22.6873   time: 428.61s   best: 22.6873
2023-11-22 15:55:27,933:INFO:  Epoch 54/500:  train Loss: 25.7400   val Loss: 28.8867   time: 428.21s   best: 28.7379
2023-11-22 15:56:52,157:INFO:  Epoch 356/500:  train Loss: 17.9394   val Loss: 24.1422   time: 433.75s   best: 22.6873
2023-11-22 16:02:36,458:INFO:  Epoch 55/500:  train Loss: 25.5096   val Loss: 29.1799   time: 428.50s   best: 28.7379
2023-11-22 16:04:04,551:INFO:  Epoch 357/500:  train Loss: 17.9774   val Loss: 23.3630   time: 432.39s   best: 22.6873
2023-11-22 16:09:49,854:INFO:  Epoch 56/500:  train Loss: 25.5344   val Loss: 29.0420   time: 433.39s   best: 28.7379
2023-11-22 16:11:15,662:INFO:  Epoch 358/500:  train Loss: 17.8840   val Loss: 24.0444   time: 431.11s   best: 22.6873
2023-11-22 16:17:01,658:INFO:  Epoch 57/500:  train Loss: 25.5144   val Loss: 28.9507   time: 431.80s   best: 28.7379
2023-11-22 16:18:25,443:INFO:  Epoch 359/500:  train Loss: 17.7716   val Loss: 23.4670   time: 429.77s   best: 22.6873
2023-11-22 16:24:13,627:INFO:  Epoch 58/500:  train Loss: 25.3321   val Loss: 29.4902   time: 431.96s   best: 28.7379
2023-11-22 16:25:39,656:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-22 16:25:39,675:INFO:  Epoch 360/500:  train Loss: 17.8922   val Loss: 22.6843   time: 434.19s   best: 22.6843
2023-11-22 16:31:25,316:INFO:  Epoch 59/500:  train Loss: 25.2448   val Loss: 28.8766   time: 431.69s   best: 28.7379
2023-11-22 16:32:53,250:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-22 16:32:53,269:INFO:  Epoch 361/500:  train Loss: 17.6817   val Loss: 22.6247   time: 433.57s   best: 22.6247
2023-11-22 16:38:33,579:INFO:  Epoch 60/500:  train Loss: 25.1316   val Loss: 28.7887   time: 428.26s   best: 28.7379
2023-11-22 16:40:06,828:INFO:  Epoch 362/500:  train Loss: 17.8183   val Loss: 24.2081   time: 433.55s   best: 22.6247
2023-11-22 16:45:42,245:INFO:  Epoch 61/500:  train Loss: 25.0701   val Loss: 28.8623   time: 428.66s   best: 28.7379
2023-11-22 16:47:20,691:INFO:  Epoch 363/500:  train Loss: 17.7450   val Loss: 23.5229   time: 433.85s   best: 22.6247
2023-11-22 16:52:55,499:INFO:  Epoch 62/500:  train Loss: 25.0750   val Loss: 29.0243   time: 433.23s   best: 28.7379
2023-11-22 16:54:34,898:INFO:  Epoch 364/500:  train Loss: 17.6349   val Loss: 23.1476   time: 434.19s   best: 22.6247
2023-11-22 17:00:09,123:INFO:  Epoch 63/500:  train Loss: 24.9855   val Loss: 30.0645   time: 433.61s   best: 28.7379
2023-11-22 17:01:46,091:INFO:  Epoch 365/500:  train Loss: 17.7863   val Loss: 22.6397   time: 431.18s   best: 22.6247
2023-11-22 17:07:17,896:INFO:  Epoch 64/500:  train Loss: 24.8895   val Loss: 28.9496   time: 428.77s   best: 28.7379
2023-11-22 17:09:00,203:INFO:  Epoch 366/500:  train Loss: 17.8050   val Loss: 24.1599   time: 434.11s   best: 22.6247
2023-11-22 17:14:27,762:INFO:  Epoch 65/500:  train Loss: 24.9839   val Loss: 29.7935   time: 429.85s   best: 28.7379
2023-11-22 17:16:13,510:INFO:  Epoch 367/500:  train Loss: 17.5802   val Loss: 23.7093   time: 433.28s   best: 22.6247
2023-11-22 17:21:40,027:INFO:  Epoch 66/500:  train Loss: 24.7229   val Loss: 29.4991   time: 432.25s   best: 28.7379
2023-11-22 17:23:23,494:INFO:  Epoch 368/500:  train Loss: 17.8158   val Loss: 23.8291   time: 429.98s   best: 22.6247
2023-11-22 17:28:50,535:INFO:  Epoch 67/500:  train Loss: 25.0036   val Loss: 30.4832   time: 430.51s   best: 28.7379
2023-11-22 17:30:34,103:INFO:  Epoch 369/500:  train Loss: 17.7115   val Loss: 23.4418   time: 430.61s   best: 22.6247
2023-11-22 17:36:04,003:INFO:  Epoch 68/500:  train Loss: 24.7619   val Loss: 28.8806   time: 433.47s   best: 28.7379
2023-11-22 17:37:47,461:INFO:  Epoch 370/500:  train Loss: 18.1761   val Loss: 23.5411   time: 433.33s   best: 22.6247
2023-11-22 17:43:16,182:INFO:  Epoch 69/500:  train Loss: 24.7866   val Loss: 29.7101   time: 432.18s   best: 28.7379
2023-11-22 17:45:01,964:INFO:  Epoch 371/500:  train Loss: 17.6489   val Loss: 23.5679   time: 434.50s   best: 22.6247
2023-11-22 17:50:26,892:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 17:50:26,914:INFO:  Epoch 70/500:  train Loss: 24.6181   val Loss: 28.7012   time: 430.70s   best: 28.7012
2023-11-22 17:52:16,913:INFO:  Epoch 372/500:  train Loss: 17.6986   val Loss: 23.1039   time: 434.95s   best: 22.6247
2023-11-22 17:57:35,792:INFO:  Epoch 71/500:  train Loss: 24.6109   val Loss: 29.7917   time: 428.88s   best: 28.7012
2023-11-22 17:59:29,979:INFO:  Epoch 373/500:  train Loss: 17.6271   val Loss: 23.5184   time: 433.06s   best: 22.6247
2023-11-22 18:04:44,324:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 18:04:44,346:INFO:  Epoch 72/500:  train Loss: 24.6471   val Loss: 28.3610   time: 428.52s   best: 28.3610
2023-11-22 18:06:41,761:INFO:  Epoch 374/500:  train Loss: 17.5786   val Loss: 23.4751   time: 431.77s   best: 22.6247
2023-11-22 18:11:53,609:INFO:  Epoch 73/500:  train Loss: 24.4651   val Loss: 28.7549   time: 429.25s   best: 28.3610
2023-11-22 18:13:56,060:INFO:  Epoch 375/500:  train Loss: 17.6608   val Loss: 23.5471   time: 434.29s   best: 22.6247
2023-11-22 18:19:05,217:INFO:  Epoch 74/500:  train Loss: 24.3009   val Loss: 28.4435   time: 431.61s   best: 28.3610
2023-11-22 18:21:07,436:INFO:  Epoch 376/500:  train Loss: 17.6546   val Loss: 23.6572   time: 431.37s   best: 22.6247
2023-11-22 18:26:16,121:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 18:26:16,187:INFO:  Epoch 75/500:  train Loss: 24.3269   val Loss: 28.0368   time: 430.88s   best: 28.0368
2023-11-22 18:28:20,346:INFO:  Epoch 377/500:  train Loss: 17.6708   val Loss: 23.9383   time: 432.90s   best: 22.6247
2023-11-22 18:33:25,194:INFO:  Epoch 76/500:  train Loss: 24.3420   val Loss: 28.3549   time: 429.00s   best: 28.0368
2023-11-22 18:35:30,233:INFO:  Epoch 378/500:  train Loss: 17.6974   val Loss: 22.7053   time: 429.88s   best: 22.6247
2023-11-22 18:40:38,020:INFO:  Epoch 77/500:  train Loss: 24.2059   val Loss: 29.5093   time: 432.82s   best: 28.0368
2023-11-22 18:42:40,951:INFO:  Epoch 379/500:  train Loss: 17.6703   val Loss: 22.7832   time: 430.69s   best: 22.6247
2023-11-22 18:47:46,145:INFO:  Epoch 78/500:  train Loss: 24.1732   val Loss: 28.8378   time: 428.11s   best: 28.0368
2023-11-22 18:49:54,444:INFO:  Epoch 380/500:  train Loss: 17.5559   val Loss: 23.4607   time: 433.49s   best: 22.6247
2023-11-22 18:54:58,743:INFO:  Epoch 79/500:  train Loss: 24.1032   val Loss: 28.6193   time: 432.59s   best: 28.0368
2023-11-22 18:57:09,305:INFO:  Epoch 381/500:  train Loss: 17.6115   val Loss: 23.8172   time: 434.86s   best: 22.6247
2023-11-22 19:02:07,497:INFO:  Epoch 80/500:  train Loss: 24.0741   val Loss: 28.9383   time: 428.74s   best: 28.0368
2023-11-22 19:04:24,942:INFO:  Epoch 382/500:  train Loss: 17.7230   val Loss: 23.2651   time: 435.64s   best: 22.6247
2023-11-22 19:09:19,047:INFO:  Epoch 81/500:  train Loss: 24.1029   val Loss: 28.1608   time: 431.55s   best: 28.0368
2023-11-22 19:11:34,996:INFO:  Epoch 383/500:  train Loss: 17.6275   val Loss: 24.0204   time: 430.05s   best: 22.6247
2023-11-22 19:16:27,630:INFO:  Epoch 82/500:  train Loss: 24.0552   val Loss: 28.5895   time: 428.57s   best: 28.0368
2023-11-22 19:18:44,673:INFO:  Epoch 384/500:  train Loss: 17.6488   val Loss: 23.0256   time: 429.68s   best: 22.6247
2023-11-22 19:23:40,853:INFO:  Epoch 83/500:  train Loss: 23.9760   val Loss: 28.7835   time: 433.22s   best: 28.0368
2023-11-22 19:25:57,554:INFO:  Epoch 385/500:  train Loss: 17.6931   val Loss: 23.2172   time: 432.88s   best: 22.6247
2023-11-22 19:30:48,519:INFO:  Epoch 84/500:  train Loss: 23.9935   val Loss: 28.7913   time: 427.64s   best: 28.0368
2023-11-22 19:33:11,560:INFO:  Epoch 386/500:  train Loss: 18.1447   val Loss: 22.6477   time: 434.00s   best: 22.6247
2023-11-22 19:37:59,130:INFO:  Epoch 85/500:  train Loss: 23.8894   val Loss: 29.3411   time: 430.59s   best: 28.0368
2023-11-22 19:40:24,032:INFO:  Epoch 387/500:  train Loss: 17.8064   val Loss: 23.1651   time: 432.47s   best: 22.6247
2023-11-22 19:45:07,702:INFO:  Epoch 86/500:  train Loss: 23.8049   val Loss: 28.4195   time: 428.56s   best: 28.0368
2023-11-22 19:47:36,467:INFO:  Epoch 388/500:  train Loss: 17.5250   val Loss: 24.4920   time: 432.42s   best: 22.6247
2023-11-22 19:52:19,686:INFO:  Epoch 87/500:  train Loss: 23.7766   val Loss: 28.0618   time: 431.98s   best: 28.0368
2023-11-22 19:54:50,330:INFO:  Epoch 389/500:  train Loss: 17.6880   val Loss: 23.2656   time: 433.85s   best: 22.6247
2023-11-22 19:59:29,925:INFO:  Epoch 88/500:  train Loss: 23.9755   val Loss: 28.5007   time: 430.24s   best: 28.0368
2023-11-22 20:02:02,996:INFO:  Epoch 390/500:  train Loss: 17.5615   val Loss: 23.5560   time: 432.65s   best: 22.6247
2023-11-22 20:06:42,325:INFO:  Epoch 89/500:  train Loss: 23.6712   val Loss: 28.3526   time: 432.38s   best: 28.0368
2023-11-22 20:09:17,248:INFO:  Epoch 391/500:  train Loss: 17.5477   val Loss: 23.7286   time: 434.25s   best: 22.6247
2023-11-22 20:13:54,677:INFO:  Epoch 90/500:  train Loss: 23.6596   val Loss: 28.4029   time: 432.34s   best: 28.0368
2023-11-22 20:16:31,677:INFO:  Epoch 392/500:  train Loss: 17.7630   val Loss: 22.9868   time: 434.43s   best: 22.6247
2023-11-22 20:21:03,805:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 20:21:03,829:INFO:  Epoch 91/500:  train Loss: 23.7079   val Loss: 27.9631   time: 429.11s   best: 27.9631
2023-11-22 20:23:45,227:INFO:  Epoch 393/500:  train Loss: 17.6613   val Loss: 23.4522   time: 433.53s   best: 22.6247
2023-11-22 20:28:11,662:INFO:  Epoch 92/500:  train Loss: 23.6407   val Loss: 28.0374   time: 427.83s   best: 27.9631
2023-11-22 20:30:55,490:INFO:  Epoch 394/500:  train Loss: 17.5714   val Loss: 23.1473   time: 430.26s   best: 22.6247
2023-11-22 20:35:24,464:INFO:  Epoch 93/500:  train Loss: 23.6852   val Loss: 28.0915   time: 432.80s   best: 27.9631
2023-11-22 20:38:10,841:INFO:  Epoch 395/500:  train Loss: 17.8380   val Loss: 23.1774   time: 435.35s   best: 22.6247
2023-11-22 20:42:35,472:INFO:  Epoch 94/500:  train Loss: 23.8357   val Loss: 28.7290   time: 431.00s   best: 27.9631
2023-11-22 20:45:20,145:INFO:  Epoch 396/500:  train Loss: 17.5481   val Loss: 23.2173   time: 429.30s   best: 22.6247
2023-11-22 20:49:44,504:INFO:  Epoch 95/500:  train Loss: 23.5898   val Loss: 28.5059   time: 429.02s   best: 27.9631
2023-11-22 20:52:32,457:INFO:  Epoch 397/500:  train Loss: 18.0745   val Loss: 23.3984   time: 432.30s   best: 22.6247
2023-11-22 20:56:55,145:INFO:  Epoch 96/500:  train Loss: 23.4247   val Loss: 28.1489   time: 430.64s   best: 27.9631
2023-11-22 20:59:42,612:INFO:  Epoch 398/500:  train Loss: 17.6031   val Loss: 23.5462   time: 430.14s   best: 22.6247
2023-11-22 21:04:03,388:INFO:  Epoch 97/500:  train Loss: 23.5638   val Loss: 28.3397   time: 428.24s   best: 27.9631
2023-11-22 21:06:57,870:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 (0.05 dropout)_fa4a.pt
2023-11-22 21:06:57,889:INFO:  Epoch 399/500:  train Loss: 17.6221   val Loss: 22.0977   time: 435.24s   best: 22.0977
2023-11-22 21:11:11,700:INFO:  Epoch 98/500:  train Loss: 23.5381   val Loss: 28.3081   time: 428.31s   best: 27.9631
2023-11-22 21:14:11,210:INFO:  Epoch 400/500:  train Loss: 17.9642   val Loss: 22.9866   time: 433.31s   best: 22.0977
2023-11-22 21:18:19,211:INFO:  Epoch 99/500:  train Loss: 23.4218   val Loss: 28.0564   time: 427.50s   best: 27.9631
2023-11-22 21:21:21,194:INFO:  Epoch 401/500:  train Loss: 17.5480   val Loss: 22.6352   time: 429.98s   best: 22.0977
2023-11-22 21:25:30,919:INFO:  Epoch 100/500:  train Loss: 23.2927   val Loss: 28.5241   time: 431.69s   best: 27.9631
2023-11-22 21:28:35,618:INFO:  Epoch 402/500:  train Loss: 17.5990   val Loss: 24.0405   time: 434.41s   best: 22.0977
2023-11-22 21:32:43,302:INFO:  Epoch 101/500:  train Loss: 23.4553   val Loss: 30.2718   time: 432.37s   best: 27.9631
2023-11-22 21:35:48,952:INFO:  Epoch 403/500:  train Loss: 17.6733   val Loss: 23.1959   time: 433.32s   best: 22.0977
2023-11-22 21:39:54,550:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 21:39:54,584:INFO:  Epoch 102/500:  train Loss: 23.3653   val Loss: 27.8526   time: 431.23s   best: 27.8526
2023-11-22 21:42:59,071:INFO:  Epoch 404/500:  train Loss: 17.7042   val Loss: 23.3760   time: 430.12s   best: 22.0977
2023-11-22 21:47:04,727:INFO:  Epoch 103/500:  train Loss: 23.5922   val Loss: 28.0161   time: 430.14s   best: 27.8526
2023-11-22 21:50:12,577:INFO:  Epoch 405/500:  train Loss: 17.7619   val Loss: 23.2804   time: 433.47s   best: 22.0977
2023-11-22 21:54:14,706:INFO:  Epoch 104/500:  train Loss: 23.2444   val Loss: 28.3801   time: 429.96s   best: 27.8526
2023-11-22 21:57:25,349:INFO:  Epoch 406/500:  train Loss: 17.7336   val Loss: 23.4550   time: 432.76s   best: 22.0977
2023-11-22 22:01:28,009:INFO:  Epoch 105/500:  train Loss: 23.2448   val Loss: 27.9063   time: 433.30s   best: 27.8526
2023-11-22 22:04:35,697:INFO:  Epoch 407/500:  train Loss: 17.5644   val Loss: 22.9214   time: 430.35s   best: 22.0977
2023-11-22 22:08:40,196:INFO:  Epoch 106/500:  train Loss: 23.1033   val Loss: 29.3004   time: 432.17s   best: 27.8526
2023-11-22 22:11:44,906:INFO:  Epoch 408/500:  train Loss: 17.6531   val Loss: 24.2741   time: 429.21s   best: 22.0977
2023-11-22 22:15:52,979:INFO:  Epoch 107/500:  train Loss: 23.1519   val Loss: 28.1186   time: 432.78s   best: 27.8526
2023-11-22 22:18:59,038:INFO:  Epoch 409/500:  train Loss: 17.4736   val Loss: 23.8510   time: 434.13s   best: 22.0977
2023-11-22 22:23:05,498:INFO:  Epoch 108/500:  train Loss: 23.1577   val Loss: 28.0106   time: 432.52s   best: 27.8526
2023-11-22 22:26:13,507:INFO:  Epoch 410/500:  train Loss: 17.6144   val Loss: 23.6441   time: 434.45s   best: 22.0977
2023-11-22 22:30:16,908:INFO:  Epoch 109/500:  train Loss: 23.0745   val Loss: 29.4009   time: 431.41s   best: 27.8526
2023-11-22 22:33:26,718:INFO:  Epoch 411/500:  train Loss: 17.6841   val Loss: 23.1529   time: 433.21s   best: 22.0977
2023-11-22 22:37:25,910:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 22:37:25,931:INFO:  Epoch 110/500:  train Loss: 23.5011   val Loss: 27.7236   time: 428.98s   best: 27.7236
2023-11-22 22:40:36,831:INFO:  Epoch 412/500:  train Loss: 17.6319   val Loss: 23.1648   time: 430.10s   best: 22.0977
2023-11-22 22:44:36,047:INFO:  Epoch 111/500:  train Loss: 23.0322   val Loss: 27.9274   time: 430.10s   best: 27.7236
2023-11-22 22:47:50,785:INFO:  Epoch 413/500:  train Loss: 17.4327   val Loss: 23.5563   time: 433.95s   best: 22.0977
2023-11-22 22:51:44,732:INFO:  Epoch 112/500:  train Loss: 22.9933   val Loss: 28.1811   time: 428.68s   best: 27.7236
2023-11-22 22:55:02,974:INFO:  Epoch 414/500:  train Loss: 17.4337   val Loss: 23.2304   time: 432.17s   best: 22.0977
2023-11-22 22:58:52,733:INFO:  Epoch 113/500:  train Loss: 23.0683   val Loss: 27.8866   time: 428.00s   best: 27.7236
2023-11-22 23:02:18,003:INFO:  Epoch 415/500:  train Loss: 17.4908   val Loss: 23.1606   time: 435.03s   best: 22.0977
2023-11-22 23:06:01,871:INFO:  Epoch 114/500:  train Loss: 22.9631   val Loss: 29.0898   time: 429.13s   best: 27.7236
2023-11-22 23:09:28,526:INFO:  Epoch 416/500:  train Loss: 17.5131   val Loss: 23.5101   time: 430.51s   best: 22.0977
2023-11-22 23:13:14,445:INFO:  Epoch 115/500:  train Loss: 23.1947   val Loss: 27.8285   time: 432.56s   best: 27.7236
2023-11-22 23:16:40,771:INFO:  Epoch 417/500:  train Loss: 17.4911   val Loss: 23.2771   time: 432.23s   best: 22.0977
2023-11-22 23:20:23,051:INFO:  Epoch 116/500:  train Loss: 23.1610   val Loss: 28.3718   time: 428.60s   best: 27.7236
2023-11-22 23:23:50,509:INFO:  Epoch 418/500:  train Loss: 17.5291   val Loss: 23.9106   time: 429.73s   best: 22.0977
2023-11-22 23:27:33,847:INFO:  Epoch 117/500:  train Loss: 22.9301   val Loss: 28.3916   time: 430.78s   best: 27.7236
2023-11-22 23:30:59,988:INFO:  Epoch 419/500:  train Loss: 17.4512   val Loss: 23.3355   time: 429.46s   best: 22.0977
2023-11-22 23:34:42,740:INFO:  Epoch 118/500:  train Loss: 22.8767   val Loss: 27.8414   time: 428.89s   best: 27.7236
2023-11-22 23:38:09,949:INFO:  Epoch 420/500:  train Loss: 17.4359   val Loss: 23.7605   time: 429.93s   best: 22.0977
2023-11-22 23:41:51,260:INFO:  Epoch 119/500:  train Loss: 23.3494   val Loss: 27.7687   time: 428.50s   best: 27.7236
2023-11-22 23:45:19,120:INFO:  Epoch 421/500:  train Loss: 17.4929   val Loss: 23.8309   time: 429.17s   best: 22.0977
2023-11-22 23:49:03,641:INFO:  Epoch 120/500:  train Loss: 22.7556   val Loss: 27.9582   time: 432.38s   best: 27.7236
2023-11-22 23:52:29,155:INFO:  Epoch 422/500:  train Loss: 17.5557   val Loss: 23.5765   time: 430.03s   best: 22.0977
2023-11-22 23:56:11,951:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-22 23:56:11,972:INFO:  Epoch 121/500:  train Loss: 22.8496   val Loss: 27.6601   time: 428.30s   best: 27.6601
2023-11-22 23:59:42,126:INFO:  Epoch 423/500:  train Loss: 18.2348   val Loss: 24.2047   time: 432.96s   best: 22.0977
2023-11-23 00:03:23,885:INFO:  Epoch 122/500:  train Loss: 22.8897   val Loss: 27.6944   time: 431.90s   best: 27.6601
2023-11-23 00:06:51,930:INFO:  Epoch 424/500:  train Loss: 17.4956   val Loss: 24.3984   time: 429.80s   best: 22.0977
2023-11-23 00:10:32,266:INFO:  Epoch 123/500:  train Loss: 22.7966   val Loss: 27.8710   time: 428.37s   best: 27.6601
2023-11-23 00:14:06,198:INFO:  Epoch 425/500:  train Loss: 17.4235   val Loss: 22.7180   time: 434.27s   best: 22.0977
2023-11-23 00:17:43,766:INFO:  Epoch 124/500:  train Loss: 22.7606   val Loss: 28.3167   time: 431.50s   best: 27.6601
2023-11-23 00:21:19,163:INFO:  Epoch 426/500:  train Loss: 17.6898   val Loss: 22.9614   time: 432.95s   best: 22.0977
2023-11-23 00:24:55,647:INFO:  Epoch 125/500:  train Loss: 22.7010   val Loss: 27.7526   time: 431.88s   best: 27.6601
2023-11-23 00:28:33,918:INFO:  Epoch 427/500:  train Loss: 17.4141   val Loss: 23.3590   time: 434.75s   best: 22.0977
2023-11-23 00:32:04,967:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-23 00:32:05,009:INFO:  Epoch 126/500:  train Loss: 22.7316   val Loss: 27.6380   time: 429.31s   best: 27.6380
2023-11-23 00:35:44,026:INFO:  Epoch 428/500:  train Loss: 17.6151   val Loss: 23.1453   time: 430.11s   best: 22.0977
2023-11-23 00:39:15,573:INFO:  Epoch 127/500:  train Loss: 22.6739   val Loss: 28.0465   time: 430.56s   best: 27.6380
2023-11-23 00:42:57,775:INFO:  Epoch 429/500:  train Loss: 17.6456   val Loss: 23.2253   time: 433.75s   best: 22.0977
2023-11-23 00:46:27,875:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-23 00:46:27,894:INFO:  Epoch 128/500:  train Loss: 22.6792   val Loss: 27.6093   time: 432.28s   best: 27.6093
2023-11-23 00:50:12,120:INFO:  Epoch 430/500:  train Loss: 17.5853   val Loss: 22.7781   time: 434.33s   best: 22.0977
2023-11-23 00:53:41,292:INFO:  Epoch 129/500:  train Loss: 22.6978   val Loss: 28.1797   time: 433.39s   best: 27.6093
2023-11-23 00:57:26,987:INFO:  Epoch 431/500:  train Loss: 17.8570   val Loss: 22.7655   time: 434.83s   best: 22.0977
2023-11-23 01:00:52,482:INFO:  Epoch 130/500:  train Loss: 22.5446   val Loss: 27.7738   time: 431.18s   best: 27.6093
2023-11-23 01:04:39,935:INFO:  Epoch 432/500:  train Loss: 17.4076   val Loss: 23.8083   time: 432.95s   best: 22.0977
2023-11-23 01:08:03,402:INFO:  Epoch 131/500:  train Loss: 22.6027   val Loss: 28.1975   time: 430.92s   best: 27.6093
2023-11-23 01:11:53,668:INFO:  Epoch 433/500:  train Loss: 17.5008   val Loss: 23.0211   time: 433.73s   best: 22.0977
2023-11-23 01:15:16,270:INFO:  Epoch 132/500:  train Loss: 22.5665   val Loss: 29.4772   time: 432.86s   best: 27.6093
2023-11-23 01:19:05,960:INFO:  Epoch 434/500:  train Loss: 17.5636   val Loss: 22.5479   time: 432.29s   best: 22.0977
2023-11-23 01:22:28,525:INFO:  Epoch 133/500:  train Loss: 22.4950   val Loss: 28.1510   time: 432.24s   best: 27.6093
2023-11-23 01:26:17,373:INFO:  Epoch 435/500:  train Loss: 17.2873   val Loss: 22.8443   time: 431.41s   best: 22.0977
2023-11-23 01:29:41,505:INFO:  Epoch 134/500:  train Loss: 22.4890   val Loss: 28.5509   time: 432.96s   best: 27.6093
2023-11-23 01:33:26,675:INFO:  Epoch 436/500:  train Loss: 17.5356   val Loss: 23.5068   time: 429.30s   best: 22.0977
2023-11-23 01:36:51,985:INFO:  Epoch 135/500:  train Loss: 22.4888   val Loss: 28.0992   time: 430.47s   best: 27.6093
2023-11-23 01:40:35,694:INFO:  Epoch 437/500:  train Loss: 17.5817   val Loss: 22.8347   time: 429.01s   best: 22.0977
2023-11-23 01:44:02,667:INFO:  Epoch 136/500:  train Loss: 22.6855   val Loss: 28.0499   time: 430.68s   best: 27.6093
2023-11-23 01:47:45,132:INFO:  Epoch 438/500:  train Loss: 17.4095   val Loss: 23.2471   time: 429.41s   best: 22.0977
2023-11-23 01:51:09,861:INFO:  Epoch 137/500:  train Loss: 22.5059   val Loss: 29.0802   time: 427.19s   best: 27.6093
2023-11-23 01:54:58,353:INFO:  Epoch 439/500:  train Loss: 17.4280   val Loss: 23.8645   time: 433.21s   best: 22.0977
2023-11-23 01:58:21,378:INFO:  Epoch 138/500:  train Loss: 22.4066   val Loss: 28.5621   time: 431.51s   best: 27.6093
2023-11-23 02:02:12,309:INFO:  Epoch 440/500:  train Loss: 17.3342   val Loss: 23.2064   time: 433.95s   best: 22.0977
2023-11-23 02:05:34,872:INFO:  Epoch 139/500:  train Loss: 22.8147   val Loss: 28.8729   time: 433.48s   best: 27.6093
2023-11-23 02:09:25,464:INFO:  Epoch 441/500:  train Loss: 17.4515   val Loss: 23.0314   time: 433.14s   best: 22.0977
2023-11-23 02:12:47,130:INFO:  Epoch 140/500:  train Loss: 22.4841   val Loss: 28.2011   time: 432.25s   best: 27.6093
2023-11-23 02:16:38,299:INFO:  Epoch 442/500:  train Loss: 17.5180   val Loss: 23.2985   time: 432.82s   best: 22.0977
2023-11-23 02:19:55,716:INFO:  Epoch 141/500:  train Loss: 22.3241   val Loss: 28.1262   time: 428.56s   best: 27.6093
2023-11-23 02:23:50,860:INFO:  Epoch 443/500:  train Loss: 17.5466   val Loss: 23.9543   time: 432.54s   best: 22.0977
2023-11-23 02:27:04,562:INFO:  Epoch 142/500:  train Loss: 22.3825   val Loss: 28.6876   time: 428.84s   best: 27.6093
2023-11-23 02:31:04,983:INFO:  Epoch 444/500:  train Loss: 17.3580   val Loss: 23.7851   time: 434.12s   best: 22.0977
2023-11-23 02:34:18,290:INFO:  Epoch 143/500:  train Loss: 22.6283   val Loss: 27.6466   time: 433.71s   best: 27.6093
2023-11-23 02:38:18,741:INFO:  Epoch 445/500:  train Loss: 17.3536   val Loss: 23.4028   time: 433.74s   best: 22.0977
2023-11-23 02:41:32,953:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-23 02:41:32,973:INFO:  Epoch 144/500:  train Loss: 22.4319   val Loss: 27.5952   time: 434.65s   best: 27.5952
2023-11-23 02:45:31,669:INFO:  Epoch 446/500:  train Loss: 17.4134   val Loss: 23.3699   time: 432.90s   best: 22.0977
2023-11-23 02:48:45,234:INFO:  Epoch 145/500:  train Loss: 22.3781   val Loss: 27.6442   time: 432.25s   best: 27.5952
2023-11-23 02:52:46,118:INFO:  Epoch 447/500:  train Loss: 17.5350   val Loss: 23.3186   time: 434.45s   best: 22.0977
2023-11-23 02:55:54,752:INFO:  Epoch 146/500:  train Loss: 22.4129   val Loss: 28.6759   time: 429.50s   best: 27.5952
2023-11-23 02:59:59,633:INFO:  Epoch 448/500:  train Loss: 17.4150   val Loss: 23.2115   time: 433.51s   best: 22.0977
2023-11-23 03:03:06,562:INFO:  Epoch 147/500:  train Loss: 22.2895   val Loss: 28.0063   time: 431.81s   best: 27.5952
2023-11-23 03:07:13,517:INFO:  Epoch 449/500:  train Loss: 17.4921   val Loss: 23.7494   time: 433.87s   best: 22.0977
2023-11-23 03:10:14,982:INFO:  Epoch 148/500:  train Loss: 22.2863   val Loss: 27.8561   time: 428.42s   best: 27.5952
2023-11-23 03:14:25,251:INFO:  Epoch 450/500:  train Loss: 17.4222   val Loss: 23.4840   time: 431.73s   best: 22.0977
2023-11-23 03:17:22,521:INFO:  Epoch 149/500:  train Loss: 22.1556   val Loss: 27.7748   time: 427.51s   best: 27.5952
2023-11-23 03:21:38,958:INFO:  Epoch 451/500:  train Loss: 17.6528   val Loss: 23.2959   time: 433.70s   best: 22.0977
2023-11-23 03:24:33,715:INFO:  Epoch 150/500:  train Loss: 22.2119   val Loss: 28.0826   time: 431.19s   best: 27.5952
2023-11-23 03:28:53,688:INFO:  Epoch 452/500:  train Loss: 17.5541   val Loss: 23.2937   time: 434.72s   best: 22.0977
2023-11-23 03:31:42,563:INFO:  Epoch 151/500:  train Loss: 22.1716   val Loss: 27.9990   time: 428.84s   best: 27.5952
2023-11-23 03:36:08,338:INFO:  Epoch 453/500:  train Loss: 17.5062   val Loss: 23.1811   time: 434.65s   best: 22.0977
2023-11-23 03:38:53,307:INFO:  Epoch 152/500:  train Loss: 22.1504   val Loss: 28.0924   time: 430.73s   best: 27.5952
2023-11-23 03:43:22,168:INFO:  Epoch 454/500:  train Loss: 17.5438   val Loss: 23.2281   time: 433.83s   best: 22.0977
2023-11-23 03:46:02,395:INFO:  Epoch 153/500:  train Loss: 22.0999   val Loss: 27.9561   time: 429.07s   best: 27.5952
2023-11-23 03:50:35,652:INFO:  Epoch 455/500:  train Loss: 17.3342   val Loss: 22.9839   time: 433.46s   best: 22.0977
2023-11-23 03:53:12,020:INFO:  Epoch 154/500:  train Loss: 22.2002   val Loss: 27.7004   time: 429.61s   best: 27.5952
2023-11-23 03:57:44,855:INFO:  Epoch 456/500:  train Loss: 17.3632   val Loss: 23.8302   time: 429.19s   best: 22.0977
2023-11-23 04:00:20,906:INFO:  Epoch 155/500:  train Loss: 22.0832   val Loss: 28.1326   time: 428.87s   best: 27.5952
2023-11-23 04:04:54,910:INFO:  Epoch 457/500:  train Loss: 17.3124   val Loss: 23.5388   time: 430.03s   best: 22.0977
2023-11-23 04:07:33,250:INFO:  Epoch 156/500:  train Loss: 22.2845   val Loss: 27.9708   time: 432.34s   best: 27.5952
2023-11-23 04:12:08,983:INFO:  Epoch 458/500:  train Loss: 17.4352   val Loss: 23.5652   time: 434.06s   best: 22.0977
2023-11-23 04:14:42,301:INFO:  Epoch 157/500:  train Loss: 22.0228   val Loss: 27.8940   time: 429.05s   best: 27.5952
2023-11-23 04:19:22,808:INFO:  Epoch 459/500:  train Loss: 17.4501   val Loss: 23.8050   time: 433.81s   best: 22.0977
2023-11-23 04:21:54,854:INFO:  Epoch 158/500:  train Loss: 22.0502   val Loss: 28.1585   time: 432.55s   best: 27.5952
2023-11-23 04:26:37,559:INFO:  Epoch 460/500:  train Loss: 17.4882   val Loss: 23.6083   time: 434.75s   best: 22.0977
2023-11-23 04:29:07,392:INFO:  Epoch 159/500:  train Loss: 22.0617   val Loss: 28.0160   time: 432.52s   best: 27.5952
2023-11-23 04:33:47,030:INFO:  Epoch 461/500:  train Loss: 17.3339   val Loss: 23.9393   time: 429.46s   best: 22.0977
2023-11-23 04:36:15,685:INFO:  Epoch 160/500:  train Loss: 22.0964   val Loss: 27.7099   time: 428.29s   best: 27.5952
2023-11-23 04:41:00,689:INFO:  Epoch 462/500:  train Loss: 17.5273   val Loss: 22.9791   time: 433.66s   best: 22.0977
2023-11-23 04:43:24,383:INFO:  Epoch 161/500:  train Loss: 21.9901   val Loss: 27.7271   time: 428.68s   best: 27.5952
2023-11-23 04:48:13,973:INFO:  Epoch 463/500:  train Loss: 17.4389   val Loss: 23.3962   time: 433.26s   best: 22.0977
2023-11-23 04:50:36,902:INFO:  Epoch 162/500:  train Loss: 22.0355   val Loss: 27.8072   time: 432.51s   best: 27.5952
2023-11-23 04:55:27,097:INFO:  Epoch 464/500:  train Loss: 17.3865   val Loss: 23.2205   time: 433.12s   best: 22.0977
2023-11-23 04:57:48,088:INFO:  Epoch 163/500:  train Loss: 21.9776   val Loss: 27.8349   time: 431.17s   best: 27.5952
2023-11-23 05:02:39,787:INFO:  Epoch 465/500:  train Loss: 17.3870   val Loss: 23.3062   time: 432.69s   best: 22.0977
2023-11-23 05:05:00,691:INFO:  Epoch 164/500:  train Loss: 22.1322   val Loss: 27.8379   time: 432.60s   best: 27.5952
2023-11-23 05:09:53,151:INFO:  Epoch 466/500:  train Loss: 17.6619   val Loss: 23.5037   time: 433.36s   best: 22.0977
2023-11-23 05:12:08,926:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-23 05:12:08,946:INFO:  Epoch 165/500:  train Loss: 21.9858   val Loss: 27.5588   time: 428.22s   best: 27.5588
2023-11-23 05:17:05,478:INFO:  Epoch 467/500:  train Loss: 17.3072   val Loss: 23.8236   time: 432.30s   best: 22.0977
2023-11-23 05:19:18,226:INFO:  Epoch 166/500:  train Loss: 22.2119   val Loss: 27.7061   time: 429.28s   best: 27.5588
2023-11-23 05:24:18,181:INFO:  Epoch 468/500:  train Loss: 17.3276   val Loss: 24.1323   time: 432.70s   best: 22.0977
2023-11-23 05:26:30,470:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-23 05:26:30,490:INFO:  Epoch 167/500:  train Loss: 22.0301   val Loss: 27.4562   time: 432.23s   best: 27.4562
2023-11-23 05:31:27,919:INFO:  Epoch 469/500:  train Loss: 17.4429   val Loss: 23.4500   time: 429.74s   best: 22.0977
2023-11-23 05:33:40,675:INFO:  Epoch 168/500:  train Loss: 22.0772   val Loss: 27.9034   time: 430.18s   best: 27.4562
2023-11-23 05:38:40,598:INFO:  Epoch 470/500:  train Loss: 17.3003   val Loss: 23.9492   time: 432.68s   best: 22.0977
2023-11-23 05:40:49,673:INFO:  Epoch 169/500:  train Loss: 21.9415   val Loss: 27.8293   time: 429.00s   best: 27.4562
2023-11-23 05:45:49,580:INFO:  Epoch 471/500:  train Loss: 17.3396   val Loss: 23.6760   time: 428.97s   best: 22.0977
2023-11-23 05:47:59,998:INFO:  Epoch 170/500:  train Loss: 21.8496   val Loss: 27.9820   time: 430.32s   best: 27.4562
2023-11-23 05:53:04,496:INFO:  Epoch 472/500:  train Loss: 17.4628   val Loss: 23.5076   time: 434.90s   best: 22.0977
2023-11-23 05:55:11,969:INFO:  Epoch 171/500:  train Loss: 21.9746   val Loss: 28.1732   time: 431.97s   best: 27.4562
2023-11-23 06:00:16,462:INFO:  Epoch 473/500:  train Loss: 17.4540   val Loss: 23.6125   time: 431.95s   best: 22.0977
2023-11-23 06:02:20,703:INFO:  Epoch 172/500:  train Loss: 21.8966   val Loss: 27.9830   time: 428.72s   best: 27.4562
2023-11-23 06:07:31,083:INFO:  Epoch 474/500:  train Loss: 17.3645   val Loss: 24.1437   time: 434.62s   best: 22.0977
2023-11-23 06:09:29,768:INFO:  Epoch 173/500:  train Loss: 22.1640   val Loss: 28.0056   time: 429.05s   best: 27.4562
2023-11-23 06:14:45,295:INFO:  Epoch 475/500:  train Loss: 17.5527   val Loss: 27.5284   time: 434.20s   best: 22.0977
2023-11-23 06:16:41,965:INFO:  Epoch 174/500:  train Loss: 21.9117   val Loss: 27.8807   time: 432.18s   best: 27.4562
2023-11-23 06:21:58,648:INFO:  Epoch 476/500:  train Loss: 17.8784   val Loss: 23.6876   time: 433.34s   best: 22.0977
2023-11-23 06:23:54,882:INFO:  Epoch 175/500:  train Loss: 21.9811   val Loss: 30.4408   time: 432.91s   best: 27.4562
2023-11-23 06:29:09,371:INFO:  Epoch 477/500:  train Loss: 17.5482   val Loss: 23.8891   time: 430.71s   best: 22.0977
2023-11-23 06:31:08,000:INFO:  Epoch 176/500:  train Loss: 21.9694   val Loss: 27.7118   time: 433.08s   best: 27.4562
2023-11-23 06:36:22,735:INFO:  Epoch 478/500:  train Loss: 17.3786   val Loss: 24.5654   time: 433.36s   best: 22.0977
2023-11-23 06:38:15,682:INFO:  Epoch 177/500:  train Loss: 21.8169   val Loss: 27.6479   time: 427.67s   best: 27.4562
2023-11-23 06:43:34,555:INFO:  Epoch 479/500:  train Loss: 17.5002   val Loss: 23.4989   time: 431.82s   best: 22.0977
2023-11-23 06:45:27,865:INFO:  Epoch 178/500:  train Loss: 21.7439   val Loss: 30.9878   time: 432.17s   best: 27.4562
2023-11-23 06:50:47,251:INFO:  Epoch 480/500:  train Loss: 17.4331   val Loss: 23.9006   time: 432.69s   best: 22.0977
2023-11-23 06:52:35,870:INFO:  Epoch 179/500:  train Loss: 22.1070   val Loss: 28.1275   time: 427.99s   best: 27.4562
2023-11-23 06:58:01,172:INFO:  Epoch 481/500:  train Loss: 18.1166   val Loss: 24.7430   time: 433.90s   best: 22.0977
2023-11-23 06:59:47,996:INFO:  Epoch 180/500:  train Loss: 21.8060   val Loss: 36.2722   time: 432.11s   best: 27.4562
2023-11-23 07:05:11,020:INFO:  Epoch 482/500:  train Loss: 17.6680   val Loss: 24.0241   time: 429.85s   best: 22.0977
2023-11-23 07:06:55,151:INFO:  Epoch 181/500:  train Loss: 21.8085   val Loss: 27.5068   time: 427.14s   best: 27.4562
2023-11-23 07:12:21,714:INFO:  Epoch 483/500:  train Loss: 17.7186   val Loss: 23.3152   time: 430.69s   best: 22.0977
2023-11-23 07:14:04,624:INFO:  Epoch 182/500:  train Loss: 21.8443   val Loss: 27.6744   time: 429.47s   best: 27.4562
2023-11-23 07:19:33,387:INFO:  Epoch 484/500:  train Loss: 17.6079   val Loss: 23.7091   time: 431.65s   best: 22.0977
2023-11-23 07:21:10,847:INFO:  Epoch 183/500:  train Loss: 22.0252   val Loss: 27.7212   time: 426.21s   best: 27.4562
2023-11-23 07:26:47,611:INFO:  Epoch 485/500:  train Loss: 17.5826   val Loss: 22.9192   time: 434.21s   best: 22.0977
2023-11-23 07:28:18,841:INFO:  Epoch 184/500:  train Loss: 21.8406   val Loss: 28.3179   time: 427.99s   best: 27.4562
2023-11-23 07:34:01,174:INFO:  Epoch 486/500:  train Loss: 17.3873   val Loss: 23.6366   time: 433.54s   best: 22.0977
2023-11-23 07:35:27,090:INFO:  Epoch 185/500:  train Loss: 21.8365   val Loss: 27.5313   time: 428.23s   best: 27.4562
2023-11-23 07:41:12,091:INFO:  Epoch 487/500:  train Loss: 17.3384   val Loss: 23.4881   time: 430.91s   best: 22.0977
2023-11-23 07:42:36,612:INFO:  Epoch 186/500:  train Loss: 22.1866   val Loss: 27.6806   time: 429.51s   best: 27.4562
2023-11-23 07:48:21,540:INFO:  Epoch 488/500:  train Loss: 17.4706   val Loss: 23.2528   time: 429.43s   best: 22.0977
2023-11-23 07:49:49,412:INFO:  Epoch 187/500:  train Loss: 21.6883   val Loss: 28.8404   time: 432.80s   best: 27.4562
2023-11-23 07:55:34,451:INFO:  Epoch 489/500:  train Loss: 17.5201   val Loss: 23.2884   time: 432.91s   best: 22.0977
2023-11-23 07:57:02,494:INFO:  Epoch 188/500:  train Loss: 21.6316   val Loss: 28.2767   time: 433.07s   best: 27.4562
2023-11-23 08:02:45,246:INFO:  Epoch 490/500:  train Loss: 17.2744   val Loss: 23.6065   time: 430.79s   best: 22.0977
2023-11-23 08:04:13,147:INFO:  Epoch 189/500:  train Loss: 21.7667   val Loss: 27.9179   time: 430.62s   best: 27.4562
2023-11-23 08:09:54,187:INFO:  Epoch 491/500:  train Loss: 17.4327   val Loss: 23.3090   time: 428.94s   best: 22.0977
2023-11-23 08:11:21,282:INFO:  Epoch 190/500:  train Loss: 21.6829   val Loss: 27.8845   time: 428.13s   best: 27.4562
2023-11-23 08:17:06,583:INFO:  Epoch 492/500:  train Loss: 17.4665   val Loss: 23.4743   time: 432.39s   best: 22.0977
2023-11-23 08:18:30,174:INFO:  Epoch 191/500:  train Loss: 21.7674   val Loss: 27.8324   time: 428.89s   best: 27.4562
2023-11-23 08:24:16,796:INFO:  Epoch 493/500:  train Loss: 17.4040   val Loss: 23.3892   time: 430.20s   best: 22.0977
2023-11-23 08:25:42,259:INFO:  Epoch 192/500:  train Loss: 21.8164   val Loss: 27.6379   time: 432.08s   best: 27.4562
2023-11-23 08:31:28,483:INFO:  Epoch 494/500:  train Loss: 17.2566   val Loss: 22.6792   time: 431.67s   best: 22.0977
2023-11-23 08:32:50,998:INFO:  Epoch 193/500:  train Loss: 21.6138   val Loss: 28.1400   time: 428.72s   best: 27.4562
2023-11-23 08:38:39,695:INFO:  Epoch 495/500:  train Loss: 17.3023   val Loss: 23.3653   time: 431.20s   best: 22.0977
2023-11-23 08:39:58,708:INFO:  Epoch 194/500:  train Loss: 21.7929   val Loss: 28.2503   time: 427.70s   best: 27.4562
2023-11-23 08:45:50,077:INFO:  Epoch 496/500:  train Loss: 17.3716   val Loss: 23.5327   time: 430.38s   best: 22.0977
2023-11-23 08:47:10,773:INFO:  Epoch 195/500:  train Loss: 21.5747   val Loss: 27.9718   time: 432.05s   best: 27.4562
2023-11-23 08:52:58,888:INFO:  Epoch 497/500:  train Loss: 17.2437   val Loss: 23.2223   time: 428.81s   best: 22.0977
2023-11-23 08:54:19,222:INFO:  Epoch 196/500:  train Loss: 21.6062   val Loss: 27.6638   time: 428.43s   best: 27.4562
2023-11-23 09:00:12,541:INFO:  Epoch 498/500:  train Loss: 17.1901   val Loss: 23.2373   time: 433.64s   best: 22.0977
2023-11-23 09:01:27,821:INFO:  Epoch 197/500:  train Loss: 21.6255   val Loss: 28.0931   time: 428.60s   best: 27.4562
2023-11-23 09:07:22,562:INFO:  Epoch 499/500:  train Loss: 17.1603   val Loss: 24.3597   time: 430.00s   best: 22.0977
2023-11-23 09:08:39,586:INFO:  Epoch 198/500:  train Loss: 21.6246   val Loss: 28.1415   time: 431.74s   best: 27.4562
2023-11-23 09:14:35,085:INFO:  Epoch 500/500:  train Loss: 17.2387   val Loss: 23.3578   time: 432.52s   best: 22.0977
2023-11-23 09:14:35,099:INFO:  -----> Training complete in 3599m 16s   best validation loss: 22.0977
 
2023-11-23 09:15:51,908:INFO:  Epoch 199/500:  train Loss: 21.7088   val Loss: 27.7447   time: 432.31s   best: 27.4562
2023-11-23 09:23:04,345:INFO:  Epoch 200/500:  train Loss: 21.6581   val Loss: 28.1307   time: 432.42s   best: 27.4562
2023-11-23 09:30:15,289:INFO:  Epoch 201/500:  train Loss: 21.5701   val Loss: 27.5154   time: 430.93s   best: 27.4562
2023-11-23 09:37:23,976:INFO:  Epoch 202/500:  train Loss: 21.6764   val Loss: 27.6946   time: 428.68s   best: 27.4562
2023-11-23 09:44:34,783:INFO:  Epoch 203/500:  train Loss: 21.5624   val Loss: 27.8216   time: 430.80s   best: 27.4562
2023-11-23 09:51:48,005:INFO:  Epoch 204/500:  train Loss: 21.8069   val Loss: 27.8124   time: 433.21s   best: 27.4562
2023-11-23 09:59:00,146:INFO:  Epoch 205/500:  train Loss: 21.5075   val Loss: 27.9372   time: 432.13s   best: 27.4562
2023-11-23 10:06:07,620:INFO:  Epoch 206/500:  train Loss: 21.4829   val Loss: 28.5566   time: 427.47s   best: 27.4562
2023-11-23 10:13:19,561:INFO:  Epoch 207/500:  train Loss: 21.5503   val Loss: 28.2469   time: 431.94s   best: 27.4562
2023-11-23 10:20:33,051:INFO:  Epoch 208/500:  train Loss: 21.6419   val Loss: 27.5390   time: 433.49s   best: 27.4562
2023-11-23 10:27:43,361:INFO:  Epoch 209/500:  train Loss: 21.8774   val Loss: 27.6658   time: 430.31s   best: 27.4562
2023-11-23 10:34:55,637:INFO:  Epoch 210/500:  train Loss: 21.4932   val Loss: 27.5686   time: 432.26s   best: 27.4562
2023-11-23 10:42:03,200:INFO:  Epoch 211/500:  train Loss: 21.5158   val Loss: 27.9300   time: 427.55s   best: 27.4562
2023-11-23 10:49:15,678:INFO:  Epoch 212/500:  train Loss: 21.4951   val Loss: 27.5681   time: 432.46s   best: 27.4562
2023-11-23 10:56:27,906:INFO:  Epoch 213/500:  train Loss: 21.5778   val Loss: 27.7049   time: 432.23s   best: 27.4562
2023-11-23 11:03:39,408:INFO:  Epoch 214/500:  train Loss: 21.4757   val Loss: 28.1878   time: 431.49s   best: 27.4562
2023-11-23 11:10:49,926:INFO:  Epoch 215/500:  train Loss: 21.8277   val Loss: 27.5291   time: 430.50s   best: 27.4562
2023-11-23 11:18:02,566:INFO:  Epoch 216/500:  train Loss: 21.4512   val Loss: 27.7698   time: 432.64s   best: 27.4562
2023-11-23 11:25:10,963:INFO:  Epoch 217/500:  train Loss: 21.4920   val Loss: 27.7608   time: 428.38s   best: 27.4562
2023-11-23 11:32:23,696:INFO:  Epoch 218/500:  train Loss: 21.5912   val Loss: 27.9597   time: 432.72s   best: 27.4562
2023-11-23 11:39:33,641:INFO:  Epoch 219/500:  train Loss: 21.3884   val Loss: 36.2607   time: 429.93s   best: 27.4562
2023-11-23 11:46:46,542:INFO:  Epoch 220/500:  train Loss: 21.4646   val Loss: 27.7632   time: 432.89s   best: 27.4562
2023-11-23 11:53:55,385:INFO:  Epoch 221/500:  train Loss: 21.4967   val Loss: 28.2021   time: 428.83s   best: 27.4562
2023-11-23 12:01:04,976:INFO:  Epoch 222/500:  train Loss: 21.5797   val Loss: 27.9565   time: 429.59s   best: 27.4562
2023-11-23 12:08:14,865:INFO:  Epoch 223/500:  train Loss: 21.3655   val Loss: 27.8628   time: 429.89s   best: 27.4562
2023-11-23 12:15:23,841:INFO:  Epoch 224/500:  train Loss: 21.3976   val Loss: 27.9585   time: 428.97s   best: 27.4562
2023-11-23 12:22:36,987:INFO:  Epoch 225/500:  train Loss: 21.3477   val Loss: 27.7802   time: 433.13s   best: 27.4562
2023-11-23 12:29:47,610:INFO:  Epoch 226/500:  train Loss: 21.6619   val Loss: 28.0008   time: 430.61s   best: 27.4562
2023-11-23 12:37:00,212:INFO:  Epoch 227/500:  train Loss: 21.3048   val Loss: 27.7822   time: 432.60s   best: 27.4562
2023-11-23 12:44:09,256:INFO:  Epoch 228/500:  train Loss: 21.2825   val Loss: 27.6877   time: 429.04s   best: 27.4562
2023-11-23 12:51:19,880:INFO:  Epoch 229/500:  train Loss: 21.5422   val Loss: 27.6255   time: 430.62s   best: 27.4562
2023-11-23 12:58:33,223:INFO:  Epoch 230/500:  train Loss: 21.4270   val Loss: 27.7302   time: 433.33s   best: 27.4562
2023-11-23 13:05:45,560:INFO:  Epoch 231/500:  train Loss: 21.3895   val Loss: 28.0359   time: 432.32s   best: 27.4562
2023-11-23 13:12:58,069:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-23 13:12:58,088:INFO:  Epoch 232/500:  train Loss: 21.4341   val Loss: 27.3407   time: 432.48s   best: 27.3407
2023-11-23 13:20:10,613:INFO:  Epoch 233/500:  train Loss: 21.2804   val Loss: 28.3984   time: 432.51s   best: 27.3407
2023-11-23 13:27:21,697:INFO:  Epoch 234/500:  train Loss: 21.3296   val Loss: 27.8363   time: 431.07s   best: 27.3407
2023-11-23 13:34:33,373:INFO:  Epoch 235/500:  train Loss: 21.3419   val Loss: 27.8969   time: 431.68s   best: 27.3407
2023-11-23 13:41:45,744:INFO:  Epoch 236/500:  train Loss: 21.2883   val Loss: 27.6917   time: 432.36s   best: 27.3407
2023-11-23 13:48:52,989:INFO:  Epoch 237/500:  train Loss: 21.4312   val Loss: 27.5885   time: 427.23s   best: 27.3407
2023-11-23 13:56:03,323:INFO:  Epoch 238/500:  train Loss: 21.2157   val Loss: 27.8941   time: 430.33s   best: 27.3407
2023-11-23 14:03:14,727:INFO:  Epoch 239/500:  train Loss: 21.3529   val Loss: 27.7700   time: 431.39s   best: 27.3407
2023-11-23 14:10:26,043:INFO:  Epoch 240/500:  train Loss: 21.1909   val Loss: 27.9510   time: 431.30s   best: 27.3407
2023-11-23 14:17:38,127:INFO:  Epoch 241/500:  train Loss: 21.2591   val Loss: 27.5658   time: 432.08s   best: 27.3407
2023-11-23 14:24:47,237:INFO:  Epoch 242/500:  train Loss: 21.2497   val Loss: 28.1234   time: 429.10s   best: 27.3407
2023-11-23 14:31:57,581:INFO:  Epoch 243/500:  train Loss: 21.1931   val Loss: 28.2890   time: 430.34s   best: 27.3407
2023-11-23 14:39:08,362:INFO:  Epoch 244/500:  train Loss: 21.1998   val Loss: 28.0467   time: 430.78s   best: 27.3407
2023-11-23 14:46:18,956:INFO:  Epoch 245/500:  train Loss: 21.2575   val Loss: 28.5430   time: 430.58s   best: 27.3407
2023-11-23 14:53:29,494:INFO:  Epoch 246/500:  train Loss: 21.2338   val Loss: 28.0623   time: 430.54s   best: 27.3407
2023-11-23 15:00:37,967:INFO:  Epoch 247/500:  train Loss: 22.0370   val Loss: 28.3634   time: 428.47s   best: 27.3407
2023-11-23 15:07:50,284:INFO:  Epoch 248/500:  train Loss: 21.2857   val Loss: 28.2406   time: 432.30s   best: 27.3407
2023-11-23 15:14:57,967:INFO:  Epoch 249/500:  train Loss: 21.3176   val Loss: 28.4268   time: 427.68s   best: 27.3407
2023-11-23 15:22:10,387:INFO:  Epoch 250/500:  train Loss: 21.1352   val Loss: 27.5083   time: 432.42s   best: 27.3407
2023-11-23 15:29:21,830:INFO:  Epoch 251/500:  train Loss: 21.1076   val Loss: 28.4713   time: 431.44s   best: 27.3407
2023-11-23 15:36:34,955:INFO:  Epoch 252/500:  train Loss: 21.1717   val Loss: 27.8574   time: 433.12s   best: 27.3407
2023-11-23 15:43:45,355:INFO:  Epoch 253/500:  train Loss: 21.1911   val Loss: 27.6248   time: 430.40s   best: 27.3407
2023-11-23 15:50:52,650:INFO:  Epoch 254/500:  train Loss: 21.1088   val Loss: 27.9733   time: 427.28s   best: 27.3407
2023-11-23 15:57:59,144:INFO:  Epoch 255/500:  train Loss: 21.1311   val Loss: 28.0582   time: 426.48s   best: 27.3407
2023-11-23 16:05:09,977:INFO:  Epoch 256/500:  train Loss: 21.0427   val Loss: 28.0707   time: 430.83s   best: 27.3407
2023-11-23 16:12:21,179:INFO:  Epoch 257/500:  train Loss: 21.0736   val Loss: 28.4001   time: 431.20s   best: 27.3407
2023-11-23 16:19:32,034:INFO:  Epoch 258/500:  train Loss: 21.3293   val Loss: 28.2571   time: 430.84s   best: 27.3407
2023-11-23 16:26:43,440:INFO:  Epoch 259/500:  train Loss: 21.1703   val Loss: 28.4923   time: 431.40s   best: 27.3407
2023-11-23 16:33:55,702:INFO:  Epoch 260/500:  train Loss: 21.0766   val Loss: 28.2149   time: 432.26s   best: 27.3407
2023-11-23 16:41:07,745:INFO:  Epoch 261/500:  train Loss: 21.2682   val Loss: 28.8077   time: 432.04s   best: 27.3407
2023-11-23 16:48:14,535:INFO:  Epoch 262/500:  train Loss: 21.1420   val Loss: 31.2424   time: 426.78s   best: 27.3407
2023-11-23 16:55:27,489:INFO:  Epoch 263/500:  train Loss: 21.0533   val Loss: 28.5713   time: 432.94s   best: 27.3407
2023-11-23 17:02:37,601:INFO:  Epoch 264/500:  train Loss: 21.0273   val Loss: 28.0186   time: 430.11s   best: 27.3407
2023-11-23 17:09:45,558:INFO:  Epoch 265/500:  train Loss: 21.0558   val Loss: 28.0850   time: 427.94s   best: 27.3407
2023-11-23 17:16:54,353:INFO:  Epoch 266/500:  train Loss: 21.0597   val Loss: 27.9105   time: 428.79s   best: 27.3407
2023-11-23 17:24:04,625:INFO:  Epoch 267/500:  train Loss: 21.1329   val Loss: 28.0730   time: 430.27s   best: 27.3407
2023-11-23 17:31:11,938:INFO:  Epoch 268/500:  train Loss: 21.0039   val Loss: 27.9455   time: 427.31s   best: 27.3407
2023-11-23 17:38:23,823:INFO:  Epoch 269/500:  train Loss: 21.0347   val Loss: 28.4218   time: 431.88s   best: 27.3407
2023-11-23 17:45:31,210:INFO:  Epoch 270/500:  train Loss: 21.0041   val Loss: 27.6873   time: 427.38s   best: 27.3407
2023-11-23 17:52:42,098:INFO:  Epoch 271/500:  train Loss: 21.1294   val Loss: 28.2627   time: 430.88s   best: 27.3407
2023-11-23 17:59:51,022:INFO:  Epoch 272/500:  train Loss: 20.9955   val Loss: 27.8519   time: 428.90s   best: 27.3407
2023-11-23 18:06:58,011:INFO:  Epoch 273/500:  train Loss: 21.0674   val Loss: 27.6764   time: 426.97s   best: 27.3407
2023-11-23 18:14:09,975:INFO:  Epoch 274/500:  train Loss: 20.9913   val Loss: 28.4228   time: 431.96s   best: 27.3407
2023-11-23 18:21:22,698:INFO:  Epoch 275/500:  train Loss: 20.9600   val Loss: 27.9558   time: 432.71s   best: 27.3407
2023-11-23 18:28:29,731:INFO:  Epoch 276/500:  train Loss: 21.0005   val Loss: 27.8992   time: 427.03s   best: 27.3407
2023-11-23 18:35:38,934:INFO:  Epoch 277/500:  train Loss: 21.1279   val Loss: 27.5739   time: 429.20s   best: 27.3407
2023-11-23 18:42:51,606:INFO:  Epoch 278/500:  train Loss: 20.9803   val Loss: 28.1587   time: 432.67s   best: 27.3407
2023-11-23 18:50:01,956:INFO:  Epoch 279/500:  train Loss: 20.8972   val Loss: 27.9585   time: 430.34s   best: 27.3407
2023-11-23 18:57:09,394:INFO:  Epoch 280/500:  train Loss: 20.9752   val Loss: 27.9461   time: 427.44s   best: 27.3407
2023-11-23 19:04:16,708:INFO:  Epoch 281/500:  train Loss: 20.9411   val Loss: 27.9646   time: 427.31s   best: 27.3407
2023-11-23 19:11:28,422:INFO:  Epoch 282/500:  train Loss: 21.2482   val Loss: 27.9648   time: 431.70s   best: 27.3407
2023-11-23 19:18:40,813:INFO:  Epoch 283/500:  train Loss: 21.0494   val Loss: 28.8758   time: 432.39s   best: 27.3407
2023-11-23 19:25:48,964:INFO:  Epoch 284/500:  train Loss: 20.9610   val Loss: 28.1897   time: 428.15s   best: 27.3407
2023-11-23 19:32:56,783:INFO:  Epoch 285/500:  train Loss: 20.9249   val Loss: 27.6354   time: 427.81s   best: 27.3407
2023-11-23 19:40:06,471:INFO:  Epoch 286/500:  train Loss: 21.0089   val Loss: 27.7752   time: 429.68s   best: 27.3407
2023-11-23 19:47:13,334:INFO:  Epoch 287/500:  train Loss: 21.0275   val Loss: 27.9591   time: 426.86s   best: 27.3407
2023-11-23 19:54:25,071:INFO:  Epoch 288/500:  train Loss: 21.0068   val Loss: 28.2884   time: 431.72s   best: 27.3407
2023-11-23 20:01:33,629:INFO:  Epoch 289/500:  train Loss: 20.9278   val Loss: 27.6646   time: 428.54s   best: 27.3407
2023-11-23 20:08:40,913:INFO:  Epoch 290/500:  train Loss: 20.9295   val Loss: 28.1529   time: 427.28s   best: 27.3407
2023-11-23 20:15:51,527:INFO:  Epoch 291/500:  train Loss: 20.8847   val Loss: 28.0963   time: 430.60s   best: 27.3407
2023-11-23 20:23:03,751:INFO:  Epoch 292/500:  train Loss: 21.0738   val Loss: 27.7356   time: 432.22s   best: 27.3407
2023-11-23 20:30:14,557:INFO:  Epoch 293/500:  train Loss: 21.0072   val Loss: 27.6498   time: 430.80s   best: 27.3407
2023-11-23 20:37:23,979:INFO:  Epoch 294/500:  train Loss: 20.8209   val Loss: 27.8364   time: 429.42s   best: 27.3407
2023-11-23 20:44:30,887:INFO:  Epoch 295/500:  train Loss: 20.8914   val Loss: 28.3360   time: 426.91s   best: 27.3407
2023-11-23 20:51:42,879:INFO:  Epoch 296/500:  train Loss: 21.0517   val Loss: 27.9897   time: 431.99s   best: 27.3407
2023-11-23 20:58:55,441:INFO:  Epoch 297/500:  train Loss: 20.8749   val Loss: 27.9511   time: 432.55s   best: 27.3407
2023-11-23 21:06:06,088:INFO:  Epoch 298/500:  train Loss: 20.8914   val Loss: 27.8181   time: 430.63s   best: 27.3407
2023-11-23 21:13:15,830:INFO:  Epoch 299/500:  train Loss: 20.8308   val Loss: 27.6041   time: 429.73s   best: 27.3407
2023-11-23 21:20:27,190:INFO:  Epoch 300/500:  train Loss: 20.8252   val Loss: 28.3229   time: 431.36s   best: 27.3407
2023-11-23 21:27:34,523:INFO:  Epoch 301/500:  train Loss: 20.9191   val Loss: 28.0051   time: 427.33s   best: 27.3407
2023-11-23 21:34:48,366:INFO:  Epoch 302/500:  train Loss: 20.9388   val Loss: 27.9322   time: 433.84s   best: 27.3407
2023-11-23 21:42:00,916:INFO:  Epoch 303/500:  train Loss: 20.7824   val Loss: 28.1162   time: 432.54s   best: 27.3407
2023-11-23 21:49:08,044:INFO:  Epoch 304/500:  train Loss: 21.0673   val Loss: 27.9621   time: 427.12s   best: 27.3407
2023-11-23 21:56:20,122:INFO:  Epoch 305/500:  train Loss: 20.8289   val Loss: 27.9825   time: 432.07s   best: 27.3407
2023-11-23 22:03:27,274:INFO:  Epoch 306/500:  train Loss: 20.7285   val Loss: 28.0898   time: 427.15s   best: 27.3407
2023-11-23 22:10:34,053:INFO:  Epoch 307/500:  train Loss: 20.9973   val Loss: 27.8822   time: 426.77s   best: 27.3407
2023-11-23 22:17:46,321:INFO:  Epoch 308/500:  train Loss: 20.8734   val Loss: 28.3885   time: 432.27s   best: 27.3407
2023-11-23 22:24:52,593:INFO:  Epoch 309/500:  train Loss: 20.7901   val Loss: 28.0707   time: 426.25s   best: 27.3407
2023-11-23 22:31:58,884:INFO:  Epoch 310/500:  train Loss: 20.8022   val Loss: 28.1042   time: 426.28s   best: 27.3407
2023-11-23 22:39:05,734:INFO:  Epoch 311/500:  train Loss: 20.7736   val Loss: 28.4293   time: 426.84s   best: 27.3407
2023-11-23 22:46:12,772:INFO:  Epoch 312/500:  train Loss: 20.8385   val Loss: 28.4354   time: 427.02s   best: 27.3407
2023-11-23 22:53:24,776:INFO:  Epoch 313/500:  train Loss: 20.7912   val Loss: 28.0266   time: 432.00s   best: 27.3407
2023-11-23 23:00:37,768:INFO:  Epoch 314/500:  train Loss: 20.7926   val Loss: 27.9118   time: 432.98s   best: 27.3407
2023-11-23 23:07:50,644:INFO:  Epoch 315/500:  train Loss: 20.7048   val Loss: 28.9594   time: 432.86s   best: 27.3407
2023-11-23 23:14:05,510:INFO:  Starting experiment lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)
2023-11-23 23:14:05,512:INFO:  Defining the model
2023-11-23 23:14:05,607:INFO:  Reading the dataset
2023-11-23 23:14:57,458:INFO:  Epoch 316/500:  train Loss: 20.9534   val Loss: 27.7590   time: 426.81s   best: 27.3407
2023-11-23 23:20:49,085:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:20:49,105:INFO:  Epoch 1/500:  train Loss: 90.6169   val Loss: 87.2058   time: 90.00s   best: 87.2058
2023-11-23 23:22:04,191:INFO:  Epoch 317/500:  train Loss: 20.7081   val Loss: 27.8909   time: 426.72s   best: 27.3407
2023-11-23 23:22:17,367:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:22:17,387:INFO:  Epoch 2/500:  train Loss: 87.7213   val Loss: 85.7128   time: 88.25s   best: 85.7128
2023-11-23 23:23:45,368:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:23:45,388:INFO:  Epoch 3/500:  train Loss: 83.5772   val Loss: 78.8998   time: 87.98s   best: 78.8998
2023-11-23 23:25:13,970:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:25:13,988:INFO:  Epoch 4/500:  train Loss: 77.7207   val Loss: 74.3130   time: 88.57s   best: 74.3130
2023-11-23 23:26:42,484:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:26:42,502:INFO:  Epoch 5/500:  train Loss: 74.6126   val Loss: 72.9023   time: 88.49s   best: 72.9023
2023-11-23 23:28:10,940:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:28:10,958:INFO:  Epoch 6/500:  train Loss: 73.3080   val Loss: 71.6289   time: 88.43s   best: 71.6289
2023-11-23 23:29:12,750:INFO:  Epoch 318/500:  train Loss: 20.7800   val Loss: 29.4022   time: 428.56s   best: 27.3407
2023-11-23 23:29:39,249:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:29:39,270:INFO:  Epoch 7/500:  train Loss: 71.9962   val Loss: 70.1408   time: 88.29s   best: 70.1408
2023-11-23 23:31:07,881:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:31:07,899:INFO:  Epoch 8/500:  train Loss: 70.0648   val Loss: 68.7969   time: 88.61s   best: 68.7969
2023-11-23 23:32:36,254:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:32:36,272:INFO:  Epoch 9/500:  train Loss: 69.0985   val Loss: 67.2880   time: 88.35s   best: 67.2880
2023-11-23 23:34:04,722:INFO:  Epoch 10/500:  train Loss: 68.2356   val Loss: 67.4299   time: 88.44s   best: 67.2880
2023-11-23 23:35:33,631:INFO:  Epoch 11/500:  train Loss: 67.4234   val Loss: 68.2092   time: 88.90s   best: 67.2880
2023-11-23 23:36:23,760:INFO:  Epoch 319/500:  train Loss: 20.7704   val Loss: 27.5543   time: 430.99s   best: 27.3407
2023-11-23 23:37:02,196:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:37:02,216:INFO:  Epoch 12/500:  train Loss: 66.8947   val Loss: 66.4677   time: 88.56s   best: 66.4677
2023-11-23 23:38:30,716:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:38:30,749:INFO:  Epoch 13/500:  train Loss: 66.2435   val Loss: 65.0304   time: 88.48s   best: 65.0304
2023-11-23 23:39:59,393:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:39:59,411:INFO:  Epoch 14/500:  train Loss: 65.3318   val Loss: 64.6544   time: 88.63s   best: 64.6544
2023-11-23 23:41:28,489:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:41:28,508:INFO:  Epoch 15/500:  train Loss: 64.6287   val Loss: 64.3630   time: 89.06s   best: 64.3630
2023-11-23 23:42:57,274:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:42:57,293:INFO:  Epoch 16/500:  train Loss: 64.0212   val Loss: 63.4722   time: 88.75s   best: 63.4722
2023-11-23 23:43:33,044:INFO:  Epoch 320/500:  train Loss: 21.1909   val Loss: 29.9234   time: 429.27s   best: 27.3407
2023-11-23 23:44:25,859:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:44:25,880:INFO:  Epoch 17/500:  train Loss: 63.3074   val Loss: 62.5114   time: 88.55s   best: 62.5114
2023-11-23 23:45:54,214:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:45:54,233:INFO:  Epoch 18/500:  train Loss: 62.8188   val Loss: 62.4510   time: 88.33s   best: 62.4510
2023-11-23 23:47:22,532:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:47:22,551:INFO:  Epoch 19/500:  train Loss: 62.2815   val Loss: 61.6601   time: 88.28s   best: 61.6601
2023-11-23 23:48:50,982:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:48:51,001:INFO:  Epoch 20/500:  train Loss: 61.6791   val Loss: 61.2455   time: 88.41s   best: 61.2455
2023-11-23 23:50:19,863:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:50:19,882:INFO:  Epoch 21/500:  train Loss: 61.1635   val Loss: 61.0549   time: 88.85s   best: 61.0549
2023-11-23 23:50:41,351:INFO:  Epoch 321/500:  train Loss: 21.3445   val Loss: 27.7993   time: 428.30s   best: 27.3407
2023-11-23 23:51:48,276:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:51:48,306:INFO:  Epoch 22/500:  train Loss: 60.7042   val Loss: 60.4586   time: 88.38s   best: 60.4586
2023-11-23 23:53:16,734:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:53:16,753:INFO:  Epoch 23/500:  train Loss: 60.0790   val Loss: 60.0491   time: 88.42s   best: 60.0491
2023-11-23 23:54:45,538:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:54:45,567:INFO:  Epoch 24/500:  train Loss: 59.6136   val Loss: 58.9180   time: 88.77s   best: 58.9180
2023-11-23 23:56:14,776:INFO:  Epoch 25/500:  train Loss: 59.5007   val Loss: 59.8454   time: 89.20s   best: 58.9180
2023-11-23 23:57:43,470:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:57:43,489:INFO:  Epoch 26/500:  train Loss: 58.7484   val Loss: 58.1364   time: 88.69s   best: 58.1364
2023-11-23 23:57:48,705:INFO:  Epoch 322/500:  train Loss: 20.8146   val Loss: 27.7889   time: 427.35s   best: 27.3407
2023-11-23 23:59:12,238:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-23 23:59:12,259:INFO:  Epoch 27/500:  train Loss: 58.2252   val Loss: 57.5364   time: 88.74s   best: 57.5364
2023-11-24 00:00:41,032:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:00:41,051:INFO:  Epoch 28/500:  train Loss: 57.5613   val Loss: 57.4853   time: 88.77s   best: 57.4853
2023-11-24 00:02:10,132:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:02:10,151:INFO:  Epoch 29/500:  train Loss: 57.3507   val Loss: 56.6643   time: 89.08s   best: 56.6643
2023-11-24 00:03:38,943:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:03:38,962:INFO:  Epoch 30/500:  train Loss: 56.5718   val Loss: 56.4814   time: 88.79s   best: 56.4814
2023-11-24 00:04:56,677:INFO:  Epoch 323/500:  train Loss: 20.6955   val Loss: 28.3779   time: 427.96s   best: 27.3407
2023-11-24 00:05:07,536:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:05:07,556:INFO:  Epoch 31/500:  train Loss: 56.0262   val Loss: 56.0336   time: 88.57s   best: 56.0336
2023-11-24 00:06:36,549:INFO:  Epoch 32/500:  train Loss: 55.4417   val Loss: 56.0907   time: 88.99s   best: 56.0336
2023-11-24 00:08:05,045:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:08:05,065:INFO:  Epoch 33/500:  train Loss: 54.8304   val Loss: 54.3955   time: 88.48s   best: 54.3955
2023-11-24 00:09:33,498:INFO:  Epoch 34/500:  train Loss: 54.4071   val Loss: 54.7578   time: 88.43s   best: 54.3955
2023-11-24 00:11:02,237:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:11:02,270:INFO:  Epoch 35/500:  train Loss: 53.8113   val Loss: 54.0583   time: 88.73s   best: 54.0583
2023-11-24 00:12:09,280:INFO:  Epoch 324/500:  train Loss: 20.7316   val Loss: 28.3172   time: 432.60s   best: 27.3407
2023-11-24 00:12:30,975:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:12:31,005:INFO:  Epoch 36/500:  train Loss: 53.2427   val Loss: 52.8575   time: 88.69s   best: 52.8575
2023-11-24 00:13:59,564:INFO:  Epoch 37/500:  train Loss: 52.7863   val Loss: 53.1733   time: 88.55s   best: 52.8575
2023-11-24 00:15:28,769:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:15:28,788:INFO:  Epoch 38/500:  train Loss: 52.6329   val Loss: 52.6407   time: 89.19s   best: 52.6407
2023-11-24 00:16:57,565:INFO:  Epoch 39/500:  train Loss: 51.9193   val Loss: 53.3175   time: 88.77s   best: 52.6407
2023-11-24 00:18:26,016:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:18:26,036:INFO:  Epoch 40/500:  train Loss: 51.5407   val Loss: 51.2320   time: 88.45s   best: 51.2320
2023-11-24 00:19:21,893:INFO:  Epoch 325/500:  train Loss: 20.7370   val Loss: 27.6513   time: 432.58s   best: 27.3407
2023-11-24 00:19:55,034:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:19:55,055:INFO:  Epoch 41/500:  train Loss: 51.1252   val Loss: 50.9483   time: 88.99s   best: 50.9483
2023-11-24 00:21:23,583:INFO:  Epoch 42/500:  train Loss: 50.5947   val Loss: 53.3786   time: 88.52s   best: 50.9483
2023-11-24 00:22:52,247:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:22:52,267:INFO:  Epoch 43/500:  train Loss: 50.4071   val Loss: 50.6477   time: 88.66s   best: 50.6477
2023-11-24 00:24:21,438:INFO:  Epoch 44/500:  train Loss: 49.9050   val Loss: 50.8664   time: 89.17s   best: 50.6477
2023-11-24 00:25:49,985:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:25:50,005:INFO:  Epoch 45/500:  train Loss: 49.4884   val Loss: 49.9289   time: 88.53s   best: 49.9289
2023-11-24 00:26:33,632:INFO:  Epoch 326/500:  train Loss: 20.7688   val Loss: 27.9565   time: 431.73s   best: 27.3407
2023-11-24 00:27:18,299:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:27:18,320:INFO:  Epoch 46/500:  train Loss: 49.1234   val Loss: 49.6468   time: 88.28s   best: 49.6468
2023-11-24 00:28:46,927:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:28:46,946:INFO:  Epoch 47/500:  train Loss: 48.8480   val Loss: 49.0978   time: 88.59s   best: 49.0978
2023-11-24 00:30:15,627:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:30:15,646:INFO:  Epoch 48/500:  train Loss: 48.1476   val Loss: 48.6435   time: 88.66s   best: 48.6435
2023-11-24 00:31:44,467:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:31:44,487:INFO:  Epoch 49/500:  train Loss: 48.1374   val Loss: 47.8557   time: 88.81s   best: 47.8557
2023-11-24 00:33:13,430:INFO:  Epoch 50/500:  train Loss: 47.6120   val Loss: 48.1074   time: 88.94s   best: 47.8557
2023-11-24 00:33:44,580:INFO:  Epoch 327/500:  train Loss: 20.8799   val Loss: 28.0022   time: 430.93s   best: 27.3407
2023-11-24 00:34:42,084:INFO:  Epoch 51/500:  train Loss: 47.2484   val Loss: 49.4427   time: 88.64s   best: 47.8557
2023-11-24 00:36:11,119:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:36:11,151:INFO:  Epoch 52/500:  train Loss: 46.9758   val Loss: 47.7102   time: 89.02s   best: 47.7102
2023-11-24 00:37:39,812:INFO:  Epoch 53/500:  train Loss: 46.7252   val Loss: 47.7957   time: 88.66s   best: 47.7102
2023-11-24 00:39:08,697:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:39:08,716:INFO:  Epoch 54/500:  train Loss: 46.3358   val Loss: 46.6710   time: 88.87s   best: 46.6710
2023-11-24 00:40:37,426:INFO:  Epoch 55/500:  train Loss: 45.9113   val Loss: 46.9247   time: 88.70s   best: 46.6710
2023-11-24 00:40:55,630:INFO:  Epoch 328/500:  train Loss: 20.6187   val Loss: 28.1227   time: 431.05s   best: 27.3407
2023-11-24 00:42:06,437:INFO:  Epoch 56/500:  train Loss: 45.8477   val Loss: 47.4133   time: 89.00s   best: 46.6710
2023-11-24 00:43:35,179:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:43:35,198:INFO:  Epoch 57/500:  train Loss: 45.9321   val Loss: 46.4561   time: 88.72s   best: 46.4561
2023-11-24 00:45:04,048:INFO:  Epoch 58/500:  train Loss: 45.3009   val Loss: 46.4730   time: 88.85s   best: 46.4561
2023-11-24 00:46:33,390:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:46:33,409:INFO:  Epoch 59/500:  train Loss: 45.1450   val Loss: 46.0148   time: 89.33s   best: 46.0148
2023-11-24 00:48:02,484:INFO:  Epoch 60/500:  train Loss: 44.5266   val Loss: 46.6334   time: 89.07s   best: 46.0148
2023-11-24 00:48:07,675:INFO:  Epoch 329/500:  train Loss: 20.6563   val Loss: 28.4343   time: 432.04s   best: 27.3407
2023-11-24 00:49:31,187:INFO:  Epoch 61/500:  train Loss: 44.8944   val Loss: 46.2272   time: 88.70s   best: 46.0148
2023-11-24 00:50:59,734:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:50:59,753:INFO:  Epoch 62/500:  train Loss: 44.4227   val Loss: 45.3031   time: 88.53s   best: 45.3031
2023-11-24 00:52:28,438:INFO:  Epoch 63/500:  train Loss: 43.9257   val Loss: 45.7742   time: 88.68s   best: 45.3031
2023-11-24 00:53:57,704:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:53:57,723:INFO:  Epoch 64/500:  train Loss: 43.6774   val Loss: 44.7260   time: 89.26s   best: 44.7260
2023-11-24 00:55:17,275:INFO:  Epoch 330/500:  train Loss: 20.7403   val Loss: 29.4013   time: 429.60s   best: 27.3407
2023-11-24 00:55:26,842:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:55:26,863:INFO:  Epoch 65/500:  train Loss: 43.3208   val Loss: 44.5395   time: 89.11s   best: 44.5395
2023-11-24 00:56:55,963:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:56:55,982:INFO:  Epoch 66/500:  train Loss: 43.0221   val Loss: 44.0576   time: 89.09s   best: 44.0576
2023-11-24 00:58:24,614:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 00:58:24,633:INFO:  Epoch 67/500:  train Loss: 42.8984   val Loss: 44.0042   time: 88.63s   best: 44.0042
2023-11-24 00:59:53,302:INFO:  Epoch 68/500:  train Loss: 42.5895   val Loss: 44.5399   time: 88.67s   best: 44.0042
2023-11-24 01:01:22,134:INFO:  Epoch 69/500:  train Loss: 42.3474   val Loss: 44.2570   time: 88.83s   best: 44.0042
2023-11-24 01:02:28,130:INFO:  Epoch 331/500:  train Loss: 20.8486   val Loss: 28.3152   time: 430.85s   best: 27.3407
2023-11-24 01:02:51,066:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:02:51,088:INFO:  Epoch 70/500:  train Loss: 41.9837   val Loss: 43.5409   time: 88.93s   best: 43.5409
2023-11-24 01:04:19,781:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:04:19,800:INFO:  Epoch 71/500:  train Loss: 41.9188   val Loss: 42.9575   time: 88.69s   best: 42.9575
2023-11-24 01:05:48,428:INFO:  Epoch 72/500:  train Loss: 41.6114   val Loss: 43.4695   time: 88.62s   best: 42.9575
2023-11-24 01:07:17,688:INFO:  Epoch 73/500:  train Loss: 41.4276   val Loss: 43.5600   time: 89.26s   best: 42.9575
2023-11-24 01:08:46,647:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:08:46,667:INFO:  Epoch 74/500:  train Loss: 41.1838   val Loss: 42.3449   time: 88.94s   best: 42.3449
2023-11-24 01:09:35,601:INFO:  Epoch 332/500:  train Loss: 20.7934   val Loss: 28.1009   time: 427.46s   best: 27.3407
2023-11-24 01:10:15,412:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:10:15,432:INFO:  Epoch 75/500:  train Loss: 40.8609   val Loss: 42.0714   time: 88.73s   best: 42.0714
2023-11-24 01:11:44,453:INFO:  Epoch 76/500:  train Loss: 40.6672   val Loss: 44.4151   time: 89.01s   best: 42.0714
2023-11-24 01:13:13,434:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:13:13,454:INFO:  Epoch 77/500:  train Loss: 41.2098   val Loss: 42.0197   time: 88.98s   best: 42.0197
2023-11-24 01:14:42,338:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:14:42,358:INFO:  Epoch 78/500:  train Loss: 40.2645   val Loss: 41.9232   time: 88.88s   best: 41.9232
2023-11-24 01:16:11,208:INFO:  Epoch 79/500:  train Loss: 40.9177   val Loss: 42.3136   time: 88.85s   best: 41.9232
2023-11-24 01:16:45,683:INFO:  Epoch 333/500:  train Loss: 20.7333   val Loss: 28.2798   time: 430.07s   best: 27.3407
2023-11-24 01:17:40,172:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:17:40,193:INFO:  Epoch 80/500:  train Loss: 40.0503   val Loss: 41.4878   time: 88.96s   best: 41.4878
2023-11-24 01:19:09,215:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:19:09,244:INFO:  Epoch 81/500:  train Loss: 39.7992   val Loss: 41.2604   time: 89.02s   best: 41.2604
2023-11-24 01:20:38,464:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:20:38,483:INFO:  Epoch 82/500:  train Loss: 39.7642   val Loss: 41.0257   time: 89.20s   best: 41.0257
2023-11-24 01:22:07,637:INFO:  Epoch 83/500:  train Loss: 39.5846   val Loss: 41.4331   time: 89.15s   best: 41.0257
2023-11-24 01:23:36,314:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:23:36,343:INFO:  Epoch 84/500:  train Loss: 39.3903   val Loss: 40.4426   time: 88.67s   best: 40.4426
2023-11-24 01:23:58,076:INFO:  Epoch 334/500:  train Loss: 20.7829   val Loss: 27.6499   time: 432.38s   best: 27.3407
2023-11-24 01:25:06,070:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:25:06,091:INFO:  Epoch 85/500:  train Loss: 38.9664   val Loss: 40.2865   time: 89.72s   best: 40.2865
2023-11-24 01:26:34,754:INFO:  Epoch 86/500:  train Loss: 38.8335   val Loss: 41.8186   time: 88.65s   best: 40.2865
2023-11-24 01:28:03,297:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:28:03,316:INFO:  Epoch 87/500:  train Loss: 38.5835   val Loss: 39.8733   time: 88.54s   best: 39.8733
2023-11-24 01:29:32,305:INFO:  Epoch 88/500:  train Loss: 39.3937   val Loss: 40.4063   time: 88.98s   best: 39.8733
2023-11-24 01:31:00,985:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:31:01,004:INFO:  Epoch 89/500:  train Loss: 38.5418   val Loss: 39.4157   time: 88.68s   best: 39.4157
2023-11-24 01:31:09,641:INFO:  Epoch 335/500:  train Loss: 20.6483   val Loss: 28.0260   time: 431.55s   best: 27.3407
2023-11-24 01:32:29,561:INFO:  Epoch 90/500:  train Loss: 38.3075   val Loss: 39.7052   time: 88.54s   best: 39.4157
2023-11-24 01:33:58,242:INFO:  Epoch 91/500:  train Loss: 38.5740   val Loss: 39.6985   time: 88.66s   best: 39.4157
2023-11-24 01:35:27,117:INFO:  Epoch 92/500:  train Loss: 37.9822   val Loss: 39.7896   time: 88.87s   best: 39.4157
2023-11-24 01:36:56,133:INFO:  Epoch 93/500:  train Loss: 37.6071   val Loss: 40.6955   time: 89.01s   best: 39.4157
2023-11-24 01:38:19,342:INFO:  Epoch 336/500:  train Loss: 20.6548   val Loss: 27.4750   time: 429.70s   best: 27.3407
2023-11-24 01:38:24,766:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:38:24,787:INFO:  Epoch 94/500:  train Loss: 37.5555   val Loss: 39.3267   time: 88.62s   best: 39.3267
2023-11-24 01:39:53,744:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:39:53,763:INFO:  Epoch 95/500:  train Loss: 37.6977   val Loss: 38.6466   time: 88.94s   best: 38.6466
2023-11-24 01:41:22,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:41:22,464:INFO:  Epoch 96/500:  train Loss: 37.0854   val Loss: 38.4612   time: 88.67s   best: 38.4612
2023-11-24 01:42:51,311:INFO:  Epoch 97/500:  train Loss: 37.0642   val Loss: 39.6088   time: 88.83s   best: 38.4612
2023-11-24 01:44:20,249:INFO:  Epoch 98/500:  train Loss: 36.6404   val Loss: 40.2912   time: 88.93s   best: 38.4612
2023-11-24 01:45:30,465:INFO:  Epoch 337/500:  train Loss: 20.6916   val Loss: 28.4589   time: 431.11s   best: 27.3407
2023-11-24 01:45:49,419:INFO:  Epoch 99/500:  train Loss: 36.7522   val Loss: 38.8799   time: 89.17s   best: 38.4612
2023-11-24 01:47:18,712:INFO:  Epoch 100/500:  train Loss: 36.7330   val Loss: 38.6664   time: 89.27s   best: 38.4612
2023-11-24 01:48:47,670:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:48:47,689:INFO:  Epoch 101/500:  train Loss: 36.5101   val Loss: 37.5978   time: 88.94s   best: 37.5978
2023-11-24 01:50:16,354:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:50:16,373:INFO:  Epoch 102/500:  train Loss: 36.1710   val Loss: 37.1985   time: 88.65s   best: 37.1985
2023-11-24 01:51:45,136:INFO:  Epoch 103/500:  train Loss: 35.9201   val Loss: 37.8722   time: 88.75s   best: 37.1985
2023-11-24 01:52:38,283:INFO:  Epoch 338/500:  train Loss: 20.8844   val Loss: 28.4791   time: 427.82s   best: 27.3407
2023-11-24 01:53:13,854:INFO:  Epoch 104/500:  train Loss: 36.3777   val Loss: 41.2359   time: 88.71s   best: 37.1985
2023-11-24 01:54:42,881:INFO:  Epoch 105/500:  train Loss: 36.1859   val Loss: 37.4061   time: 89.01s   best: 37.1985
2023-11-24 01:56:11,707:INFO:  Epoch 106/500:  train Loss: 35.5229   val Loss: 37.4410   time: 88.81s   best: 37.1985
2023-11-24 01:57:40,194:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 01:57:40,213:INFO:  Epoch 107/500:  train Loss: 35.3871   val Loss: 36.6605   time: 88.47s   best: 36.6605
2023-11-24 01:59:08,985:INFO:  Epoch 108/500:  train Loss: 35.1183   val Loss: 39.3391   time: 88.76s   best: 36.6605
2023-11-24 01:59:48,992:INFO:  Epoch 339/500:  train Loss: 20.6359   val Loss: 27.7419   time: 430.71s   best: 27.3407
2023-11-24 02:00:38,371:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 02:00:38,392:INFO:  Epoch 109/500:  train Loss: 35.1930   val Loss: 36.2363   time: 89.38s   best: 36.2363
2023-11-24 02:02:07,523:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 02:02:07,543:INFO:  Epoch 110/500:  train Loss: 34.8816   val Loss: 36.2235   time: 89.12s   best: 36.2235
2023-11-24 02:03:36,774:INFO:  Epoch 111/500:  train Loss: 35.0324   val Loss: 36.3116   time: 89.22s   best: 36.2235
2023-11-24 02:05:05,885:INFO:  Epoch 112/500:  train Loss: 34.7974   val Loss: 36.5954   time: 89.10s   best: 36.2235
2023-11-24 02:06:34,844:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 02:06:34,864:INFO:  Epoch 113/500:  train Loss: 34.3257   val Loss: 35.5130   time: 88.95s   best: 35.5130
2023-11-24 02:06:56,990:INFO:  Epoch 340/500:  train Loss: 20.7211   val Loss: 28.0046   time: 428.00s   best: 27.3407
2023-11-24 02:08:04,220:INFO:  Epoch 114/500:  train Loss: 34.0953   val Loss: 35.8620   time: 89.36s   best: 35.5130
2023-11-24 02:09:33,256:INFO:  Epoch 115/500:  train Loss: 34.2913   val Loss: 37.2127   time: 89.03s   best: 35.5130
2023-11-24 02:11:01,925:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 02:11:01,945:INFO:  Epoch 116/500:  train Loss: 34.1915   val Loss: 35.3998   time: 88.66s   best: 35.3998
2023-11-24 02:12:31,042:INFO:  Epoch 117/500:  train Loss: 33.8609   val Loss: 35.8655   time: 89.10s   best: 35.3998
2023-11-24 02:13:59,657:INFO:  Epoch 118/500:  train Loss: 34.0755   val Loss: 37.6440   time: 88.61s   best: 35.3998
2023-11-24 02:14:04,302:INFO:  Epoch 341/500:  train Loss: 20.6325   val Loss: 28.1967   time: 427.30s   best: 27.3407
2023-11-24 02:15:28,402:INFO:  Epoch 119/500:  train Loss: 34.3190   val Loss: 36.5131   time: 88.74s   best: 35.3998
2023-11-24 02:16:57,126:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 02:16:57,155:INFO:  Epoch 120/500:  train Loss: 33.5668   val Loss: 34.9570   time: 88.71s   best: 34.9570
2023-11-24 02:18:26,197:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 02:18:26,216:INFO:  Epoch 121/500:  train Loss: 33.4684   val Loss: 34.3352   time: 89.03s   best: 34.3352
2023-11-24 02:19:54,734:INFO:  Epoch 122/500:  train Loss: 33.5685   val Loss: 34.8498   time: 88.52s   best: 34.3352
2023-11-24 02:21:13,417:INFO:  Epoch 342/500:  train Loss: 20.5808   val Loss: 28.8758   time: 429.10s   best: 27.3407
2023-11-24 02:21:23,377:INFO:  Epoch 123/500:  train Loss: 33.3967   val Loss: 35.7446   time: 88.63s   best: 34.3352
2023-11-24 02:22:51,849:INFO:  Epoch 124/500:  train Loss: 33.2168   val Loss: 34.9044   time: 88.46s   best: 34.3352
2023-11-24 02:24:20,812:INFO:  Epoch 125/500:  train Loss: 33.8581   val Loss: 40.7682   time: 88.95s   best: 34.3352
2023-11-24 02:25:49,427:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 02:25:49,446:INFO:  Epoch 126/500:  train Loss: 33.6780   val Loss: 34.1496   time: 88.60s   best: 34.1496
2023-11-24 02:27:18,568:INFO:  Epoch 127/500:  train Loss: 33.2633   val Loss: 35.2897   time: 89.12s   best: 34.1496
2023-11-24 02:28:24,349:INFO:  Epoch 343/500:  train Loss: 20.8406   val Loss: 27.7498   time: 430.93s   best: 27.3407
2023-11-24 02:28:47,227:INFO:  Epoch 128/500:  train Loss: 32.8291   val Loss: 35.8969   time: 88.66s   best: 34.1496
2023-11-24 02:30:15,920:INFO:  Epoch 129/500:  train Loss: 32.7304   val Loss: 35.4539   time: 88.69s   best: 34.1496
2023-11-24 02:31:44,735:INFO:  Epoch 130/500:  train Loss: 32.7943   val Loss: 34.9910   time: 88.81s   best: 34.1496
2023-11-24 02:33:13,342:INFO:  Epoch 131/500:  train Loss: 32.7014   val Loss: 35.4319   time: 88.61s   best: 34.1496
2023-11-24 02:34:42,226:INFO:  Epoch 132/500:  train Loss: 32.5501   val Loss: 34.3454   time: 88.88s   best: 34.1496
2023-11-24 02:35:34,728:INFO:  Epoch 344/500:  train Loss: 20.7200   val Loss: 27.8552   time: 430.38s   best: 27.3407
2023-11-24 02:36:11,241:INFO:  Epoch 133/500:  train Loss: 32.5909   val Loss: 34.2399   time: 89.01s   best: 34.1496
2023-11-24 02:37:40,371:INFO:  Epoch 134/500:  train Loss: 32.3397   val Loss: 34.3513   time: 89.13s   best: 34.1496
2023-11-24 02:39:09,205:INFO:  Epoch 135/500:  train Loss: 32.3458   val Loss: 37.6272   time: 88.82s   best: 34.1496
2023-11-24 02:40:37,944:INFO:  Epoch 136/500:  train Loss: 32.3431   val Loss: 34.5128   time: 88.74s   best: 34.1496
2023-11-24 02:42:06,630:INFO:  Epoch 137/500:  train Loss: 32.1535   val Loss: 34.4277   time: 88.67s   best: 34.1496
2023-11-24 02:42:46,328:INFO:  Epoch 345/500:  train Loss: 20.5952   val Loss: 28.3199   time: 431.60s   best: 27.3407
2023-11-24 02:43:35,292:INFO:  Epoch 138/500:  train Loss: 32.1498   val Loss: 35.9095   time: 88.66s   best: 34.1496
2023-11-24 02:45:03,962:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 02:45:03,981:INFO:  Epoch 139/500:  train Loss: 32.1199   val Loss: 33.6431   time: 88.63s   best: 33.6431
2023-11-24 02:46:32,681:INFO:  Epoch 140/500:  train Loss: 32.2864   val Loss: 34.2751   time: 88.69s   best: 33.6431
2023-11-24 02:48:01,764:INFO:  Epoch 141/500:  train Loss: 31.9192   val Loss: 34.1404   time: 89.07s   best: 33.6431
2023-11-24 02:49:30,413:INFO:  Epoch 142/500:  train Loss: 31.9242   val Loss: 34.8838   time: 88.64s   best: 33.6431
2023-11-24 02:49:59,425:INFO:  Epoch 346/500:  train Loss: 20.6772   val Loss: 29.4294   time: 433.08s   best: 27.3407
2023-11-24 02:50:59,180:INFO:  Epoch 143/500:  train Loss: 31.6761   val Loss: 33.8669   time: 88.75s   best: 33.6431
2023-11-24 02:52:27,805:INFO:  Epoch 144/500:  train Loss: 31.8267   val Loss: 35.3573   time: 88.61s   best: 33.6431
2023-11-24 02:53:56,593:INFO:  Epoch 145/500:  train Loss: 31.6658   val Loss: 33.6911   time: 88.78s   best: 33.6431
2023-11-24 02:55:25,037:INFO:  Epoch 146/500:  train Loss: 31.3126   val Loss: 37.9753   time: 88.43s   best: 33.6431
2023-11-24 02:56:53,795:INFO:  Epoch 147/500:  train Loss: 31.6216   val Loss: 33.8867   time: 88.75s   best: 33.6431
2023-11-24 02:57:11,717:INFO:  Epoch 347/500:  train Loss: 20.6375   val Loss: 27.9736   time: 432.28s   best: 27.3407
2023-11-24 02:58:22,397:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 02:58:22,418:INFO:  Epoch 148/500:  train Loss: 31.0991   val Loss: 32.5848   time: 88.60s   best: 32.5848
2023-11-24 02:59:51,186:INFO:  Epoch 149/500:  train Loss: 31.1906   val Loss: 33.2299   time: 88.76s   best: 32.5848
2023-11-24 03:01:20,503:INFO:  Epoch 150/500:  train Loss: 31.1600   val Loss: 33.1778   time: 89.32s   best: 32.5848
2023-11-24 03:02:50,024:INFO:  Epoch 151/500:  train Loss: 31.1052   val Loss: 33.7958   time: 89.50s   best: 32.5848
2023-11-24 03:04:18,858:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 03:04:18,878:INFO:  Epoch 152/500:  train Loss: 30.9454   val Loss: 32.3991   time: 88.82s   best: 32.3991
2023-11-24 03:04:19,440:INFO:  Epoch 348/500:  train Loss: 20.5463   val Loss: 28.1213   time: 427.71s   best: 27.3407
2023-11-24 03:05:47,592:INFO:  Epoch 153/500:  train Loss: 30.8366   val Loss: 34.2198   time: 88.70s   best: 32.3991
2023-11-24 03:07:16,207:INFO:  Epoch 154/500:  train Loss: 30.8155   val Loss: 32.8860   time: 88.59s   best: 32.3991
2023-11-24 03:08:45,182:INFO:  Epoch 155/500:  train Loss: 30.7662   val Loss: 32.9129   time: 88.97s   best: 32.3991
2023-11-24 03:10:14,328:INFO:  Epoch 156/500:  train Loss: 30.5136   val Loss: 32.8380   time: 89.13s   best: 32.3991
2023-11-24 03:11:31,757:INFO:  Epoch 349/500:  train Loss: 20.6251   val Loss: 28.3006   time: 432.30s   best: 27.3407
2023-11-24 03:11:43,382:INFO:  Epoch 157/500:  train Loss: 31.3108   val Loss: 34.7817   time: 89.05s   best: 32.3991
2023-11-24 03:13:12,239:INFO:  Epoch 158/500:  train Loss: 30.8810   val Loss: 33.2398   time: 88.84s   best: 32.3991
2023-11-24 03:14:40,959:INFO:  Epoch 159/500:  train Loss: 30.6902   val Loss: 33.7749   time: 88.71s   best: 32.3991
2023-11-24 03:16:09,465:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 03:16:09,484:INFO:  Epoch 160/500:  train Loss: 30.3555   val Loss: 32.1511   time: 88.50s   best: 32.1511
2023-11-24 03:17:38,050:INFO:  Epoch 161/500:  train Loss: 30.3925   val Loss: 33.2772   time: 88.57s   best: 32.1511
2023-11-24 03:18:39,195:INFO:  Epoch 350/500:  train Loss: 20.7531   val Loss: 27.8847   time: 427.42s   best: 27.3407
2023-11-24 03:19:06,961:INFO:  Epoch 162/500:  train Loss: 30.2335   val Loss: 33.4809   time: 88.91s   best: 32.1511
2023-11-24 03:20:35,722:INFO:  Epoch 163/500:  train Loss: 31.2460   val Loss: 34.6798   time: 88.75s   best: 32.1511
2023-11-24 03:22:04,412:INFO:  Epoch 164/500:  train Loss: 30.4815   val Loss: 32.5780   time: 88.68s   best: 32.1511
2023-11-24 03:23:33,109:INFO:  Epoch 165/500:  train Loss: 30.1507   val Loss: 32.1863   time: 88.68s   best: 32.1511
2023-11-24 03:25:02,008:INFO:  Epoch 166/500:  train Loss: 29.8577   val Loss: 32.3186   time: 88.90s   best: 32.1511
2023-11-24 03:25:47,822:INFO:  Epoch 351/500:  train Loss: 20.6747   val Loss: 27.8956   time: 428.62s   best: 27.3407
2023-11-24 03:26:30,869:INFO:  Epoch 167/500:  train Loss: 30.0215   val Loss: 33.3892   time: 88.85s   best: 32.1511
2023-11-24 03:27:59,592:INFO:  Epoch 168/500:  train Loss: 30.1001   val Loss: 33.2085   time: 88.71s   best: 32.1511
2023-11-24 03:29:28,274:INFO:  Epoch 169/500:  train Loss: 29.9351   val Loss: 32.2713   time: 88.68s   best: 32.1511
2023-11-24 03:30:57,146:INFO:  Epoch 170/500:  train Loss: 29.8037   val Loss: 33.4047   time: 88.87s   best: 32.1511
2023-11-24 03:32:26,277:INFO:  Epoch 171/500:  train Loss: 29.8165   val Loss: 34.0471   time: 89.12s   best: 32.1511
2023-11-24 03:32:57,067:INFO:  Epoch 352/500:  train Loss: 20.7611   val Loss: 28.1495   time: 429.24s   best: 27.3407
2023-11-24 03:33:54,702:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 03:33:54,723:INFO:  Epoch 172/500:  train Loss: 29.7955   val Loss: 31.9015   time: 88.41s   best: 31.9015
2023-11-24 03:35:23,261:INFO:  Epoch 173/500:  train Loss: 30.4179   val Loss: 32.4738   time: 88.54s   best: 31.9015
2023-11-24 03:36:52,552:INFO:  Epoch 174/500:  train Loss: 29.8479   val Loss: 33.3953   time: 89.28s   best: 31.9015
2023-11-24 03:38:21,445:INFO:  Epoch 175/500:  train Loss: 29.8794   val Loss: 32.2691   time: 88.89s   best: 31.9015
2023-11-24 03:39:49,903:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 03:39:49,922:INFO:  Epoch 176/500:  train Loss: 29.3175   val Loss: 31.8138   time: 88.45s   best: 31.8138
2023-11-24 03:40:09,468:INFO:  Epoch 353/500:  train Loss: 20.6027   val Loss: 27.5201   time: 432.40s   best: 27.3407
2023-11-24 03:41:18,621:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 03:41:18,645:INFO:  Epoch 177/500:  train Loss: 29.4422   val Loss: 31.4746   time: 88.69s   best: 31.4746
2023-11-24 03:42:47,239:INFO:  Epoch 178/500:  train Loss: 29.4033   val Loss: 31.8307   time: 88.59s   best: 31.4746
2023-11-24 03:44:16,375:INFO:  Epoch 179/500:  train Loss: 29.4706   val Loss: 32.1053   time: 89.13s   best: 31.4746
2023-11-24 03:45:45,039:INFO:  Epoch 180/500:  train Loss: 29.2749   val Loss: 31.7521   time: 88.66s   best: 31.4746
2023-11-24 03:47:13,998:INFO:  Epoch 181/500:  train Loss: 29.2693   val Loss: 32.2133   time: 88.95s   best: 31.4746
2023-11-24 03:47:19,739:INFO:  Epoch 354/500:  train Loss: 20.6413   val Loss: 28.2322   time: 430.27s   best: 27.3407
2023-11-24 03:48:42,958:INFO:  Epoch 182/500:  train Loss: 29.5169   val Loss: 32.3161   time: 88.96s   best: 31.4746
2023-11-24 03:50:12,081:INFO:  Epoch 183/500:  train Loss: 29.4297   val Loss: 32.6105   time: 89.12s   best: 31.4746
2023-11-24 03:51:40,722:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 03:51:40,750:INFO:  Epoch 184/500:  train Loss: 29.0470   val Loss: 31.4646   time: 88.62s   best: 31.4646
2023-11-24 03:53:09,983:INFO:  Epoch 185/500:  train Loss: 28.9786   val Loss: 32.4594   time: 89.22s   best: 31.4646
2023-11-24 03:54:32,822:INFO:  Epoch 355/500:  train Loss: 20.5627   val Loss: 28.0880   time: 433.07s   best: 27.3407
2023-11-24 03:54:38,574:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 03:54:38,595:INFO:  Epoch 186/500:  train Loss: 29.0458   val Loss: 31.3701   time: 88.59s   best: 31.3701
2023-11-24 03:56:07,238:INFO:  Epoch 187/500:  train Loss: 28.8760   val Loss: 32.8665   time: 88.64s   best: 31.3701
2023-11-24 03:57:35,656:INFO:  Epoch 188/500:  train Loss: 28.9553   val Loss: 32.6147   time: 88.42s   best: 31.3701
2023-11-24 03:59:04,130:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 03:59:04,149:INFO:  Epoch 189/500:  train Loss: 28.9329   val Loss: 31.3089   time: 88.47s   best: 31.3089
2023-11-24 04:00:32,820:INFO:  Epoch 190/500:  train Loss: 28.7301   val Loss: 32.1698   time: 88.66s   best: 31.3089
2023-11-24 04:01:41,896:INFO:  Epoch 356/500:  train Loss: 20.6173   val Loss: 27.8771   time: 429.07s   best: 27.3407
2023-11-24 04:02:01,694:INFO:  Epoch 191/500:  train Loss: 28.8731   val Loss: 32.6389   time: 88.86s   best: 31.3089
2023-11-24 04:03:30,343:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 04:03:30,363:INFO:  Epoch 192/500:  train Loss: 28.5960   val Loss: 31.0801   time: 88.64s   best: 31.0801
2023-11-24 04:04:59,189:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 04:04:59,208:INFO:  Epoch 193/500:  train Loss: 28.6362   val Loss: 31.0660   time: 88.81s   best: 31.0660
2023-11-24 04:06:28,323:INFO:  Epoch 194/500:  train Loss: 28.5547   val Loss: 32.6915   time: 89.10s   best: 31.0660
2023-11-24 04:07:57,285:INFO:  Epoch 195/500:  train Loss: 28.7032   val Loss: 32.0898   time: 88.95s   best: 31.0660
2023-11-24 04:08:51,341:INFO:  Epoch 357/500:  train Loss: 20.6312   val Loss: 27.7964   time: 429.44s   best: 27.3407
2023-11-24 04:09:26,074:INFO:  Epoch 196/500:  train Loss: 28.4871   val Loss: 31.9471   time: 88.78s   best: 31.0660
2023-11-24 04:10:54,935:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 04:10:54,954:INFO:  Epoch 197/500:  train Loss: 28.4443   val Loss: 30.8294   time: 88.84s   best: 30.8294
2023-11-24 04:12:23,492:INFO:  Epoch 198/500:  train Loss: 28.7267   val Loss: 31.2248   time: 88.52s   best: 30.8294
2023-11-24 04:13:52,739:INFO:  Epoch 199/500:  train Loss: 28.4391   val Loss: 31.8747   time: 89.25s   best: 30.8294
2023-11-24 04:15:21,763:INFO:  Epoch 200/500:  train Loss: 28.5551   val Loss: 31.9653   time: 89.01s   best: 30.8294
2023-11-24 04:16:04,294:INFO:  Epoch 358/500:  train Loss: 20.5095   val Loss: 28.0056   time: 432.94s   best: 27.3407
2023-11-24 04:16:50,864:INFO:  Epoch 201/500:  train Loss: 28.0837   val Loss: 31.0118   time: 89.09s   best: 30.8294
2023-11-24 04:18:19,619:INFO:  Epoch 202/500:  train Loss: 28.1089   val Loss: 32.0079   time: 88.75s   best: 30.8294
2023-11-24 04:19:48,641:INFO:  Epoch 203/500:  train Loss: 28.4974   val Loss: 31.7440   time: 89.02s   best: 30.8294
2023-11-24 04:21:17,164:INFO:  Epoch 204/500:  train Loss: 28.3099   val Loss: 31.8640   time: 88.51s   best: 30.8294
2023-11-24 04:22:46,656:INFO:  Epoch 205/500:  train Loss: 28.0760   val Loss: 31.2562   time: 89.49s   best: 30.8294
2023-11-24 04:23:13,204:INFO:  Epoch 359/500:  train Loss: 20.5579   val Loss: 27.9856   time: 428.91s   best: 27.3407
2023-11-24 04:24:15,284:INFO:  Epoch 206/500:  train Loss: 28.0334   val Loss: 31.1179   time: 88.62s   best: 30.8294
2023-11-24 04:25:43,959:INFO:  Epoch 207/500:  train Loss: 28.0804   val Loss: 31.5164   time: 88.67s   best: 30.8294
2023-11-24 04:27:12,784:INFO:  Epoch 208/500:  train Loss: 27.9442   val Loss: 30.8587   time: 88.82s   best: 30.8294
2023-11-24 04:28:41,694:INFO:  Epoch 209/500:  train Loss: 27.9932   val Loss: 30.9122   time: 88.91s   best: 30.8294
2023-11-24 04:30:10,610:INFO:  Epoch 210/500:  train Loss: 27.9344   val Loss: 32.2736   time: 88.91s   best: 30.8294
2023-11-24 04:30:26,724:INFO:  Epoch 360/500:  train Loss: 20.6191   val Loss: 28.2237   time: 433.51s   best: 27.3407
2023-11-24 04:31:39,598:INFO:  Epoch 211/500:  train Loss: 27.9686   val Loss: 31.8889   time: 88.98s   best: 30.8294
2023-11-24 04:33:08,617:INFO:  Epoch 212/500:  train Loss: 27.7877   val Loss: 31.5686   time: 89.01s   best: 30.8294
2023-11-24 04:34:37,931:INFO:  Epoch 213/500:  train Loss: 28.0223   val Loss: 31.3781   time: 89.31s   best: 30.8294
2023-11-24 04:36:06,960:INFO:  Epoch 214/500:  train Loss: 27.7110   val Loss: 31.5602   time: 89.03s   best: 30.8294
2023-11-24 04:37:36,133:INFO:  Epoch 215/500:  train Loss: 27.6873   val Loss: 32.1242   time: 89.16s   best: 30.8294
2023-11-24 04:37:38,176:INFO:  Epoch 361/500:  train Loss: 20.6338   val Loss: 28.8314   time: 431.43s   best: 27.3407
2023-11-24 04:39:04,797:INFO:  Epoch 216/500:  train Loss: 27.6486   val Loss: 31.2764   time: 88.66s   best: 30.8294
2023-11-24 04:40:33,928:INFO:  Epoch 217/500:  train Loss: 27.9129   val Loss: 31.9110   time: 89.13s   best: 30.8294
2023-11-24 04:42:02,977:INFO:  Epoch 218/500:  train Loss: 27.6986   val Loss: 32.6440   time: 89.05s   best: 30.8294
2023-11-24 04:43:31,750:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 04:43:31,769:INFO:  Epoch 219/500:  train Loss: 27.9240   val Loss: 30.4146   time: 88.76s   best: 30.4146
2023-11-24 04:44:46,588:INFO:  Epoch 362/500:  train Loss: 20.6697   val Loss: 28.0575   time: 428.41s   best: 27.3407
2023-11-24 04:45:00,343:INFO:  Epoch 220/500:  train Loss: 28.1493   val Loss: 30.6124   time: 88.56s   best: 30.4146
2023-11-24 04:46:29,454:INFO:  Epoch 221/500:  train Loss: 27.7272   val Loss: 30.4730   time: 89.11s   best: 30.4146
2023-11-24 04:47:58,213:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 04:47:58,232:INFO:  Epoch 222/500:  train Loss: 27.5707   val Loss: 30.3920   time: 88.75s   best: 30.3920
2023-11-24 04:49:27,666:INFO:  Epoch 223/500:  train Loss: 27.4727   val Loss: 33.4641   time: 89.43s   best: 30.3920
2023-11-24 04:50:56,722:INFO:  Epoch 224/500:  train Loss: 27.8196   val Loss: 31.1050   time: 89.04s   best: 30.3920
2023-11-24 04:51:58,659:INFO:  Epoch 363/500:  train Loss: 20.7366   val Loss: 28.6669   time: 432.07s   best: 27.3407
2023-11-24 04:52:25,959:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 04:52:25,980:INFO:  Epoch 225/500:  train Loss: 27.3031   val Loss: 30.3488   time: 89.23s   best: 30.3488
2023-11-24 04:53:55,047:INFO:  Epoch 226/500:  train Loss: 27.3155   val Loss: 31.2715   time: 89.05s   best: 30.3488
2023-11-24 04:55:23,874:INFO:  Epoch 227/500:  train Loss: 27.2491   val Loss: 31.7434   time: 88.83s   best: 30.3488
2023-11-24 04:56:53,210:INFO:  Epoch 228/500:  train Loss: 27.4819   val Loss: 30.8089   time: 89.33s   best: 30.3488
2023-11-24 04:58:22,555:INFO:  Epoch 229/500:  train Loss: 27.6770   val Loss: 30.9001   time: 89.34s   best: 30.3488
2023-11-24 04:59:06,113:INFO:  Epoch 364/500:  train Loss: 20.7419   val Loss: 28.2992   time: 427.45s   best: 27.3407
2023-11-24 04:59:51,560:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 04:59:51,584:INFO:  Epoch 230/500:  train Loss: 27.1369   val Loss: 29.7834   time: 88.99s   best: 29.7834
2023-11-24 05:01:20,980:INFO:  Epoch 231/500:  train Loss: 27.1473   val Loss: 31.1399   time: 89.38s   best: 29.7834
2023-11-24 05:02:49,583:INFO:  Epoch 232/500:  train Loss: 27.0417   val Loss: 30.3721   time: 88.60s   best: 29.7834
2023-11-24 05:04:18,249:INFO:  Epoch 233/500:  train Loss: 27.2624   val Loss: 31.1839   time: 88.67s   best: 29.7834
2023-11-24 05:05:46,764:INFO:  Epoch 234/500:  train Loss: 27.6644   val Loss: 30.5478   time: 88.50s   best: 29.7834
2023-11-24 05:06:17,564:INFO:  Epoch 365/500:  train Loss: 20.6133   val Loss: 31.4009   time: 431.44s   best: 27.3407
2023-11-24 05:07:15,471:INFO:  Epoch 235/500:  train Loss: 26.9608   val Loss: 30.0221   time: 88.69s   best: 29.7834
2023-11-24 05:08:44,501:INFO:  Epoch 236/500:  train Loss: 26.8488   val Loss: 29.9846   time: 89.00s   best: 29.7834
2023-11-24 05:10:13,157:INFO:  Epoch 237/500:  train Loss: 27.0073   val Loss: 30.9146   time: 88.65s   best: 29.7834
2023-11-24 05:11:41,545:INFO:  Epoch 238/500:  train Loss: 26.8762   val Loss: 30.6649   time: 88.38s   best: 29.7834
2023-11-24 05:13:10,142:INFO:  Epoch 239/500:  train Loss: 27.0307   val Loss: 30.9178   time: 88.58s   best: 29.7834
2023-11-24 05:13:29,553:INFO:  Epoch 366/500:  train Loss: 20.6165   val Loss: 28.9458   time: 431.99s   best: 27.3407
2023-11-24 05:14:38,659:INFO:  Epoch 240/500:  train Loss: 27.2495   val Loss: 31.6496   time: 88.50s   best: 29.7834
2023-11-24 05:16:07,340:INFO:  Epoch 241/500:  train Loss: 27.0369   val Loss: 30.3146   time: 88.68s   best: 29.7834
2023-11-24 05:17:36,537:INFO:  Epoch 242/500:  train Loss: 26.7288   val Loss: 30.8403   time: 89.20s   best: 29.7834
2023-11-24 05:19:05,332:INFO:  Epoch 243/500:  train Loss: 26.8288   val Loss: 30.2672   time: 88.79s   best: 29.7834
2023-11-24 05:20:34,307:INFO:  Epoch 244/500:  train Loss: 26.8379   val Loss: 31.5508   time: 88.96s   best: 29.7834
2023-11-24 05:20:37,284:INFO:  Epoch 367/500:  train Loss: 20.6279   val Loss: 28.4090   time: 427.72s   best: 27.3407
2023-11-24 05:22:02,958:INFO:  Epoch 245/500:  train Loss: 26.7499   val Loss: 30.4541   time: 88.64s   best: 29.7834
2023-11-24 05:23:31,513:INFO:  Epoch 246/500:  train Loss: 26.9174   val Loss: 30.7172   time: 88.55s   best: 29.7834
2023-11-24 05:25:00,158:INFO:  Epoch 247/500:  train Loss: 26.5075   val Loss: 30.4085   time: 88.63s   best: 29.7834
2023-11-24 05:26:28,728:INFO:  Epoch 248/500:  train Loss: 26.4847   val Loss: 30.7199   time: 88.56s   best: 29.7834
2023-11-24 05:27:49,408:INFO:  Epoch 368/500:  train Loss: 20.5811   val Loss: 28.0517   time: 432.11s   best: 27.3407
2023-11-24 05:27:57,358:INFO:  Epoch 249/500:  train Loss: 26.5207   val Loss: 30.0540   time: 88.63s   best: 29.7834
2023-11-24 05:29:26,335:INFO:  Epoch 250/500:  train Loss: 26.8158   val Loss: 30.4329   time: 88.96s   best: 29.7834
2023-11-24 05:30:55,598:INFO:  Epoch 251/500:  train Loss: 26.4653   val Loss: 30.4800   time: 89.26s   best: 29.7834
2023-11-24 05:32:24,147:INFO:  Epoch 252/500:  train Loss: 26.5638   val Loss: 30.1048   time: 88.55s   best: 29.7834
2023-11-24 05:33:53,306:INFO:  Epoch 253/500:  train Loss: 26.8119   val Loss: 30.2750   time: 89.16s   best: 29.7834
2023-11-24 05:34:58,849:INFO:  Epoch 369/500:  train Loss: 20.9113   val Loss: 28.9856   time: 429.43s   best: 27.3407
2023-11-24 05:35:22,387:INFO:  Epoch 254/500:  train Loss: 26.4785   val Loss: 30.7958   time: 89.08s   best: 29.7834
2023-11-24 05:36:51,904:INFO:  Epoch 255/500:  train Loss: 26.4389   val Loss: 30.5507   time: 89.51s   best: 29.7834
2023-11-24 05:38:21,090:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 05:38:21,109:INFO:  Epoch 256/500:  train Loss: 26.2402   val Loss: 29.7599   time: 89.18s   best: 29.7599
2023-11-24 05:39:50,542:INFO:  Epoch 257/500:  train Loss: 26.4565   val Loss: 30.6444   time: 89.42s   best: 29.7599
2023-11-24 05:41:20,352:INFO:  Epoch 258/500:  train Loss: 26.3306   val Loss: 30.3589   time: 89.80s   best: 29.7599
2023-11-24 05:42:10,602:INFO:  Epoch 370/500:  train Loss: 20.5478   val Loss: 28.3281   time: 431.74s   best: 27.3407
2023-11-24 05:42:49,457:INFO:  Epoch 259/500:  train Loss: 26.1719   val Loss: 30.1772   time: 89.10s   best: 29.7599
2023-11-24 05:44:18,664:INFO:  Epoch 260/500:  train Loss: 26.4234   val Loss: 30.0091   time: 89.19s   best: 29.7599
2023-11-24 05:45:47,460:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 05:45:47,480:INFO:  Epoch 261/500:  train Loss: 26.2133   val Loss: 29.7156   time: 88.79s   best: 29.7156
2023-11-24 05:47:16,636:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 05:47:16,655:INFO:  Epoch 262/500:  train Loss: 26.0842   val Loss: 29.1729   time: 89.15s   best: 29.1729
2023-11-24 05:48:45,786:INFO:  Epoch 263/500:  train Loss: 26.1907   val Loss: 30.1967   time: 89.13s   best: 29.1729
2023-11-24 05:49:22,368:INFO:  Epoch 371/500:  train Loss: 21.3857   val Loss: 27.8391   time: 431.74s   best: 27.3407
2023-11-24 05:50:14,437:INFO:  Epoch 264/500:  train Loss: 26.4991   val Loss: 30.1350   time: 88.65s   best: 29.1729
2023-11-24 05:51:43,190:INFO:  Epoch 265/500:  train Loss: 25.9362   val Loss: 30.1998   time: 88.73s   best: 29.1729
2023-11-24 05:53:12,055:INFO:  Epoch 266/500:  train Loss: 26.3876   val Loss: 33.2125   time: 88.85s   best: 29.1729
2023-11-24 05:54:40,829:INFO:  Epoch 267/500:  train Loss: 26.4002   val Loss: 30.4340   time: 88.76s   best: 29.1729
2023-11-24 05:56:09,468:INFO:  Epoch 268/500:  train Loss: 25.8261   val Loss: 29.3910   time: 88.63s   best: 29.1729
2023-11-24 05:56:30,240:INFO:  Epoch 372/500:  train Loss: 20.6271   val Loss: 27.6230   time: 427.85s   best: 27.3407
2023-11-24 05:57:38,137:INFO:  Epoch 269/500:  train Loss: 26.3712   val Loss: 31.0290   time: 88.67s   best: 29.1729
2023-11-24 05:59:06,625:INFO:  Epoch 270/500:  train Loss: 25.9868   val Loss: 29.9738   time: 88.47s   best: 29.1729
2023-11-24 06:00:35,368:INFO:  Epoch 271/500:  train Loss: 25.8368   val Loss: 29.5950   time: 88.73s   best: 29.1729
2023-11-24 06:02:04,063:INFO:  Epoch 272/500:  train Loss: 26.0332   val Loss: 31.3049   time: 88.68s   best: 29.1729
2023-11-24 06:03:33,060:INFO:  Epoch 273/500:  train Loss: 26.0008   val Loss: 30.4167   time: 89.00s   best: 29.1729
2023-11-24 06:03:43,631:INFO:  Epoch 373/500:  train Loss: 20.5215   val Loss: 27.6210   time: 433.37s   best: 27.3407
2023-11-24 06:05:01,633:INFO:  Epoch 274/500:  train Loss: 26.1624   val Loss: 30.2033   time: 88.57s   best: 29.1729
2023-11-24 06:06:30,352:INFO:  Epoch 275/500:  train Loss: 26.0680   val Loss: 29.8056   time: 88.72s   best: 29.1729
2023-11-24 06:07:59,743:INFO:  Epoch 276/500:  train Loss: 25.6643   val Loss: 29.2709   time: 89.38s   best: 29.1729
2023-11-24 06:09:28,336:INFO:  Epoch 277/500:  train Loss: 25.8628   val Loss: 30.3877   time: 88.58s   best: 29.1729
2023-11-24 06:10:53,038:INFO:  Epoch 374/500:  train Loss: 20.5237   val Loss: 27.7343   time: 429.39s   best: 27.3407
2023-11-24 06:10:57,128:INFO:  Epoch 278/500:  train Loss: 25.6373   val Loss: 30.5923   time: 88.78s   best: 29.1729
2023-11-24 06:12:26,285:INFO:  Epoch 279/500:  train Loss: 25.7827   val Loss: 30.5417   time: 89.14s   best: 29.1729
2023-11-24 06:13:55,005:INFO:  Epoch 280/500:  train Loss: 25.6600   val Loss: 30.4577   time: 88.71s   best: 29.1729
2023-11-24 06:15:23,675:INFO:  Epoch 281/500:  train Loss: 25.5660   val Loss: 29.7038   time: 88.66s   best: 29.1729
2023-11-24 06:16:52,434:INFO:  Epoch 282/500:  train Loss: 25.9541   val Loss: 29.4024   time: 88.75s   best: 29.1729
2023-11-24 06:18:05,506:INFO:  Epoch 375/500:  train Loss: 20.7725   val Loss: 27.9522   time: 432.46s   best: 27.3407
2023-11-24 06:18:21,230:INFO:  Epoch 283/500:  train Loss: 25.5706   val Loss: 29.9478   time: 88.78s   best: 29.1729
2023-11-24 06:19:49,544:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 06:19:49,563:INFO:  Epoch 284/500:  train Loss: 25.5272   val Loss: 29.0808   time: 88.30s   best: 29.0808
2023-11-24 06:21:18,165:INFO:  Epoch 285/500:  train Loss: 25.6460   val Loss: 29.2977   time: 88.59s   best: 29.0808
2023-11-24 06:22:46,717:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 06:22:46,736:INFO:  Epoch 286/500:  train Loss: 25.6362   val Loss: 28.9223   time: 88.54s   best: 28.9223
2023-11-24 06:24:15,710:INFO:  Epoch 287/500:  train Loss: 25.5808   val Loss: 29.7781   time: 88.97s   best: 28.9223
2023-11-24 06:25:14,534:INFO:  Epoch 376/500:  train Loss: 20.4922   val Loss: 27.6968   time: 429.03s   best: 27.3407
2023-11-24 06:25:44,335:INFO:  Epoch 288/500:  train Loss: 25.5035   val Loss: 30.2538   time: 88.61s   best: 28.9223
2023-11-24 06:27:13,065:INFO:  Epoch 289/500:  train Loss: 25.4748   val Loss: 29.9054   time: 88.73s   best: 28.9223
2023-11-24 06:28:41,989:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 06:28:42,017:INFO:  Epoch 290/500:  train Loss: 25.2802   val Loss: 28.9120   time: 88.92s   best: 28.9120
2023-11-24 06:30:11,354:INFO:  Epoch 291/500:  train Loss: 25.7278   val Loss: 29.0826   time: 89.34s   best: 28.9120
2023-11-24 06:31:40,371:INFO:  Epoch 292/500:  train Loss: 25.2966   val Loss: 28.9296   time: 89.01s   best: 28.9120
2023-11-24 06:32:23,342:INFO:  Epoch 377/500:  train Loss: 20.5957   val Loss: 27.9488   time: 428.80s   best: 27.3407
2023-11-24 06:33:09,205:INFO:  Epoch 293/500:  train Loss: 25.3609   val Loss: 29.2692   time: 88.83s   best: 28.9120
2023-11-24 06:34:38,586:INFO:  Epoch 294/500:  train Loss: 25.2093   val Loss: 29.5060   time: 89.37s   best: 28.9120
2023-11-24 06:36:07,442:INFO:  Epoch 295/500:  train Loss: 25.4734   val Loss: 29.3903   time: 88.85s   best: 28.9120
2023-11-24 06:37:36,274:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 06:37:36,293:INFO:  Epoch 296/500:  train Loss: 25.1779   val Loss: 28.2789   time: 88.83s   best: 28.2789
2023-11-24 06:39:05,159:INFO:  Epoch 297/500:  train Loss: 25.7388   val Loss: 29.3580   time: 88.86s   best: 28.2789
2023-11-24 06:39:35,905:INFO:  Epoch 378/500:  train Loss: 20.4210   val Loss: 28.1374   time: 432.55s   best: 27.3407
2023-11-24 06:40:33,933:INFO:  Epoch 298/500:  train Loss: 25.2043   val Loss: 28.7950   time: 88.76s   best: 28.2789
2023-11-24 06:42:02,526:INFO:  Epoch 299/500:  train Loss: 25.4120   val Loss: 28.9795   time: 88.58s   best: 28.2789
2023-11-24 06:43:31,279:INFO:  Epoch 300/500:  train Loss: 25.2609   val Loss: 29.6795   time: 88.75s   best: 28.2789
2023-11-24 06:45:00,402:INFO:  Epoch 301/500:  train Loss: 25.2714   val Loss: 29.7906   time: 89.11s   best: 28.2789
2023-11-24 06:46:28,778:INFO:  Epoch 302/500:  train Loss: 25.0161   val Loss: 28.6687   time: 88.37s   best: 28.2789
2023-11-24 06:46:44,941:INFO:  Epoch 379/500:  train Loss: 21.2036   val Loss: 28.3371   time: 429.01s   best: 27.3407
2023-11-24 06:47:57,491:INFO:  Epoch 303/500:  train Loss: 25.1169   val Loss: 30.0142   time: 88.71s   best: 28.2789
2023-11-24 06:49:26,066:INFO:  Epoch 304/500:  train Loss: 25.0733   val Loss: 29.2959   time: 88.57s   best: 28.2789
2023-11-24 06:50:55,177:INFO:  Epoch 305/500:  train Loss: 26.1711   val Loss: 29.3063   time: 89.11s   best: 28.2789
2023-11-24 06:52:24,128:INFO:  Epoch 306/500:  train Loss: 25.4540   val Loss: 29.7527   time: 88.95s   best: 28.2789
2023-11-24 06:53:53,220:INFO:  Epoch 307/500:  train Loss: 24.9554   val Loss: 29.1758   time: 89.09s   best: 28.2789
2023-11-24 06:53:56,026:INFO:  Epoch 380/500:  train Loss: 20.6245   val Loss: 28.0505   time: 431.06s   best: 27.3407
2023-11-24 06:55:22,289:INFO:  Epoch 308/500:  train Loss: 24.9486   val Loss: 29.1013   time: 89.07s   best: 28.2789
2023-11-24 06:56:51,346:INFO:  Epoch 309/500:  train Loss: 25.3770   val Loss: 29.8721   time: 89.05s   best: 28.2789
2023-11-24 06:58:20,036:INFO:  Epoch 310/500:  train Loss: 25.0028   val Loss: 29.5010   time: 88.68s   best: 28.2789
2023-11-24 06:59:48,935:INFO:  Epoch 311/500:  train Loss: 24.8831   val Loss: 28.7606   time: 88.90s   best: 28.2789
2023-11-24 07:01:04,666:INFO:  Epoch 381/500:  train Loss: 20.5430   val Loss: 28.4303   time: 428.61s   best: 27.3407
2023-11-24 07:01:17,990:INFO:  Epoch 312/500:  train Loss: 24.8249   val Loss: 28.8196   time: 89.05s   best: 28.2789
2023-11-24 07:02:47,104:INFO:  Epoch 313/500:  train Loss: 25.0655   val Loss: 30.8829   time: 89.10s   best: 28.2789
2023-11-24 07:04:15,966:INFO:  Epoch 314/500:  train Loss: 24.8424   val Loss: 29.4607   time: 88.85s   best: 28.2789
2023-11-24 07:05:44,847:INFO:  Epoch 315/500:  train Loss: 24.9421   val Loss: 28.8891   time: 88.88s   best: 28.2789
2023-11-24 07:07:13,559:INFO:  Epoch 316/500:  train Loss: 24.9709   val Loss: 28.9102   time: 88.71s   best: 28.2789
2023-11-24 07:08:13,336:INFO:  Epoch 382/500:  train Loss: 20.6531   val Loss: 28.0599   time: 428.66s   best: 27.3407
2023-11-24 07:08:42,405:INFO:  Epoch 317/500:  train Loss: 24.7153   val Loss: 28.8745   time: 88.85s   best: 28.2789
2023-11-24 07:10:11,336:INFO:  Epoch 318/500:  train Loss: 24.8663   val Loss: 29.4844   time: 88.93s   best: 28.2789
2023-11-24 07:11:39,993:INFO:  Epoch 319/500:  train Loss: 25.0157   val Loss: 28.8542   time: 88.66s   best: 28.2789
2023-11-24 07:13:08,811:INFO:  Epoch 320/500:  train Loss: 24.6842   val Loss: 28.9585   time: 88.82s   best: 28.2789
2023-11-24 07:14:37,514:INFO:  Epoch 321/500:  train Loss: 24.7491   val Loss: 29.1714   time: 88.70s   best: 28.2789
2023-11-24 07:15:25,157:INFO:  Epoch 383/500:  train Loss: 20.5808   val Loss: 27.9429   time: 431.81s   best: 27.3407
2023-11-24 07:16:06,980:INFO:  Epoch 322/500:  train Loss: 24.7965   val Loss: 28.9626   time: 89.47s   best: 28.2789
2023-11-24 07:17:36,161:INFO:  Epoch 323/500:  train Loss: 25.2307   val Loss: 28.8662   time: 89.17s   best: 28.2789
2023-11-24 07:19:04,689:INFO:  Epoch 324/500:  train Loss: 24.8968   val Loss: 28.5255   time: 88.52s   best: 28.2789
2023-11-24 07:20:33,479:INFO:  Epoch 325/500:  train Loss: 24.6852   val Loss: 29.0624   time: 88.79s   best: 28.2789
2023-11-24 07:22:02,253:INFO:  Epoch 326/500:  train Loss: 24.5476   val Loss: 31.6560   time: 88.77s   best: 28.2789
2023-11-24 07:22:37,896:INFO:  Epoch 384/500:  train Loss: 20.5580   val Loss: 27.8450   time: 432.72s   best: 27.3407
2023-11-24 07:23:31,082:INFO:  Epoch 327/500:  train Loss: 24.7107   val Loss: 29.6013   time: 88.83s   best: 28.2789
2023-11-24 07:24:59,693:INFO:  Epoch 328/500:  train Loss: 24.7503   val Loss: 28.6795   time: 88.61s   best: 28.2789
2023-11-24 07:26:28,829:INFO:  Epoch 329/500:  train Loss: 24.4968   val Loss: 28.5118   time: 89.12s   best: 28.2789
2023-11-24 07:27:57,798:INFO:  Epoch 330/500:  train Loss: 24.6030   val Loss: 30.8691   time: 88.97s   best: 28.2789
2023-11-24 07:29:26,757:INFO:  Epoch 331/500:  train Loss: 24.4646   val Loss: 30.0410   time: 88.95s   best: 28.2789
2023-11-24 07:29:46,443:INFO:  Epoch 385/500:  train Loss: 20.4228   val Loss: 28.1427   time: 428.53s   best: 27.3407
2023-11-24 07:30:55,657:INFO:  Epoch 332/500:  train Loss: 24.6875   val Loss: 29.8118   time: 88.89s   best: 28.2789
2023-11-24 07:32:24,523:INFO:  Epoch 333/500:  train Loss: 24.4163   val Loss: 29.2411   time: 88.86s   best: 28.2789
2023-11-24 07:33:53,572:INFO:  Epoch 334/500:  train Loss: 24.4478   val Loss: 29.1700   time: 89.05s   best: 28.2789
2023-11-24 07:35:22,677:INFO:  Epoch 335/500:  train Loss: 24.3578   val Loss: 28.5569   time: 89.10s   best: 28.2789
2023-11-24 07:36:51,770:INFO:  Epoch 336/500:  train Loss: 24.5455   val Loss: 29.1001   time: 89.08s   best: 28.2789
2023-11-24 07:36:56,099:INFO:  Epoch 386/500:  train Loss: 20.5708   val Loss: 28.4010   time: 429.64s   best: 27.3407
2023-11-24 07:38:20,683:INFO:  Epoch 337/500:  train Loss: 24.3541   val Loss: 28.3651   time: 88.91s   best: 28.2789
2023-11-24 07:39:49,423:INFO:  Epoch 338/500:  train Loss: 24.4345   val Loss: 30.2200   time: 88.74s   best: 28.2789
2023-11-24 07:41:18,119:INFO:  Epoch 339/500:  train Loss: 24.5622   val Loss: 28.4966   time: 88.68s   best: 28.2789
2023-11-24 07:42:46,959:INFO:  Epoch 340/500:  train Loss: 24.1825   val Loss: 29.2596   time: 88.83s   best: 28.2789
2023-11-24 07:44:08,151:INFO:  Epoch 387/500:  train Loss: 20.5101   val Loss: 28.7693   time: 432.05s   best: 27.3407
2023-11-24 07:44:16,483:INFO:  Epoch 341/500:  train Loss: 24.3787   val Loss: 29.0286   time: 89.51s   best: 28.2789
2023-11-24 07:45:45,517:INFO:  Epoch 342/500:  train Loss: 24.1630   val Loss: 30.0580   time: 89.02s   best: 28.2789
2023-11-24 07:47:14,095:INFO:  Epoch 343/500:  train Loss: 24.3034   val Loss: 28.8000   time: 88.57s   best: 28.2789
2023-11-24 07:48:42,586:INFO:  Epoch 344/500:  train Loss: 24.4702   val Loss: 30.3556   time: 88.49s   best: 28.2789
2023-11-24 07:50:11,212:INFO:  Epoch 345/500:  train Loss: 24.8854   val Loss: 29.9008   time: 88.63s   best: 28.2789
2023-11-24 07:51:17,577:INFO:  Epoch 388/500:  train Loss: 20.5338   val Loss: 27.9320   time: 429.41s   best: 27.3407
2023-11-24 07:51:39,773:INFO:  Epoch 346/500:  train Loss: 24.2261   val Loss: 28.6609   time: 88.55s   best: 28.2789
2023-11-24 07:53:08,341:INFO:  Epoch 347/500:  train Loss: 24.1264   val Loss: 28.8477   time: 88.55s   best: 28.2789
2023-11-24 07:54:36,768:INFO:  Epoch 348/500:  train Loss: 24.1513   val Loss: 28.6609   time: 88.41s   best: 28.2789
2023-11-24 07:56:05,340:INFO:  Epoch 349/500:  train Loss: 24.7388   val Loss: 29.9372   time: 88.57s   best: 28.2789
2023-11-24 07:57:33,922:INFO:  Epoch 350/500:  train Loss: 24.4877   val Loss: 29.7209   time: 88.58s   best: 28.2789
2023-11-24 07:58:29,934:INFO:  Epoch 389/500:  train Loss: 20.4982   val Loss: 27.9722   time: 432.35s   best: 27.3407
2023-11-24 07:59:02,793:INFO:  Epoch 351/500:  train Loss: 24.1029   val Loss: 28.9574   time: 88.86s   best: 28.2789
2023-11-24 08:00:31,699:INFO:  Epoch 352/500:  train Loss: 24.0361   val Loss: 29.9814   time: 88.89s   best: 28.2789
2023-11-24 08:02:00,178:INFO:  Epoch 353/500:  train Loss: 25.1622   val Loss: 29.2821   time: 88.47s   best: 28.2789
2023-11-24 08:03:29,153:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 08:03:29,171:INFO:  Epoch 354/500:  train Loss: 24.2594   val Loss: 28.0633   time: 88.97s   best: 28.0633
2023-11-24 08:04:58,183:INFO:  Epoch 355/500:  train Loss: 24.0145   val Loss: 28.2436   time: 89.00s   best: 28.0633
2023-11-24 08:05:43,626:INFO:  Epoch 390/500:  train Loss: 20.4027   val Loss: 28.3462   time: 433.69s   best: 27.3407
2023-11-24 08:06:27,148:INFO:  Epoch 356/500:  train Loss: 24.0023   val Loss: 28.8294   time: 88.95s   best: 28.0633
2023-11-24 08:07:55,643:INFO:  Epoch 357/500:  train Loss: 23.9392   val Loss: 29.1228   time: 88.49s   best: 28.0633
2023-11-24 08:09:23,954:INFO:  Epoch 358/500:  train Loss: 24.1302   val Loss: 29.5343   time: 88.30s   best: 28.0633
2023-11-24 08:10:52,874:INFO:  Epoch 359/500:  train Loss: 24.5666   val Loss: 28.7353   time: 88.92s   best: 28.0633
2023-11-24 08:12:21,815:INFO:  Epoch 360/500:  train Loss: 23.8976   val Loss: 29.5864   time: 88.92s   best: 28.0633
2023-11-24 08:12:55,664:INFO:  Epoch 391/500:  train Loss: 20.5353   val Loss: 28.1193   time: 432.03s   best: 27.3407
2023-11-24 08:13:50,379:INFO:  Epoch 361/500:  train Loss: 24.2992   val Loss: 28.7085   time: 88.55s   best: 28.0633
2023-11-24 08:15:19,161:INFO:  Epoch 362/500:  train Loss: 23.9093   val Loss: 28.9431   time: 88.77s   best: 28.0633
2023-11-24 08:16:48,316:INFO:  Epoch 363/500:  train Loss: 24.0400   val Loss: 29.3847   time: 89.15s   best: 28.0633
2023-11-24 08:18:17,130:INFO:  Epoch 364/500:  train Loss: 23.9594   val Loss: 28.5759   time: 88.80s   best: 28.0633
2023-11-24 08:19:45,905:INFO:  Epoch 365/500:  train Loss: 23.8421   val Loss: 28.3995   time: 88.77s   best: 28.0633
2023-11-24 08:20:03,494:INFO:  Epoch 392/500:  train Loss: 20.8245   val Loss: 27.7535   time: 427.83s   best: 27.3407
2023-11-24 08:21:14,938:INFO:  Epoch 366/500:  train Loss: 23.7764   val Loss: 29.3201   time: 89.02s   best: 28.0633
2023-11-24 08:22:43,750:INFO:  Epoch 367/500:  train Loss: 23.7965   val Loss: 29.0079   time: 88.80s   best: 28.0633
2023-11-24 08:24:12,540:INFO:  Epoch 368/500:  train Loss: 24.0140   val Loss: 32.5343   time: 88.79s   best: 28.0633
2023-11-24 08:25:41,292:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 08:25:41,312:INFO:  Epoch 369/500:  train Loss: 23.8646   val Loss: 28.0456   time: 88.75s   best: 28.0456
2023-11-24 08:27:10,201:INFO:  Epoch 370/500:  train Loss: 24.5344   val Loss: 29.5482   time: 88.88s   best: 28.0456
2023-11-24 08:27:12,295:INFO:  Epoch 393/500:  train Loss: 20.7419   val Loss: 27.8198   time: 428.80s   best: 27.3407
2023-11-24 08:28:38,901:INFO:  Epoch 371/500:  train Loss: 24.1853   val Loss: 28.2214   time: 88.70s   best: 28.0456
2023-11-24 08:30:07,465:INFO:  Epoch 372/500:  train Loss: 23.8206   val Loss: 28.7945   time: 88.55s   best: 28.0456
2023-11-24 08:31:35,874:INFO:  Epoch 373/500:  train Loss: 24.7266   val Loss: 29.3661   time: 88.41s   best: 28.0456
2023-11-24 08:33:04,792:INFO:  Epoch 374/500:  train Loss: 23.9189   val Loss: 28.3115   time: 88.92s   best: 28.0456
2023-11-24 08:34:20,413:INFO:  Epoch 394/500:  train Loss: 20.5461   val Loss: 27.7969   time: 428.10s   best: 27.3407
2023-11-24 08:34:33,417:INFO:  Epoch 375/500:  train Loss: 23.8148   val Loss: 29.8670   time: 88.61s   best: 28.0456
2023-11-24 08:36:02,564:INFO:  Epoch 376/500:  train Loss: 23.9041   val Loss: 28.3032   time: 89.14s   best: 28.0456
2023-11-24 08:37:31,383:INFO:  Epoch 377/500:  train Loss: 24.1407   val Loss: 31.0667   time: 88.82s   best: 28.0456
2023-11-24 08:39:00,178:INFO:  Epoch 378/500:  train Loss: 23.9160   val Loss: 28.6109   time: 88.79s   best: 28.0456
2023-11-24 08:40:28,790:INFO:  Epoch 379/500:  train Loss: 23.5376   val Loss: 28.6677   time: 88.61s   best: 28.0456
2023-11-24 08:41:28,650:INFO:  Epoch 395/500:  train Loss: 20.3698   val Loss: 28.1137   time: 428.21s   best: 27.3407
2023-11-24 08:41:57,278:INFO:  Epoch 380/500:  train Loss: 23.6186   val Loss: 28.2387   time: 88.48s   best: 28.0456
2023-11-24 08:43:25,845:INFO:  Epoch 381/500:  train Loss: 23.6839   val Loss: 28.4750   time: 88.56s   best: 28.0456
2023-11-24 08:44:54,629:INFO:  Epoch 382/500:  train Loss: 23.6534   val Loss: 29.1508   time: 88.77s   best: 28.0456
2023-11-24 08:46:23,286:INFO:  Epoch 383/500:  train Loss: 23.5222   val Loss: 28.1646   time: 88.66s   best: 28.0456
2023-11-24 08:47:51,786:INFO:  Epoch 384/500:  train Loss: 23.5758   val Loss: 29.4501   time: 88.49s   best: 28.0456
2023-11-24 08:48:37,510:INFO:  Epoch 396/500:  train Loss: 20.5272   val Loss: 28.4115   time: 428.85s   best: 27.3407
2023-11-24 08:49:20,423:INFO:  Epoch 385/500:  train Loss: 23.7590   val Loss: 28.7467   time: 88.64s   best: 28.0456
2023-11-24 08:50:48,906:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 08:50:48,925:INFO:  Epoch 386/500:  train Loss: 23.3937   val Loss: 27.7680   time: 88.46s   best: 27.7680
2023-11-24 08:52:17,605:INFO:  Epoch 387/500:  train Loss: 23.8088   val Loss: 28.6337   time: 88.68s   best: 27.7680
2023-11-24 08:53:45,970:INFO:  Epoch 388/500:  train Loss: 23.5303   val Loss: 28.7461   time: 88.36s   best: 27.7680
2023-11-24 08:55:14,354:INFO:  Epoch 389/500:  train Loss: 23.3688   val Loss: 28.4863   time: 88.37s   best: 27.7680
2023-11-24 08:55:50,209:INFO:  Epoch 397/500:  train Loss: 20.3979   val Loss: 27.9780   time: 432.68s   best: 27.3407
2023-11-24 08:56:42,722:INFO:  Epoch 390/500:  train Loss: 23.5766   val Loss: 28.0061   time: 88.37s   best: 27.7680
2023-11-24 08:58:11,378:INFO:  Epoch 391/500:  train Loss: 23.3886   val Loss: 28.3933   time: 88.64s   best: 27.7680
2023-11-24 08:59:39,943:INFO:  Epoch 392/500:  train Loss: 23.7714   val Loss: 28.7269   time: 88.55s   best: 27.7680
2023-11-24 09:01:08,594:INFO:  Epoch 393/500:  train Loss: 23.6628   val Loss: 29.2238   time: 88.65s   best: 27.7680
2023-11-24 09:02:37,093:INFO:  Epoch 394/500:  train Loss: 23.4323   val Loss: 28.3413   time: 88.48s   best: 27.7680
2023-11-24 09:03:02,358:INFO:  Epoch 398/500:  train Loss: 20.4511   val Loss: 28.1892   time: 432.13s   best: 27.3407
2023-11-24 09:04:05,746:INFO:  Epoch 395/500:  train Loss: 23.6585   val Loss: 28.8507   time: 88.64s   best: 27.7680
2023-11-24 09:05:34,127:INFO:  Epoch 396/500:  train Loss: 23.5673   val Loss: 29.1214   time: 88.38s   best: 27.7680
2023-11-24 09:07:03,483:INFO:  Epoch 397/500:  train Loss: 23.5869   val Loss: 28.7721   time: 89.35s   best: 27.7680
2023-11-24 09:08:31,835:INFO:  Epoch 398/500:  train Loss: 23.2696   val Loss: 27.9087   time: 88.35s   best: 27.7680
2023-11-24 09:10:00,740:INFO:  Epoch 399/500:  train Loss: 23.2401   val Loss: 27.9609   time: 88.90s   best: 27.7680
2023-11-24 09:10:15,200:INFO:  Epoch 399/500:  train Loss: 20.4741   val Loss: 27.7061   time: 432.82s   best: 27.3407
2023-11-24 09:11:29,125:INFO:  Epoch 400/500:  train Loss: 23.2808   val Loss: 28.0474   time: 88.38s   best: 27.7680
2023-11-24 09:12:57,985:INFO:  Epoch 401/500:  train Loss: 23.2745   val Loss: 28.2275   time: 88.86s   best: 27.7680
2023-11-24 09:14:27,147:INFO:  Epoch 402/500:  train Loss: 23.2346   val Loss: 27.8884   time: 89.15s   best: 27.7680
2023-11-24 09:15:55,823:INFO:  Epoch 403/500:  train Loss: 23.2485   val Loss: 28.3394   time: 88.66s   best: 27.7680
2023-11-24 09:17:22,836:INFO:  Epoch 400/500:  train Loss: 20.4294   val Loss: 29.0920   time: 427.63s   best: 27.3407
2023-11-24 09:17:24,476:INFO:  Epoch 404/500:  train Loss: 23.4659   val Loss: 28.3739   time: 88.64s   best: 27.7680
2023-11-24 09:18:53,222:INFO:  Epoch 405/500:  train Loss: 23.4426   val Loss: 28.5615   time: 88.73s   best: 27.7680
2023-11-24 09:20:22,079:INFO:  Epoch 406/500:  train Loss: 23.2163   val Loss: 28.1147   time: 88.84s   best: 27.7680
2023-11-24 09:21:50,584:INFO:  Epoch 407/500:  train Loss: 23.2234   val Loss: 27.8846   time: 88.50s   best: 27.7680
2023-11-24 09:23:19,236:INFO:  Epoch 408/500:  train Loss: 23.2335   val Loss: 27.8648   time: 88.65s   best: 27.7680
2023-11-24 09:24:34,968:INFO:  Epoch 401/500:  train Loss: 20.4660   val Loss: 28.4281   time: 432.13s   best: 27.3407
2023-11-24 09:24:47,922:INFO:  Epoch 409/500:  train Loss: 23.2645   val Loss: 28.6671   time: 88.67s   best: 27.7680
2023-11-24 09:26:16,744:INFO:  Epoch 410/500:  train Loss: 23.3185   val Loss: 30.2940   time: 88.82s   best: 27.7680
2023-11-24 09:27:45,751:INFO:  Epoch 411/500:  train Loss: 23.1768   val Loss: 27.7790   time: 89.01s   best: 27.7680
2023-11-24 09:29:14,449:INFO:  Epoch 412/500:  train Loss: 23.1538   val Loss: 28.7038   time: 88.69s   best: 27.7680
2023-11-24 09:30:43,259:INFO:  Epoch 413/500:  train Loss: 23.1535   val Loss: 28.7237   time: 88.81s   best: 27.7680
2023-11-24 09:31:47,251:INFO:  Epoch 402/500:  train Loss: 20.4163   val Loss: 28.0116   time: 432.27s   best: 27.3407
2023-11-24 09:32:11,817:INFO:  Epoch 414/500:  train Loss: 23.0927   val Loss: 27.9031   time: 88.56s   best: 27.7680
2023-11-24 09:33:40,306:INFO:  Epoch 415/500:  train Loss: 23.0688   val Loss: 27.9076   time: 88.48s   best: 27.7680
2023-11-24 09:35:08,587:INFO:  Epoch 416/500:  train Loss: 23.3686   val Loss: 29.1953   time: 88.28s   best: 27.7680
2023-11-24 09:36:37,229:INFO:  Epoch 417/500:  train Loss: 23.3208   val Loss: 28.9625   time: 88.64s   best: 27.7680
2023-11-24 09:38:05,986:INFO:  Epoch 418/500:  train Loss: 23.5960   val Loss: 28.4080   time: 88.76s   best: 27.7680
2023-11-24 09:38:58,045:INFO:  Epoch 403/500:  train Loss: 20.4166   val Loss: 29.0345   time: 430.79s   best: 27.3407
2023-11-24 09:39:34,374:INFO:  Epoch 419/500:  train Loss: 23.1534   val Loss: 29.6310   time: 88.39s   best: 27.7680
2023-11-24 09:41:02,765:INFO:  Epoch 420/500:  train Loss: 23.0004   val Loss: 28.5195   time: 88.39s   best: 27.7680
2023-11-24 09:42:31,558:INFO:  Epoch 421/500:  train Loss: 23.0008   val Loss: 29.6750   time: 88.79s   best: 27.7680
2023-11-24 09:44:00,026:INFO:  Epoch 422/500:  train Loss: 23.2378   val Loss: 27.9897   time: 88.47s   best: 27.7680
2023-11-24 09:45:28,530:INFO:  Epoch 423/500:  train Loss: 23.0861   val Loss: 28.8026   time: 88.50s   best: 27.7680
2023-11-24 09:46:10,754:INFO:  Epoch 404/500:  train Loss: 20.8847   val Loss: 27.9834   time: 432.69s   best: 27.3407
2023-11-24 09:46:56,923:INFO:  Epoch 424/500:  train Loss: 23.3366   val Loss: 29.4440   time: 88.38s   best: 27.7680
2023-11-24 09:48:25,783:INFO:  Epoch 425/500:  train Loss: 22.9201   val Loss: 28.6821   time: 88.86s   best: 27.7680
2023-11-24 09:49:54,842:INFO:  Epoch 426/500:  train Loss: 22.9036   val Loss: 28.7732   time: 89.05s   best: 27.7680
2023-11-24 09:51:23,531:INFO:  Epoch 427/500:  train Loss: 23.2595   val Loss: 29.0185   time: 88.69s   best: 27.7680
2023-11-24 09:52:51,960:INFO:  Epoch 428/500:  train Loss: 22.9848   val Loss: 28.2443   time: 88.43s   best: 27.7680
2023-11-24 09:53:22,793:INFO:  Epoch 405/500:  train Loss: 20.3539   val Loss: 27.9014   time: 432.03s   best: 27.3407
2023-11-24 09:54:20,844:INFO:  Epoch 429/500:  train Loss: 23.0605   val Loss: 29.7686   time: 88.88s   best: 27.7680
2023-11-24 09:55:49,222:INFO:  Epoch 430/500:  train Loss: 22.9446   val Loss: 28.6217   time: 88.38s   best: 27.7680
2023-11-24 09:57:18,205:INFO:  Epoch 431/500:  train Loss: 23.5019   val Loss: 28.1620   time: 88.97s   best: 27.7680
2023-11-24 09:58:46,601:INFO:  Epoch 432/500:  train Loss: 23.1876   val Loss: 28.2221   time: 88.40s   best: 27.7680
2023-11-24 10:00:14,983:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 10:00:15,002:INFO:  Epoch 433/500:  train Loss: 23.3082   val Loss: 27.7625   time: 88.38s   best: 27.7625
2023-11-24 10:00:34,633:INFO:  Epoch 406/500:  train Loss: 20.4234   val Loss: 33.9740   time: 431.82s   best: 27.3407
2023-11-24 10:01:43,908:INFO:  Epoch 434/500:  train Loss: 22.7904   val Loss: 27.8747   time: 88.89s   best: 27.7625
2023-11-24 10:03:12,612:INFO:  Epoch 435/500:  train Loss: 22.7772   val Loss: 28.2295   time: 88.70s   best: 27.7625
2023-11-24 10:04:41,158:INFO:  Epoch 436/500:  train Loss: 22.8872   val Loss: 27.8215   time: 88.55s   best: 27.7625
2023-11-24 10:06:09,887:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 10:06:09,906:INFO:  Epoch 437/500:  train Loss: 23.3833   val Loss: 27.4155   time: 88.71s   best: 27.4155
2023-11-24 10:07:38,271:INFO:  Epoch 438/500:  train Loss: 22.8001   val Loss: 28.3185   time: 88.35s   best: 27.4155
2023-11-24 10:07:43,287:INFO:  Epoch 407/500:  train Loss: 20.5831   val Loss: 27.4533   time: 428.65s   best: 27.3407
2023-11-24 10:09:07,512:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 10:09:07,534:INFO:  Epoch 439/500:  train Loss: 22.8110   val Loss: 27.3281   time: 89.24s   best: 27.3281
2023-11-24 10:10:36,461:INFO:  Epoch 440/500:  train Loss: 22.9617   val Loss: 28.6276   time: 88.93s   best: 27.3281
2023-11-24 10:12:05,142:INFO:  Epoch 441/500:  train Loss: 23.0738   val Loss: 30.5975   time: 88.67s   best: 27.3281
2023-11-24 10:13:33,599:INFO:  Epoch 442/500:  train Loss: 22.9384   val Loss: 27.9052   time: 88.44s   best: 27.3281
2023-11-24 10:14:55,469:INFO:  Epoch 408/500:  train Loss: 20.4392   val Loss: 29.1054   time: 432.18s   best: 27.3407
2023-11-24 10:15:02,160:INFO:  Epoch 443/500:  train Loss: 22.7618   val Loss: 28.2470   time: 88.56s   best: 27.3281
2023-11-24 10:16:30,686:INFO:  Epoch 444/500:  train Loss: 22.8582   val Loss: 29.3481   time: 88.52s   best: 27.3281
2023-11-24 10:17:59,193:INFO:  Epoch 445/500:  train Loss: 22.6601   val Loss: 27.6708   time: 88.51s   best: 27.3281
2023-11-24 10:19:27,590:INFO:  Epoch 446/500:  train Loss: 23.5795   val Loss: 27.5701   time: 88.38s   best: 27.3281
2023-11-24 10:20:56,183:INFO:  Epoch 447/500:  train Loss: 24.2292   val Loss: 27.5391   time: 88.58s   best: 27.3281
2023-11-24 10:22:06,004:INFO:  Epoch 409/500:  train Loss: 20.3889   val Loss: 28.8590   time: 430.53s   best: 27.3407
2023-11-24 10:22:25,134:INFO:  Epoch 448/500:  train Loss: 22.9459   val Loss: 28.3927   time: 88.95s   best: 27.3281
2023-11-24 10:23:53,756:INFO:  Epoch 449/500:  train Loss: 22.6663   val Loss: 27.5566   time: 88.62s   best: 27.3281
2023-11-24 10:25:22,371:INFO:  Epoch 450/500:  train Loss: 22.6480   val Loss: 28.1519   time: 88.60s   best: 27.3281
2023-11-24 10:26:51,162:INFO:  Epoch 451/500:  train Loss: 22.7077   val Loss: 27.4051   time: 88.77s   best: 27.3281
2023-11-24 10:28:19,820:INFO:  Epoch 452/500:  train Loss: 22.8073   val Loss: 28.3624   time: 88.66s   best: 27.3281
2023-11-24 10:29:18,381:INFO:  Epoch 410/500:  train Loss: 20.4185   val Loss: 28.2458   time: 432.36s   best: 27.3407
2023-11-24 10:29:48,343:INFO:  Epoch 453/500:  train Loss: 22.9235   val Loss: 28.0757   time: 88.52s   best: 27.3281
2023-11-24 10:31:16,600:INFO:  Epoch 454/500:  train Loss: 22.6721   val Loss: 27.9480   time: 88.24s   best: 27.3281
2023-11-24 10:32:45,033:INFO:  Epoch 455/500:  train Loss: 22.6522   val Loss: 27.9251   time: 88.42s   best: 27.3281
2023-11-24 10:34:13,547:INFO:  Epoch 456/500:  train Loss: 22.6433   val Loss: 28.1073   time: 88.51s   best: 27.3281
2023-11-24 10:35:42,183:INFO:  Epoch 457/500:  train Loss: 22.9065   val Loss: 28.3574   time: 88.64s   best: 27.3281
2023-11-24 10:36:30,124:INFO:  Epoch 411/500:  train Loss: 20.3213   val Loss: 27.6881   time: 431.73s   best: 27.3407
2023-11-24 10:37:10,678:INFO:  Epoch 458/500:  train Loss: 23.4069   val Loss: 29.8903   time: 88.48s   best: 27.3281
2023-11-24 10:38:38,833:INFO:  Epoch 459/500:  train Loss: 22.8672   val Loss: 27.4777   time: 88.14s   best: 27.3281
2023-11-24 10:40:07,666:INFO:  Epoch 460/500:  train Loss: 22.5908   val Loss: 28.2600   time: 88.83s   best: 27.3281
2023-11-24 10:41:36,287:INFO:  Epoch 461/500:  train Loss: 22.5763   val Loss: 27.7071   time: 88.62s   best: 27.3281
2023-11-24 10:43:05,094:INFO:  Epoch 462/500:  train Loss: 22.6363   val Loss: 27.7289   time: 88.81s   best: 27.3281
2023-11-24 10:43:42,495:INFO:  Epoch 412/500:  train Loss: 20.6535   val Loss: 27.8622   time: 432.36s   best: 27.3407
2023-11-24 10:44:33,328:INFO:  Epoch 463/500:  train Loss: 22.5898   val Loss: 30.3842   time: 88.23s   best: 27.3281
2023-11-24 10:46:01,850:INFO:  Epoch 464/500:  train Loss: 22.5120   val Loss: 28.2806   time: 88.52s   best: 27.3281
2023-11-24 10:47:29,917:INFO:  Epoch 465/500:  train Loss: 22.9221   val Loss: 27.8433   time: 88.06s   best: 27.3281
2023-11-24 10:48:57,863:INFO:  Epoch 466/500:  train Loss: 22.9538   val Loss: 27.3896   time: 87.94s   best: 27.3281
2023-11-24 10:50:26,085:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_cc30.pt
2023-11-24 10:50:26,106:INFO:  Epoch 467/500:  train Loss: 22.5559   val Loss: 27.1855   time: 88.22s   best: 27.1855
2023-11-24 10:50:51,112:INFO:  Epoch 413/500:  train Loss: 20.4562   val Loss: 28.0901   time: 428.61s   best: 27.3407
2023-11-24 10:51:54,133:INFO:  Epoch 468/500:  train Loss: 22.4396   val Loss: 28.3019   time: 88.02s   best: 27.1855
2023-11-24 10:53:22,312:INFO:  Epoch 469/500:  train Loss: 22.4486   val Loss: 27.4468   time: 88.17s   best: 27.1855
2023-11-24 10:54:50,347:INFO:  Epoch 470/500:  train Loss: 22.4577   val Loss: 28.7585   time: 88.02s   best: 27.1855
2023-11-24 10:56:18,380:INFO:  Epoch 471/500:  train Loss: 22.5166   val Loss: 27.9885   time: 88.02s   best: 27.1855
2023-11-24 10:57:46,425:INFO:  Epoch 472/500:  train Loss: 22.3950   val Loss: 31.7586   time: 88.04s   best: 27.1855
2023-11-24 10:58:00,611:INFO:  Epoch 414/500:  train Loss: 20.3459   val Loss: 27.7334   time: 429.48s   best: 27.3407
2023-11-24 10:59:14,613:INFO:  Epoch 473/500:  train Loss: 22.6450   val Loss: 28.0228   time: 88.19s   best: 27.1855
2023-11-24 11:00:42,619:INFO:  Epoch 474/500:  train Loss: 22.5010   val Loss: 29.2759   time: 88.00s   best: 27.1855
2023-11-24 11:02:10,539:INFO:  Epoch 475/500:  train Loss: 22.6991   val Loss: 28.0752   time: 87.92s   best: 27.1855
2023-11-24 11:03:38,878:INFO:  Epoch 476/500:  train Loss: 22.8834   val Loss: 30.0622   time: 88.34s   best: 27.1855
2023-11-24 11:05:07,207:INFO:  Epoch 477/500:  train Loss: 22.7452   val Loss: 27.6615   time: 88.32s   best: 27.1855
2023-11-24 11:05:13,691:INFO:  Epoch 415/500:  train Loss: 20.3343   val Loss: 27.8777   time: 433.08s   best: 27.3407
2023-11-24 11:06:35,114:INFO:  Epoch 478/500:  train Loss: 22.3960   val Loss: 27.6454   time: 87.91s   best: 27.1855
2023-11-24 11:08:03,023:INFO:  Epoch 479/500:  train Loss: 22.3361   val Loss: 27.5083   time: 87.91s   best: 27.1855
2023-11-24 11:09:30,838:INFO:  Epoch 480/500:  train Loss: 22.3854   val Loss: 27.9552   time: 87.81s   best: 27.1855
2023-11-24 11:10:59,121:INFO:  Epoch 481/500:  train Loss: 22.3398   val Loss: 27.6546   time: 88.28s   best: 27.1855
2023-11-24 11:12:26,689:INFO:  Epoch 416/500:  train Loss: 20.3519   val Loss: 28.0808   time: 432.99s   best: 27.3407
2023-11-24 11:12:27,749:INFO:  Epoch 482/500:  train Loss: 22.3069   val Loss: 28.9576   time: 88.62s   best: 27.1855
2023-11-24 11:13:55,985:INFO:  Epoch 483/500:  train Loss: 24.3154   val Loss: 28.3628   time: 88.23s   best: 27.1855
2023-11-24 11:15:24,373:INFO:  Epoch 484/500:  train Loss: 23.8597   val Loss: 27.8806   time: 88.39s   best: 27.1855
2023-11-24 11:16:52,736:INFO:  Epoch 485/500:  train Loss: 22.8089   val Loss: 28.0568   time: 88.36s   best: 27.1855
2023-11-24 11:18:20,642:INFO:  Epoch 486/500:  train Loss: 22.4986   val Loss: 28.3942   time: 87.89s   best: 27.1855
2023-11-24 11:19:39,260:INFO:  Epoch 417/500:  train Loss: 20.4206   val Loss: 28.1113   time: 432.56s   best: 27.3407
2023-11-24 11:19:48,700:INFO:  Epoch 487/500:  train Loss: 22.4152   val Loss: 29.8558   time: 88.06s   best: 27.1855
2023-11-24 11:21:16,651:INFO:  Epoch 488/500:  train Loss: 22.8076   val Loss: 28.2720   time: 87.95s   best: 27.1855
2023-11-24 11:22:44,794:INFO:  Epoch 489/500:  train Loss: 22.2553   val Loss: 27.6985   time: 88.14s   best: 27.1855
2023-11-24 11:24:13,179:INFO:  Epoch 490/500:  train Loss: 22.3435   val Loss: 28.6160   time: 88.38s   best: 27.1855
2023-11-24 11:25:41,285:INFO:  Epoch 491/500:  train Loss: 22.2659   val Loss: 28.7637   time: 88.11s   best: 27.1855
2023-11-24 11:26:48,736:INFO:  Epoch 418/500:  train Loss: 20.3821   val Loss: 28.0918   time: 429.45s   best: 27.3407
2023-11-24 11:27:09,771:INFO:  Epoch 492/500:  train Loss: 22.3217   val Loss: 28.5800   time: 88.48s   best: 27.1855
2023-11-24 11:28:38,360:INFO:  Epoch 493/500:  train Loss: 22.1902   val Loss: 28.7974   time: 88.59s   best: 27.1855
2023-11-24 11:30:06,453:INFO:  Epoch 494/500:  train Loss: 22.2532   val Loss: 28.3127   time: 88.08s   best: 27.1855
2023-11-24 11:31:34,318:INFO:  Epoch 495/500:  train Loss: 22.3894   val Loss: 27.7049   time: 87.86s   best: 27.1855
2023-11-24 11:33:02,326:INFO:  Epoch 496/500:  train Loss: 22.2757   val Loss: 27.6153   time: 88.00s   best: 27.1855
2023-11-24 11:34:00,176:INFO:  Epoch 419/500:  train Loss: 20.4044   val Loss: 28.2334   time: 431.44s   best: 27.3407
2023-11-24 11:34:30,092:INFO:  Epoch 497/500:  train Loss: 22.1570   val Loss: 28.1526   time: 87.77s   best: 27.1855
2023-11-24 11:35:57,966:INFO:  Epoch 498/500:  train Loss: 22.6780   val Loss: 27.4699   time: 87.86s   best: 27.1855
2023-11-24 11:37:26,032:INFO:  Epoch 499/500:  train Loss: 22.1163   val Loss: 27.3298   time: 88.05s   best: 27.1855
2023-11-24 11:38:53,953:INFO:  Epoch 500/500:  train Loss: 22.1307   val Loss: 27.9803   time: 87.92s   best: 27.1855
2023-11-24 11:38:53,954:INFO:  -----> Training complete in 739m 35s   best validation loss: 27.1855
 
2023-11-24 11:41:14,040:INFO:  Epoch 420/500:  train Loss: 20.5119   val Loss: 27.5794   time: 433.85s   best: 27.3407
2023-11-24 11:48:23,237:INFO:  Epoch 421/500:  train Loss: 20.3605   val Loss: 27.8383   time: 429.18s   best: 27.3407
2023-11-24 11:55:36,502:INFO:  Epoch 422/500:  train Loss: 20.3727   val Loss: 27.9474   time: 433.26s   best: 27.3407
2023-11-24 12:02:45,062:INFO:  Epoch 423/500:  train Loss: 20.3830   val Loss: 30.3962   time: 428.55s   best: 27.3407
2023-11-24 12:09:55,709:INFO:  Epoch 424/500:  train Loss: 20.6176   val Loss: 27.8528   time: 430.65s   best: 27.3407
2023-11-24 12:17:03,501:INFO:  Epoch 425/500:  train Loss: 20.3900   val Loss: 27.9176   time: 427.79s   best: 27.3407
2023-11-24 12:24:12,124:INFO:  Epoch 426/500:  train Loss: 20.4167   val Loss: 27.8430   time: 428.61s   best: 27.3407
2023-11-24 12:31:23,480:INFO:  Epoch 427/500:  train Loss: 20.3751   val Loss: 27.8667   time: 431.34s   best: 27.3407
2023-11-24 12:38:36,404:INFO:  Epoch 428/500:  train Loss: 20.2798   val Loss: 28.0555   time: 432.91s   best: 27.3407
2023-11-24 12:45:49,418:INFO:  Epoch 429/500:  train Loss: 20.5428   val Loss: 28.1704   time: 433.01s   best: 27.3407
2023-11-24 12:52:57,135:INFO:  Epoch 430/500:  train Loss: 20.4694   val Loss: 28.0983   time: 427.72s   best: 27.3407
2023-11-24 13:00:10,049:INFO:  Epoch 431/500:  train Loss: 20.3908   val Loss: 28.7191   time: 432.90s   best: 27.3407
2023-11-24 13:07:23,106:INFO:  Epoch 432/500:  train Loss: 20.4728   val Loss: 28.3264   time: 433.05s   best: 27.3407
2023-11-24 13:14:35,072:INFO:  Epoch 433/500:  train Loss: 20.3630   val Loss: 27.7147   time: 431.96s   best: 27.3407
2023-11-24 13:21:45,179:INFO:  Epoch 434/500:  train Loss: 20.4391   val Loss: 28.3457   time: 430.09s   best: 27.3407
2023-11-24 13:28:56,210:INFO:  Epoch 435/500:  train Loss: 20.3788   val Loss: 27.7938   time: 431.03s   best: 27.3407
2023-11-24 13:36:07,788:INFO:  Epoch 436/500:  train Loss: 20.3989   val Loss: 27.8281   time: 431.56s   best: 27.3407
2023-11-24 13:43:20,507:INFO:  Epoch 437/500:  train Loss: 20.8582   val Loss: 31.4101   time: 432.71s   best: 27.3407
2023-11-24 13:50:28,549:INFO:  Epoch 438/500:  train Loss: 20.5060   val Loss: 28.1007   time: 428.03s   best: 27.3407
2023-11-24 13:57:40,704:INFO:  Epoch 439/500:  train Loss: 20.3850   val Loss: 28.0841   time: 432.14s   best: 27.3407
2023-11-24 14:04:53,006:INFO:  Epoch 440/500:  train Loss: 20.4411   val Loss: 28.1076   time: 432.30s   best: 27.3407
2023-11-24 14:12:04,232:INFO:  Epoch 441/500:  train Loss: 20.3120   val Loss: 32.9996   time: 431.21s   best: 27.3407
2023-11-24 14:19:17,303:INFO:  Epoch 442/500:  train Loss: 20.3126   val Loss: 27.5981   time: 433.07s   best: 27.3407
2023-11-24 14:26:30,351:INFO:  Epoch 443/500:  train Loss: 20.2875   val Loss: 28.9106   time: 433.05s   best: 27.3407
2023-11-24 14:33:43,822:INFO:  Epoch 444/500:  train Loss: 20.3174   val Loss: 27.8535   time: 433.47s   best: 27.3407
2023-11-24 14:40:56,280:INFO:  Epoch 445/500:  train Loss: 20.2654   val Loss: 29.2688   time: 432.46s   best: 27.3407
2023-11-24 14:48:09,340:INFO:  Epoch 446/500:  train Loss: 20.2332   val Loss: 27.9728   time: 433.05s   best: 27.3407
2023-11-24 14:55:19,909:INFO:  Epoch 447/500:  train Loss: 20.5998   val Loss: 28.5862   time: 430.57s   best: 27.3407
2023-11-24 15:02:29,261:INFO:  Epoch 448/500:  train Loss: 20.3288   val Loss: 27.5057   time: 429.35s   best: 27.3407
2023-11-24 15:09:40,602:INFO:  Epoch 449/500:  train Loss: 20.2585   val Loss: 28.0253   time: 431.32s   best: 27.3407
2023-11-24 15:16:49,705:INFO:  Epoch 450/500:  train Loss: 20.4903   val Loss: 27.8649   time: 429.10s   best: 27.3407
2023-11-24 15:23:59,449:INFO:  Epoch 451/500:  train Loss: 20.2752   val Loss: 28.1126   time: 429.74s   best: 27.3407
2023-11-24 15:31:07,577:INFO:  Epoch 452/500:  train Loss: 20.2242   val Loss: 28.0542   time: 428.12s   best: 27.3407
2023-11-24 15:38:20,510:INFO:  Epoch 453/500:  train Loss: 20.3260   val Loss: 28.1388   time: 432.93s   best: 27.3407
2023-11-24 15:45:34,501:INFO:  Epoch 454/500:  train Loss: 20.6067   val Loss: 29.1250   time: 433.99s   best: 27.3407
2023-11-24 15:52:45,648:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 15:52:45,668:INFO:  Epoch 455/500:  train Loss: 20.5530   val Loss: 27.3068   time: 431.14s   best: 27.3068
2023-11-24 15:59:55,355:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 15:59:55,373:INFO:  Epoch 456/500:  train Loss: 19.9862   val Loss: 27.2445   time: 429.67s   best: 27.2445
2023-11-24 16:07:08,822:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 16:07:08,839:INFO:  Epoch 457/500:  train Loss: 19.8102   val Loss: 26.4348   time: 433.43s   best: 26.4348
2023-11-24 16:14:17,999:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 16:14:18,028:INFO:  Epoch 458/500:  train Loss: 19.6410   val Loss: 25.9231   time: 429.15s   best: 25.9231
2023-11-24 16:21:31,702:INFO:  Epoch 459/500:  train Loss: 19.6379   val Loss: 26.1528   time: 433.67s   best: 25.9231
2023-11-24 16:28:44,906:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 16:28:44,924:INFO:  Epoch 460/500:  train Loss: 19.3893   val Loss: 25.4184   time: 433.20s   best: 25.4184
2023-11-24 16:35:58,164:INFO:  Epoch 461/500:  train Loss: 19.7115   val Loss: 26.7726   time: 433.24s   best: 25.4184
2023-11-24 16:43:09,846:INFO:  Epoch 462/500:  train Loss: 19.3000   val Loss: 26.1787   time: 431.68s   best: 25.4184
2023-11-24 16:50:22,913:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 16:50:22,931:INFO:  Epoch 463/500:  train Loss: 19.2122   val Loss: 25.2754   time: 433.06s   best: 25.2754
2023-11-24 16:57:36,764:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 16:57:36,782:INFO:  Epoch 464/500:  train Loss: 19.2408   val Loss: 25.2603   time: 433.83s   best: 25.2603
2023-11-24 17:04:51,180:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 17:04:51,198:INFO:  Epoch 465/500:  train Loss: 19.0339   val Loss: 24.7052   time: 434.38s   best: 24.7052
2023-11-24 17:12:03,622:INFO:  Epoch 466/500:  train Loss: 18.9942   val Loss: 24.9604   time: 432.41s   best: 24.7052
2023-11-24 17:19:14,676:INFO:  Epoch 467/500:  train Loss: 19.1269   val Loss: 25.5393   time: 431.05s   best: 24.7052
2023-11-24 17:26:29,025:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 17:26:29,065:INFO:  Epoch 468/500:  train Loss: 18.9995   val Loss: 24.6195   time: 434.34s   best: 24.6195
2023-11-24 17:33:42,897:INFO:  Epoch 469/500:  train Loss: 18.9263   val Loss: 26.8938   time: 433.83s   best: 24.6195
2023-11-24 17:40:55,897:INFO:  Epoch 470/500:  train Loss: 18.8812   val Loss: 24.9944   time: 433.00s   best: 24.6195
2023-11-24 17:48:09,100:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 17:48:09,118:INFO:  Epoch 471/500:  train Loss: 18.8835   val Loss: 24.2337   time: 433.20s   best: 24.2337
2023-11-24 17:55:21,632:INFO:  Epoch 472/500:  train Loss: 18.9023   val Loss: 24.6314   time: 432.50s   best: 24.2337
2023-11-24 18:02:33,400:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 18:02:33,418:INFO:  Epoch 473/500:  train Loss: 19.1226   val Loss: 24.2022   time: 431.76s   best: 24.2022
2023-11-24 18:09:48,098:INFO:  Epoch 474/500:  train Loss: 18.9610   val Loss: 24.3189   time: 434.68s   best: 24.2022
2023-11-24 18:16:57,921:INFO:  Epoch 475/500:  train Loss: 18.7967   val Loss: 24.7045   time: 429.82s   best: 24.2022
2023-11-24 18:24:09,380:INFO:  Epoch 476/500:  train Loss: 18.7759   val Loss: 24.2866   time: 431.45s   best: 24.2022
2023-11-24 18:31:23,055:INFO:  Epoch 477/500:  train Loss: 18.6742   val Loss: 24.3223   time: 433.67s   best: 24.2022
2023-11-24 18:38:34,781:INFO:  Epoch 478/500:  train Loss: 18.6228   val Loss: 24.4630   time: 431.73s   best: 24.2022
2023-11-24 18:45:46,318:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 18:45:46,337:INFO:  Epoch 479/500:  train Loss: 18.7008   val Loss: 23.9608   time: 431.53s   best: 23.9608
2023-11-24 18:52:58,615:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 18:52:58,633:INFO:  Epoch 480/500:  train Loss: 18.6201   val Loss: 23.9366   time: 432.27s   best: 23.9366
2023-11-24 19:00:07,960:INFO:  Epoch 481/500:  train Loss: 18.5909   val Loss: 25.2970   time: 429.32s   best: 23.9366
2023-11-24 19:07:16,215:INFO:  Epoch 482/500:  train Loss: 18.6889   val Loss: 24.3746   time: 428.24s   best: 23.9366
2023-11-24 19:14:28,693:INFO:  Epoch 483/500:  train Loss: 18.7116   val Loss: 25.3306   time: 432.48s   best: 23.9366
2023-11-24 19:21:40,503:INFO:  Epoch 484/500:  train Loss: 18.7467   val Loss: 23.9884   time: 431.79s   best: 23.9366
2023-11-24 19:28:53,101:INFO:  Epoch 485/500:  train Loss: 18.5377   val Loss: 25.2715   time: 432.60s   best: 23.9366
2023-11-24 19:36:05,544:INFO:  Epoch 486/500:  train Loss: 18.5819   val Loss: 24.3571   time: 432.43s   best: 23.9366
2023-11-24 19:43:14,429:INFO:  Epoch 487/500:  train Loss: 18.5820   val Loss: 24.0697   time: 428.88s   best: 23.9366
2023-11-24 19:50:26,108:INFO:  Epoch 488/500:  train Loss: 18.5640   val Loss: 23.9652   time: 431.68s   best: 23.9366
2023-11-24 19:57:33,996:INFO:  Epoch 489/500:  train Loss: 18.5645   val Loss: 24.6302   time: 427.87s   best: 23.9366
2023-11-24 20:04:44,520:INFO:  Epoch 490/500:  train Loss: 18.4623   val Loss: 24.0980   time: 430.51s   best: 23.9366
2023-11-24 20:11:54,237:INFO:  Epoch 491/500:  train Loss: 18.4777   val Loss: 23.9820   time: 429.71s   best: 23.9366
2023-11-24 20:19:04,074:INFO:  Epoch 492/500:  train Loss: 18.3688   val Loss: 24.1773   time: 429.84s   best: 23.9366
2023-11-24 20:26:17,778:INFO:  Epoch 493/500:  train Loss: 18.6434   val Loss: 24.1180   time: 433.69s   best: 23.9366
2023-11-24 20:33:26,921:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 20:33:26,940:INFO:  Epoch 494/500:  train Loss: 18.3549   val Loss: 23.8605   time: 429.13s   best: 23.8605
2023-11-24 20:40:39,457:INFO:  Epoch 495/500:  train Loss: 18.5261   val Loss: 24.2377   time: 432.52s   best: 23.8605
2023-11-24 20:47:52,325:INFO:  Epoch 496/500:  train Loss: 18.3915   val Loss: 24.0043   time: 432.87s   best: 23.8605
2023-11-24 20:55:01,640:INFO:  Epoch 497/500:  train Loss: 18.4561   val Loss: 24.1520   time: 429.31s   best: 23.8605
2023-11-24 21:02:10,447:INFO:  Epoch 498/500:  train Loss: 18.4740   val Loss: 23.9757   time: 428.80s   best: 23.8605
2023-11-24 21:09:19,425:INFO:  Epoch 499/500:  train Loss: 18.5509   val Loss: 23.9042   time: 428.97s   best: 23.8605
2023-11-24 21:16:33,132:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm100 with 0.2 dataset (0.05 dropout)_fc8c.pt
2023-11-24 21:16:33,150:INFO:  Epoch 500/500:  train Loss: 18.3534   val Loss: 23.4836   time: 433.69s   best: 23.4836
2023-11-24 21:16:33,161:INFO:  -----> Training complete in 3588m 21s   best validation loss: 23.4836
 
2023-11-25 20:48:55,464:INFO:  Starting experiment lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)
2023-11-25 20:48:55,466:INFO:  Defining the model
2023-11-25 20:48:55,546:INFO:  Reading the dataset
2023-11-25 20:55:18,985:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 20:55:19,005:INFO:  Epoch 1/600:  train Loss: 90.2397   val Loss: 87.5242   time: 90.99s   best: 87.5242
2023-11-25 20:56:47,677:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 20:56:47,696:INFO:  Epoch 2/600:  train Loss: 86.8199   val Loss: 85.6075   time: 88.67s   best: 85.6075
2023-11-25 20:58:16,551:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 20:58:16,570:INFO:  Epoch 3/600:  train Loss: 85.5657   val Loss: 84.3977   time: 88.85s   best: 84.3977
2023-11-25 20:59:45,422:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 20:59:45,442:INFO:  Epoch 4/600:  train Loss: 83.6906   val Loss: 81.8369   time: 88.84s   best: 81.8369
2023-11-25 21:01:14,508:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:01:14,527:INFO:  Epoch 5/600:  train Loss: 78.6207   val Loss: 77.3493   time: 89.06s   best: 77.3493
2023-11-25 21:02:43,383:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:02:43,402:INFO:  Epoch 6/600:  train Loss: 75.8914   val Loss: 76.9060   time: 88.84s   best: 76.9060
2023-11-25 21:04:12,841:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:04:12,861:INFO:  Epoch 7/600:  train Loss: 73.7213   val Loss: 74.7100   time: 89.43s   best: 74.7100
2023-11-25 21:05:42,335:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:05:42,354:INFO:  Epoch 8/600:  train Loss: 72.5691   val Loss: 71.8796   time: 89.46s   best: 71.8796
2023-11-25 21:07:12,579:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:07:12,599:INFO:  Epoch 9/600:  train Loss: 70.8657   val Loss: 71.2225   time: 90.21s   best: 71.2225
2023-11-25 21:08:42,473:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:08:42,492:INFO:  Epoch 10/600:  train Loss: 69.5417   val Loss: 68.3116   time: 89.87s   best: 68.3116
2023-11-25 21:10:12,800:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:10:12,820:INFO:  Epoch 11/600:  train Loss: 68.5347   val Loss: 67.9394   time: 90.30s   best: 67.9394
2023-11-25 21:11:42,882:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:11:42,901:INFO:  Epoch 12/600:  train Loss: 67.6158   val Loss: 66.5030   time: 90.06s   best: 66.5030
2023-11-25 21:13:12,666:INFO:  Epoch 13/600:  train Loss: 67.0282   val Loss: 66.5182   time: 89.76s   best: 66.5030
2023-11-25 21:14:42,283:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:14:42,302:INFO:  Epoch 14/600:  train Loss: 66.1696   val Loss: 66.4181   time: 89.61s   best: 66.4181
2023-11-25 21:16:11,650:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:16:11,685:INFO:  Epoch 15/600:  train Loss: 65.7110   val Loss: 64.8207   time: 89.34s   best: 64.8207
2023-11-25 21:17:41,182:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:17:41,202:INFO:  Epoch 16/600:  train Loss: 65.4195   val Loss: 64.8020   time: 89.49s   best: 64.8020
2023-11-25 21:19:10,854:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:19:10,873:INFO:  Epoch 17/600:  train Loss: 64.8783   val Loss: 64.2227   time: 89.64s   best: 64.2227
2023-11-25 21:20:40,367:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:20:40,386:INFO:  Epoch 18/600:  train Loss: 64.2195   val Loss: 63.1245   time: 89.48s   best: 63.1245
2023-11-25 21:22:09,802:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:22:09,823:INFO:  Epoch 19/600:  train Loss: 63.5663   val Loss: 62.7708   time: 89.40s   best: 62.7708
2023-11-25 21:23:39,223:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:23:39,243:INFO:  Epoch 20/600:  train Loss: 63.0404   val Loss: 62.2651   time: 89.39s   best: 62.2651
2023-11-25 21:25:09,193:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:25:09,212:INFO:  Epoch 21/600:  train Loss: 62.4124   val Loss: 62.1375   time: 89.95s   best: 62.1375
2023-11-25 21:26:39,220:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:26:39,249:INFO:  Epoch 22/600:  train Loss: 62.2379   val Loss: 61.7000   time: 90.00s   best: 61.7000
2023-11-25 21:28:09,202:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:28:09,222:INFO:  Epoch 23/600:  train Loss: 61.4521   val Loss: 60.6603   time: 89.94s   best: 60.6603
2023-11-25 21:29:38,851:INFO:  Epoch 24/600:  train Loss: 61.0721   val Loss: 60.8272   time: 89.62s   best: 60.6603
2023-11-25 21:31:08,544:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:31:08,563:INFO:  Epoch 25/600:  train Loss: 60.8196   val Loss: 59.9514   time: 89.69s   best: 59.9514
2023-11-25 21:32:38,138:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:32:38,158:INFO:  Epoch 26/600:  train Loss: 60.0965   val Loss: 59.5343   time: 89.56s   best: 59.5343
2023-11-25 21:34:07,401:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:34:07,420:INFO:  Epoch 27/600:  train Loss: 59.5898   val Loss: 58.6800   time: 89.23s   best: 58.6800
2023-11-25 21:35:37,500:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:35:37,519:INFO:  Epoch 28/600:  train Loss: 59.1101   val Loss: 58.1480   time: 90.06s   best: 58.1480
2023-11-25 21:37:07,350:INFO:  Epoch 29/600:  train Loss: 58.4706   val Loss: 58.4065   time: 89.82s   best: 58.1480
2023-11-25 21:38:36,641:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:38:36,661:INFO:  Epoch 30/600:  train Loss: 58.0846   val Loss: 57.5631   time: 89.28s   best: 57.5631
2023-11-25 21:40:06,235:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:40:06,254:INFO:  Epoch 31/600:  train Loss: 57.4498   val Loss: 56.8453   time: 89.56s   best: 56.8453
2023-11-25 21:41:35,978:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:41:35,997:INFO:  Epoch 32/600:  train Loss: 56.8533   val Loss: 56.4154   time: 89.72s   best: 56.4154
2023-11-25 21:43:05,352:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:43:05,372:INFO:  Epoch 33/600:  train Loss: 56.7530   val Loss: 56.1989   time: 89.34s   best: 56.1989
2023-11-25 21:44:34,749:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:44:34,769:INFO:  Epoch 34/600:  train Loss: 55.9781   val Loss: 55.5164   time: 89.36s   best: 55.5164
2023-11-25 21:46:04,074:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:46:04,093:INFO:  Epoch 35/600:  train Loss: 55.3738   val Loss: 55.1483   time: 89.29s   best: 55.1483
2023-11-25 21:47:33,518:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:47:33,537:INFO:  Epoch 36/600:  train Loss: 54.9930   val Loss: 54.5626   time: 89.41s   best: 54.5626
2023-11-25 21:49:03,556:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:49:03,575:INFO:  Epoch 37/600:  train Loss: 54.6315   val Loss: 54.3993   time: 90.01s   best: 54.3993
2023-11-25 21:50:32,936:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:50:32,956:INFO:  Epoch 38/600:  train Loss: 53.8816   val Loss: 53.4130   time: 89.35s   best: 53.4130
2023-11-25 21:52:02,429:INFO:  Epoch 39/600:  train Loss: 53.5540   val Loss: 53.7888   time: 89.46s   best: 53.4130
2023-11-25 21:53:31,793:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:53:31,813:INFO:  Epoch 40/600:  train Loss: 53.0195   val Loss: 52.3884   time: 89.34s   best: 52.3884
2023-11-25 21:55:01,302:INFO:  Epoch 41/600:  train Loss: 52.2104   val Loss: 52.4409   time: 89.48s   best: 52.3884
2023-11-25 21:56:31,477:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:56:31,497:INFO:  Epoch 42/600:  train Loss: 51.8522   val Loss: 51.4101   time: 90.17s   best: 51.4101
2023-11-25 21:58:01,096:INFO:  Epoch 43/600:  train Loss: 51.1904   val Loss: 51.5200   time: 89.60s   best: 51.4101
2023-11-25 21:59:30,954:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 21:59:30,973:INFO:  Epoch 44/600:  train Loss: 50.8641   val Loss: 50.6518   time: 89.85s   best: 50.6518
2023-11-25 22:01:00,663:INFO:  Epoch 45/600:  train Loss: 50.4272   val Loss: 52.4468   time: 89.68s   best: 50.6518
2023-11-25 22:02:30,068:INFO:  Epoch 46/600:  train Loss: 49.9597   val Loss: 51.3878   time: 89.40s   best: 50.6518
2023-11-25 22:03:59,579:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:03:59,598:INFO:  Epoch 47/600:  train Loss: 49.3630   val Loss: 49.8725   time: 89.50s   best: 49.8725
2023-11-25 22:05:29,011:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:05:29,030:INFO:  Epoch 48/600:  train Loss: 48.9079   val Loss: 49.0281   time: 89.41s   best: 49.0281
2023-11-25 22:06:58,323:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:06:58,342:INFO:  Epoch 49/600:  train Loss: 48.4070   val Loss: 49.0091   time: 89.29s   best: 49.0091
2023-11-25 22:08:27,735:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:08:27,755:INFO:  Epoch 50/600:  train Loss: 47.8448   val Loss: 48.3592   time: 89.38s   best: 48.3592
2023-11-25 22:09:57,430:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:09:57,449:INFO:  Epoch 51/600:  train Loss: 47.6663   val Loss: 47.9679   time: 89.67s   best: 47.9679
2023-11-25 22:11:26,796:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:11:26,816:INFO:  Epoch 52/600:  train Loss: 47.0913   val Loss: 47.9154   time: 89.33s   best: 47.9154
2023-11-25 22:12:56,216:INFO:  Epoch 53/600:  train Loss: 46.8469   val Loss: 48.6582   time: 89.40s   best: 47.9154
2023-11-25 22:14:26,037:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:14:26,057:INFO:  Epoch 54/600:  train Loss: 46.6207   val Loss: 47.1377   time: 89.81s   best: 47.1377
2023-11-25 22:15:55,675:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:15:55,694:INFO:  Epoch 55/600:  train Loss: 46.1113   val Loss: 46.6732   time: 89.60s   best: 46.6732
2023-11-25 22:17:26,018:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:17:26,037:INFO:  Epoch 56/600:  train Loss: 45.7092   val Loss: 46.4505   time: 90.30s   best: 46.4505
2023-11-25 22:18:56,186:INFO:  Epoch 57/600:  train Loss: 45.4673   val Loss: 47.3475   time: 90.14s   best: 46.4505
2023-11-25 22:20:25,825:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:20:25,844:INFO:  Epoch 58/600:  train Loss: 45.1516   val Loss: 45.7324   time: 89.62s   best: 45.7324
2023-11-25 22:21:55,323:INFO:  Epoch 59/600:  train Loss: 44.7559   val Loss: 46.6417   time: 89.47s   best: 45.7324
2023-11-25 22:23:25,022:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:23:25,041:INFO:  Epoch 60/600:  train Loss: 44.4789   val Loss: 45.3077   time: 89.69s   best: 45.3077
2023-11-25 22:24:54,499:INFO:  Epoch 61/600:  train Loss: 44.5451   val Loss: 46.8044   time: 89.46s   best: 45.3077
2023-11-25 22:26:23,786:INFO:  Epoch 62/600:  train Loss: 43.8986   val Loss: 45.5689   time: 89.28s   best: 45.3077
2023-11-25 22:27:53,166:INFO:  Epoch 63/600:  train Loss: 43.8478   val Loss: 45.4504   time: 89.37s   best: 45.3077
2023-11-25 22:29:23,053:INFO:  Epoch 64/600:  train Loss: 43.4308   val Loss: 46.0376   time: 89.88s   best: 45.3077
2023-11-25 22:30:52,740:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:30:52,759:INFO:  Epoch 65/600:  train Loss: 43.0908   val Loss: 44.2450   time: 89.68s   best: 44.2450
2023-11-25 22:32:22,133:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:32:22,152:INFO:  Epoch 66/600:  train Loss: 43.4210   val Loss: 43.8795   time: 89.36s   best: 43.8795
2023-11-25 22:33:51,924:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:33:51,943:INFO:  Epoch 67/600:  train Loss: 42.6056   val Loss: 43.7717   time: 89.76s   best: 43.7717
2023-11-25 22:35:21,914:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:35:21,933:INFO:  Epoch 68/600:  train Loss: 42.3979   val Loss: 43.6147   time: 89.97s   best: 43.6147
2023-11-25 22:36:51,826:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:36:51,846:INFO:  Epoch 69/600:  train Loss: 42.0292   val Loss: 43.5869   time: 89.89s   best: 43.5869
2023-11-25 22:38:21,230:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:38:21,250:INFO:  Epoch 70/600:  train Loss: 41.9323   val Loss: 43.3157   time: 89.37s   best: 43.3157
2023-11-25 22:39:51,064:INFO:  Epoch 71/600:  train Loss: 42.0175   val Loss: 43.3907   time: 89.80s   best: 43.3157
2023-11-25 22:41:20,523:INFO:  Epoch 72/600:  train Loss: 41.4999   val Loss: 44.3306   time: 89.46s   best: 43.3157
2023-11-25 22:42:49,954:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:42:49,973:INFO:  Epoch 73/600:  train Loss: 41.2353   val Loss: 43.0166   time: 89.42s   best: 43.0166
2023-11-25 22:44:19,280:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:44:19,300:INFO:  Epoch 74/600:  train Loss: 41.2407   val Loss: 42.6388   time: 89.30s   best: 42.6388
2023-11-25 22:45:48,652:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:45:48,671:INFO:  Epoch 75/600:  train Loss: 40.7042   val Loss: 42.0336   time: 89.35s   best: 42.0336
2023-11-25 22:47:18,323:INFO:  Epoch 76/600:  train Loss: 40.8419   val Loss: 42.2541   time: 89.64s   best: 42.0336
2023-11-25 22:48:48,342:INFO:  Epoch 77/600:  train Loss: 40.5175   val Loss: 42.0593   time: 90.01s   best: 42.0336
2023-11-25 22:50:17,862:INFO:  Epoch 78/600:  train Loss: 40.1997   val Loss: 43.9422   time: 89.52s   best: 42.0336
2023-11-25 22:51:47,628:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:51:47,647:INFO:  Epoch 79/600:  train Loss: 40.1101   val Loss: 41.7316   time: 89.76s   best: 41.7316
2023-11-25 22:53:17,353:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:53:17,381:INFO:  Epoch 80/600:  train Loss: 39.7242   val Loss: 41.2404   time: 89.70s   best: 41.2404
2023-11-25 22:54:47,260:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:54:47,279:INFO:  Epoch 81/600:  train Loss: 39.7419   val Loss: 41.0712   time: 89.86s   best: 41.0712
2023-11-25 22:56:16,634:INFO:  Epoch 82/600:  train Loss: 39.3322   val Loss: 41.6966   time: 89.34s   best: 41.0712
2023-11-25 22:57:45,931:INFO:  Epoch 83/600:  train Loss: 39.2316   val Loss: 41.3662   time: 89.30s   best: 41.0712
2023-11-25 22:59:15,720:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 22:59:15,738:INFO:  Epoch 84/600:  train Loss: 38.9577   val Loss: 40.4098   time: 89.78s   best: 40.4098
2023-11-25 23:00:45,201:INFO:  Epoch 85/600:  train Loss: 38.5650   val Loss: 41.6468   time: 89.45s   best: 40.4098
2023-11-25 23:02:14,759:INFO:  Epoch 86/600:  train Loss: 38.4207   val Loss: 40.4646   time: 89.55s   best: 40.4098
2023-11-25 23:03:44,246:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:03:44,265:INFO:  Epoch 87/600:  train Loss: 38.0735   val Loss: 39.7783   time: 89.47s   best: 39.7783
2023-11-25 23:05:13,710:INFO:  Epoch 88/600:  train Loss: 38.5686   val Loss: 44.2426   time: 89.43s   best: 39.7783
2023-11-25 23:06:43,426:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:06:43,446:INFO:  Epoch 89/600:  train Loss: 38.2185   val Loss: 39.6161   time: 89.70s   best: 39.6161
2023-11-25 23:08:12,730:INFO:  Epoch 90/600:  train Loss: 38.3255   val Loss: 40.1489   time: 89.28s   best: 39.6161
2023-11-25 23:09:41,908:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:09:41,927:INFO:  Epoch 91/600:  train Loss: 37.5039   val Loss: 38.8898   time: 89.17s   best: 38.8898
2023-11-25 23:11:11,276:INFO:  Epoch 92/600:  train Loss: 37.3475   val Loss: 39.0830   time: 89.34s   best: 38.8898
2023-11-25 23:12:40,604:INFO:  Epoch 93/600:  train Loss: 37.1473   val Loss: 39.5996   time: 89.32s   best: 38.8898
2023-11-25 23:14:09,900:INFO:  Epoch 94/600:  train Loss: 36.8853   val Loss: 39.0053   time: 89.30s   best: 38.8898
2023-11-25 23:15:39,815:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:15:39,834:INFO:  Epoch 95/600:  train Loss: 36.6901   val Loss: 38.6463   time: 89.90s   best: 38.6463
2023-11-25 23:17:09,510:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:17:09,529:INFO:  Epoch 96/600:  train Loss: 36.3754   val Loss: 37.6381   time: 89.66s   best: 37.6381
2023-11-25 23:18:38,804:INFO:  Epoch 97/600:  train Loss: 36.6306   val Loss: 38.5671   time: 89.27s   best: 37.6381
2023-11-25 23:20:08,139:INFO:  Epoch 98/600:  train Loss: 36.3507   val Loss: 38.6029   time: 89.33s   best: 37.6381
2023-11-25 23:21:37,966:INFO:  Epoch 99/600:  train Loss: 35.9947   val Loss: 37.8976   time: 89.82s   best: 37.6381
2023-11-25 23:23:07,927:INFO:  Epoch 100/600:  train Loss: 36.0940   val Loss: 38.3206   time: 89.95s   best: 37.6381
2023-11-25 23:24:37,147:INFO:  Epoch 101/600:  train Loss: 35.7094   val Loss: 38.4234   time: 89.21s   best: 37.6381
2023-11-25 23:26:06,795:INFO:  Epoch 102/600:  train Loss: 35.6522   val Loss: 39.0758   time: 89.64s   best: 37.6381
2023-11-25 23:27:36,429:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:27:36,448:INFO:  Epoch 103/600:  train Loss: 35.5364   val Loss: 37.3120   time: 89.63s   best: 37.3120
2023-11-25 23:29:06,097:INFO:  Epoch 104/600:  train Loss: 35.6069   val Loss: 38.8543   time: 89.65s   best: 37.3120
2023-11-25 23:30:35,945:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:30:35,964:INFO:  Epoch 105/600:  train Loss: 35.1946   val Loss: 37.2214   time: 89.84s   best: 37.2214
2023-11-25 23:32:05,584:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:32:05,603:INFO:  Epoch 106/600:  train Loss: 35.4706   val Loss: 37.1352   time: 89.60s   best: 37.1352
2023-11-25 23:33:34,897:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:33:34,916:INFO:  Epoch 107/600:  train Loss: 35.0204   val Loss: 37.0081   time: 89.28s   best: 37.0081
2023-11-25 23:35:04,549:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:35:04,568:INFO:  Epoch 108/600:  train Loss: 34.9156   val Loss: 36.9796   time: 89.62s   best: 36.9796
2023-11-25 23:36:33,898:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:36:33,917:INFO:  Epoch 109/600:  train Loss: 34.6434   val Loss: 36.3575   time: 89.33s   best: 36.3575
2023-11-25 23:38:03,364:INFO:  Epoch 110/600:  train Loss: 35.3864   val Loss: 39.4901   time: 89.45s   best: 36.3575
2023-11-25 23:39:32,826:INFO:  Epoch 111/600:  train Loss: 34.9231   val Loss: 36.5968   time: 89.45s   best: 36.3575
2023-11-25 23:41:02,436:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:41:02,456:INFO:  Epoch 112/600:  train Loss: 34.2104   val Loss: 35.7453   time: 89.61s   best: 35.7453
2023-11-25 23:42:32,143:INFO:  Epoch 113/600:  train Loss: 34.3746   val Loss: 37.4545   time: 89.69s   best: 35.7453
2023-11-25 23:44:01,391:INFO:  Epoch 114/600:  train Loss: 34.2425   val Loss: 36.2573   time: 89.24s   best: 35.7453
2023-11-25 23:45:30,787:INFO:  Epoch 115/600:  train Loss: 34.0996   val Loss: 36.6405   time: 89.38s   best: 35.7453
2023-11-25 23:47:00,226:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:47:00,245:INFO:  Epoch 116/600:  train Loss: 33.8545   val Loss: 35.4601   time: 89.42s   best: 35.4601
2023-11-25 23:48:30,052:INFO:  Epoch 117/600:  train Loss: 33.4664   val Loss: 35.5054   time: 89.81s   best: 35.4601
2023-11-25 23:49:59,364:INFO:  Epoch 118/600:  train Loss: 33.7906   val Loss: 35.5535   time: 89.30s   best: 35.4601
2023-11-25 23:51:29,033:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-25 23:51:29,052:INFO:  Epoch 119/600:  train Loss: 33.4255   val Loss: 34.8068   time: 89.66s   best: 34.8068
2023-11-25 23:52:58,830:INFO:  Epoch 120/600:  train Loss: 33.3032   val Loss: 36.0218   time: 89.78s   best: 34.8068
2023-11-25 23:54:28,721:INFO:  Epoch 121/600:  train Loss: 33.1920   val Loss: 34.9267   time: 89.89s   best: 34.8068
2023-11-25 23:55:58,742:INFO:  Epoch 122/600:  train Loss: 33.1106   val Loss: 35.5210   time: 90.02s   best: 34.8068
2023-11-25 23:57:28,311:INFO:  Epoch 123/600:  train Loss: 32.9035   val Loss: 35.9563   time: 89.57s   best: 34.8068
2023-11-25 23:58:57,790:INFO:  Epoch 124/600:  train Loss: 33.1920   val Loss: 34.8387   time: 89.48s   best: 34.8068
2023-11-26 00:00:27,065:INFO:  Epoch 125/600:  train Loss: 32.8179   val Loss: 35.7352   time: 89.27s   best: 34.8068
2023-11-26 00:01:56,796:INFO:  Epoch 126/600:  train Loss: 32.7861   val Loss: 34.8683   time: 89.72s   best: 34.8068
2023-11-26 00:03:26,304:INFO:  Epoch 127/600:  train Loss: 33.6355   val Loss: 37.8055   time: 89.51s   best: 34.8068
2023-11-26 00:04:56,241:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 00:04:56,278:INFO:  Epoch 128/600:  train Loss: 33.4202   val Loss: 34.3602   time: 89.93s   best: 34.3602
2023-11-26 00:06:26,054:INFO:  Epoch 129/600:  train Loss: 32.9498   val Loss: 34.7302   time: 89.77s   best: 34.3602
2023-11-26 00:07:55,378:INFO:  Epoch 130/600:  train Loss: 33.1659   val Loss: 35.1185   time: 89.32s   best: 34.3602
2023-11-26 00:09:24,852:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 00:09:24,872:INFO:  Epoch 131/600:  train Loss: 32.2901   val Loss: 33.7614   time: 89.47s   best: 33.7614
2023-11-26 00:10:54,410:INFO:  Epoch 132/600:  train Loss: 32.3057   val Loss: 34.4548   time: 89.54s   best: 33.7614
2023-11-26 00:12:23,716:INFO:  Epoch 133/600:  train Loss: 32.6931   val Loss: 34.2104   time: 89.29s   best: 33.7614
2023-11-26 00:13:53,060:INFO:  Epoch 134/600:  train Loss: 32.6085   val Loss: 41.8849   time: 89.34s   best: 33.7614
2023-11-26 00:15:22,521:INFO:  Epoch 135/600:  train Loss: 32.8321   val Loss: 33.9222   time: 89.46s   best: 33.7614
2023-11-26 00:16:52,251:INFO:  Epoch 136/600:  train Loss: 32.1818   val Loss: 33.8079   time: 89.73s   best: 33.7614
2023-11-26 00:18:22,041:INFO:  Epoch 137/600:  train Loss: 32.5699   val Loss: 34.9408   time: 89.78s   best: 33.7614
2023-11-26 00:19:51,433:INFO:  Epoch 138/600:  train Loss: 32.2343   val Loss: 33.9777   time: 89.38s   best: 33.7614
2023-11-26 00:21:21,120:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 00:21:21,139:INFO:  Epoch 139/600:  train Loss: 31.8634   val Loss: 33.5049   time: 89.68s   best: 33.5049
2023-11-26 00:22:50,648:INFO:  Epoch 140/600:  train Loss: 31.9284   val Loss: 33.8634   time: 89.50s   best: 33.5049
2023-11-26 00:24:20,010:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 00:24:20,029:INFO:  Epoch 141/600:  train Loss: 31.7535   val Loss: 33.3652   time: 89.35s   best: 33.3652
2023-11-26 00:25:49,265:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 00:25:49,284:INFO:  Epoch 142/600:  train Loss: 31.4608   val Loss: 33.3172   time: 89.22s   best: 33.3172
2023-11-26 00:27:18,603:INFO:  Epoch 143/600:  train Loss: 32.1497   val Loss: 33.8429   time: 89.31s   best: 33.3172
2023-11-26 00:28:48,333:INFO:  Epoch 144/600:  train Loss: 31.3181   val Loss: 33.3384   time: 89.73s   best: 33.3172
2023-11-26 00:30:17,677:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 00:30:17,696:INFO:  Epoch 145/600:  train Loss: 31.9159   val Loss: 32.9725   time: 89.34s   best: 32.9725
2023-11-26 00:31:47,078:INFO:  Epoch 146/600:  train Loss: 31.2828   val Loss: 32.9909   time: 89.38s   best: 32.9725
2023-11-26 00:33:16,683:INFO:  Epoch 147/600:  train Loss: 31.3717   val Loss: 33.1762   time: 89.60s   best: 32.9725
2023-11-26 00:34:45,945:INFO:  Epoch 148/600:  train Loss: 31.1591   val Loss: 33.2239   time: 89.26s   best: 32.9725
2023-11-26 00:36:15,211:INFO:  Epoch 149/600:  train Loss: 31.1881   val Loss: 33.3362   time: 89.25s   best: 32.9725
2023-11-26 00:37:44,501:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 00:37:44,519:INFO:  Epoch 150/600:  train Loss: 31.4480   val Loss: 32.9664   time: 89.28s   best: 32.9664
2023-11-26 00:39:14,021:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 00:39:14,040:INFO:  Epoch 151/600:  train Loss: 30.8350   val Loss: 32.8372   time: 89.50s   best: 32.8372
2023-11-26 00:40:43,774:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 00:40:43,793:INFO:  Epoch 152/600:  train Loss: 30.8552   val Loss: 32.5279   time: 89.72s   best: 32.5279
2023-11-26 00:42:13,530:INFO:  Epoch 153/600:  train Loss: 31.1913   val Loss: 33.2592   time: 89.74s   best: 32.5279
2023-11-26 00:43:42,807:INFO:  Epoch 154/600:  train Loss: 30.7114   val Loss: 32.7506   time: 89.28s   best: 32.5279
2023-11-26 00:45:12,215:INFO:  Epoch 155/600:  train Loss: 30.5520   val Loss: 33.8447   time: 89.40s   best: 32.5279
2023-11-26 00:46:41,548:INFO:  Epoch 156/600:  train Loss: 31.4338   val Loss: 34.1521   time: 89.33s   best: 32.5279
2023-11-26 00:48:11,250:INFO:  Epoch 157/600:  train Loss: 30.7813   val Loss: 32.7814   time: 89.70s   best: 32.5279
2023-11-26 00:49:40,610:INFO:  Epoch 158/600:  train Loss: 30.5622   val Loss: 34.3105   time: 89.35s   best: 32.5279
2023-11-26 00:51:09,839:INFO:  Epoch 159/600:  train Loss: 30.4532   val Loss: 32.6797   time: 89.23s   best: 32.5279
2023-11-26 00:52:39,075:INFO:  Epoch 160/600:  train Loss: 32.5296   val Loss: 32.6489   time: 89.23s   best: 32.5279
2023-11-26 00:54:08,224:INFO:  Epoch 161/600:  train Loss: 30.8047   val Loss: 32.8467   time: 89.15s   best: 32.5279
2023-11-26 00:55:37,486:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 00:55:37,504:INFO:  Epoch 162/600:  train Loss: 30.2250   val Loss: 32.3549   time: 89.25s   best: 32.3549
2023-11-26 00:57:07,625:INFO:  Epoch 163/600:  train Loss: 30.3037   val Loss: 32.5007   time: 90.10s   best: 32.3549
2023-11-26 00:58:36,853:INFO:  Epoch 164/600:  train Loss: 30.9254   val Loss: 34.0849   time: 89.23s   best: 32.3549
2023-11-26 01:00:06,039:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 01:00:06,058:INFO:  Epoch 165/600:  train Loss: 30.4150   val Loss: 32.1866   time: 89.18s   best: 32.1866
2023-11-26 01:01:35,380:INFO:  Epoch 166/600:  train Loss: 30.2531   val Loss: 33.2035   time: 89.32s   best: 32.1866
2023-11-26 01:03:04,794:INFO:  Epoch 167/600:  train Loss: 30.3125   val Loss: 32.3522   time: 89.40s   best: 32.1866
2023-11-26 01:04:34,063:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 01:04:34,081:INFO:  Epoch 168/600:  train Loss: 30.0120   val Loss: 31.9921   time: 89.26s   best: 31.9921
2023-11-26 01:06:03,296:INFO:  Epoch 169/600:  train Loss: 29.9173   val Loss: 32.2226   time: 89.21s   best: 31.9921
2023-11-26 01:07:32,642:INFO:  Epoch 170/600:  train Loss: 29.8148   val Loss: 32.1628   time: 89.34s   best: 31.9921
2023-11-26 01:09:02,432:INFO:  Epoch 171/600:  train Loss: 29.9528   val Loss: 32.9786   time: 89.79s   best: 31.9921
2023-11-26 01:10:31,924:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 01:10:31,943:INFO:  Epoch 172/600:  train Loss: 29.7068   val Loss: 31.4546   time: 89.49s   best: 31.4546
2023-11-26 01:12:01,522:INFO:  Epoch 173/600:  train Loss: 30.1251   val Loss: 32.8786   time: 89.58s   best: 31.4546
2023-11-26 01:13:31,860:INFO:  Epoch 174/600:  train Loss: 29.5319   val Loss: 31.7292   time: 90.33s   best: 31.4546
2023-11-26 01:15:01,060:INFO:  Epoch 175/600:  train Loss: 30.1695   val Loss: 32.5412   time: 89.19s   best: 31.4546
2023-11-26 01:16:30,258:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 01:16:30,277:INFO:  Epoch 176/600:  train Loss: 29.6231   val Loss: 31.4471   time: 89.18s   best: 31.4471
2023-11-26 01:17:59,541:INFO:  Epoch 177/600:  train Loss: 30.0500   val Loss: 31.9086   time: 89.26s   best: 31.4471
2023-11-26 01:19:29,025:INFO:  Epoch 178/600:  train Loss: 29.4796   val Loss: 31.6278   time: 89.47s   best: 31.4471
2023-11-26 01:20:58,333:INFO:  Epoch 179/600:  train Loss: 29.8263   val Loss: 31.8834   time: 89.30s   best: 31.4471
2023-11-26 01:22:27,510:INFO:  Epoch 180/600:  train Loss: 29.2943   val Loss: 32.1755   time: 89.16s   best: 31.4471
2023-11-26 01:23:56,612:INFO:  Epoch 181/600:  train Loss: 29.2774   val Loss: 31.6027   time: 89.09s   best: 31.4471
2023-11-26 01:25:25,924:INFO:  Epoch 182/600:  train Loss: 29.1366   val Loss: 31.5636   time: 89.30s   best: 31.4471
2023-11-26 01:26:55,533:INFO:  Epoch 183/600:  train Loss: 29.5143   val Loss: 31.8387   time: 89.61s   best: 31.4471
2023-11-26 01:28:24,847:INFO:  Epoch 184/600:  train Loss: 31.3308   val Loss: 31.9760   time: 89.31s   best: 31.4471
2023-11-26 01:29:54,159:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 01:29:54,178:INFO:  Epoch 185/600:  train Loss: 29.3897   val Loss: 31.4191   time: 89.31s   best: 31.4191
2023-11-26 01:31:23,548:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 01:31:23,567:INFO:  Epoch 186/600:  train Loss: 28.9685   val Loss: 31.1215   time: 89.36s   best: 31.1215
2023-11-26 01:32:53,745:INFO:  Epoch 187/600:  train Loss: 29.4465   val Loss: 31.7702   time: 90.18s   best: 31.1215
2023-11-26 01:34:23,236:INFO:  Epoch 188/600:  train Loss: 28.9547   val Loss: 31.4066   time: 89.49s   best: 31.1215
2023-11-26 01:35:52,872:INFO:  Epoch 189/600:  train Loss: 28.8971   val Loss: 31.5857   time: 89.63s   best: 31.1215
2023-11-26 01:37:22,477:INFO:  Epoch 190/600:  train Loss: 29.5574   val Loss: 32.6576   time: 89.59s   best: 31.1215
2023-11-26 01:38:51,985:INFO:  Epoch 191/600:  train Loss: 28.8748   val Loss: 31.1338   time: 89.50s   best: 31.1215
2023-11-26 01:40:21,476:INFO:  Epoch 192/600:  train Loss: 28.9167   val Loss: 31.6190   time: 89.49s   best: 31.1215
2023-11-26 01:41:51,082:INFO:  Epoch 193/600:  train Loss: 28.6982   val Loss: 31.7974   time: 89.61s   best: 31.1215
2023-11-26 01:43:20,680:INFO:  Epoch 194/600:  train Loss: 28.7123   val Loss: 33.8578   time: 89.60s   best: 31.1215
2023-11-26 01:44:50,069:INFO:  Epoch 195/600:  train Loss: 28.7925   val Loss: 31.5216   time: 89.38s   best: 31.1215
2023-11-26 01:46:19,657:INFO:  Epoch 196/600:  train Loss: 28.6066   val Loss: 31.2629   time: 89.58s   best: 31.1215
2023-11-26 01:47:48,858:INFO:  Epoch 197/600:  train Loss: 28.5383   val Loss: 31.6022   time: 89.19s   best: 31.1215
2023-11-26 01:49:18,342:INFO:  Epoch 198/600:  train Loss: 28.8508   val Loss: 31.3256   time: 89.47s   best: 31.1215
2023-11-26 01:50:48,548:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 01:50:48,566:INFO:  Epoch 199/600:  train Loss: 28.7344   val Loss: 30.6545   time: 90.20s   best: 30.6545
2023-11-26 01:52:18,161:INFO:  Epoch 200/600:  train Loss: 28.3503   val Loss: 30.8777   time: 89.58s   best: 30.6545
2023-11-26 01:53:47,672:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 01:53:47,690:INFO:  Epoch 201/600:  train Loss: 28.3435   val Loss: 30.5202   time: 89.50s   best: 30.5202
2023-11-26 01:55:17,496:INFO:  Epoch 202/600:  train Loss: 28.3781   val Loss: 31.8512   time: 89.79s   best: 30.5202
2023-11-26 01:56:46,784:INFO:  Epoch 203/600:  train Loss: 28.2532   val Loss: 31.0452   time: 89.27s   best: 30.5202
2023-11-26 01:58:16,413:INFO:  Epoch 204/600:  train Loss: 28.4957   val Loss: 33.4680   time: 89.63s   best: 30.5202
2023-11-26 01:59:46,356:INFO:  Epoch 205/600:  train Loss: 28.9731   val Loss: 32.7781   time: 89.93s   best: 30.5202
2023-11-26 02:01:16,109:INFO:  Epoch 206/600:  train Loss: 28.3402   val Loss: 30.6540   time: 89.75s   best: 30.5202
2023-11-26 02:02:45,668:INFO:  Epoch 207/600:  train Loss: 28.4473   val Loss: 30.7968   time: 89.56s   best: 30.5202
2023-11-26 02:04:15,397:INFO:  Epoch 208/600:  train Loss: 28.1204   val Loss: 31.8419   time: 89.72s   best: 30.5202
2023-11-26 02:05:45,050:INFO:  Epoch 209/600:  train Loss: 28.0465   val Loss: 30.5825   time: 89.65s   best: 30.5202
2023-11-26 02:07:14,429:INFO:  Epoch 210/600:  train Loss: 28.3719   val Loss: 30.6580   time: 89.37s   best: 30.5202
2023-11-26 02:08:43,904:INFO:  Epoch 211/600:  train Loss: 28.0248   val Loss: 31.5804   time: 89.46s   best: 30.5202
2023-11-26 02:10:13,152:INFO:  Epoch 212/600:  train Loss: 27.9754   val Loss: 30.6117   time: 89.24s   best: 30.5202
2023-11-26 02:11:42,988:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 02:11:43,007:INFO:  Epoch 213/600:  train Loss: 28.0140   val Loss: 30.3487   time: 89.82s   best: 30.3487
2023-11-26 02:13:12,883:INFO:  Epoch 214/600:  train Loss: 27.9398   val Loss: 30.8779   time: 89.88s   best: 30.3487
2023-11-26 02:14:42,491:INFO:  Epoch 215/600:  train Loss: 28.2533   val Loss: 30.3764   time: 89.61s   best: 30.3487
2023-11-26 02:16:11,809:INFO:  Epoch 216/600:  train Loss: 27.7164   val Loss: 30.4432   time: 89.32s   best: 30.3487
2023-11-26 02:17:41,176:INFO:  Epoch 217/600:  train Loss: 28.7380   val Loss: 31.7991   time: 89.36s   best: 30.3487
2023-11-26 02:19:10,997:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 02:19:11,016:INFO:  Epoch 218/600:  train Loss: 27.7555   val Loss: 30.2804   time: 89.81s   best: 30.2804
2023-11-26 02:20:40,326:INFO:  Epoch 219/600:  train Loss: 27.6027   val Loss: 30.3718   time: 89.31s   best: 30.2804
2023-11-26 02:22:09,670:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 02:22:09,689:INFO:  Epoch 220/600:  train Loss: 27.5921   val Loss: 30.0615   time: 89.33s   best: 30.0615
2023-11-26 02:23:39,510:INFO:  Epoch 221/600:  train Loss: 27.7542   val Loss: 31.4065   time: 89.82s   best: 30.0615
2023-11-26 02:25:08,996:INFO:  Epoch 222/600:  train Loss: 27.7931   val Loss: 30.8455   time: 89.47s   best: 30.0615
2023-11-26 02:26:38,179:INFO:  Epoch 223/600:  train Loss: 27.6443   val Loss: 30.5875   time: 89.18s   best: 30.0615
2023-11-26 02:28:07,376:INFO:  Epoch 224/600:  train Loss: 27.6286   val Loss: 31.0014   time: 89.19s   best: 30.0615
2023-11-26 02:29:36,992:INFO:  Epoch 225/600:  train Loss: 27.6064   val Loss: 30.2667   time: 89.62s   best: 30.0615
2023-11-26 02:31:06,537:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 02:31:06,556:INFO:  Epoch 226/600:  train Loss: 27.5160   val Loss: 29.6538   time: 89.54s   best: 29.6538
2023-11-26 02:32:36,257:INFO:  Epoch 227/600:  train Loss: 27.2734   val Loss: 30.0421   time: 89.70s   best: 29.6538
2023-11-26 02:34:05,472:INFO:  Epoch 228/600:  train Loss: 27.9417   val Loss: 30.7135   time: 89.21s   best: 29.6538
2023-11-26 02:35:34,833:INFO:  Epoch 229/600:  train Loss: 27.7628   val Loss: 30.8502   time: 89.36s   best: 29.6538
2023-11-26 02:37:04,131:INFO:  Epoch 230/600:  train Loss: 27.2941   val Loss: 30.0167   time: 89.29s   best: 29.6538
2023-11-26 02:38:33,332:INFO:  Epoch 231/600:  train Loss: 28.2688   val Loss: 31.3590   time: 89.19s   best: 29.6538
2023-11-26 02:40:02,621:INFO:  Epoch 232/600:  train Loss: 27.7463   val Loss: 31.2352   time: 89.29s   best: 29.6538
2023-11-26 02:41:31,967:INFO:  Epoch 233/600:  train Loss: 27.3508   val Loss: 30.5179   time: 89.35s   best: 29.6538
2023-11-26 02:43:01,426:INFO:  Epoch 234/600:  train Loss: 27.4618   val Loss: 34.5477   time: 89.45s   best: 29.6538
2023-11-26 02:44:30,766:INFO:  Epoch 235/600:  train Loss: 27.3439   val Loss: 30.1252   time: 89.33s   best: 29.6538
2023-11-26 02:46:00,147:INFO:  Epoch 236/600:  train Loss: 27.1497   val Loss: 30.2208   time: 89.38s   best: 29.6538
2023-11-26 02:47:29,743:INFO:  Epoch 237/600:  train Loss: 27.6122   val Loss: 33.3376   time: 89.60s   best: 29.6538
2023-11-26 02:48:59,434:INFO:  Epoch 238/600:  train Loss: 27.1665   val Loss: 30.3782   time: 89.69s   best: 29.6538
2023-11-26 02:50:28,824:INFO:  Epoch 239/600:  train Loss: 27.0520   val Loss: 30.6090   time: 89.38s   best: 29.6538
2023-11-26 02:51:58,768:INFO:  Epoch 240/600:  train Loss: 26.9926   val Loss: 30.1094   time: 89.93s   best: 29.6538
2023-11-26 02:53:28,377:INFO:  Epoch 241/600:  train Loss: 26.8800   val Loss: 29.9445   time: 89.61s   best: 29.6538
2023-11-26 02:54:57,975:INFO:  Epoch 242/600:  train Loss: 28.0047   val Loss: 30.1547   time: 89.60s   best: 29.6538
2023-11-26 02:56:27,170:INFO:  Epoch 243/600:  train Loss: 26.7993   val Loss: 29.7866   time: 89.19s   best: 29.6538
2023-11-26 02:57:57,062:INFO:  Epoch 244/600:  train Loss: 27.0157   val Loss: 30.3048   time: 89.88s   best: 29.6538
2023-11-26 02:59:26,529:INFO:  Epoch 245/600:  train Loss: 26.9247   val Loss: 30.3850   time: 89.46s   best: 29.6538
2023-11-26 03:00:56,344:INFO:  Epoch 246/600:  train Loss: 27.1591   val Loss: 30.4856   time: 89.81s   best: 29.6538
2023-11-26 03:02:26,221:INFO:  Epoch 247/600:  train Loss: 27.3418   val Loss: 30.5349   time: 89.87s   best: 29.6538
2023-11-26 03:03:56,333:INFO:  Epoch 248/600:  train Loss: 26.8571   val Loss: 29.7764   time: 90.10s   best: 29.6538
2023-11-26 03:05:25,807:INFO:  Epoch 249/600:  train Loss: 27.2196   val Loss: 29.7666   time: 89.46s   best: 29.6538
2023-11-26 03:06:55,776:INFO:  Epoch 250/600:  train Loss: 26.6243   val Loss: 29.9199   time: 89.96s   best: 29.6538
2023-11-26 03:08:25,619:INFO:  Epoch 251/600:  train Loss: 26.7079   val Loss: 29.8398   time: 89.84s   best: 29.6538
2023-11-26 03:09:55,474:INFO:  Epoch 252/600:  train Loss: 26.5231   val Loss: 29.9840   time: 89.84s   best: 29.6538
2023-11-26 03:11:25,124:INFO:  Epoch 253/600:  train Loss: 27.3765   val Loss: 29.9416   time: 89.65s   best: 29.6538
2023-11-26 03:12:54,792:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 03:12:54,811:INFO:  Epoch 254/600:  train Loss: 26.5285   val Loss: 29.6389   time: 89.65s   best: 29.6389
2023-11-26 03:14:24,721:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 03:14:24,740:INFO:  Epoch 255/600:  train Loss: 26.5457   val Loss: 29.4524   time: 89.89s   best: 29.4524
2023-11-26 03:15:54,412:INFO:  Epoch 256/600:  train Loss: 26.4311   val Loss: 29.6495   time: 89.66s   best: 29.4524
2023-11-26 03:17:24,463:INFO:  Epoch 257/600:  train Loss: 26.6330   val Loss: 29.7266   time: 90.05s   best: 29.4524
2023-11-26 03:18:53,718:INFO:  Epoch 258/600:  train Loss: 26.4098   val Loss: 29.7689   time: 89.25s   best: 29.4524
2023-11-26 03:20:23,141:INFO:  Epoch 259/600:  train Loss: 26.3630   val Loss: 29.7154   time: 89.42s   best: 29.4524
2023-11-26 03:21:53,312:INFO:  Epoch 260/600:  train Loss: 26.3157   val Loss: 29.5075   time: 90.16s   best: 29.4524
2023-11-26 03:23:23,130:INFO:  Epoch 261/600:  train Loss: 26.2781   val Loss: 29.5943   time: 89.81s   best: 29.4524
2023-11-26 03:24:52,552:INFO:  Epoch 262/600:  train Loss: 26.2372   val Loss: 29.5625   time: 89.42s   best: 29.4524
2023-11-26 03:26:21,939:INFO:  Epoch 263/600:  train Loss: 26.4637   val Loss: 30.7514   time: 89.37s   best: 29.4524
2023-11-26 03:27:51,259:INFO:  Epoch 264/600:  train Loss: 26.1566   val Loss: 29.7152   time: 89.31s   best: 29.4524
2023-11-26 03:29:20,583:INFO:  Epoch 265/600:  train Loss: 26.1471   val Loss: 29.4849   time: 89.31s   best: 29.4524
2023-11-26 03:30:50,370:INFO:  Epoch 266/600:  train Loss: 26.3934   val Loss: 29.6143   time: 89.79s   best: 29.4524
2023-11-26 03:32:19,731:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 03:32:19,750:INFO:  Epoch 267/600:  train Loss: 26.1514   val Loss: 29.3500   time: 89.36s   best: 29.3500
2023-11-26 03:33:49,160:INFO:  Epoch 268/600:  train Loss: 26.7340   val Loss: 32.2402   time: 89.41s   best: 29.3500
2023-11-26 03:35:18,699:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 03:35:18,718:INFO:  Epoch 269/600:  train Loss: 26.9306   val Loss: 29.0898   time: 89.51s   best: 29.0898
2023-11-26 03:36:47,974:INFO:  Epoch 270/600:  train Loss: 26.1010   val Loss: 29.1868   time: 89.24s   best: 29.0898
2023-11-26 03:38:17,030:INFO:  Epoch 271/600:  train Loss: 25.9062   val Loss: 29.4062   time: 89.05s   best: 29.0898
2023-11-26 03:39:46,268:INFO:  Epoch 272/600:  train Loss: 25.9214   val Loss: 29.3606   time: 89.24s   best: 29.0898
2023-11-26 03:41:15,466:INFO:  Epoch 273/600:  train Loss: 25.9659   val Loss: 29.1686   time: 89.19s   best: 29.0898
2023-11-26 03:42:44,741:INFO:  Epoch 274/600:  train Loss: 26.5805   val Loss: 30.1356   time: 89.26s   best: 29.0898
2023-11-26 03:44:14,319:INFO:  Epoch 275/600:  train Loss: 26.0102   val Loss: 29.7777   time: 89.57s   best: 29.0898
2023-11-26 03:45:44,080:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 03:45:44,099:INFO:  Epoch 276/600:  train Loss: 25.7967   val Loss: 29.0585   time: 89.74s   best: 29.0585
2023-11-26 03:47:13,549:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 03:47:13,568:INFO:  Epoch 277/600:  train Loss: 25.9005   val Loss: 28.8317   time: 89.45s   best: 28.8317
2023-11-26 03:48:43,024:INFO:  Epoch 278/600:  train Loss: 27.4076   val Loss: 30.4360   time: 89.44s   best: 28.8317
2023-11-26 03:50:12,307:INFO:  Epoch 279/600:  train Loss: 25.9713   val Loss: 29.1545   time: 89.28s   best: 28.8317
2023-11-26 03:51:41,664:INFO:  Epoch 280/600:  train Loss: 25.7715   val Loss: 32.7925   time: 89.36s   best: 28.8317
2023-11-26 03:53:11,029:INFO:  Epoch 281/600:  train Loss: 25.8547   val Loss: 29.0373   time: 89.35s   best: 28.8317
2023-11-26 03:54:40,311:INFO:  Epoch 282/600:  train Loss: 25.7106   val Loss: 29.2936   time: 89.27s   best: 28.8317
2023-11-26 03:56:09,416:INFO:  Epoch 283/600:  train Loss: 25.8718   val Loss: 28.9401   time: 89.10s   best: 28.8317
2023-11-26 03:57:38,662:INFO:  Epoch 284/600:  train Loss: 26.4214   val Loss: 30.7223   time: 89.24s   best: 28.8317
2023-11-26 03:59:07,875:INFO:  Epoch 285/600:  train Loss: 26.2253   val Loss: 29.2279   time: 89.20s   best: 28.8317
2023-11-26 04:00:37,173:INFO:  Epoch 286/600:  train Loss: 25.5427   val Loss: 28.9581   time: 89.29s   best: 28.8317
2023-11-26 04:02:06,486:INFO:  Epoch 287/600:  train Loss: 27.3843   val Loss: 31.1998   time: 89.31s   best: 28.8317
2023-11-26 04:03:35,840:INFO:  Epoch 288/600:  train Loss: 26.5385   val Loss: 28.9071   time: 89.35s   best: 28.8317
2023-11-26 04:05:05,138:INFO:  Epoch 289/600:  train Loss: 26.7145   val Loss: 29.0795   time: 89.29s   best: 28.8317
2023-11-26 04:06:34,859:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 04:06:34,877:INFO:  Epoch 290/600:  train Loss: 25.6605   val Loss: 28.7456   time: 89.70s   best: 28.7456
2023-11-26 04:08:04,063:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 04:08:04,082:INFO:  Epoch 291/600:  train Loss: 25.4419   val Loss: 28.4882   time: 89.17s   best: 28.4882
2023-11-26 04:09:33,502:INFO:  Epoch 292/600:  train Loss: 26.0013   val Loss: 28.8922   time: 89.42s   best: 28.4882
2023-11-26 04:11:03,370:INFO:  Epoch 293/600:  train Loss: 25.4766   val Loss: 28.7353   time: 89.86s   best: 28.4882
2023-11-26 04:12:32,574:INFO:  Epoch 294/600:  train Loss: 25.9704   val Loss: 29.5128   time: 89.19s   best: 28.4882
2023-11-26 04:14:01,840:INFO:  Epoch 295/600:  train Loss: 25.4870   val Loss: 28.8841   time: 89.27s   best: 28.4882
2023-11-26 04:15:31,457:INFO:  Epoch 296/600:  train Loss: 25.4076   val Loss: 28.9837   time: 89.61s   best: 28.4882
2023-11-26 04:17:00,728:INFO:  Epoch 297/600:  train Loss: 25.7980   val Loss: 28.8981   time: 89.26s   best: 28.4882
2023-11-26 04:18:30,295:INFO:  Epoch 298/600:  train Loss: 25.7275   val Loss: 29.1412   time: 89.57s   best: 28.4882
2023-11-26 04:19:59,929:INFO:  Epoch 299/600:  train Loss: 25.7153   val Loss: 28.5856   time: 89.63s   best: 28.4882
2023-11-26 04:21:29,010:INFO:  Epoch 300/600:  train Loss: 25.5863   val Loss: 29.9051   time: 89.07s   best: 28.4882
2023-11-26 04:22:58,219:INFO:  Epoch 301/600:  train Loss: 26.8652   val Loss: 28.9425   time: 89.21s   best: 28.4882
2023-11-26 04:24:27,300:INFO:  Epoch 302/600:  train Loss: 25.3803   val Loss: 28.5207   time: 89.08s   best: 28.4882
2023-11-26 04:25:56,968:INFO:  Epoch 303/600:  train Loss: 25.1854   val Loss: 28.6234   time: 89.65s   best: 28.4882
2023-11-26 04:27:26,571:INFO:  Epoch 304/600:  train Loss: 25.4558   val Loss: 28.7134   time: 89.60s   best: 28.4882
2023-11-26 04:28:56,540:INFO:  Epoch 305/600:  train Loss: 25.1511   val Loss: 28.7315   time: 89.96s   best: 28.4882
2023-11-26 04:30:25,948:INFO:  Epoch 306/600:  train Loss: 25.2382   val Loss: 28.5213   time: 89.40s   best: 28.4882
2023-11-26 04:31:55,503:INFO:  Epoch 307/600:  train Loss: 25.3237   val Loss: 30.1302   time: 89.54s   best: 28.4882
2023-11-26 04:33:25,489:INFO:  Epoch 308/600:  train Loss: 25.7075   val Loss: 28.8266   time: 89.97s   best: 28.4882
2023-11-26 04:34:54,745:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 04:34:54,764:INFO:  Epoch 309/600:  train Loss: 25.1207   val Loss: 28.3139   time: 89.25s   best: 28.3139
2023-11-26 04:36:23,865:INFO:  Epoch 310/600:  train Loss: 25.4476   val Loss: 28.7549   time: 89.10s   best: 28.3139
2023-11-26 04:37:53,429:INFO:  Epoch 311/600:  train Loss: 25.1899   val Loss: 28.5009   time: 89.55s   best: 28.3139
2023-11-26 04:39:22,680:INFO:  Epoch 312/600:  train Loss: 25.0383   val Loss: 28.4334   time: 89.24s   best: 28.3139
2023-11-26 04:40:52,190:INFO:  Epoch 313/600:  train Loss: 25.0396   val Loss: 29.1992   time: 89.51s   best: 28.3139
2023-11-26 04:42:21,451:INFO:  Epoch 314/600:  train Loss: 25.1256   val Loss: 28.5742   time: 89.26s   best: 28.3139
2023-11-26 04:43:50,658:INFO:  Epoch 315/600:  train Loss: 25.2113   val Loss: 28.4068   time: 89.21s   best: 28.3139
2023-11-26 04:45:19,837:INFO:  Epoch 316/600:  train Loss: 25.2776   val Loss: 29.4838   time: 89.17s   best: 28.3139
2023-11-26 04:46:49,279:INFO:  Epoch 317/600:  train Loss: 24.9870   val Loss: 28.5867   time: 89.44s   best: 28.3139
2023-11-26 04:48:18,743:INFO:  Epoch 318/600:  train Loss: 25.1243   val Loss: 28.6636   time: 89.46s   best: 28.3139
2023-11-26 04:49:48,659:INFO:  Epoch 319/600:  train Loss: 25.8341   val Loss: 30.1093   time: 89.90s   best: 28.3139
2023-11-26 04:51:18,166:INFO:  Epoch 320/600:  train Loss: 25.3600   val Loss: 28.7583   time: 89.51s   best: 28.3139
2023-11-26 04:52:47,577:INFO:  Epoch 321/600:  train Loss: 24.8510   val Loss: 28.4319   time: 89.41s   best: 28.3139
2023-11-26 04:54:16,783:INFO:  Epoch 322/600:  train Loss: 24.9833   val Loss: 28.4429   time: 89.20s   best: 28.3139
2023-11-26 04:55:46,602:INFO:  Epoch 323/600:  train Loss: 24.8584   val Loss: 29.1693   time: 89.82s   best: 28.3139
2023-11-26 04:57:16,171:INFO:  Epoch 324/600:  train Loss: 24.8196   val Loss: 28.5001   time: 89.57s   best: 28.3139
2023-11-26 04:58:45,632:INFO:  Epoch 325/600:  train Loss: 25.0978   val Loss: 28.3758   time: 89.46s   best: 28.3139
2023-11-26 05:00:15,145:INFO:  Epoch 326/600:  train Loss: 24.9085   val Loss: 28.3196   time: 89.50s   best: 28.3139
2023-11-26 05:01:44,644:INFO:  Epoch 327/600:  train Loss: 25.0165   val Loss: 29.8503   time: 89.50s   best: 28.3139
2023-11-26 05:03:14,173:INFO:  Epoch 328/600:  train Loss: 24.8359   val Loss: 28.5002   time: 89.53s   best: 28.3139
2023-11-26 05:04:43,785:INFO:  Epoch 329/600:  train Loss: 24.7331   val Loss: 28.5323   time: 89.61s   best: 28.3139
2023-11-26 05:06:13,267:INFO:  Epoch 330/600:  train Loss: 24.6653   val Loss: 28.7986   time: 89.47s   best: 28.3139
2023-11-26 05:07:43,100:INFO:  Epoch 331/600:  train Loss: 25.3558   val Loss: 29.0722   time: 89.83s   best: 28.3139
2023-11-26 05:09:12,636:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 05:09:12,655:INFO:  Epoch 332/600:  train Loss: 24.7874   val Loss: 28.2578   time: 89.53s   best: 28.2578
2023-11-26 05:10:42,156:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 05:10:42,185:INFO:  Epoch 333/600:  train Loss: 24.5824   val Loss: 28.1812   time: 89.50s   best: 28.1812
2023-11-26 05:12:11,627:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 05:12:11,646:INFO:  Epoch 334/600:  train Loss: 24.6908   val Loss: 28.1309   time: 89.44s   best: 28.1309
2023-11-26 05:13:40,844:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 05:13:40,863:INFO:  Epoch 335/600:  train Loss: 24.6808   val Loss: 28.1251   time: 89.18s   best: 28.1251
2023-11-26 05:15:10,536:INFO:  Epoch 336/600:  train Loss: 24.7959   val Loss: 28.2855   time: 89.66s   best: 28.1251
2023-11-26 05:16:40,115:INFO:  Epoch 337/600:  train Loss: 24.4850   val Loss: 28.1707   time: 89.57s   best: 28.1251
2023-11-26 05:18:09,460:INFO:  Epoch 338/600:  train Loss: 25.5422   val Loss: 28.2804   time: 89.33s   best: 28.1251
2023-11-26 05:19:38,706:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 05:19:38,725:INFO:  Epoch 339/600:  train Loss: 24.5159   val Loss: 27.9730   time: 89.23s   best: 27.9730
2023-11-26 05:21:07,853:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 05:21:07,872:INFO:  Epoch 340/600:  train Loss: 24.5203   val Loss: 27.8855   time: 89.11s   best: 27.8855
2023-11-26 05:22:37,530:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 05:22:37,550:INFO:  Epoch 341/600:  train Loss: 24.5651   val Loss: 27.8433   time: 89.65s   best: 27.8433
2023-11-26 05:24:07,112:INFO:  Epoch 342/600:  train Loss: 24.5821   val Loss: 28.0894   time: 89.55s   best: 27.8433
2023-11-26 05:25:36,777:INFO:  Epoch 343/600:  train Loss: 25.2658   val Loss: 29.2568   time: 89.66s   best: 27.8433
2023-11-26 05:27:06,818:INFO:  Epoch 344/600:  train Loss: 24.8472   val Loss: 28.6484   time: 90.04s   best: 27.8433
2023-11-26 05:28:36,290:INFO:  Epoch 345/600:  train Loss: 24.4906   val Loss: 28.3201   time: 89.47s   best: 27.8433
2023-11-26 05:30:05,474:INFO:  Epoch 346/600:  train Loss: 25.9702   val Loss: 29.5252   time: 89.18s   best: 27.8433
2023-11-26 05:31:35,061:INFO:  Epoch 347/600:  train Loss: 24.7833   val Loss: 28.1089   time: 89.59s   best: 27.8433
2023-11-26 05:33:04,257:INFO:  Epoch 348/600:  train Loss: 25.0157   val Loss: 28.4472   time: 89.20s   best: 27.8433
2023-11-26 05:34:33,604:INFO:  Epoch 349/600:  train Loss: 24.4018   val Loss: 27.8887   time: 89.34s   best: 27.8433
2023-11-26 05:36:03,196:INFO:  Epoch 350/600:  train Loss: 24.5054   val Loss: 30.3466   time: 89.58s   best: 27.8433
2023-11-26 05:37:32,335:INFO:  Epoch 351/600:  train Loss: 24.5945   val Loss: 28.2307   time: 89.13s   best: 27.8433
2023-11-26 05:39:01,856:INFO:  Epoch 352/600:  train Loss: 24.2399   val Loss: 28.3744   time: 89.52s   best: 27.8433
2023-11-26 05:40:31,292:INFO:  Epoch 353/600:  train Loss: 24.4803   val Loss: 28.1336   time: 89.43s   best: 27.8433
2023-11-26 05:42:00,527:INFO:  Epoch 354/600:  train Loss: 24.3072   val Loss: 28.0871   time: 89.23s   best: 27.8433
2023-11-26 05:43:29,721:INFO:  Epoch 355/600:  train Loss: 24.4404   val Loss: 28.8402   time: 89.19s   best: 27.8433
2023-11-26 05:44:59,191:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 05:44:59,209:INFO:  Epoch 356/600:  train Loss: 24.2365   val Loss: 27.7968   time: 89.44s   best: 27.7968
2023-11-26 05:46:28,321:INFO:  Epoch 357/600:  train Loss: 24.1760   val Loss: 27.8681   time: 89.11s   best: 27.7968
2023-11-26 05:47:57,475:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 05:47:57,493:INFO:  Epoch 358/600:  train Loss: 24.2107   val Loss: 27.6159   time: 89.15s   best: 27.6159
2023-11-26 05:49:26,812:INFO:  Epoch 359/600:  train Loss: 24.2162   val Loss: 28.1124   time: 89.32s   best: 27.6159
2023-11-26 05:50:56,437:INFO:  Epoch 360/600:  train Loss: 24.2203   val Loss: 28.3918   time: 89.61s   best: 27.6159
2023-11-26 05:52:25,857:INFO:  Epoch 361/600:  train Loss: 24.4189   val Loss: 28.4546   time: 89.42s   best: 27.6159
2023-11-26 05:53:55,065:INFO:  Epoch 362/600:  train Loss: 24.1138   val Loss: 28.0705   time: 89.20s   best: 27.6159
2023-11-26 05:55:24,284:INFO:  Epoch 363/600:  train Loss: 24.3390   val Loss: 29.1590   time: 89.21s   best: 27.6159
2023-11-26 05:56:53,338:INFO:  Epoch 364/600:  train Loss: 24.2590   val Loss: 27.8010   time: 89.05s   best: 27.6159
2023-11-26 05:58:22,788:INFO:  Epoch 365/600:  train Loss: 24.4224   val Loss: 28.1067   time: 89.45s   best: 27.6159
2023-11-26 05:59:52,412:INFO:  Epoch 366/600:  train Loss: 24.3276   val Loss: 28.1022   time: 89.62s   best: 27.6159
2023-11-26 06:01:22,014:INFO:  Epoch 367/600:  train Loss: 24.0081   val Loss: 27.9549   time: 89.60s   best: 27.6159
2023-11-26 06:02:51,925:INFO:  Epoch 368/600:  train Loss: 24.2691   val Loss: 28.1471   time: 89.90s   best: 27.6159
2023-11-26 06:04:21,710:INFO:  Epoch 369/600:  train Loss: 24.2113   val Loss: 27.6291   time: 89.77s   best: 27.6159
2023-11-26 06:05:51,836:INFO:  Epoch 370/600:  train Loss: 23.8736   val Loss: 27.9485   time: 90.11s   best: 27.6159
2023-11-26 06:07:21,926:INFO:  Epoch 371/600:  train Loss: 23.9543   val Loss: 28.5762   time: 90.08s   best: 27.6159
2023-11-26 06:08:51,023:INFO:  Epoch 372/600:  train Loss: 23.8993   val Loss: 28.0499   time: 89.09s   best: 27.6159
2023-11-26 06:10:20,107:INFO:  Epoch 373/600:  train Loss: 24.5575   val Loss: 28.6078   time: 89.07s   best: 27.6159
2023-11-26 06:11:49,627:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 06:11:49,646:INFO:  Epoch 374/600:  train Loss: 24.3910   val Loss: 27.5422   time: 89.52s   best: 27.5422
2023-11-26 06:13:19,098:INFO:  Epoch 375/600:  train Loss: 23.7389   val Loss: 27.8656   time: 89.45s   best: 27.5422
2023-11-26 06:14:48,484:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 06:14:48,503:INFO:  Epoch 376/600:  train Loss: 23.9784   val Loss: 27.4784   time: 89.37s   best: 27.4784
2023-11-26 06:16:17,980:INFO:  Epoch 377/600:  train Loss: 23.8093   val Loss: 27.9403   time: 89.48s   best: 27.4784
2023-11-26 06:17:47,109:INFO:  Epoch 378/600:  train Loss: 24.6390   val Loss: 28.3451   time: 89.13s   best: 27.4784
2023-11-26 06:19:16,327:INFO:  Epoch 379/600:  train Loss: 24.4113   val Loss: 28.9731   time: 89.22s   best: 27.4784
2023-11-26 06:20:46,017:INFO:  Epoch 380/600:  train Loss: 24.5314   val Loss: 27.8601   time: 89.68s   best: 27.4784
2023-11-26 06:22:15,650:INFO:  Epoch 381/600:  train Loss: 23.8112   val Loss: 27.7374   time: 89.62s   best: 27.4784
2023-11-26 06:23:45,088:INFO:  Epoch 382/600:  train Loss: 23.7096   val Loss: 28.0008   time: 89.42s   best: 27.4784
2023-11-26 06:25:14,822:INFO:  Epoch 383/600:  train Loss: 23.7769   val Loss: 27.6543   time: 89.72s   best: 27.4784
2023-11-26 06:26:44,042:INFO:  Epoch 384/600:  train Loss: 23.8531   val Loss: 28.4132   time: 89.22s   best: 27.4784
2023-11-26 06:28:13,346:INFO:  Epoch 385/600:  train Loss: 23.9856   val Loss: 28.4459   time: 89.29s   best: 27.4784
2023-11-26 06:29:42,716:INFO:  Epoch 386/600:  train Loss: 24.0437   val Loss: 27.5167   time: 89.36s   best: 27.4784
2023-11-26 06:31:12,159:INFO:  Epoch 387/600:  train Loss: 23.8140   val Loss: 28.1969   time: 89.44s   best: 27.4784
2023-11-26 06:32:41,435:INFO:  Epoch 388/600:  train Loss: 23.8135   val Loss: 28.1646   time: 89.27s   best: 27.4784
2023-11-26 06:34:10,801:INFO:  Epoch 389/600:  train Loss: 25.2230   val Loss: 35.0423   time: 89.36s   best: 27.4784
2023-11-26 06:35:40,199:INFO:  Epoch 390/600:  train Loss: 25.8676   val Loss: 28.3517   time: 89.40s   best: 27.4784
2023-11-26 06:37:10,037:INFO:  Epoch 391/600:  train Loss: 25.4106   val Loss: 28.1453   time: 89.83s   best: 27.4784
2023-11-26 06:38:39,675:INFO:  Epoch 392/600:  train Loss: 24.1327   val Loss: 28.4385   time: 89.63s   best: 27.4784
2023-11-26 06:40:09,249:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 06:40:09,268:INFO:  Epoch 393/600:  train Loss: 23.7616   val Loss: 27.2049   time: 89.56s   best: 27.2049
2023-11-26 06:41:38,437:INFO:  Epoch 394/600:  train Loss: 23.7115   val Loss: 27.3679   time: 89.17s   best: 27.2049
2023-11-26 06:43:07,829:INFO:  Epoch 395/600:  train Loss: 23.5921   val Loss: 27.8503   time: 89.38s   best: 27.2049
2023-11-26 06:44:37,476:INFO:  Epoch 396/600:  train Loss: 23.8353   val Loss: 27.4758   time: 89.65s   best: 27.2049
2023-11-26 06:46:06,793:INFO:  Epoch 397/600:  train Loss: 24.2271   val Loss: 27.6251   time: 89.30s   best: 27.2049
2023-11-26 06:47:36,200:INFO:  Epoch 398/600:  train Loss: 23.6397   val Loss: 27.3544   time: 89.40s   best: 27.2049
2023-11-26 06:49:05,581:INFO:  Epoch 399/600:  train Loss: 23.5428   val Loss: 27.3995   time: 89.37s   best: 27.2049
2023-11-26 06:50:34,955:INFO:  Epoch 400/600:  train Loss: 23.5591   val Loss: 27.4682   time: 89.37s   best: 27.2049
2023-11-26 06:52:04,210:INFO:  Epoch 401/600:  train Loss: 23.7604   val Loss: 27.7927   time: 89.24s   best: 27.2049
2023-11-26 06:53:33,621:INFO:  Epoch 402/600:  train Loss: 23.6445   val Loss: 27.4165   time: 89.41s   best: 27.2049
2023-11-26 06:55:02,828:INFO:  Epoch 403/600:  train Loss: 23.8492   val Loss: 28.5884   time: 89.19s   best: 27.2049
2023-11-26 06:56:31,845:INFO:  Epoch 404/600:  train Loss: 23.8507   val Loss: 27.5378   time: 89.02s   best: 27.2049
2023-11-26 06:58:00,948:INFO:  Epoch 405/600:  train Loss: 23.7133   val Loss: 27.5672   time: 89.09s   best: 27.2049
2023-11-26 06:59:30,374:INFO:  Epoch 406/600:  train Loss: 23.9197   val Loss: 28.0783   time: 89.42s   best: 27.2049
2023-11-26 07:00:59,545:INFO:  Epoch 407/600:  train Loss: 23.4957   val Loss: 27.7362   time: 89.17s   best: 27.2049
2023-11-26 07:02:28,646:INFO:  Epoch 408/600:  train Loss: 23.6055   val Loss: 27.3871   time: 89.09s   best: 27.2049
2023-11-26 07:03:57,685:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 07:03:57,705:INFO:  Epoch 409/600:  train Loss: 23.4645   val Loss: 26.9442   time: 89.03s   best: 26.9442
2023-11-26 07:05:27,100:INFO:  Epoch 410/600:  train Loss: 23.3895   val Loss: 27.3716   time: 89.38s   best: 26.9442
2023-11-26 07:06:56,264:INFO:  Epoch 411/600:  train Loss: 23.7287   val Loss: 28.8609   time: 89.16s   best: 26.9442
2023-11-26 07:08:25,238:INFO:  Epoch 412/600:  train Loss: 23.4799   val Loss: 27.4699   time: 88.97s   best: 26.9442
2023-11-26 07:09:54,183:INFO:  Epoch 413/600:  train Loss: 23.4519   val Loss: 26.9639   time: 88.94s   best: 26.9442
2023-11-26 07:11:23,363:INFO:  Epoch 414/600:  train Loss: 23.5533   val Loss: 27.2442   time: 89.17s   best: 26.9442
2023-11-26 07:12:52,291:INFO:  Epoch 415/600:  train Loss: 23.3088   val Loss: 27.3685   time: 88.93s   best: 26.9442
2023-11-26 07:14:21,144:INFO:  Epoch 416/600:  train Loss: 23.6007   val Loss: 27.6797   time: 88.85s   best: 26.9442
2023-11-26 07:15:50,056:INFO:  Epoch 417/600:  train Loss: 23.5161   val Loss: 27.6184   time: 88.91s   best: 26.9442
2023-11-26 07:17:19,788:INFO:  Epoch 418/600:  train Loss: 23.4959   val Loss: 27.6740   time: 89.72s   best: 26.9442
2023-11-26 07:18:48,848:INFO:  Epoch 419/600:  train Loss: 23.1703   val Loss: 27.2661   time: 89.05s   best: 26.9442
2023-11-26 07:20:18,320:INFO:  Epoch 420/600:  train Loss: 23.2292   val Loss: 27.0733   time: 89.46s   best: 26.9442
2023-11-26 07:21:47,224:INFO:  Epoch 421/600:  train Loss: 23.4874   val Loss: 27.3458   time: 88.90s   best: 26.9442
2023-11-26 07:23:16,599:INFO:  Epoch 422/600:  train Loss: 23.2622   val Loss: 27.3212   time: 89.36s   best: 26.9442
2023-11-26 07:24:45,509:INFO:  Epoch 423/600:  train Loss: 23.6236   val Loss: 28.4649   time: 88.91s   best: 26.9442
2023-11-26 07:26:14,352:INFO:  Epoch 424/600:  train Loss: 23.9719   val Loss: 27.7360   time: 88.84s   best: 26.9442
2023-11-26 07:27:43,289:INFO:  Epoch 425/600:  train Loss: 23.2191   val Loss: 27.3132   time: 88.94s   best: 26.9442
2023-11-26 07:29:12,215:INFO:  Epoch 426/600:  train Loss: 23.2140   val Loss: 27.5882   time: 88.92s   best: 26.9442
2023-11-26 07:30:41,249:INFO:  Epoch 427/600:  train Loss: 23.1294   val Loss: 27.5826   time: 89.03s   best: 26.9442
2023-11-26 07:32:10,226:INFO:  Epoch 428/600:  train Loss: 23.1456   val Loss: 28.1845   time: 88.98s   best: 26.9442
2023-11-26 07:33:39,216:INFO:  Epoch 429/600:  train Loss: 23.5425   val Loss: 27.2926   time: 88.99s   best: 26.9442
2023-11-26 07:35:08,962:INFO:  Epoch 430/600:  train Loss: 23.1942   val Loss: 27.7144   time: 89.75s   best: 26.9442
2023-11-26 07:36:37,884:INFO:  Epoch 431/600:  train Loss: 23.2040   val Loss: 27.6447   time: 88.92s   best: 26.9442
2023-11-26 07:38:06,821:INFO:  Epoch 432/600:  train Loss: 23.1180   val Loss: 27.2574   time: 88.93s   best: 26.9442
2023-11-26 07:39:36,255:INFO:  Epoch 433/600:  train Loss: 23.6482   val Loss: 27.3489   time: 89.42s   best: 26.9442
2023-11-26 07:41:05,419:INFO:  Epoch 434/600:  train Loss: 23.0075   val Loss: 27.1693   time: 89.15s   best: 26.9442
2023-11-26 07:42:34,338:INFO:  Epoch 435/600:  train Loss: 23.0581   val Loss: 27.2323   time: 88.91s   best: 26.9442
2023-11-26 07:44:03,222:INFO:  Epoch 436/600:  train Loss: 23.5033   val Loss: 27.4335   time: 88.88s   best: 26.9442
2023-11-26 07:45:32,169:INFO:  Epoch 437/600:  train Loss: 23.0438   val Loss: 27.0587   time: 88.95s   best: 26.9442
2023-11-26 07:47:01,045:INFO:  Epoch 438/600:  train Loss: 23.0080   val Loss: 27.4858   time: 88.88s   best: 26.9442
2023-11-26 07:48:29,919:INFO:  Epoch 439/600:  train Loss: 23.4129   val Loss: 27.5250   time: 88.87s   best: 26.9442
2023-11-26 07:49:58,731:INFO:  Epoch 440/600:  train Loss: 23.0183   val Loss: 27.7505   time: 88.81s   best: 26.9442
2023-11-26 07:51:27,711:INFO:  Epoch 441/600:  train Loss: 23.6197   val Loss: 31.6557   time: 88.97s   best: 26.9442
2023-11-26 07:52:57,022:INFO:  Epoch 442/600:  train Loss: 23.8926   val Loss: 27.0389   time: 89.31s   best: 26.9442
2023-11-26 07:54:26,313:INFO:  Epoch 443/600:  train Loss: 23.9818   val Loss: 27.3828   time: 89.28s   best: 26.9442
2023-11-26 07:55:55,762:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 07:55:55,782:INFO:  Epoch 444/600:  train Loss: 23.2013   val Loss: 26.9394   time: 89.43s   best: 26.9394
2023-11-26 07:57:24,879:INFO:  Epoch 445/600:  train Loss: 23.1127   val Loss: 27.0520   time: 89.09s   best: 26.9394
2023-11-26 07:58:53,910:INFO:  Epoch 446/600:  train Loss: 22.9349   val Loss: 27.3204   time: 89.02s   best: 26.9394
2023-11-26 08:00:23,095:INFO:  Epoch 447/600:  train Loss: 23.1520   val Loss: 27.0350   time: 89.17s   best: 26.9394
2023-11-26 08:01:52,730:INFO:  Epoch 448/600:  train Loss: 23.3325   val Loss: 27.9773   time: 89.62s   best: 26.9394
2023-11-26 08:03:22,004:INFO:  Epoch 449/600:  train Loss: 23.0451   val Loss: 27.2901   time: 89.26s   best: 26.9394
2023-11-26 08:04:51,144:INFO:  Epoch 450/600:  train Loss: 22.8412   val Loss: 27.8034   time: 89.13s   best: 26.9394
2023-11-26 08:06:20,602:INFO:  Epoch 451/600:  train Loss: 22.9431   val Loss: 27.0403   time: 89.46s   best: 26.9394
2023-11-26 08:07:49,786:INFO:  Epoch 452/600:  train Loss: 22.8721   val Loss: 27.0718   time: 89.18s   best: 26.9394
2023-11-26 08:09:19,367:INFO:  Epoch 453/600:  train Loss: 22.9906   val Loss: 27.1586   time: 89.57s   best: 26.9394
2023-11-26 08:10:48,798:INFO:  Epoch 454/600:  train Loss: 22.8236   val Loss: 27.3676   time: 89.43s   best: 26.9394
2023-11-26 08:12:18,048:INFO:  Epoch 455/600:  train Loss: 22.8072   val Loss: 27.3372   time: 89.25s   best: 26.9394
2023-11-26 08:13:47,401:INFO:  Epoch 456/600:  train Loss: 22.7384   val Loss: 27.3023   time: 89.35s   best: 26.9394
2023-11-26 08:15:16,441:INFO:  Epoch 457/600:  train Loss: 22.7559   val Loss: 27.4573   time: 89.04s   best: 26.9394
2023-11-26 08:16:45,419:INFO:  Epoch 458/600:  train Loss: 23.1862   val Loss: 28.6197   time: 88.97s   best: 26.9394
2023-11-26 08:18:14,353:INFO:  Epoch 459/600:  train Loss: 22.8116   val Loss: 27.0704   time: 88.92s   best: 26.9394
2023-11-26 08:19:43,629:INFO:  Epoch 460/600:  train Loss: 22.7043   val Loss: 27.1249   time: 89.28s   best: 26.9394
2023-11-26 08:21:13,117:INFO:  Epoch 461/600:  train Loss: 22.8702   val Loss: 27.3828   time: 89.49s   best: 26.9394
2023-11-26 08:22:42,929:INFO:  Epoch 462/600:  train Loss: 22.8451   val Loss: 27.3695   time: 89.80s   best: 26.9394
2023-11-26 08:24:11,863:INFO:  Epoch 463/600:  train Loss: 22.6946   val Loss: 27.2036   time: 88.93s   best: 26.9394
2023-11-26 08:25:40,876:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 08:25:40,895:INFO:  Epoch 464/600:  train Loss: 22.7661   val Loss: 26.9034   time: 89.01s   best: 26.9034
2023-11-26 08:27:09,741:INFO:  Epoch 465/600:  train Loss: 23.5598   val Loss: 27.3169   time: 88.85s   best: 26.9034
2023-11-26 08:28:38,677:INFO:  Epoch 466/600:  train Loss: 22.9309   val Loss: 27.3675   time: 88.93s   best: 26.9034
2023-11-26 08:30:07,691:INFO:  Epoch 467/600:  train Loss: 22.7085   val Loss: 27.0829   time: 89.01s   best: 26.9034
2023-11-26 08:31:36,704:INFO:  Epoch 468/600:  train Loss: 22.6066   val Loss: 27.4063   time: 89.00s   best: 26.9034
2023-11-26 08:33:05,623:INFO:  Epoch 469/600:  train Loss: 22.6382   val Loss: 27.3825   time: 88.92s   best: 26.9034
2023-11-26 08:34:35,169:INFO:  Epoch 470/600:  train Loss: 22.7135   val Loss: 27.0780   time: 89.55s   best: 26.9034
2023-11-26 08:36:04,375:INFO:  Epoch 471/600:  train Loss: 22.6185   val Loss: 27.2211   time: 89.21s   best: 26.9034
2023-11-26 08:37:33,305:INFO:  Epoch 472/600:  train Loss: 23.6067   val Loss: 27.5075   time: 88.93s   best: 26.9034
2023-11-26 08:39:02,669:INFO:  Epoch 473/600:  train Loss: 22.7584   val Loss: 27.0381   time: 89.35s   best: 26.9034
2023-11-26 08:40:31,892:INFO:  Epoch 474/600:  train Loss: 22.5259   val Loss: 27.1342   time: 89.21s   best: 26.9034
2023-11-26 08:42:01,378:INFO:  Epoch 475/600:  train Loss: 22.5912   val Loss: 27.1807   time: 89.47s   best: 26.9034
2023-11-26 08:43:30,359:INFO:  Epoch 476/600:  train Loss: 22.8598   val Loss: 27.0098   time: 88.98s   best: 26.9034
2023-11-26 08:44:59,350:INFO:  Epoch 477/600:  train Loss: 22.7451   val Loss: 26.9372   time: 88.98s   best: 26.9034
2023-11-26 08:46:28,101:INFO:  Epoch 478/600:  train Loss: 22.6090   val Loss: 28.4919   time: 88.74s   best: 26.9034
2023-11-26 08:47:56,949:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 08:47:56,968:INFO:  Epoch 479/600:  train Loss: 22.6722   val Loss: 26.6413   time: 88.84s   best: 26.6413
2023-11-26 08:49:26,344:INFO:  Epoch 480/600:  train Loss: 22.6265   val Loss: 26.9812   time: 89.36s   best: 26.6413
2023-11-26 08:50:55,322:INFO:  Epoch 481/600:  train Loss: 22.4851   val Loss: 26.9845   time: 88.98s   best: 26.6413
2023-11-26 08:52:24,270:INFO:  Epoch 482/600:  train Loss: 22.5467   val Loss: 27.1447   time: 88.94s   best: 26.6413
2023-11-26 08:53:53,260:INFO:  Epoch 483/600:  train Loss: 22.4390   val Loss: 27.1171   time: 88.98s   best: 26.6413
2023-11-26 08:55:22,393:INFO:  Epoch 484/600:  train Loss: 22.4558   val Loss: 26.9147   time: 89.13s   best: 26.6413
2023-11-26 08:56:51,531:INFO:  Epoch 485/600:  train Loss: 22.5772   val Loss: 27.0583   time: 89.13s   best: 26.6413
2023-11-26 08:58:20,762:INFO:  Epoch 486/600:  train Loss: 22.4712   val Loss: 26.9515   time: 89.23s   best: 26.6413
2023-11-26 08:59:49,856:INFO:  Epoch 487/600:  train Loss: 22.4002   val Loss: 26.8108   time: 89.08s   best: 26.6413
2023-11-26 09:01:19,049:INFO:  Epoch 488/600:  train Loss: 22.5377   val Loss: 27.1560   time: 89.18s   best: 26.6413
2023-11-26 09:02:48,179:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 09:02:48,198:INFO:  Epoch 489/600:  train Loss: 22.7527   val Loss: 26.3257   time: 89.11s   best: 26.3257
2023-11-26 09:04:17,293:INFO:  Epoch 490/600:  train Loss: 22.3562   val Loss: 26.6600   time: 89.09s   best: 26.3257
2023-11-26 09:05:46,595:INFO:  Epoch 491/600:  train Loss: 22.6816   val Loss: 26.9496   time: 89.30s   best: 26.3257
2023-11-26 09:07:16,114:INFO:  Epoch 492/600:  train Loss: 22.2686   val Loss: 27.0584   time: 89.52s   best: 26.3257
2023-11-26 09:08:45,193:INFO:  Epoch 493/600:  train Loss: 22.7188   val Loss: 27.2443   time: 89.08s   best: 26.3257
2023-11-26 09:10:14,574:INFO:  Epoch 494/600:  train Loss: 22.2481   val Loss: 27.0360   time: 89.38s   best: 26.3257
2023-11-26 09:11:43,551:INFO:  Epoch 495/600:  train Loss: 22.3037   val Loss: 27.5696   time: 88.97s   best: 26.3257
2023-11-26 09:13:12,432:INFO:  Epoch 496/600:  train Loss: 23.4830   val Loss: 27.6291   time: 88.88s   best: 26.3257
2023-11-26 09:14:41,333:INFO:  Epoch 497/600:  train Loss: 22.6599   val Loss: 26.8235   time: 88.90s   best: 26.3257
2023-11-26 09:16:10,174:INFO:  Epoch 498/600:  train Loss: 22.3347   val Loss: 26.8766   time: 88.83s   best: 26.3257
2023-11-26 09:17:39,470:INFO:  Epoch 499/600:  train Loss: 22.4662   val Loss: 27.1763   time: 89.28s   best: 26.3257
2023-11-26 09:19:08,509:INFO:  Epoch 500/600:  train Loss: 22.1843   val Loss: 27.3743   time: 89.04s   best: 26.3257
2023-11-26 09:20:38,174:INFO:  Epoch 501/600:  train Loss: 22.3340   val Loss: 26.7971   time: 89.66s   best: 26.3257
2023-11-26 09:22:07,249:INFO:  Epoch 502/600:  train Loss: 22.3981   val Loss: 26.7887   time: 89.07s   best: 26.3257
2023-11-26 09:23:36,938:INFO:  Epoch 503/600:  train Loss: 22.3346   val Loss: 26.7658   time: 89.69s   best: 26.3257
2023-11-26 09:25:06,010:INFO:  Epoch 504/600:  train Loss: 22.1854   val Loss: 27.0392   time: 89.07s   best: 26.3257
2023-11-26 09:26:35,273:INFO:  Epoch 505/600:  train Loss: 22.2801   val Loss: 26.6617   time: 89.26s   best: 26.3257
2023-11-26 09:28:04,290:INFO:  Epoch 506/600:  train Loss: 22.1731   val Loss: 34.4274   time: 89.01s   best: 26.3257
2023-11-26 09:29:33,213:INFO:  Epoch 507/600:  train Loss: 22.7472   val Loss: 26.6353   time: 88.92s   best: 26.3257
2023-11-26 09:31:02,265:INFO:  Epoch 508/600:  train Loss: 22.1392   val Loss: 26.9024   time: 89.04s   best: 26.3257
2023-11-26 09:32:31,432:INFO:  Epoch 509/600:  train Loss: 22.1755   val Loss: 27.6660   time: 89.17s   best: 26.3257
2023-11-26 09:34:00,169:INFO:  Epoch 510/600:  train Loss: 22.2782   val Loss: 26.8009   time: 88.72s   best: 26.3257
2023-11-26 09:35:29,189:INFO:  Epoch 511/600:  train Loss: 22.1644   val Loss: 27.1815   time: 89.00s   best: 26.3257
2023-11-26 09:36:58,387:INFO:  Epoch 512/600:  train Loss: 22.0722   val Loss: 27.2503   time: 89.19s   best: 26.3257
2023-11-26 09:38:27,523:INFO:  Epoch 513/600:  train Loss: 22.0777   val Loss: 26.9268   time: 89.13s   best: 26.3257
2023-11-26 09:39:56,567:INFO:  Epoch 514/600:  train Loss: 22.1146   val Loss: 26.8793   time: 89.04s   best: 26.3257
2023-11-26 09:41:25,808:INFO:  Epoch 515/600:  train Loss: 22.0754   val Loss: 26.6096   time: 89.23s   best: 26.3257
2023-11-26 09:42:54,813:INFO:  Epoch 516/600:  train Loss: 22.0100   val Loss: 27.1626   time: 88.99s   best: 26.3257
2023-11-26 09:44:23,689:INFO:  Epoch 517/600:  train Loss: 22.2714   val Loss: 26.8113   time: 88.86s   best: 26.3257
2023-11-26 09:45:52,615:INFO:  Epoch 518/600:  train Loss: 22.2376   val Loss: 26.9280   time: 88.91s   best: 26.3257
2023-11-26 09:47:21,815:INFO:  Epoch 519/600:  train Loss: 22.0736   val Loss: 26.8697   time: 89.19s   best: 26.3257
2023-11-26 09:48:51,266:INFO:  Epoch 520/600:  train Loss: 22.1554   val Loss: 26.6202   time: 89.45s   best: 26.3257
2023-11-26 09:50:20,895:INFO:  Epoch 521/600:  train Loss: 21.9743   val Loss: 26.8418   time: 89.62s   best: 26.3257
2023-11-26 09:51:49,963:INFO:  Epoch 522/600:  train Loss: 22.2181   val Loss: 26.7740   time: 89.07s   best: 26.3257
2023-11-26 09:53:19,039:INFO:  Epoch 523/600:  train Loss: 21.9995   val Loss: 27.0958   time: 89.07s   best: 26.3257
2023-11-26 09:54:47,873:INFO:  Epoch 524/600:  train Loss: 21.9609   val Loss: 27.0740   time: 88.83s   best: 26.3257
2023-11-26 09:56:16,738:INFO:  Epoch 525/600:  train Loss: 22.0322   val Loss: 26.7890   time: 88.86s   best: 26.3257
2023-11-26 09:57:46,024:INFO:  Epoch 526/600:  train Loss: 21.9400   val Loss: 27.1512   time: 89.28s   best: 26.3257
2023-11-26 09:59:15,321:INFO:  Epoch 527/600:  train Loss: 22.9489   val Loss: 27.3798   time: 89.29s   best: 26.3257
2023-11-26 10:00:44,251:INFO:  Epoch 528/600:  train Loss: 22.3367   val Loss: 26.6394   time: 88.92s   best: 26.3257
2023-11-26 10:02:13,283:INFO:  Epoch 529/600:  train Loss: 21.8771   val Loss: 26.9350   time: 89.02s   best: 26.3257
2023-11-26 10:03:42,335:INFO:  Epoch 530/600:  train Loss: 21.8938   val Loss: 27.0534   time: 89.04s   best: 26.3257
2023-11-26 10:05:11,235:INFO:  Epoch 531/600:  train Loss: 21.9026   val Loss: 26.7480   time: 88.90s   best: 26.3257
2023-11-26 10:06:40,049:INFO:  Epoch 532/600:  train Loss: 21.9116   val Loss: 26.6873   time: 88.80s   best: 26.3257
2023-11-26 10:08:08,734:INFO:  Epoch 533/600:  train Loss: 21.8440   val Loss: 27.0650   time: 88.68s   best: 26.3257
2023-11-26 10:09:37,636:INFO:  Epoch 534/600:  train Loss: 22.3362   val Loss: 27.1529   time: 88.90s   best: 26.3257
2023-11-26 10:11:07,159:INFO:  Epoch 535/600:  train Loss: 21.9339   val Loss: 26.7413   time: 89.50s   best: 26.3257
2023-11-26 10:12:35,918:INFO:  Epoch 536/600:  train Loss: 21.9932   val Loss: 26.5895   time: 88.75s   best: 26.3257
2023-11-26 10:14:04,689:INFO:  Epoch 537/600:  train Loss: 21.8358   val Loss: 26.5288   time: 88.77s   best: 26.3257
2023-11-26 10:15:33,671:INFO:  Epoch 538/600:  train Loss: 21.9772   val Loss: 27.1831   time: 88.97s   best: 26.3257
2023-11-26 10:17:02,588:INFO:  Epoch 539/600:  train Loss: 22.3625   val Loss: 28.4088   time: 88.92s   best: 26.3257
2023-11-26 10:18:32,112:INFO:  Epoch 540/600:  train Loss: 22.3923   val Loss: 27.0405   time: 89.52s   best: 26.3257
2023-11-26 10:20:01,290:INFO:  Epoch 541/600:  train Loss: 21.8273   val Loss: 26.8580   time: 89.18s   best: 26.3257
2023-11-26 10:21:30,320:INFO:  Epoch 542/600:  train Loss: 21.8728   val Loss: 26.4087   time: 89.03s   best: 26.3257
2023-11-26 10:22:59,531:INFO:  Epoch 543/600:  train Loss: 23.1988   val Loss: 27.4746   time: 89.21s   best: 26.3257
2023-11-26 10:24:28,711:INFO:  Epoch 544/600:  train Loss: 22.1177   val Loss: 26.5752   time: 89.18s   best: 26.3257
2023-11-26 10:25:57,585:INFO:  Epoch 545/600:  train Loss: 21.8366   val Loss: 26.5121   time: 88.87s   best: 26.3257
2023-11-26 10:27:26,439:INFO:  Epoch 546/600:  train Loss: 21.7776   val Loss: 27.0497   time: 88.85s   best: 26.3257
2023-11-26 10:28:55,161:INFO:  Epoch 547/600:  train Loss: 21.7011   val Loss: 26.9148   time: 88.72s   best: 26.3257
2023-11-26 10:30:23,858:INFO:  Epoch 548/600:  train Loss: 21.8165   val Loss: 27.1382   time: 88.68s   best: 26.3257
2023-11-26 10:31:52,808:INFO:  Epoch 549/600:  train Loss: 21.8749   val Loss: 26.7707   time: 88.95s   best: 26.3257
2023-11-26 10:33:21,625:INFO:  Epoch 550/600:  train Loss: 21.8531   val Loss: 26.5227   time: 88.81s   best: 26.3257
2023-11-26 10:34:50,596:INFO:  Epoch 551/600:  train Loss: 21.7066   val Loss: 27.0612   time: 88.96s   best: 26.3257
2023-11-26 10:36:19,768:INFO:  Epoch 552/600:  train Loss: 21.9060   val Loss: 26.5629   time: 89.16s   best: 26.3257
2023-11-26 10:37:48,666:INFO:  Epoch 553/600:  train Loss: 21.7597   val Loss: 26.7000   time: 88.89s   best: 26.3257
2023-11-26 10:39:17,627:INFO:  Epoch 554/600:  train Loss: 21.7597   val Loss: 26.5537   time: 88.96s   best: 26.3257
2023-11-26 10:40:46,425:INFO:  Epoch 555/600:  train Loss: 21.7401   val Loss: 27.0716   time: 88.79s   best: 26.3257
2023-11-26 10:42:15,209:INFO:  Epoch 556/600:  train Loss: 21.6883   val Loss: 26.6318   time: 88.77s   best: 26.3257
2023-11-26 10:43:43,945:INFO:  Epoch 557/600:  train Loss: 21.9187   val Loss: 27.2020   time: 88.74s   best: 26.3257
2023-11-26 10:45:12,927:INFO:  Epoch 558/600:  train Loss: 21.6776   val Loss: 26.8135   time: 88.97s   best: 26.3257
2023-11-26 10:46:41,723:INFO:  Epoch 559/600:  train Loss: 22.5058   val Loss: 27.7088   time: 88.78s   best: 26.3257
2023-11-26 10:48:10,987:INFO:  Epoch 560/600:  train Loss: 21.7927   val Loss: 26.7605   time: 89.25s   best: 26.3257
2023-11-26 10:49:39,844:INFO:  Epoch 561/600:  train Loss: 21.7726   val Loss: 28.7493   time: 88.85s   best: 26.3257
2023-11-26 10:51:08,686:INFO:  Epoch 562/600:  train Loss: 22.3169   val Loss: 27.0951   time: 88.84s   best: 26.3257
2023-11-26 10:52:37,723:INFO:  Epoch 563/600:  train Loss: 21.6305   val Loss: 27.2845   time: 89.04s   best: 26.3257
2023-11-26 10:54:07,125:INFO:  Epoch 564/600:  train Loss: 21.6175   val Loss: 26.8667   time: 89.40s   best: 26.3257
2023-11-26 10:55:36,114:INFO:  Epoch 565/600:  train Loss: 21.5851   val Loss: 27.0806   time: 88.98s   best: 26.3257
2023-11-26 10:57:05,242:INFO:  Epoch 566/600:  train Loss: 21.6424   val Loss: 26.4460   time: 89.13s   best: 26.3257
2023-11-26 10:58:34,546:INFO:  Epoch 567/600:  train Loss: 22.0618   val Loss: 28.9067   time: 89.29s   best: 26.3257
2023-11-26 11:00:03,743:INFO:  Epoch 568/600:  train Loss: 22.0637   val Loss: 26.5543   time: 89.20s   best: 26.3257
2023-11-26 11:01:32,683:INFO:  Epoch 569/600:  train Loss: 21.5388   val Loss: 26.5010   time: 88.94s   best: 26.3257
2023-11-26 11:03:01,761:INFO:  Epoch 570/600:  train Loss: 21.6212   val Loss: 26.6827   time: 89.08s   best: 26.3257
2023-11-26 11:04:30,551:INFO:  Epoch 571/600:  train Loss: 21.5796   val Loss: 26.5025   time: 88.79s   best: 26.3257
2023-11-26 11:05:59,790:INFO:  Epoch 572/600:  train Loss: 21.6158   val Loss: 26.9605   time: 89.23s   best: 26.3257
2023-11-26 11:07:29,313:INFO:  Epoch 573/600:  train Loss: 21.4972   val Loss: 27.1656   time: 89.52s   best: 26.3257
2023-11-26 11:08:58,553:INFO:  Epoch 574/600:  train Loss: 21.5007   val Loss: 26.8017   time: 89.22s   best: 26.3257
2023-11-26 11:10:27,885:INFO:  Epoch 575/600:  train Loss: 21.5991   val Loss: 26.7276   time: 89.33s   best: 26.3257
2023-11-26 11:11:57,054:INFO:  Epoch 576/600:  train Loss: 22.8041   val Loss: 29.4207   time: 89.16s   best: 26.3257
2023-11-26 11:13:26,070:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_78dd.pt
2023-11-26 11:13:26,088:INFO:  Epoch 577/600:  train Loss: 21.8070   val Loss: 26.2289   time: 88.99s   best: 26.2289
2023-11-26 11:14:55,009:INFO:  Epoch 578/600:  train Loss: 21.6202   val Loss: 26.7087   time: 88.92s   best: 26.2289
2023-11-26 11:16:23,790:INFO:  Epoch 579/600:  train Loss: 21.4994   val Loss: 26.8846   time: 88.77s   best: 26.2289
2023-11-26 11:17:52,692:INFO:  Epoch 580/600:  train Loss: 21.4866   val Loss: 26.6379   time: 88.90s   best: 26.2289
2023-11-26 11:19:21,779:INFO:  Epoch 581/600:  train Loss: 21.6516   val Loss: 27.5911   time: 89.08s   best: 26.2289
2023-11-26 11:20:50,832:INFO:  Epoch 582/600:  train Loss: 21.8209   val Loss: 26.5559   time: 89.04s   best: 26.2289
2023-11-26 11:22:19,725:INFO:  Epoch 583/600:  train Loss: 21.3957   val Loss: 26.3153   time: 88.88s   best: 26.2289
2023-11-26 11:23:48,907:INFO:  Epoch 584/600:  train Loss: 21.5770   val Loss: 28.1371   time: 89.18s   best: 26.2289
2023-11-26 11:25:17,776:INFO:  Epoch 585/600:  train Loss: 21.8088   val Loss: 29.8247   time: 88.86s   best: 26.2289
2023-11-26 11:26:47,116:INFO:  Epoch 586/600:  train Loss: 21.8232   val Loss: 26.3859   time: 89.34s   best: 26.2289
2023-11-26 11:28:15,947:INFO:  Epoch 587/600:  train Loss: 21.6060   val Loss: 27.0797   time: 88.83s   best: 26.2289
2023-11-26 11:29:44,811:INFO:  Epoch 588/600:  train Loss: 21.4125   val Loss: 26.7121   time: 88.85s   best: 26.2289
2023-11-26 11:31:14,179:INFO:  Epoch 589/600:  train Loss: 21.4520   val Loss: 26.3451   time: 89.36s   best: 26.2289
2023-11-26 11:32:43,086:INFO:  Epoch 590/600:  train Loss: 21.3769   val Loss: 26.4886   time: 88.91s   best: 26.2289
2023-11-26 11:34:12,034:INFO:  Epoch 591/600:  train Loss: 22.9501   val Loss: 30.2167   time: 88.95s   best: 26.2289
2023-11-26 11:35:41,484:INFO:  Epoch 592/600:  train Loss: 22.1859   val Loss: 26.5027   time: 89.44s   best: 26.2289
2023-11-26 11:37:10,748:INFO:  Epoch 593/600:  train Loss: 21.4737   val Loss: 26.3456   time: 89.26s   best: 26.2289
2023-11-26 11:38:39,607:INFO:  Epoch 594/600:  train Loss: 21.3993   val Loss: 26.5228   time: 88.86s   best: 26.2289
2023-11-26 11:40:08,804:INFO:  Epoch 595/600:  train Loss: 21.4398   val Loss: 27.1272   time: 89.20s   best: 26.2289
2023-11-26 11:41:37,820:INFO:  Epoch 596/600:  train Loss: 21.3583   val Loss: 26.8066   time: 89.00s   best: 26.2289
2023-11-26 11:43:06,814:INFO:  Epoch 597/600:  train Loss: 21.4203   val Loss: 26.3759   time: 88.99s   best: 26.2289
2023-11-26 11:44:35,655:INFO:  Epoch 598/600:  train Loss: 21.3735   val Loss: 26.7663   time: 88.84s   best: 26.2289
2023-11-26 11:46:04,690:INFO:  Epoch 599/600:  train Loss: 21.4187   val Loss: 27.1128   time: 89.03s   best: 26.2289
2023-11-26 11:47:33,483:INFO:  Epoch 600/600:  train Loss: 21.4820   val Loss: 26.5426   time: 88.78s   best: 26.2289
2023-11-26 11:47:33,490:INFO:  -----> Training complete in 893m 46s   best validation loss: 26.2289
 
2023-11-26 22:02:51,842:INFO:  Starting experiment lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)
2023-11-26 22:02:51,844:INFO:  Defining the model
2023-11-26 22:02:51,939:INFO:  Reading the dataset
2023-11-26 22:08:22,308:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:08:22,327:INFO:  Epoch 1/600:  train Loss: 89.4369   val Loss: 86.1429   time: 89.17s   best: 86.1429
2023-11-26 22:09:50,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:09:50,106:INFO:  Epoch 2/600:  train Loss: 84.4249   val Loss: 80.0878   time: 87.76s   best: 80.0878
2023-11-26 22:11:17,964:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:11:17,982:INFO:  Epoch 3/600:  train Loss: 77.8319   val Loss: 76.7836   time: 87.85s   best: 76.7836
2023-11-26 22:12:46,081:INFO:  Epoch 4/600:  train Loss: 75.5534   val Loss: 78.6251   time: 88.09s   best: 76.7836
2023-11-26 22:14:13,865:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:14:13,884:INFO:  Epoch 5/600:  train Loss: 73.5658   val Loss: 75.9461   time: 87.77s   best: 75.9461
2023-11-26 22:15:42,355:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:15:42,374:INFO:  Epoch 6/600:  train Loss: 72.0258   val Loss: 71.2213   time: 88.47s   best: 71.2213
2023-11-26 22:17:10,701:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:17:10,734:INFO:  Epoch 7/600:  train Loss: 70.6746   val Loss: 70.6946   time: 88.31s   best: 70.6946
2023-11-26 22:18:39,357:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:18:39,375:INFO:  Epoch 8/600:  train Loss: 69.3329   val Loss: 68.9213   time: 88.61s   best: 68.9213
2023-11-26 22:20:07,757:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:20:07,775:INFO:  Epoch 9/600:  train Loss: 68.3740   val Loss: 67.3520   time: 88.37s   best: 67.3520
2023-11-26 22:21:36,312:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:21:36,330:INFO:  Epoch 10/600:  train Loss: 67.6187   val Loss: 66.9040   time: 88.53s   best: 66.9040
2023-11-26 22:23:05,075:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:23:05,094:INFO:  Epoch 11/600:  train Loss: 66.7823   val Loss: 66.2444   time: 88.74s   best: 66.2444
2023-11-26 22:24:33,563:INFO:  Epoch 12/600:  train Loss: 66.3706   val Loss: 66.5640   time: 88.46s   best: 66.2444
2023-11-26 22:26:02,322:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:26:02,340:INFO:  Epoch 13/600:  train Loss: 65.5069   val Loss: 65.0413   time: 88.74s   best: 65.0413
2023-11-26 22:27:31,356:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:27:31,375:INFO:  Epoch 14/600:  train Loss: 64.8120   val Loss: 64.6202   time: 89.01s   best: 64.6202
2023-11-26 22:28:59,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:29:00,022:INFO:  Epoch 15/600:  train Loss: 64.5108   val Loss: 63.7594   time: 88.61s   best: 63.7594
2023-11-26 22:30:28,440:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:30:28,459:INFO:  Epoch 16/600:  train Loss: 64.0247   val Loss: 63.2809   time: 88.40s   best: 63.2809
2023-11-26 22:31:56,760:INFO:  Epoch 17/600:  train Loss: 63.3114   val Loss: 63.3783   time: 88.29s   best: 63.2809
2023-11-26 22:33:25,082:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:33:25,100:INFO:  Epoch 18/600:  train Loss: 62.7398   val Loss: 62.4147   time: 88.31s   best: 62.4147
2023-11-26 22:34:53,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:34:53,684:INFO:  Epoch 19/600:  train Loss: 62.2086   val Loss: 61.3337   time: 88.56s   best: 61.3337
2023-11-26 22:36:21,864:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:36:21,883:INFO:  Epoch 20/600:  train Loss: 61.6434   val Loss: 60.9018   time: 88.18s   best: 60.9018
2023-11-26 22:37:50,224:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:37:50,243:INFO:  Epoch 21/600:  train Loss: 61.1215   val Loss: 60.8139   time: 88.34s   best: 60.8139
2023-11-26 22:39:18,747:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:39:18,781:INFO:  Epoch 22/600:  train Loss: 60.7845   val Loss: 60.6662   time: 88.49s   best: 60.6662
2023-11-26 22:40:47,007:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:40:47,025:INFO:  Epoch 23/600:  train Loss: 60.2435   val Loss: 59.9014   time: 88.21s   best: 59.9014
2023-11-26 22:42:15,486:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:42:15,505:INFO:  Epoch 24/600:  train Loss: 59.8024   val Loss: 59.2417   time: 88.46s   best: 59.2417
2023-11-26 22:43:43,614:INFO:  Epoch 25/600:  train Loss: 59.3379   val Loss: 59.4979   time: 88.10s   best: 59.2417
2023-11-26 22:45:11,751:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:45:11,769:INFO:  Epoch 26/600:  train Loss: 58.9205   val Loss: 58.6327   time: 88.12s   best: 58.6327
2023-11-26 22:46:39,931:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:46:39,949:INFO:  Epoch 27/600:  train Loss: 58.4427   val Loss: 58.5351   time: 88.15s   best: 58.5351
2023-11-26 22:48:08,177:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:48:08,195:INFO:  Epoch 28/600:  train Loss: 57.9723   val Loss: 57.5843   time: 88.22s   best: 57.5843
2023-11-26 22:49:36,501:INFO:  Epoch 29/600:  train Loss: 57.4696   val Loss: 58.2117   time: 88.30s   best: 57.5843
2023-11-26 22:51:05,032:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:51:05,060:INFO:  Epoch 30/600:  train Loss: 56.9677   val Loss: 56.9495   time: 88.53s   best: 56.9495
2023-11-26 22:52:33,471:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:52:33,489:INFO:  Epoch 31/600:  train Loss: 56.4924   val Loss: 55.9489   time: 88.40s   best: 55.9489
2023-11-26 22:54:01,687:INFO:  Epoch 32/600:  train Loss: 55.6681   val Loss: 56.1670   time: 88.19s   best: 55.9489
2023-11-26 22:55:29,822:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:55:29,840:INFO:  Epoch 33/600:  train Loss: 55.0581   val Loss: 55.3414   time: 88.13s   best: 55.3414
2023-11-26 22:56:58,314:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:56:58,332:INFO:  Epoch 34/600:  train Loss: 54.8083   val Loss: 54.1190   time: 88.47s   best: 54.1190
2023-11-26 22:58:26,452:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:58:26,470:INFO:  Epoch 35/600:  train Loss: 53.9741   val Loss: 54.0042   time: 88.11s   best: 54.0042
2023-11-26 22:59:54,883:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 22:59:54,911:INFO:  Epoch 36/600:  train Loss: 53.5316   val Loss: 53.3908   time: 88.41s   best: 53.3908
2023-11-26 23:01:23,229:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:01:23,248:INFO:  Epoch 37/600:  train Loss: 52.8121   val Loss: 53.1592   time: 88.31s   best: 53.1592
2023-11-26 23:02:51,618:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:02:51,636:INFO:  Epoch 38/600:  train Loss: 52.3430   val Loss: 52.3552   time: 88.37s   best: 52.3552
2023-11-26 23:04:19,828:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:04:19,847:INFO:  Epoch 39/600:  train Loss: 51.9652   val Loss: 52.0769   time: 88.18s   best: 52.0769
2023-11-26 23:05:48,123:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:05:48,142:INFO:  Epoch 40/600:  train Loss: 51.0516   val Loss: 51.0120   time: 88.26s   best: 51.0120
2023-11-26 23:07:16,149:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:07:16,167:INFO:  Epoch 41/600:  train Loss: 50.6803   val Loss: 50.8052   time: 88.00s   best: 50.8052
2023-11-26 23:08:44,395:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:08:44,414:INFO:  Epoch 42/600:  train Loss: 50.0351   val Loss: 50.1257   time: 88.22s   best: 50.1257
2023-11-26 23:10:12,884:INFO:  Epoch 43/600:  train Loss: 49.6390   val Loss: 50.2788   time: 88.47s   best: 50.1257
2023-11-26 23:11:41,302:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:11:41,321:INFO:  Epoch 44/600:  train Loss: 48.9594   val Loss: 49.3400   time: 88.41s   best: 49.3400
2023-11-26 23:13:09,401:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:13:09,420:INFO:  Epoch 45/600:  train Loss: 48.6603   val Loss: 49.0325   time: 88.06s   best: 49.0325
2023-11-26 23:14:37,691:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:14:37,710:INFO:  Epoch 46/600:  train Loss: 48.0752   val Loss: 48.9731   time: 88.26s   best: 48.9731
2023-11-26 23:16:05,776:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:16:05,804:INFO:  Epoch 47/600:  train Loss: 47.8915   val Loss: 48.3031   time: 88.06s   best: 48.3031
2023-11-26 23:17:33,932:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:17:33,951:INFO:  Epoch 48/600:  train Loss: 47.4117   val Loss: 48.2187   time: 88.12s   best: 48.2187
2023-11-26 23:19:02,085:INFO:  Epoch 49/600:  train Loss: 46.8759   val Loss: 48.2526   time: 88.12s   best: 48.2187
2023-11-26 23:20:30,621:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:20:30,651:INFO:  Epoch 50/600:  train Loss: 46.5008   val Loss: 47.9345   time: 88.52s   best: 47.9345
2023-11-26 23:21:58,911:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:21:58,930:INFO:  Epoch 51/600:  train Loss: 46.2343   val Loss: 47.4643   time: 88.26s   best: 47.4643
2023-11-26 23:23:27,332:INFO:  Epoch 52/600:  train Loss: 45.9852   val Loss: 47.8809   time: 88.39s   best: 47.4643
2023-11-26 23:24:55,629:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:24:55,648:INFO:  Epoch 53/600:  train Loss: 45.6206   val Loss: 46.6392   time: 88.28s   best: 46.6392
2023-11-26 23:26:23,938:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:26:23,957:INFO:  Epoch 54/600:  train Loss: 45.2543   val Loss: 46.4312   time: 88.29s   best: 46.4312
2023-11-26 23:27:52,090:INFO:  Epoch 55/600:  train Loss: 44.9028   val Loss: 47.2707   time: 88.12s   best: 46.4312
2023-11-26 23:29:20,470:INFO:  Epoch 56/600:  train Loss: 45.2317   val Loss: 46.4921   time: 88.38s   best: 46.4312
2023-11-26 23:30:48,847:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:30:48,866:INFO:  Epoch 57/600:  train Loss: 44.5052   val Loss: 46.2944   time: 88.36s   best: 46.2944
2023-11-26 23:32:17,061:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:32:17,080:INFO:  Epoch 58/600:  train Loss: 44.1242   val Loss: 45.7589   time: 88.19s   best: 45.7589
2023-11-26 23:33:45,356:INFO:  Epoch 59/600:  train Loss: 43.8485   val Loss: 46.2776   time: 88.26s   best: 45.7589
2023-11-26 23:35:13,651:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:35:13,679:INFO:  Epoch 60/600:  train Loss: 43.8328   val Loss: 45.4474   time: 88.29s   best: 45.4474
2023-11-26 23:36:41,854:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:36:41,873:INFO:  Epoch 61/600:  train Loss: 43.3038   val Loss: 44.9144   time: 88.17s   best: 44.9144
2023-11-26 23:38:10,150:INFO:  Epoch 62/600:  train Loss: 43.0848   val Loss: 45.7246   time: 88.27s   best: 44.9144
2023-11-26 23:39:38,298:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:39:38,317:INFO:  Epoch 63/600:  train Loss: 42.9065   val Loss: 44.4343   time: 88.14s   best: 44.4343
2023-11-26 23:41:06,743:INFO:  Epoch 64/600:  train Loss: 42.9801   val Loss: 45.3504   time: 88.43s   best: 44.4343
2023-11-26 23:42:34,750:INFO:  Epoch 65/600:  train Loss: 42.4553   val Loss: 44.5628   time: 88.01s   best: 44.4343
2023-11-26 23:44:03,021:INFO:  Epoch 66/600:  train Loss: 42.2025   val Loss: 45.1769   time: 88.27s   best: 44.4343
2023-11-26 23:45:31,080:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:45:31,099:INFO:  Epoch 67/600:  train Loss: 41.9606   val Loss: 44.1630   time: 88.05s   best: 44.1630
2023-11-26 23:46:59,458:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:46:59,476:INFO:  Epoch 68/600:  train Loss: 41.5013   val Loss: 43.5680   time: 88.35s   best: 43.5680
2023-11-26 23:48:27,665:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:48:27,684:INFO:  Epoch 69/600:  train Loss: 41.4587   val Loss: 43.3976   time: 88.18s   best: 43.3976
2023-11-26 23:49:56,246:INFO:  Epoch 70/600:  train Loss: 41.2411   val Loss: 43.6065   time: 88.56s   best: 43.3976
2023-11-26 23:51:24,586:INFO:  Epoch 71/600:  train Loss: 41.0274   val Loss: 43.4670   time: 88.33s   best: 43.3976
2023-11-26 23:52:52,968:INFO:  Epoch 72/600:  train Loss: 41.1133   val Loss: 43.8240   time: 88.37s   best: 43.3976
2023-11-26 23:54:21,021:INFO:  Epoch 73/600:  train Loss: 40.7868   val Loss: 43.4310   time: 88.04s   best: 43.3976
2023-11-26 23:55:49,159:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-26 23:55:49,178:INFO:  Epoch 74/600:  train Loss: 40.3003   val Loss: 42.6705   time: 88.12s   best: 42.6705
2023-11-26 23:57:17,271:INFO:  Epoch 75/600:  train Loss: 40.2457   val Loss: 42.8932   time: 88.08s   best: 42.6705
2023-11-26 23:58:45,434:INFO:  Epoch 76/600:  train Loss: 39.8885   val Loss: 42.8470   time: 88.16s   best: 42.6705
2023-11-27 00:00:13,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:00:13,464:INFO:  Epoch 77/600:  train Loss: 39.8736   val Loss: 41.9935   time: 88.01s   best: 41.9935
2023-11-27 00:01:41,681:INFO:  Epoch 78/600:  train Loss: 39.5135   val Loss: 42.1233   time: 88.22s   best: 41.9935
2023-11-27 00:03:09,817:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:03:09,835:INFO:  Epoch 79/600:  train Loss: 39.4629   val Loss: 41.8621   time: 88.13s   best: 41.8621
2023-11-27 00:04:38,097:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:04:38,116:INFO:  Epoch 80/600:  train Loss: 39.1361   val Loss: 41.1736   time: 88.25s   best: 41.1736
2023-11-27 00:06:06,213:INFO:  Epoch 81/600:  train Loss: 39.1747   val Loss: 41.3852   time: 88.10s   best: 41.1736
2023-11-27 00:07:34,309:INFO:  Epoch 82/600:  train Loss: 38.8452   val Loss: 41.9987   time: 88.10s   best: 41.1736
2023-11-27 00:09:02,422:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:09:02,441:INFO:  Epoch 83/600:  train Loss: 38.7578   val Loss: 41.1648   time: 88.11s   best: 41.1648
2023-11-27 00:10:30,624:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:10:30,643:INFO:  Epoch 84/600:  train Loss: 38.6063   val Loss: 40.4621   time: 88.17s   best: 40.4621
2023-11-27 00:11:58,812:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:11:58,831:INFO:  Epoch 85/600:  train Loss: 38.3826   val Loss: 40.4604   time: 88.16s   best: 40.4604
2023-11-27 00:13:27,072:INFO:  Epoch 86/600:  train Loss: 38.1921   val Loss: 40.4793   time: 88.24s   best: 40.4604
2023-11-27 00:14:55,305:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:14:55,324:INFO:  Epoch 87/600:  train Loss: 37.8838   val Loss: 40.1547   time: 88.22s   best: 40.1547
2023-11-27 00:16:23,485:INFO:  Epoch 88/600:  train Loss: 38.0742   val Loss: 40.2718   time: 88.16s   best: 40.1547
2023-11-27 00:17:52,053:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:17:52,072:INFO:  Epoch 89/600:  train Loss: 37.7408   val Loss: 39.7039   time: 88.55s   best: 39.7039
2023-11-27 00:19:20,431:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:19:20,450:INFO:  Epoch 90/600:  train Loss: 37.5719   val Loss: 39.1667   time: 88.34s   best: 39.1667
2023-11-27 00:20:48,626:INFO:  Epoch 91/600:  train Loss: 37.4008   val Loss: 40.6348   time: 88.16s   best: 39.1667
2023-11-27 00:22:17,371:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:22:17,390:INFO:  Epoch 92/600:  train Loss: 37.2442   val Loss: 39.0829   time: 88.73s   best: 39.0829
2023-11-27 00:23:46,077:INFO:  Epoch 93/600:  train Loss: 37.1373   val Loss: 39.3157   time: 88.69s   best: 39.0829
2023-11-27 00:25:14,384:INFO:  Epoch 94/600:  train Loss: 36.9163   val Loss: 39.8304   time: 88.31s   best: 39.0829
2023-11-27 00:26:42,543:INFO:  Epoch 95/600:  train Loss: 37.4448   val Loss: 39.5596   time: 88.16s   best: 39.0829
2023-11-27 00:28:11,172:INFO:  Epoch 96/600:  train Loss: 36.6136   val Loss: 40.7901   time: 88.62s   best: 39.0829
2023-11-27 00:29:39,252:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:29:39,271:INFO:  Epoch 97/600:  train Loss: 36.6100   val Loss: 38.6441   time: 88.06s   best: 38.6441
2023-11-27 00:31:07,515:INFO:  Epoch 98/600:  train Loss: 36.4889   val Loss: 39.0206   time: 88.24s   best: 38.6441
2023-11-27 00:32:35,604:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:32:35,623:INFO:  Epoch 99/600:  train Loss: 36.4106   val Loss: 38.3393   time: 88.08s   best: 38.3393
2023-11-27 00:34:04,072:INFO:  Epoch 100/600:  train Loss: 36.2937   val Loss: 39.9687   time: 88.44s   best: 38.3393
2023-11-27 00:35:32,394:INFO:  Epoch 101/600:  train Loss: 36.1125   val Loss: 39.4436   time: 88.31s   best: 38.3393
2023-11-27 00:37:00,793:INFO:  Epoch 102/600:  train Loss: 35.9731   val Loss: 38.8650   time: 88.40s   best: 38.3393
2023-11-27 00:38:29,144:INFO:  Epoch 103/600:  train Loss: 36.0590   val Loss: 38.3929   time: 88.34s   best: 38.3393
2023-11-27 00:39:57,475:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:39:57,494:INFO:  Epoch 104/600:  train Loss: 35.7430   val Loss: 37.4022   time: 88.33s   best: 37.4022
2023-11-27 00:41:25,876:INFO:  Epoch 105/600:  train Loss: 35.6690   val Loss: 39.8790   time: 88.38s   best: 37.4022
2023-11-27 00:42:54,292:INFO:  Epoch 106/600:  train Loss: 35.4676   val Loss: 37.7024   time: 88.42s   best: 37.4022
2023-11-27 00:44:22,883:INFO:  Epoch 107/600:  train Loss: 35.8350   val Loss: 37.5781   time: 88.58s   best: 37.4022
2023-11-27 00:45:51,193:INFO:  Epoch 108/600:  train Loss: 35.2432   val Loss: 37.7344   time: 88.30s   best: 37.4022
2023-11-27 00:47:19,417:INFO:  Epoch 109/600:  train Loss: 35.3665   val Loss: 38.4677   time: 88.21s   best: 37.4022
2023-11-27 00:48:47,714:INFO:  Epoch 110/600:  train Loss: 35.4716   val Loss: 37.4745   time: 88.28s   best: 37.4022
2023-11-27 00:50:16,266:INFO:  Epoch 111/600:  train Loss: 35.0124   val Loss: 38.0146   time: 88.55s   best: 37.4022
2023-11-27 00:51:44,906:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:51:44,925:INFO:  Epoch 112/600:  train Loss: 34.9604   val Loss: 36.7913   time: 88.63s   best: 36.7913
2023-11-27 00:53:13,462:INFO:  Epoch 113/600:  train Loss: 34.7441   val Loss: 37.6115   time: 88.52s   best: 36.7913
2023-11-27 00:54:41,770:INFO:  Epoch 114/600:  train Loss: 34.8144   val Loss: 36.9839   time: 88.31s   best: 36.7913
2023-11-27 00:56:09,908:INFO:  Epoch 115/600:  train Loss: 34.5855   val Loss: 36.9245   time: 88.13s   best: 36.7913
2023-11-27 00:57:38,128:INFO:  Epoch 116/600:  train Loss: 34.6567   val Loss: 38.7057   time: 88.22s   best: 36.7913
2023-11-27 00:59:06,509:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 00:59:06,528:INFO:  Epoch 117/600:  train Loss: 34.4781   val Loss: 36.2470   time: 88.37s   best: 36.2470
2023-11-27 01:00:34,820:INFO:  Epoch 118/600:  train Loss: 34.6242   val Loss: 36.8778   time: 88.29s   best: 36.2470
2023-11-27 01:02:02,875:INFO:  Epoch 119/600:  train Loss: 34.2738   val Loss: 37.2067   time: 88.05s   best: 36.2470
2023-11-27 01:03:31,241:INFO:  Epoch 120/600:  train Loss: 34.1113   val Loss: 36.6017   time: 88.37s   best: 36.2470
2023-11-27 01:04:59,724:INFO:  Epoch 121/600:  train Loss: 34.0512   val Loss: 36.8531   time: 88.47s   best: 36.2470
2023-11-27 01:06:27,935:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:06:27,984:INFO:  Epoch 122/600:  train Loss: 33.9938   val Loss: 35.9675   time: 88.20s   best: 35.9675
2023-11-27 01:07:56,180:INFO:  Epoch 123/600:  train Loss: 33.8562   val Loss: 36.1297   time: 88.18s   best: 35.9675
2023-11-27 01:09:24,699:INFO:  Epoch 124/600:  train Loss: 33.8547   val Loss: 36.3982   time: 88.51s   best: 35.9675
2023-11-27 01:10:53,250:INFO:  Epoch 125/600:  train Loss: 33.7733   val Loss: 36.1097   time: 88.55s   best: 35.9675
2023-11-27 01:12:21,775:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:12:21,803:INFO:  Epoch 126/600:  train Loss: 33.5413   val Loss: 35.8909   time: 88.51s   best: 35.8909
2023-11-27 01:13:50,025:INFO:  Epoch 127/600:  train Loss: 33.4832   val Loss: 36.2877   time: 88.22s   best: 35.8909
2023-11-27 01:15:18,412:INFO:  Epoch 128/600:  train Loss: 33.5457   val Loss: 38.7201   time: 88.39s   best: 35.8909
2023-11-27 01:16:46,483:INFO:  Epoch 129/600:  train Loss: 33.5970   val Loss: 36.3775   time: 88.07s   best: 35.8909
2023-11-27 01:18:14,657:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:18:14,676:INFO:  Epoch 130/600:  train Loss: 33.3272   val Loss: 35.5782   time: 88.17s   best: 35.5782
2023-11-27 01:19:43,080:INFO:  Epoch 131/600:  train Loss: 33.5859   val Loss: 35.8171   time: 88.40s   best: 35.5782
2023-11-27 01:21:11,781:INFO:  Epoch 132/600:  train Loss: 33.1373   val Loss: 36.0172   time: 88.70s   best: 35.5782
2023-11-27 01:22:39,868:INFO:  Epoch 133/600:  train Loss: 33.0308   val Loss: 35.6728   time: 88.09s   best: 35.5782
2023-11-27 01:24:08,169:INFO:  Epoch 134/600:  train Loss: 34.6389   val Loss: 36.6290   time: 88.30s   best: 35.5782
2023-11-27 01:25:36,420:INFO:  Epoch 135/600:  train Loss: 33.1341   val Loss: 35.8186   time: 88.24s   best: 35.5782
2023-11-27 01:27:04,793:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:27:04,812:INFO:  Epoch 136/600:  train Loss: 32.9138   val Loss: 35.4771   time: 88.37s   best: 35.4771
2023-11-27 01:28:33,304:INFO:  Epoch 137/600:  train Loss: 33.1022   val Loss: 35.6089   time: 88.48s   best: 35.4771
2023-11-27 01:30:01,615:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:30:01,634:INFO:  Epoch 138/600:  train Loss: 32.7120   val Loss: 35.3818   time: 88.31s   best: 35.3818
2023-11-27 01:31:29,869:INFO:  Epoch 139/600:  train Loss: 33.0825   val Loss: 35.7442   time: 88.22s   best: 35.3818
2023-11-27 01:32:58,156:INFO:  Epoch 140/600:  train Loss: 32.6124   val Loss: 35.8079   time: 88.28s   best: 35.3818
2023-11-27 01:34:26,221:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:34:26,240:INFO:  Epoch 141/600:  train Loss: 32.7261   val Loss: 35.1409   time: 88.06s   best: 35.1409
2023-11-27 01:35:54,730:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:35:54,749:INFO:  Epoch 142/600:  train Loss: 32.5271   val Loss: 34.3993   time: 88.49s   best: 34.3993
2023-11-27 01:37:22,903:INFO:  Epoch 143/600:  train Loss: 32.5405   val Loss: 38.2423   time: 88.15s   best: 34.3993
2023-11-27 01:38:51,322:INFO:  Epoch 144/600:  train Loss: 32.6723   val Loss: 35.9757   time: 88.42s   best: 34.3993
2023-11-27 01:40:19,388:INFO:  Epoch 145/600:  train Loss: 32.4817   val Loss: 38.3951   time: 88.06s   best: 34.3993
2023-11-27 01:41:47,920:INFO:  Epoch 146/600:  train Loss: 32.5743   val Loss: 35.2882   time: 88.53s   best: 34.3993
2023-11-27 01:43:16,301:INFO:  Epoch 147/600:  train Loss: 32.2313   val Loss: 34.9404   time: 88.38s   best: 34.3993
2023-11-27 01:44:44,766:INFO:  Epoch 148/600:  train Loss: 32.7938   val Loss: 35.1578   time: 88.46s   best: 34.3993
2023-11-27 01:46:12,838:INFO:  Epoch 149/600:  train Loss: 32.0817   val Loss: 35.4586   time: 88.06s   best: 34.3993
2023-11-27 01:47:41,091:INFO:  Epoch 150/600:  train Loss: 32.2448   val Loss: 36.2716   time: 88.25s   best: 34.3993
2023-11-27 01:49:09,286:INFO:  Epoch 151/600:  train Loss: 32.1371   val Loss: 35.9281   time: 88.18s   best: 34.3993
2023-11-27 01:50:37,479:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:50:37,498:INFO:  Epoch 152/600:  train Loss: 31.9002   val Loss: 34.1521   time: 88.19s   best: 34.1521
2023-11-27 01:52:05,763:INFO:  Epoch 153/600:  train Loss: 31.9169   val Loss: 34.9853   time: 88.25s   best: 34.1521
2023-11-27 01:53:34,070:INFO:  Epoch 154/600:  train Loss: 31.8110   val Loss: 34.5889   time: 88.31s   best: 34.1521
2023-11-27 01:55:02,410:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:55:02,429:INFO:  Epoch 155/600:  train Loss: 32.1628   val Loss: 34.1068   time: 88.34s   best: 34.1068
2023-11-27 01:56:30,746:INFO:  Epoch 156/600:  train Loss: 31.8306   val Loss: 34.3973   time: 88.31s   best: 34.1068
2023-11-27 01:57:59,329:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:57:59,348:INFO:  Epoch 157/600:  train Loss: 31.6290   val Loss: 33.9326   time: 88.57s   best: 33.9326
2023-11-27 01:59:28,366:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 01:59:28,393:INFO:  Epoch 158/600:  train Loss: 31.3146   val Loss: 33.8104   time: 89.00s   best: 33.8104
2023-11-27 02:00:56,837:INFO:  Epoch 159/600:  train Loss: 31.5807   val Loss: 34.2305   time: 88.44s   best: 33.8104
2023-11-27 02:02:24,953:INFO:  Epoch 160/600:  train Loss: 31.2405   val Loss: 33.8692   time: 88.12s   best: 33.8104
2023-11-27 02:03:53,410:INFO:  Epoch 161/600:  train Loss: 31.3695   val Loss: 36.0052   time: 88.45s   best: 33.8104
2023-11-27 02:05:21,601:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 02:05:21,620:INFO:  Epoch 162/600:  train Loss: 31.2454   val Loss: 33.7655   time: 88.19s   best: 33.7655
2023-11-27 02:06:49,700:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 02:06:49,737:INFO:  Epoch 163/600:  train Loss: 31.0025   val Loss: 33.6013   time: 88.08s   best: 33.6013
2023-11-27 02:08:17,905:INFO:  Epoch 164/600:  train Loss: 31.0892   val Loss: 34.5902   time: 88.17s   best: 33.6013
2023-11-27 02:09:45,973:INFO:  Epoch 165/600:  train Loss: 30.8831   val Loss: 34.0253   time: 88.07s   best: 33.6013
2023-11-27 02:11:14,106:INFO:  Epoch 166/600:  train Loss: 30.5319   val Loss: 37.9809   time: 88.12s   best: 33.6013
2023-11-27 02:12:42,214:INFO:  Epoch 167/600:  train Loss: 31.8109   val Loss: 34.2406   time: 88.09s   best: 33.6013
2023-11-27 02:14:10,238:INFO:  Epoch 168/600:  train Loss: 30.5765   val Loss: 35.1119   time: 88.02s   best: 33.6013
2023-11-27 02:15:38,300:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 02:15:38,318:INFO:  Epoch 169/600:  train Loss: 30.4507   val Loss: 33.4412   time: 88.06s   best: 33.4412
2023-11-27 02:17:06,966:INFO:  Epoch 170/600:  train Loss: 30.7301   val Loss: 33.7388   time: 88.65s   best: 33.4412
2023-11-27 02:18:35,069:INFO:  Epoch 171/600:  train Loss: 30.5146   val Loss: 33.9513   time: 88.09s   best: 33.4412
2023-11-27 02:20:03,224:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 02:20:03,243:INFO:  Epoch 172/600:  train Loss: 30.2203   val Loss: 33.0928   time: 88.14s   best: 33.0928
2023-11-27 02:21:31,399:INFO:  Epoch 173/600:  train Loss: 30.0747   val Loss: 33.3173   time: 88.14s   best: 33.0928
2023-11-27 02:22:59,683:INFO:  Epoch 174/600:  train Loss: 30.0702   val Loss: 33.1642   time: 88.28s   best: 33.0928
2023-11-27 02:24:28,180:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 02:24:28,199:INFO:  Epoch 175/600:  train Loss: 30.3262   val Loss: 32.6540   time: 88.48s   best: 32.6540
2023-11-27 02:25:56,658:INFO:  Epoch 176/600:  train Loss: 30.0929   val Loss: 32.8835   time: 88.46s   best: 32.6540
2023-11-27 02:27:24,771:INFO:  Epoch 177/600:  train Loss: 29.8106   val Loss: 32.7414   time: 88.11s   best: 32.6540
2023-11-27 02:28:53,039:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 02:28:53,067:INFO:  Epoch 178/600:  train Loss: 29.9456   val Loss: 32.6042   time: 88.26s   best: 32.6042
2023-11-27 02:30:21,233:INFO:  Epoch 179/600:  train Loss: 30.0485   val Loss: 33.2615   time: 88.15s   best: 32.6042
2023-11-27 02:31:49,860:INFO:  Epoch 180/600:  train Loss: 29.6932   val Loss: 33.1287   time: 88.63s   best: 32.6042
2023-11-27 02:33:18,351:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 02:33:18,370:INFO:  Epoch 181/600:  train Loss: 29.6594   val Loss: 32.1377   time: 88.49s   best: 32.1377
2023-11-27 02:34:47,171:INFO:  Epoch 182/600:  train Loss: 29.4995   val Loss: 32.3495   time: 88.80s   best: 32.1377
2023-11-27 02:36:15,456:INFO:  Epoch 183/600:  train Loss: 29.7476   val Loss: 33.9213   time: 88.27s   best: 32.1377
2023-11-27 02:37:44,108:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 02:37:44,127:INFO:  Epoch 184/600:  train Loss: 29.4252   val Loss: 31.6593   time: 88.64s   best: 31.6593
2023-11-27 02:39:12,725:INFO:  Epoch 185/600:  train Loss: 29.2892   val Loss: 33.9343   time: 88.60s   best: 31.6593
2023-11-27 02:40:41,503:INFO:  Epoch 186/600:  train Loss: 29.6850   val Loss: 33.0100   time: 88.77s   best: 31.6593
2023-11-27 02:42:09,870:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 02:42:09,889:INFO:  Epoch 187/600:  train Loss: 29.2456   val Loss: 31.3199   time: 88.35s   best: 31.3199
2023-11-27 02:43:38,537:INFO:  Epoch 188/600:  train Loss: 29.2212   val Loss: 32.7833   time: 88.64s   best: 31.3199
2023-11-27 02:45:07,090:INFO:  Epoch 189/600:  train Loss: 29.2225   val Loss: 33.1778   time: 88.55s   best: 31.3199
2023-11-27 02:46:35,613:INFO:  Epoch 190/600:  train Loss: 29.0681   val Loss: 33.4435   time: 88.52s   best: 31.3199
2023-11-27 02:48:03,902:INFO:  Epoch 191/600:  train Loss: 29.1965   val Loss: 32.0438   time: 88.29s   best: 31.3199
2023-11-27 02:49:32,806:INFO:  Epoch 192/600:  train Loss: 28.9130   val Loss: 31.7392   time: 88.90s   best: 31.3199
2023-11-27 02:51:01,169:INFO:  Epoch 193/600:  train Loss: 30.9402   val Loss: 33.2757   time: 88.36s   best: 31.3199
2023-11-27 02:52:29,622:INFO:  Epoch 194/600:  train Loss: 29.3547   val Loss: 31.8312   time: 88.44s   best: 31.3199
2023-11-27 02:53:57,970:INFO:  Epoch 195/600:  train Loss: 28.7894   val Loss: 31.7399   time: 88.35s   best: 31.3199
2023-11-27 02:55:27,102:INFO:  Epoch 196/600:  train Loss: 28.7337   val Loss: 31.9483   time: 89.12s   best: 31.3199
2023-11-27 02:56:55,608:INFO:  Epoch 197/600:  train Loss: 28.6925   val Loss: 31.6588   time: 88.49s   best: 31.3199
2023-11-27 02:58:24,119:INFO:  Epoch 198/600:  train Loss: 28.6466   val Loss: 31.7099   time: 88.50s   best: 31.3199
2023-11-27 02:59:52,586:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 02:59:52,606:INFO:  Epoch 199/600:  train Loss: 28.6383   val Loss: 31.2869   time: 88.45s   best: 31.2869
2023-11-27 03:01:21,664:INFO:  Epoch 200/600:  train Loss: 28.5572   val Loss: 31.4630   time: 89.05s   best: 31.2869
2023-11-27 03:02:50,090:INFO:  Epoch 201/600:  train Loss: 28.4756   val Loss: 32.0567   time: 88.43s   best: 31.2869
2023-11-27 03:04:18,550:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 03:04:18,569:INFO:  Epoch 202/600:  train Loss: 28.5716   val Loss: 31.0606   time: 88.45s   best: 31.0606
2023-11-27 03:05:46,919:INFO:  Epoch 203/600:  train Loss: 29.0596   val Loss: 31.2015   time: 88.35s   best: 31.0606
2023-11-27 03:07:15,478:INFO:  Epoch 204/600:  train Loss: 28.3766   val Loss: 34.0881   time: 88.56s   best: 31.0606
2023-11-27 03:08:44,115:INFO:  Epoch 205/600:  train Loss: 28.3566   val Loss: 31.3834   time: 88.62s   best: 31.0606
2023-11-27 03:10:12,509:INFO:  Epoch 206/600:  train Loss: 28.3859   val Loss: 31.6214   time: 88.39s   best: 31.0606
2023-11-27 03:11:40,894:INFO:  Epoch 207/600:  train Loss: 28.1543   val Loss: 32.7249   time: 88.38s   best: 31.0606
2023-11-27 03:13:09,514:INFO:  Epoch 208/600:  train Loss: 28.5048   val Loss: 32.9546   time: 88.61s   best: 31.0606
2023-11-27 03:14:38,042:INFO:  Epoch 209/600:  train Loss: 28.2724   val Loss: 31.1824   time: 88.53s   best: 31.0606
2023-11-27 03:16:07,154:INFO:  Epoch 210/600:  train Loss: 28.1261   val Loss: 32.6854   time: 89.10s   best: 31.0606
2023-11-27 03:17:36,497:INFO:  Epoch 211/600:  train Loss: 28.1170   val Loss: 31.4919   time: 89.34s   best: 31.0606
2023-11-27 03:19:05,502:INFO:  Epoch 212/600:  train Loss: 27.8766   val Loss: 31.4672   time: 89.00s   best: 31.0606
2023-11-27 03:20:33,790:INFO:  Epoch 213/600:  train Loss: 28.1993   val Loss: 31.5542   time: 88.28s   best: 31.0606
2023-11-27 03:22:02,734:INFO:  Epoch 214/600:  train Loss: 28.0451   val Loss: 31.9474   time: 88.93s   best: 31.0606
2023-11-27 03:23:31,093:INFO:  Epoch 215/600:  train Loss: 27.9580   val Loss: 31.2104   time: 88.36s   best: 31.0606
2023-11-27 03:24:59,726:INFO:  Epoch 216/600:  train Loss: 28.1275   val Loss: 33.5160   time: 88.63s   best: 31.0606
2023-11-27 03:26:28,507:INFO:  Epoch 217/600:  train Loss: 27.8455   val Loss: 32.1724   time: 88.77s   best: 31.0606
2023-11-27 03:27:57,401:INFO:  Epoch 218/600:  train Loss: 27.8170   val Loss: 31.2950   time: 88.89s   best: 31.0606
2023-11-27 03:29:25,792:INFO:  Epoch 219/600:  train Loss: 27.6108   val Loss: 31.3449   time: 88.39s   best: 31.0606
2023-11-27 03:30:54,436:INFO:  Epoch 220/600:  train Loss: 27.6393   val Loss: 31.3239   time: 88.64s   best: 31.0606
2023-11-27 03:32:23,141:INFO:  Epoch 221/600:  train Loss: 27.7365   val Loss: 31.2722   time: 88.69s   best: 31.0606
2023-11-27 03:33:51,964:INFO:  Epoch 222/600:  train Loss: 28.1120   val Loss: 35.4922   time: 88.82s   best: 31.0606
2023-11-27 03:35:20,669:INFO:  Epoch 223/600:  train Loss: 29.7790   val Loss: 32.3333   time: 88.69s   best: 31.0606
2023-11-27 03:36:49,176:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 03:36:49,196:INFO:  Epoch 224/600:  train Loss: 27.6439   val Loss: 30.6357   time: 88.49s   best: 30.6357
2023-11-27 03:38:17,977:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 03:38:17,996:INFO:  Epoch 225/600:  train Loss: 27.3540   val Loss: 30.3865   time: 88.78s   best: 30.3865
2023-11-27 03:39:46,803:INFO:  Epoch 226/600:  train Loss: 27.4180   val Loss: 34.3452   time: 88.79s   best: 30.3865
2023-11-27 03:41:15,335:INFO:  Epoch 227/600:  train Loss: 27.3665   val Loss: 30.4514   time: 88.52s   best: 30.3865
2023-11-27 03:42:43,934:INFO:  Epoch 228/600:  train Loss: 27.4519   val Loss: 31.4616   time: 88.59s   best: 30.3865
2023-11-27 03:44:12,336:INFO:  Epoch 229/600:  train Loss: 27.2815   val Loss: 30.5807   time: 88.40s   best: 30.3865
2023-11-27 03:45:40,945:INFO:  Epoch 230/600:  train Loss: 27.2548   val Loss: 31.7380   time: 88.61s   best: 30.3865
2023-11-27 03:47:09,447:INFO:  Epoch 231/600:  train Loss: 28.3453   val Loss: 30.7348   time: 88.50s   best: 30.3865
2023-11-27 03:48:38,009:INFO:  Epoch 232/600:  train Loss: 27.3288   val Loss: 30.3961   time: 88.55s   best: 30.3865
2023-11-27 03:50:06,296:INFO:  Epoch 233/600:  train Loss: 27.3476   val Loss: 32.8498   time: 88.27s   best: 30.3865
2023-11-27 03:51:35,306:INFO:  Epoch 234/600:  train Loss: 27.1989   val Loss: 30.4280   time: 89.01s   best: 30.3865
2023-11-27 03:53:03,939:INFO:  Epoch 235/600:  train Loss: 27.2079   val Loss: 30.9528   time: 88.62s   best: 30.3865
2023-11-27 03:54:32,730:INFO:  Epoch 236/600:  train Loss: 26.9919   val Loss: 30.4359   time: 88.78s   best: 30.3865
2023-11-27 03:56:01,170:INFO:  Epoch 237/600:  train Loss: 27.0595   val Loss: 30.9118   time: 88.43s   best: 30.3865
2023-11-27 03:57:29,731:INFO:  Epoch 238/600:  train Loss: 28.4376   val Loss: 31.9468   time: 88.56s   best: 30.3865
2023-11-27 03:58:58,198:INFO:  Epoch 239/600:  train Loss: 27.9606   val Loss: 31.3432   time: 88.45s   best: 30.3865
2023-11-27 04:00:26,569:INFO:  Epoch 240/600:  train Loss: 27.2818   val Loss: 30.5591   time: 88.37s   best: 30.3865
2023-11-27 04:01:54,928:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 04:01:54,947:INFO:  Epoch 241/600:  train Loss: 26.9255   val Loss: 30.3836   time: 88.35s   best: 30.3836
2023-11-27 04:03:23,887:INFO:  Epoch 242/600:  train Loss: 27.2281   val Loss: 30.9201   time: 88.93s   best: 30.3836
2023-11-27 04:04:52,598:INFO:  Epoch 243/600:  train Loss: 26.9009   val Loss: 31.9671   time: 88.70s   best: 30.3836
2023-11-27 04:06:21,272:INFO:  Epoch 244/600:  train Loss: 27.5690   val Loss: 30.8721   time: 88.67s   best: 30.3836
2023-11-27 04:07:49,594:INFO:  Epoch 245/600:  train Loss: 26.9763   val Loss: 30.7248   time: 88.32s   best: 30.3836
2023-11-27 04:09:18,134:INFO:  Epoch 246/600:  train Loss: 27.0440   val Loss: 33.2384   time: 88.54s   best: 30.3836
2023-11-27 04:10:46,451:INFO:  Epoch 247/600:  train Loss: 26.7883   val Loss: 30.3948   time: 88.30s   best: 30.3836
2023-11-27 04:12:15,022:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 04:12:15,041:INFO:  Epoch 248/600:  train Loss: 26.7252   val Loss: 29.9667   time: 88.57s   best: 29.9667
2023-11-27 04:13:43,314:INFO:  Epoch 249/600:  train Loss: 26.6631   val Loss: 30.5800   time: 88.27s   best: 29.9667
2023-11-27 04:15:12,442:INFO:  Epoch 250/600:  train Loss: 26.9429   val Loss: 30.8980   time: 89.12s   best: 29.9667
2023-11-27 04:16:40,783:INFO:  Epoch 251/600:  train Loss: 26.6689   val Loss: 33.0631   time: 88.33s   best: 29.9667
2023-11-27 04:18:09,391:INFO:  Epoch 252/600:  train Loss: 26.6636   val Loss: 30.3819   time: 88.61s   best: 29.9667
2023-11-27 04:19:38,457:INFO:  Epoch 253/600:  train Loss: 26.5678   val Loss: 30.5556   time: 89.05s   best: 29.9667
2023-11-27 04:21:07,070:INFO:  Epoch 254/600:  train Loss: 26.5104   val Loss: 32.6256   time: 88.61s   best: 29.9667
2023-11-27 04:22:35,686:INFO:  Epoch 255/600:  train Loss: 26.6275   val Loss: 30.0484   time: 88.59s   best: 29.9667
2023-11-27 04:24:04,525:INFO:  Epoch 256/600:  train Loss: 26.3321   val Loss: 30.6390   time: 88.83s   best: 29.9667
2023-11-27 04:25:32,823:INFO:  Epoch 257/600:  train Loss: 26.5742   val Loss: 31.9285   time: 88.30s   best: 29.9667
2023-11-27 04:27:01,352:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 04:27:01,371:INFO:  Epoch 258/600:  train Loss: 26.4067   val Loss: 29.6720   time: 88.52s   best: 29.6720
2023-11-27 04:28:29,675:INFO:  Epoch 259/600:  train Loss: 26.3127   val Loss: 30.1655   time: 88.29s   best: 29.6720
2023-11-27 04:29:58,120:INFO:  Epoch 260/600:  train Loss: 26.5406   val Loss: 30.1393   time: 88.43s   best: 29.6720
2023-11-27 04:31:26,649:INFO:  Epoch 261/600:  train Loss: 26.1849   val Loss: 30.2538   time: 88.53s   best: 29.6720
2023-11-27 04:32:55,803:INFO:  Epoch 262/600:  train Loss: 26.1735   val Loss: 30.8323   time: 89.15s   best: 29.6720
2023-11-27 04:34:24,777:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 04:34:24,802:INFO:  Epoch 263/600:  train Loss: 26.1471   val Loss: 29.3332   time: 88.96s   best: 29.3332
2023-11-27 04:35:53,304:INFO:  Epoch 264/600:  train Loss: 26.2691   val Loss: 32.1716   time: 88.50s   best: 29.3332
2023-11-27 04:37:21,720:INFO:  Epoch 265/600:  train Loss: 27.2265   val Loss: 30.1213   time: 88.40s   best: 29.3332
2023-11-27 04:38:50,263:INFO:  Epoch 266/600:  train Loss: 26.2805   val Loss: 30.0384   time: 88.53s   best: 29.3332
2023-11-27 04:40:18,761:INFO:  Epoch 267/600:  train Loss: 25.9347   val Loss: 29.5992   time: 88.50s   best: 29.3332
2023-11-27 04:41:47,199:INFO:  Epoch 268/600:  train Loss: 26.0762   val Loss: 30.6727   time: 88.44s   best: 29.3332
2023-11-27 04:43:15,869:INFO:  Epoch 269/600:  train Loss: 25.9921   val Loss: 34.0317   time: 88.67s   best: 29.3332
2023-11-27 04:44:44,595:INFO:  Epoch 270/600:  train Loss: 26.2514   val Loss: 30.5680   time: 88.71s   best: 29.3332
2023-11-27 04:46:12,966:INFO:  Epoch 271/600:  train Loss: 26.0829   val Loss: 30.1393   time: 88.37s   best: 29.3332
2023-11-27 04:47:41,475:INFO:  Epoch 272/600:  train Loss: 26.0516   val Loss: 29.4336   time: 88.50s   best: 29.3332
2023-11-27 04:49:09,842:INFO:  Epoch 273/600:  train Loss: 25.8273   val Loss: 29.6097   time: 88.37s   best: 29.3332
2023-11-27 04:50:38,237:INFO:  Epoch 274/600:  train Loss: 25.8363   val Loss: 29.5588   time: 88.39s   best: 29.3332
2023-11-27 04:52:06,543:INFO:  Epoch 275/600:  train Loss: 25.9566   val Loss: 29.8114   time: 88.31s   best: 29.3332
2023-11-27 04:53:35,300:INFO:  Epoch 276/600:  train Loss: 25.8003   val Loss: 29.9786   time: 88.76s   best: 29.3332
2023-11-27 04:55:03,809:INFO:  Epoch 277/600:  train Loss: 25.8395   val Loss: 30.4052   time: 88.50s   best: 29.3332
2023-11-27 04:56:32,326:INFO:  Epoch 278/600:  train Loss: 25.7908   val Loss: 30.3874   time: 88.52s   best: 29.3332
2023-11-27 04:58:00,629:INFO:  Epoch 279/600:  train Loss: 25.6332   val Loss: 29.8171   time: 88.30s   best: 29.3332
2023-11-27 04:59:29,026:INFO:  Epoch 280/600:  train Loss: 25.8649   val Loss: 31.1023   time: 88.40s   best: 29.3332
2023-11-27 05:00:57,365:INFO:  Epoch 281/600:  train Loss: 26.0945   val Loss: 29.6041   time: 88.34s   best: 29.3332
2023-11-27 05:02:25,823:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 05:02:25,843:INFO:  Epoch 282/600:  train Loss: 25.8017   val Loss: 28.9905   time: 88.45s   best: 28.9905
2023-11-27 05:03:54,366:INFO:  Epoch 283/600:  train Loss: 25.5701   val Loss: 30.0446   time: 88.52s   best: 28.9905
2023-11-27 05:05:22,673:INFO:  Epoch 284/600:  train Loss: 25.8289   val Loss: 29.2242   time: 88.31s   best: 28.9905
2023-11-27 05:06:50,904:INFO:  Epoch 285/600:  train Loss: 25.8677   val Loss: 31.6778   time: 88.23s   best: 28.9905
2023-11-27 05:08:19,190:INFO:  Epoch 286/600:  train Loss: 25.5439   val Loss: 29.3947   time: 88.27s   best: 28.9905
2023-11-27 05:09:47,814:INFO:  Epoch 287/600:  train Loss: 25.5904   val Loss: 30.2000   time: 88.61s   best: 28.9905
2023-11-27 05:11:16,627:INFO:  Epoch 288/600:  train Loss: 25.7129   val Loss: 29.6568   time: 88.80s   best: 28.9905
2023-11-27 05:12:44,909:INFO:  Epoch 289/600:  train Loss: 25.4953   val Loss: 30.1898   time: 88.28s   best: 28.9905
2023-11-27 05:14:13,148:INFO:  Epoch 290/600:  train Loss: 25.7471   val Loss: 30.4634   time: 88.23s   best: 28.9905
2023-11-27 05:15:41,342:INFO:  Epoch 291/600:  train Loss: 25.4064   val Loss: 29.2439   time: 88.19s   best: 28.9905
2023-11-27 05:17:10,232:INFO:  Epoch 292/600:  train Loss: 25.2968   val Loss: 29.2557   time: 88.89s   best: 28.9905
2023-11-27 05:18:39,113:INFO:  Epoch 293/600:  train Loss: 25.8554   val Loss: 29.2299   time: 88.87s   best: 28.9905
2023-11-27 05:20:07,742:INFO:  Epoch 294/600:  train Loss: 25.7805   val Loss: 29.3878   time: 88.62s   best: 28.9905
2023-11-27 05:21:36,612:INFO:  Epoch 295/600:  train Loss: 25.2390   val Loss: 29.3089   time: 88.87s   best: 28.9905
2023-11-27 05:23:05,175:INFO:  Epoch 296/600:  train Loss: 25.5279   val Loss: 30.9819   time: 88.56s   best: 28.9905
2023-11-27 05:24:33,501:INFO:  Epoch 297/600:  train Loss: 25.2105   val Loss: 29.6984   time: 88.32s   best: 28.9905
2023-11-27 05:26:02,082:INFO:  Epoch 298/600:  train Loss: 25.3167   val Loss: 30.8313   time: 88.57s   best: 28.9905
2023-11-27 05:27:30,466:INFO:  Epoch 299/600:  train Loss: 25.2881   val Loss: 29.6413   time: 88.37s   best: 28.9905
2023-11-27 05:28:59,076:INFO:  Epoch 300/600:  train Loss: 25.2075   val Loss: 29.4133   time: 88.61s   best: 28.9905
2023-11-27 05:30:27,274:INFO:  Epoch 301/600:  train Loss: 25.1884   val Loss: 29.4344   time: 88.20s   best: 28.9905
2023-11-27 05:31:55,789:INFO:  Epoch 302/600:  train Loss: 25.1069   val Loss: 29.1175   time: 88.51s   best: 28.9905
2023-11-27 05:33:24,396:INFO:  Epoch 303/600:  train Loss: 25.6960   val Loss: 29.5683   time: 88.59s   best: 28.9905
2023-11-27 05:34:53,846:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 05:34:53,865:INFO:  Epoch 304/600:  train Loss: 25.1257   val Loss: 28.9556   time: 89.45s   best: 28.9556
2023-11-27 05:36:22,340:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 05:36:22,374:INFO:  Epoch 305/600:  train Loss: 25.1154   val Loss: 28.9042   time: 88.46s   best: 28.9042
2023-11-27 05:37:51,306:INFO:  Epoch 306/600:  train Loss: 25.2953   val Loss: 29.7165   time: 88.92s   best: 28.9042
2023-11-27 05:39:19,572:INFO:  Epoch 307/600:  train Loss: 25.1032   val Loss: 29.2531   time: 88.26s   best: 28.9042
2023-11-27 05:40:48,399:INFO:  Epoch 308/600:  train Loss: 25.2235   val Loss: 31.0982   time: 88.82s   best: 28.9042
2023-11-27 05:42:16,556:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 05:42:16,575:INFO:  Epoch 309/600:  train Loss: 24.9854   val Loss: 28.6831   time: 88.14s   best: 28.6831
2023-11-27 05:43:45,003:INFO:  Epoch 310/600:  train Loss: 25.0173   val Loss: 28.8989   time: 88.42s   best: 28.6831
2023-11-27 05:45:13,503:INFO:  Epoch 311/600:  train Loss: 25.5336   val Loss: 30.3528   time: 88.50s   best: 28.6831
2023-11-27 05:46:41,962:INFO:  Epoch 312/600:  train Loss: 25.2205   val Loss: 28.8677   time: 88.46s   best: 28.6831
2023-11-27 05:48:10,689:INFO:  Epoch 313/600:  train Loss: 24.9080   val Loss: 29.3978   time: 88.71s   best: 28.6831
2023-11-27 05:49:39,630:INFO:  Epoch 314/600:  train Loss: 25.0001   val Loss: 29.4837   time: 88.94s   best: 28.6831
2023-11-27 05:51:08,409:INFO:  Epoch 315/600:  train Loss: 24.9776   val Loss: 29.2587   time: 88.78s   best: 28.6831
2023-11-27 05:52:36,922:INFO:  Epoch 316/600:  train Loss: 24.7325   val Loss: 29.3307   time: 88.50s   best: 28.6831
2023-11-27 05:54:05,703:INFO:  Epoch 317/600:  train Loss: 25.0659   val Loss: 29.2778   time: 88.77s   best: 28.6831
2023-11-27 05:55:34,541:INFO:  Epoch 318/600:  train Loss: 24.8596   val Loss: 29.2847   time: 88.84s   best: 28.6831
2023-11-27 05:57:02,983:INFO:  Epoch 319/600:  train Loss: 25.7132   val Loss: 29.6748   time: 88.44s   best: 28.6831
2023-11-27 05:58:31,392:INFO:  Epoch 320/600:  train Loss: 25.0584   val Loss: 29.7523   time: 88.40s   best: 28.6831
2023-11-27 06:00:00,323:INFO:  Epoch 321/600:  train Loss: 24.7380   val Loss: 29.0943   time: 88.93s   best: 28.6831
2023-11-27 06:01:28,965:INFO:  Epoch 322/600:  train Loss: 24.9166   val Loss: 29.3201   time: 88.64s   best: 28.6831
2023-11-27 06:02:57,458:INFO:  Epoch 323/600:  train Loss: 24.6972   val Loss: 29.2298   time: 88.48s   best: 28.6831
2023-11-27 06:04:25,896:INFO:  Epoch 324/600:  train Loss: 24.6643   val Loss: 29.9828   time: 88.43s   best: 28.6831
2023-11-27 06:05:54,171:INFO:  Epoch 325/600:  train Loss: 24.8628   val Loss: 29.4139   time: 88.27s   best: 28.6831
2023-11-27 06:07:23,118:INFO:  Epoch 326/600:  train Loss: 24.7485   val Loss: 29.4232   time: 88.95s   best: 28.6831
2023-11-27 06:08:51,883:INFO:  Epoch 327/600:  train Loss: 24.7588   val Loss: 29.0501   time: 88.75s   best: 28.6831
2023-11-27 06:10:20,337:INFO:  Epoch 328/600:  train Loss: 24.8391   val Loss: 29.5160   time: 88.45s   best: 28.6831
2023-11-27 06:11:48,623:INFO:  Epoch 329/600:  train Loss: 24.5211   val Loss: 29.0830   time: 88.27s   best: 28.6831
2023-11-27 06:13:16,999:INFO:  Epoch 330/600:  train Loss: 24.7386   val Loss: 28.9252   time: 88.36s   best: 28.6831
2023-11-27 06:14:45,409:INFO:  Epoch 331/600:  train Loss: 24.7593   val Loss: 30.8738   time: 88.39s   best: 28.6831
2023-11-27 06:16:13,768:INFO:  Epoch 332/600:  train Loss: 24.8762   val Loss: 29.1019   time: 88.36s   best: 28.6831
2023-11-27 06:17:42,069:INFO:  Epoch 333/600:  train Loss: 24.4234   val Loss: 29.5424   time: 88.30s   best: 28.6831
2023-11-27 06:19:10,558:INFO:  Epoch 334/600:  train Loss: 24.6394   val Loss: 29.1948   time: 88.48s   best: 28.6831
2023-11-27 06:20:38,800:INFO:  Epoch 335/600:  train Loss: 24.6424   val Loss: 29.3663   time: 88.24s   best: 28.6831
2023-11-27 06:22:07,549:INFO:  Epoch 336/600:  train Loss: 24.4474   val Loss: 29.7113   time: 88.74s   best: 28.6831
2023-11-27 06:23:36,147:INFO:  Epoch 337/600:  train Loss: 24.4053   val Loss: 29.0146   time: 88.60s   best: 28.6831
2023-11-27 06:25:04,765:INFO:  Epoch 338/600:  train Loss: 24.5832   val Loss: 29.3748   time: 88.61s   best: 28.6831
2023-11-27 06:26:33,075:INFO:  Epoch 339/600:  train Loss: 24.4333   val Loss: 34.1025   time: 88.31s   best: 28.6831
2023-11-27 06:28:01,470:INFO:  Epoch 340/600:  train Loss: 24.5245   val Loss: 30.7269   time: 88.39s   best: 28.6831
2023-11-27 06:29:29,956:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 06:29:29,976:INFO:  Epoch 341/600:  train Loss: 25.0343   val Loss: 28.4134   time: 88.47s   best: 28.4134
2023-11-27 06:30:58,279:INFO:  Epoch 342/600:  train Loss: 24.3573   val Loss: 29.4729   time: 88.29s   best: 28.4134
2023-11-27 06:32:26,657:INFO:  Epoch 343/600:  train Loss: 24.3816   val Loss: 28.5597   time: 88.37s   best: 28.4134
2023-11-27 06:33:55,630:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 06:33:55,650:INFO:  Epoch 344/600:  train Loss: 24.3085   val Loss: 28.3428   time: 88.97s   best: 28.3428
2023-11-27 06:35:24,404:INFO:  Epoch 345/600:  train Loss: 24.3684   val Loss: 29.0626   time: 88.74s   best: 28.3428
2023-11-27 06:36:52,800:INFO:  Epoch 346/600:  train Loss: 24.2661   val Loss: 28.3818   time: 88.38s   best: 28.3428
2023-11-27 06:38:21,473:INFO:  Epoch 347/600:  train Loss: 24.6822   val Loss: 28.5617   time: 88.67s   best: 28.3428
2023-11-27 06:39:49,877:INFO:  Epoch 348/600:  train Loss: 24.4297   val Loss: 29.0109   time: 88.40s   best: 28.3428
2023-11-27 06:41:18,555:INFO:  Epoch 349/600:  train Loss: 24.3040   val Loss: 30.0939   time: 88.68s   best: 28.3428
2023-11-27 06:42:46,978:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 06:42:46,997:INFO:  Epoch 350/600:  train Loss: 24.3667   val Loss: 28.3292   time: 88.42s   best: 28.3292
2023-11-27 06:44:15,643:INFO:  Epoch 351/600:  train Loss: 24.1968   val Loss: 28.8464   time: 88.63s   best: 28.3292
2023-11-27 06:45:44,055:INFO:  Epoch 352/600:  train Loss: 25.0392   val Loss: 30.9115   time: 88.41s   best: 28.3292
2023-11-27 06:47:12,377:INFO:  Epoch 353/600:  train Loss: 24.4671   val Loss: 28.5912   time: 88.31s   best: 28.3292
2023-11-27 06:48:41,202:INFO:  Epoch 354/600:  train Loss: 24.0749   val Loss: 30.1926   time: 88.82s   best: 28.3292
2023-11-27 06:50:09,272:INFO:  Epoch 355/600:  train Loss: 24.1515   val Loss: 29.1247   time: 88.07s   best: 28.3292
2023-11-27 06:51:38,135:INFO:  Epoch 356/600:  train Loss: 24.2584   val Loss: 28.6186   time: 88.86s   best: 28.3292
2023-11-27 06:53:06,392:INFO:  Epoch 357/600:  train Loss: 24.1538   val Loss: 29.1400   time: 88.26s   best: 28.3292
2023-11-27 06:54:34,716:INFO:  Epoch 358/600:  train Loss: 24.1751   val Loss: 28.4886   time: 88.31s   best: 28.3292
2023-11-27 06:56:03,454:INFO:  Epoch 359/600:  train Loss: 23.9565   val Loss: 28.5605   time: 88.74s   best: 28.3292
2023-11-27 06:57:31,755:INFO:  Epoch 360/600:  train Loss: 24.4464   val Loss: 33.3469   time: 88.29s   best: 28.3292
2023-11-27 06:59:00,134:INFO:  Epoch 361/600:  train Loss: 24.6711   val Loss: 28.9332   time: 88.38s   best: 28.3292
2023-11-27 07:00:28,393:INFO:  Epoch 362/600:  train Loss: 24.2037   val Loss: 28.3976   time: 88.25s   best: 28.3292
2023-11-27 07:01:56,682:INFO:  Epoch 363/600:  train Loss: 24.0003   val Loss: 29.6789   time: 88.28s   best: 28.3292
2023-11-27 07:03:25,115:INFO:  Epoch 364/600:  train Loss: 23.9189   val Loss: 28.8708   time: 88.43s   best: 28.3292
2023-11-27 07:04:53,514:INFO:  Epoch 365/600:  train Loss: 23.9830   val Loss: 28.6399   time: 88.39s   best: 28.3292
2023-11-27 07:06:22,219:INFO:  Epoch 366/600:  train Loss: 25.4905   val Loss: 30.4536   time: 88.70s   best: 28.3292
2023-11-27 07:07:50,485:INFO:  Epoch 367/600:  train Loss: 24.4865   val Loss: 28.7188   time: 88.25s   best: 28.3292
2023-11-27 07:09:18,919:INFO:  Epoch 368/600:  train Loss: 23.9910   val Loss: 28.4378   time: 88.43s   best: 28.3292
2023-11-27 07:10:47,164:INFO:  Epoch 369/600:  train Loss: 23.8705   val Loss: 28.3896   time: 88.24s   best: 28.3292
2023-11-27 07:12:15,576:INFO:  Epoch 370/600:  train Loss: 23.8638   val Loss: 28.6564   time: 88.41s   best: 28.3292
2023-11-27 07:13:43,889:INFO:  Epoch 371/600:  train Loss: 24.3856   val Loss: 29.5362   time: 88.30s   best: 28.3292
2023-11-27 07:15:12,397:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 07:15:12,416:INFO:  Epoch 372/600:  train Loss: 24.0139   val Loss: 28.1778   time: 88.50s   best: 28.1778
2023-11-27 07:16:40,840:INFO:  Epoch 373/600:  train Loss: 25.0729   val Loss: 29.0615   time: 88.41s   best: 28.1778
2023-11-27 07:18:09,409:INFO:  Epoch 374/600:  train Loss: 23.9362   val Loss: 28.7187   time: 88.57s   best: 28.1778
2023-11-27 07:19:37,898:INFO:  Epoch 375/600:  train Loss: 24.0223   val Loss: 28.4262   time: 88.48s   best: 28.1778
2023-11-27 07:21:06,833:INFO:  Epoch 376/600:  train Loss: 23.7887   val Loss: 28.3607   time: 88.92s   best: 28.1778
2023-11-27 07:22:35,110:INFO:  Epoch 377/600:  train Loss: 23.9283   val Loss: 28.5205   time: 88.27s   best: 28.1778
2023-11-27 07:24:03,459:INFO:  Epoch 378/600:  train Loss: 23.8374   val Loss: 28.5551   time: 88.34s   best: 28.1778
2023-11-27 07:25:31,721:INFO:  Epoch 379/600:  train Loss: 24.2086   val Loss: 28.2435   time: 88.25s   best: 28.1778
2023-11-27 07:27:00,224:INFO:  Epoch 380/600:  train Loss: 23.9757   val Loss: 28.4294   time: 88.49s   best: 28.1778
2023-11-27 07:28:28,609:INFO:  Epoch 381/600:  train Loss: 23.6579   val Loss: 28.5261   time: 88.37s   best: 28.1778
2023-11-27 07:29:57,017:INFO:  Epoch 382/600:  train Loss: 24.0243   val Loss: 28.2485   time: 88.40s   best: 28.1778
2023-11-27 07:31:25,259:INFO:  Epoch 383/600:  train Loss: 23.9236   val Loss: 30.8097   time: 88.23s   best: 28.1778
2023-11-27 07:32:53,714:INFO:  Epoch 384/600:  train Loss: 23.8706   val Loss: 28.6040   time: 88.44s   best: 28.1778
2023-11-27 07:34:22,222:INFO:  Epoch 385/600:  train Loss: 23.8960   val Loss: 30.0691   time: 88.51s   best: 28.1778
2023-11-27 07:35:50,653:INFO:  Epoch 386/600:  train Loss: 23.7721   val Loss: 28.7354   time: 88.42s   best: 28.1778
2023-11-27 07:37:18,899:INFO:  Epoch 387/600:  train Loss: 23.6625   val Loss: 28.1855   time: 88.23s   best: 28.1778
2023-11-27 07:38:47,273:INFO:  Epoch 388/600:  train Loss: 23.6944   val Loss: 28.3172   time: 88.37s   best: 28.1778
2023-11-27 07:40:15,965:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 07:40:16,004:INFO:  Epoch 389/600:  train Loss: 23.9213   val Loss: 28.1272   time: 88.67s   best: 28.1272
2023-11-27 07:41:44,676:INFO:  Epoch 390/600:  train Loss: 23.6327   val Loss: 28.2100   time: 88.67s   best: 28.1272
2023-11-27 07:43:13,015:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 07:43:13,034:INFO:  Epoch 391/600:  train Loss: 23.5888   val Loss: 27.8524   time: 88.32s   best: 27.8524
2023-11-27 07:44:41,703:INFO:  Epoch 392/600:  train Loss: 23.6899   val Loss: 28.1476   time: 88.67s   best: 27.8524
2023-11-27 07:46:09,893:INFO:  Epoch 393/600:  train Loss: 23.5647   val Loss: 28.4119   time: 88.18s   best: 27.8524
2023-11-27 07:47:38,192:INFO:  Epoch 394/600:  train Loss: 23.6272   val Loss: 28.6517   time: 88.30s   best: 27.8524
2023-11-27 07:49:06,421:INFO:  Epoch 395/600:  train Loss: 23.4971   val Loss: 28.5018   time: 88.22s   best: 27.8524
2023-11-27 07:50:34,793:INFO:  Epoch 396/600:  train Loss: 24.1779   val Loss: 28.5580   time: 88.37s   best: 27.8524
2023-11-27 07:52:03,067:INFO:  Epoch 397/600:  train Loss: 23.3836   val Loss: 28.4740   time: 88.27s   best: 27.8524
2023-11-27 07:53:31,456:INFO:  Epoch 398/600:  train Loss: 23.6925   val Loss: 33.2012   time: 88.39s   best: 27.8524
2023-11-27 07:54:59,916:INFO:  Epoch 399/600:  train Loss: 23.7470   val Loss: 28.2139   time: 88.45s   best: 27.8524
2023-11-27 07:56:28,702:INFO:  Epoch 400/600:  train Loss: 23.6158   val Loss: 28.9019   time: 88.79s   best: 27.8524
2023-11-27 07:57:56,967:INFO:  Epoch 401/600:  train Loss: 23.5154   val Loss: 28.2630   time: 88.25s   best: 27.8524
2023-11-27 07:59:25,411:INFO:  Epoch 402/600:  train Loss: 23.4914   val Loss: 29.7756   time: 88.44s   best: 27.8524
2023-11-27 08:00:54,065:INFO:  Epoch 403/600:  train Loss: 23.7112   val Loss: 28.8669   time: 88.65s   best: 27.8524
2023-11-27 08:02:22,342:INFO:  Epoch 404/600:  train Loss: 24.2694   val Loss: 28.8893   time: 88.28s   best: 27.8524
2023-11-27 08:03:50,690:INFO:  Epoch 405/600:  train Loss: 23.7343   val Loss: 28.1258   time: 88.35s   best: 27.8524
2023-11-27 08:05:19,461:INFO:  Epoch 406/600:  train Loss: 23.6221   val Loss: 28.9712   time: 88.76s   best: 27.8524
2023-11-27 08:06:47,614:INFO:  Epoch 407/600:  train Loss: 23.4132   val Loss: 28.0963   time: 88.14s   best: 27.8524
2023-11-27 08:08:15,982:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 08:08:16,001:INFO:  Epoch 408/600:  train Loss: 23.3031   val Loss: 27.8278   time: 88.35s   best: 27.8278
2023-11-27 08:09:44,207:INFO:  Epoch 409/600:  train Loss: 23.3799   val Loss: 28.1852   time: 88.19s   best: 27.8278
2023-11-27 08:11:12,529:INFO:  Epoch 410/600:  train Loss: 23.3189   val Loss: 28.3862   time: 88.31s   best: 27.8278
2023-11-27 08:12:40,675:INFO:  Epoch 411/600:  train Loss: 23.6024   val Loss: 29.0229   time: 88.13s   best: 27.8278
2023-11-27 08:14:08,827:INFO:  Epoch 412/600:  train Loss: 23.2828   val Loss: 28.0334   time: 88.15s   best: 27.8278
2023-11-27 08:15:37,019:INFO:  Epoch 413/600:  train Loss: 23.2529   val Loss: 28.0282   time: 88.18s   best: 27.8278
2023-11-27 08:17:05,582:INFO:  Epoch 414/600:  train Loss: 23.6231   val Loss: 28.3172   time: 88.55s   best: 27.8278
2023-11-27 08:18:33,752:INFO:  Epoch 415/600:  train Loss: 23.3173   val Loss: 28.2697   time: 88.17s   best: 27.8278
2023-11-27 08:20:02,035:INFO:  Epoch 416/600:  train Loss: 23.5098   val Loss: 33.4888   time: 88.27s   best: 27.8278
2023-11-27 08:21:30,733:INFO:  Epoch 417/600:  train Loss: 23.5715   val Loss: 28.6323   time: 88.70s   best: 27.8278
2023-11-27 08:22:59,027:INFO:  Epoch 418/600:  train Loss: 23.8109   val Loss: 28.5170   time: 88.28s   best: 27.8278
2023-11-27 08:24:27,059:INFO:  Epoch 419/600:  train Loss: 23.5525   val Loss: 29.4543   time: 88.02s   best: 27.8278
2023-11-27 08:25:55,238:INFO:  Epoch 420/600:  train Loss: 23.5505   val Loss: 28.3718   time: 88.18s   best: 27.8278
2023-11-27 08:27:23,783:INFO:  Epoch 421/600:  train Loss: 23.1562   val Loss: 28.0878   time: 88.54s   best: 27.8278
2023-11-27 08:28:52,576:INFO:  Epoch 422/600:  train Loss: 23.1063   val Loss: 28.2206   time: 88.78s   best: 27.8278
2023-11-27 08:30:20,760:INFO:  Epoch 423/600:  train Loss: 23.4247   val Loss: 27.8938   time: 88.17s   best: 27.8278
2023-11-27 08:31:49,203:INFO:  Epoch 424/600:  train Loss: 23.0846   val Loss: 28.0351   time: 88.44s   best: 27.8278
2023-11-27 08:33:17,360:INFO:  Epoch 425/600:  train Loss: 23.3592   val Loss: 28.2519   time: 88.16s   best: 27.8278
2023-11-27 08:34:45,423:INFO:  Epoch 426/600:  train Loss: 23.2084   val Loss: 30.3430   time: 88.06s   best: 27.8278
2023-11-27 08:36:13,444:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 08:36:13,463:INFO:  Epoch 427/600:  train Loss: 23.1303   val Loss: 27.4592   time: 88.00s   best: 27.4592
2023-11-27 08:37:41,985:INFO:  Epoch 428/600:  train Loss: 23.3260   val Loss: 28.0765   time: 88.51s   best: 27.4592
2023-11-27 08:39:10,199:INFO:  Epoch 429/600:  train Loss: 23.3910   val Loss: 28.2784   time: 88.20s   best: 27.4592
2023-11-27 08:40:38,489:INFO:  Epoch 430/600:  train Loss: 23.2647   val Loss: 27.6350   time: 88.28s   best: 27.4592
2023-11-27 08:42:06,655:INFO:  Epoch 431/600:  train Loss: 23.0190   val Loss: 28.3643   time: 88.16s   best: 27.4592
2023-11-27 08:43:35,117:INFO:  Epoch 432/600:  train Loss: 23.1484   val Loss: 28.0018   time: 88.45s   best: 27.4592
2023-11-27 08:45:03,318:INFO:  Epoch 433/600:  train Loss: 23.0598   val Loss: 27.8806   time: 88.19s   best: 27.4592
2023-11-27 08:46:32,019:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 08:46:32,039:INFO:  Epoch 434/600:  train Loss: 23.0776   val Loss: 27.4220   time: 88.70s   best: 27.4220
2023-11-27 08:48:00,155:INFO:  Epoch 435/600:  train Loss: 23.1784   val Loss: 28.0390   time: 88.10s   best: 27.4220
2023-11-27 08:49:28,412:INFO:  Epoch 436/600:  train Loss: 23.0216   val Loss: 28.8324   time: 88.26s   best: 27.4220
2023-11-27 08:50:56,592:INFO:  Epoch 437/600:  train Loss: 23.3326   val Loss: 27.6407   time: 88.17s   best: 27.4220
2023-11-27 08:52:24,992:INFO:  Epoch 438/600:  train Loss: 23.0869   val Loss: 28.2862   time: 88.39s   best: 27.4220
2023-11-27 08:53:53,234:INFO:  Epoch 439/600:  train Loss: 22.9622   val Loss: 27.8440   time: 88.23s   best: 27.4220
2023-11-27 08:55:21,640:INFO:  Epoch 440/600:  train Loss: 22.9239   val Loss: 27.4410   time: 88.41s   best: 27.4220
2023-11-27 08:56:49,728:INFO:  Epoch 441/600:  train Loss: 23.5895   val Loss: 28.5581   time: 88.07s   best: 27.4220
2023-11-27 08:58:17,916:INFO:  Epoch 442/600:  train Loss: 22.9135   val Loss: 27.6627   time: 88.18s   best: 27.4220
2023-11-27 08:59:46,422:INFO:  Epoch 443/600:  train Loss: 22.9235   val Loss: 27.9418   time: 88.50s   best: 27.4220
2023-11-27 09:01:15,031:INFO:  Epoch 444/600:  train Loss: 23.2422   val Loss: 28.8761   time: 88.60s   best: 27.4220
2023-11-27 09:02:43,395:INFO:  Epoch 445/600:  train Loss: 22.9363   val Loss: 28.1260   time: 88.35s   best: 27.4220
2023-11-27 09:04:11,622:INFO:  Epoch 446/600:  train Loss: 23.1772   val Loss: 27.8912   time: 88.22s   best: 27.4220
2023-11-27 09:05:40,072:INFO:  Epoch 447/600:  train Loss: 23.1880   val Loss: 44.1119   time: 88.45s   best: 27.4220
2023-11-27 09:07:08,375:INFO:  Epoch 448/600:  train Loss: 24.6142   val Loss: 27.9066   time: 88.30s   best: 27.4220
2023-11-27 09:08:36,337:INFO:  Epoch 449/600:  train Loss: 22.8485   val Loss: 27.7285   time: 87.96s   best: 27.4220
2023-11-27 09:10:04,485:INFO:  Epoch 450/600:  train Loss: 23.0038   val Loss: 27.9239   time: 88.14s   best: 27.4220
2023-11-27 09:11:32,434:INFO:  Epoch 451/600:  train Loss: 22.7870   val Loss: 28.2360   time: 87.94s   best: 27.4220
2023-11-27 09:13:00,489:INFO:  Epoch 452/600:  train Loss: 22.9377   val Loss: 27.8994   time: 88.05s   best: 27.4220
2023-11-27 09:14:29,041:INFO:  Epoch 453/600:  train Loss: 23.0019   val Loss: 28.5121   time: 88.54s   best: 27.4220
2023-11-27 09:15:57,879:INFO:  Epoch 454/600:  train Loss: 22.9503   val Loss: 27.4483   time: 88.84s   best: 27.4220
2023-11-27 09:17:25,965:INFO:  Epoch 455/600:  train Loss: 23.4125   val Loss: 28.4759   time: 88.09s   best: 27.4220
2023-11-27 09:18:54,203:INFO:  Epoch 456/600:  train Loss: 22.7917   val Loss: 27.6702   time: 88.24s   best: 27.4220
2023-11-27 09:20:22,562:INFO:  Epoch 457/600:  train Loss: 23.7472   val Loss: 31.3056   time: 88.36s   best: 27.4220
2023-11-27 09:21:50,713:INFO:  Epoch 458/600:  train Loss: 23.7472   val Loss: 28.0218   time: 88.14s   best: 27.4220
2023-11-27 09:23:18,672:INFO:  Epoch 459/600:  train Loss: 22.8860   val Loss: 27.6807   time: 87.96s   best: 27.4220
2023-11-27 09:24:47,023:INFO:  Epoch 460/600:  train Loss: 22.7707   val Loss: 27.9038   time: 88.35s   best: 27.4220
2023-11-27 09:26:15,587:INFO:  Epoch 461/600:  train Loss: 22.7674   val Loss: 28.3943   time: 88.55s   best: 27.4220
2023-11-27 09:27:43,892:INFO:  Epoch 462/600:  train Loss: 23.0727   val Loss: 28.1405   time: 88.29s   best: 27.4220
2023-11-27 09:29:11,941:INFO:  Epoch 463/600:  train Loss: 22.6524   val Loss: 29.7585   time: 88.05s   best: 27.4220
2023-11-27 09:30:40,057:INFO:  Epoch 464/600:  train Loss: 22.9055   val Loss: 28.2124   time: 88.11s   best: 27.4220
2023-11-27 09:32:08,215:INFO:  Epoch 465/600:  train Loss: 22.6689   val Loss: 28.0294   time: 88.15s   best: 27.4220
2023-11-27 09:33:36,380:INFO:  Epoch 466/600:  train Loss: 22.6171   val Loss: 27.6938   time: 88.15s   best: 27.4220
2023-11-27 09:35:04,246:INFO:  Epoch 467/600:  train Loss: 22.7061   val Loss: 27.6342   time: 87.87s   best: 27.4220
2023-11-27 09:36:32,520:INFO:  Epoch 468/600:  train Loss: 23.6948   val Loss: 27.9158   time: 88.26s   best: 27.4220
2023-11-27 09:38:00,377:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 09:38:00,396:INFO:  Epoch 469/600:  train Loss: 22.7154   val Loss: 27.2968   time: 87.85s   best: 27.2968
2023-11-27 09:39:28,698:INFO:  Epoch 470/600:  train Loss: 22.7589   val Loss: 27.6086   time: 88.30s   best: 27.2968
2023-11-27 09:40:56,623:INFO:  Epoch 471/600:  train Loss: 22.5915   val Loss: 27.9795   time: 87.91s   best: 27.2968
2023-11-27 09:42:24,476:INFO:  Epoch 472/600:  train Loss: 22.9111   val Loss: 28.8071   time: 87.84s   best: 27.2968
2023-11-27 09:43:52,844:INFO:  Epoch 473/600:  train Loss: 22.7325   val Loss: 27.9877   time: 88.37s   best: 27.2968
2023-11-27 09:45:21,215:INFO:  Epoch 474/600:  train Loss: 23.1521   val Loss: 28.3451   time: 88.37s   best: 27.2968
2023-11-27 09:46:49,694:INFO:  Epoch 475/600:  train Loss: 22.7738   val Loss: 28.3924   time: 88.48s   best: 27.2968
2023-11-27 09:48:17,800:INFO:  Epoch 476/600:  train Loss: 23.0168   val Loss: 27.8815   time: 88.11s   best: 27.2968
2023-11-27 09:49:45,731:INFO:  Epoch 477/600:  train Loss: 22.6930   val Loss: 31.9790   time: 87.93s   best: 27.2968
2023-11-27 09:51:14,219:INFO:  Epoch 478/600:  train Loss: 22.7082   val Loss: 27.9416   time: 88.49s   best: 27.2968
2023-11-27 09:52:42,186:INFO:  Epoch 479/600:  train Loss: 22.6925   val Loss: 27.8813   time: 87.97s   best: 27.2968
2023-11-27 09:54:10,299:INFO:  Epoch 480/600:  train Loss: 23.4290   val Loss: 29.8991   time: 88.10s   best: 27.2968
2023-11-27 09:55:38,190:INFO:  Epoch 481/600:  train Loss: 22.9156   val Loss: 27.8663   time: 87.89s   best: 27.2968
2023-11-27 09:57:06,133:INFO:  Epoch 482/600:  train Loss: 22.5264   val Loss: 27.6792   time: 87.94s   best: 27.2968
2023-11-27 09:58:34,053:INFO:  Epoch 483/600:  train Loss: 22.6525   val Loss: 27.4166   time: 87.92s   best: 27.2968
2023-11-27 10:00:02,644:INFO:  Epoch 484/600:  train Loss: 22.5297   val Loss: 27.8170   time: 88.59s   best: 27.2968
2023-11-27 10:01:30,533:INFO:  Epoch 485/600:  train Loss: 22.4389   val Loss: 27.6089   time: 87.88s   best: 27.2968
2023-11-27 10:02:58,941:INFO:  Epoch 486/600:  train Loss: 22.4638   val Loss: 27.8166   time: 88.40s   best: 27.2968
2023-11-27 10:04:26,849:INFO:  Epoch 487/600:  train Loss: 22.4691   val Loss: 27.7011   time: 87.91s   best: 27.2968
2023-11-27 10:05:54,832:INFO:  Epoch 488/600:  train Loss: 22.4879   val Loss: 27.6010   time: 87.98s   best: 27.2968
2023-11-27 10:07:22,704:INFO:  Epoch 489/600:  train Loss: 22.4706   val Loss: 27.6134   time: 87.87s   best: 27.2968
2023-11-27 10:08:50,758:INFO:  Epoch 490/600:  train Loss: 22.4076   val Loss: 28.1522   time: 88.04s   best: 27.2968
2023-11-27 10:10:18,830:INFO:  Epoch 491/600:  train Loss: 22.6349   val Loss: 28.1355   time: 88.07s   best: 27.2968
2023-11-27 10:11:46,907:INFO:  Epoch 492/600:  train Loss: 22.3371   val Loss: 27.8671   time: 88.08s   best: 27.2968
2023-11-27 10:13:14,760:INFO:  Epoch 493/600:  train Loss: 22.7495   val Loss: 28.2815   time: 87.85s   best: 27.2968
2023-11-27 10:14:42,694:INFO:  Epoch 494/600:  train Loss: 22.8889   val Loss: 28.2162   time: 87.93s   best: 27.2968
2023-11-27 10:16:10,535:INFO:  Epoch 495/600:  train Loss: 22.3843   val Loss: 27.5754   time: 87.84s   best: 27.2968
2023-11-27 10:17:38,568:INFO:  Epoch 496/600:  train Loss: 22.6278   val Loss: 27.5974   time: 88.02s   best: 27.2968
2023-11-27 10:19:06,374:INFO:  Epoch 497/600:  train Loss: 22.2803   val Loss: 27.8099   time: 87.81s   best: 27.2968
2023-11-27 10:20:34,861:INFO:  Epoch 498/600:  train Loss: 22.3138   val Loss: 27.8033   time: 88.47s   best: 27.2968
2023-11-27 10:22:02,574:INFO:  Epoch 499/600:  train Loss: 22.4456   val Loss: 27.8647   time: 87.70s   best: 27.2968
2023-11-27 10:23:30,430:INFO:  Epoch 500/600:  train Loss: 22.4259   val Loss: 28.7500   time: 87.84s   best: 27.2968
2023-11-27 10:24:58,227:INFO:  Epoch 501/600:  train Loss: 24.8253   val Loss: 28.2661   time: 87.80s   best: 27.2968
2023-11-27 10:26:26,147:INFO:  Epoch 502/600:  train Loss: 23.8235   val Loss: 27.4646   time: 87.91s   best: 27.2968
2023-11-27 10:27:54,110:INFO:  Epoch 503/600:  train Loss: 23.0922   val Loss: 27.6647   time: 87.96s   best: 27.2968
2023-11-27 10:29:22,076:INFO:  Epoch 504/600:  train Loss: 22.4211   val Loss: 28.5319   time: 87.96s   best: 27.2968
2023-11-27 10:30:49,962:INFO:  Epoch 505/600:  train Loss: 22.2972   val Loss: 27.4948   time: 87.87s   best: 27.2968
2023-11-27 10:32:17,980:INFO:  Epoch 506/600:  train Loss: 22.2420   val Loss: 27.3159   time: 88.02s   best: 27.2968
2023-11-27 10:33:45,878:INFO:  Epoch 507/600:  train Loss: 22.4852   val Loss: 27.8311   time: 87.90s   best: 27.2968
2023-11-27 10:35:13,978:INFO:  Epoch 508/600:  train Loss: 22.3745   val Loss: 27.5104   time: 88.09s   best: 27.2968
2023-11-27 10:36:42,513:INFO:  Epoch 509/600:  train Loss: 22.4713   val Loss: 27.4640   time: 88.53s   best: 27.2968
2023-11-27 10:38:10,742:INFO:  Epoch 510/600:  train Loss: 22.3386   val Loss: 27.6108   time: 88.23s   best: 27.2968
2023-11-27 10:39:38,995:INFO:  Epoch 511/600:  train Loss: 22.4022   val Loss: 28.8552   time: 88.25s   best: 27.2968
2023-11-27 10:41:06,921:INFO:  Epoch 512/600:  train Loss: 22.4257   val Loss: 27.5609   time: 87.91s   best: 27.2968
2023-11-27 10:42:34,776:INFO:  Epoch 513/600:  train Loss: 22.1803   val Loss: 27.4867   time: 87.84s   best: 27.2968
2023-11-27 10:44:03,037:INFO:  Epoch 514/600:  train Loss: 22.4801   val Loss: 27.7611   time: 88.26s   best: 27.2968
2023-11-27 10:45:30,753:INFO:  Epoch 515/600:  train Loss: 22.3740   val Loss: 27.3166   time: 87.69s   best: 27.2968
2023-11-27 10:46:58,594:INFO:  Epoch 516/600:  train Loss: 22.1711   val Loss: 27.5480   time: 87.84s   best: 27.2968
2023-11-27 10:48:26,748:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 10:48:26,766:INFO:  Epoch 517/600:  train Loss: 22.3701   val Loss: 27.1010   time: 88.14s   best: 27.1010
2023-11-27 10:49:54,650:INFO:  Epoch 518/600:  train Loss: 23.6150   val Loss: 32.7682   time: 87.88s   best: 27.1010
2023-11-27 10:51:22,572:INFO:  Epoch 519/600:  train Loss: 24.5893   val Loss: 30.4655   time: 87.91s   best: 27.1010
2023-11-27 10:52:50,262:INFO:  Epoch 520/600:  train Loss: 22.5080   val Loss: 27.2937   time: 87.69s   best: 27.1010
2023-11-27 10:54:17,923:INFO:  Epoch 521/600:  train Loss: 22.2066   val Loss: 27.5259   time: 87.65s   best: 27.1010
2023-11-27 10:55:45,787:INFO:  Epoch 522/600:  train Loss: 22.2013   val Loss: 27.2875   time: 87.86s   best: 27.1010
2023-11-27 10:57:13,448:INFO:  Epoch 523/600:  train Loss: 22.1055   val Loss: 27.4442   time: 87.65s   best: 27.1010
2023-11-27 10:58:41,215:INFO:  Epoch 524/600:  train Loss: 23.2646   val Loss: 29.2653   time: 87.77s   best: 27.1010
2023-11-27 11:00:08,859:INFO:  Epoch 525/600:  train Loss: 23.1141   val Loss: 27.8327   time: 87.64s   best: 27.1010
2023-11-27 11:01:36,618:INFO:  Epoch 526/600:  train Loss: 22.3029   val Loss: 27.7885   time: 87.75s   best: 27.1010
2023-11-27 11:03:04,620:INFO:  Epoch 527/600:  train Loss: 22.4987   val Loss: 27.7379   time: 88.00s   best: 27.1010
2023-11-27 11:04:32,804:INFO:  Epoch 528/600:  train Loss: 22.1752   val Loss: 27.3235   time: 88.18s   best: 27.1010
2023-11-27 11:06:00,547:INFO:  Epoch 529/600:  train Loss: 22.0807   val Loss: 27.8328   time: 87.73s   best: 27.1010
2023-11-27 11:07:28,319:INFO:  Epoch 530/600:  train Loss: 22.2276   val Loss: 27.5711   time: 87.76s   best: 27.1010
2023-11-27 11:08:56,176:INFO:  Epoch 531/600:  train Loss: 22.8071   val Loss: 27.1734   time: 87.84s   best: 27.1010
2023-11-27 11:10:24,301:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 11:10:24,321:INFO:  Epoch 532/600:  train Loss: 22.1995   val Loss: 27.0856   time: 88.12s   best: 27.0856
2023-11-27 11:11:52,282:INFO:  Epoch 533/600:  train Loss: 21.9601   val Loss: 27.4076   time: 87.96s   best: 27.0856
2023-11-27 11:13:20,366:INFO:  Epoch 534/600:  train Loss: 22.1492   val Loss: 27.8311   time: 88.07s   best: 27.0856
2023-11-27 11:14:48,682:INFO:  Epoch 535/600:  train Loss: 22.6600   val Loss: 28.8968   time: 88.30s   best: 27.0856
2023-11-27 11:16:16,635:INFO:  Epoch 536/600:  train Loss: 22.2684   val Loss: 27.6799   time: 87.95s   best: 27.0856
2023-11-27 11:17:44,324:INFO:  Epoch 537/600:  train Loss: 22.1157   val Loss: 27.3067   time: 87.68s   best: 27.0856
2023-11-27 11:19:12,371:INFO:  Epoch 538/600:  train Loss: 21.9396   val Loss: 27.2982   time: 88.04s   best: 27.0856
2023-11-27 11:20:40,299:INFO:  Epoch 539/600:  train Loss: 23.4038   val Loss: 27.8295   time: 87.92s   best: 27.0856
2023-11-27 11:22:08,363:INFO:  Epoch 540/600:  train Loss: 22.4344   val Loss: 28.5263   time: 88.05s   best: 27.0856
2023-11-27 11:23:36,240:INFO:  Epoch 541/600:  train Loss: 22.0665   val Loss: 27.1522   time: 87.88s   best: 27.0856
2023-11-27 11:25:04,034:INFO:  Epoch 542/600:  train Loss: 21.9874   val Loss: 27.1524   time: 87.79s   best: 27.0856
2023-11-27 11:26:32,128:INFO:  Epoch 543/600:  train Loss: 22.1432   val Loss: 27.1291   time: 88.08s   best: 27.0856
2023-11-27 11:27:59,946:INFO:  Epoch 544/600:  train Loss: 22.2925   val Loss: 29.4514   time: 87.82s   best: 27.0856
2023-11-27 11:29:27,826:INFO:  Epoch 545/600:  train Loss: 22.0144   val Loss: 27.5531   time: 87.87s   best: 27.0856
2023-11-27 11:30:56,319:INFO:  Epoch 546/600:  train Loss: 22.2793   val Loss: 30.4233   time: 88.48s   best: 27.0856
2023-11-27 11:32:24,300:INFO:  Epoch 547/600:  train Loss: 22.5633   val Loss: 27.1746   time: 87.97s   best: 27.0856
2023-11-27 11:33:52,207:INFO:  Epoch 548/600:  train Loss: 22.0237   val Loss: 27.4098   time: 87.89s   best: 27.0856
2023-11-27 11:35:19,878:INFO:  Epoch 549/600:  train Loss: 22.2033   val Loss: 27.2339   time: 87.67s   best: 27.0856
2023-11-27 11:36:47,734:INFO:  Epoch 550/600:  train Loss: 21.8945   val Loss: 27.4854   time: 87.84s   best: 27.0856
2023-11-27 11:38:15,535:INFO:  Epoch 551/600:  train Loss: 21.9086   val Loss: 27.5195   time: 87.80s   best: 27.0856
2023-11-27 11:39:43,407:INFO:  Epoch 552/600:  train Loss: 22.1704   val Loss: 27.8124   time: 87.86s   best: 27.0856
2023-11-27 11:41:11,150:INFO:  Epoch 553/600:  train Loss: 21.9912   val Loss: 28.0863   time: 87.74s   best: 27.0856
2023-11-27 11:42:39,105:INFO:  Epoch 554/600:  train Loss: 21.8568   val Loss: 27.2654   time: 87.94s   best: 27.0856
2023-11-27 11:44:07,354:INFO:  Epoch 555/600:  train Loss: 21.8894   val Loss: 28.1325   time: 88.25s   best: 27.0856
2023-11-27 11:45:36,109:INFO:  Epoch 556/600:  train Loss: 22.7333   val Loss: 31.1207   time: 88.75s   best: 27.0856
2023-11-27 11:47:03,987:INFO:  Epoch 557/600:  train Loss: 22.4039   val Loss: 27.1325   time: 87.87s   best: 27.0856
2023-11-27 11:48:31,828:INFO:  Epoch 558/600:  train Loss: 21.8085   val Loss: 27.2718   time: 87.83s   best: 27.0856
2023-11-27 11:49:59,712:INFO:  Epoch 559/600:  train Loss: 22.1481   val Loss: 27.2183   time: 87.87s   best: 27.0856
2023-11-27 11:51:27,786:INFO:  Epoch 560/600:  train Loss: 21.9478   val Loss: 27.9275   time: 88.07s   best: 27.0856
2023-11-27 11:52:55,656:INFO:  Epoch 561/600:  train Loss: 22.1242   val Loss: 28.4827   time: 87.86s   best: 27.0856
2023-11-27 11:54:23,572:INFO:  Epoch 562/600:  train Loss: 22.0765   val Loss: 28.1654   time: 87.91s   best: 27.0856
2023-11-27 11:55:51,459:INFO:  Epoch 563/600:  train Loss: 21.9381   val Loss: 27.7339   time: 87.87s   best: 27.0856
2023-11-27 11:57:19,681:INFO:  Epoch 564/600:  train Loss: 21.8019   val Loss: 27.0891   time: 88.21s   best: 27.0856
2023-11-27 11:58:47,994:INFO:  Epoch 565/600:  train Loss: 22.1558   val Loss: 30.9615   time: 88.31s   best: 27.0856
2023-11-27 12:00:16,004:INFO:  Epoch 566/600:  train Loss: 22.4016   val Loss: 27.2451   time: 88.00s   best: 27.0856
2023-11-27 12:01:43,887:INFO:  Epoch 567/600:  train Loss: 21.7767   val Loss: 27.6001   time: 87.87s   best: 27.0856
2023-11-27 12:03:11,812:INFO:  Epoch 568/600:  train Loss: 22.6490   val Loss: 27.6366   time: 87.91s   best: 27.0856
2023-11-27 12:04:39,569:INFO:  Epoch 569/600:  train Loss: 21.9104   val Loss: 27.6973   time: 87.76s   best: 27.0856
2023-11-27 12:06:07,301:INFO:  Epoch 570/600:  train Loss: 21.7811   val Loss: 27.2186   time: 87.73s   best: 27.0856
2023-11-27 12:07:34,999:INFO:  Epoch 571/600:  train Loss: 21.7327   val Loss: 28.0620   time: 87.70s   best: 27.0856
2023-11-27 12:09:03,245:INFO:  Epoch 572/600:  train Loss: 22.0127   val Loss: 27.4716   time: 88.23s   best: 27.0856
2023-11-27 12:10:30,974:INFO:  Epoch 573/600:  train Loss: 21.8731   val Loss: 27.6362   time: 87.73s   best: 27.0856
2023-11-27 12:11:59,171:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 12:11:59,190:INFO:  Epoch 574/600:  train Loss: 21.7410   val Loss: 27.0842   time: 88.18s   best: 27.0842
2023-11-27 12:13:26,968:INFO:  Epoch 575/600:  train Loss: 21.8385   val Loss: 27.5843   time: 87.77s   best: 27.0842
2023-11-27 12:14:54,814:INFO:  Epoch 576/600:  train Loss: 21.6683   val Loss: 27.0934   time: 87.82s   best: 27.0842
2023-11-27 12:16:22,789:INFO:  Epoch 577/600:  train Loss: 22.9097   val Loss: 27.3668   time: 87.96s   best: 27.0842
2023-11-27 12:17:50,816:INFO:  Epoch 578/600:  train Loss: 22.0575   val Loss: 27.4641   time: 88.03s   best: 27.0842
2023-11-27 12:19:18,905:INFO:  Epoch 579/600:  train Loss: 21.7331   val Loss: 27.3291   time: 88.09s   best: 27.0842
2023-11-27 12:20:47,487:INFO:  Epoch 580/600:  train Loss: 21.6818   val Loss: 27.4609   time: 88.58s   best: 27.0842
2023-11-27 12:22:15,898:INFO:  Epoch 581/600:  train Loss: 21.7275   val Loss: 27.7679   time: 88.41s   best: 27.0842
2023-11-27 12:23:43,807:INFO:  Epoch 582/600:  train Loss: 21.7915   val Loss: 27.5031   time: 87.91s   best: 27.0842
2023-11-27 12:25:11,513:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_a38c.pt
2023-11-27 12:25:11,532:INFO:  Epoch 583/600:  train Loss: 21.5997   val Loss: 26.9604   time: 87.70s   best: 26.9604
2023-11-27 12:26:39,427:INFO:  Epoch 584/600:  train Loss: 21.7121   val Loss: 27.3435   time: 87.89s   best: 26.9604
2023-11-27 12:28:07,301:INFO:  Epoch 585/600:  train Loss: 21.7631   val Loss: 27.4292   time: 87.87s   best: 26.9604
2023-11-27 12:29:35,466:INFO:  Epoch 586/600:  train Loss: 21.6883   val Loss: 27.3534   time: 88.15s   best: 26.9604
2023-11-27 12:31:03,393:INFO:  Epoch 587/600:  train Loss: 21.6743   val Loss: 27.0359   time: 87.93s   best: 26.9604
2023-11-27 12:32:31,437:INFO:  Epoch 588/600:  train Loss: 21.7857   val Loss: 27.1967   time: 88.04s   best: 26.9604
2023-11-27 12:33:59,249:INFO:  Epoch 589/600:  train Loss: 21.6826   val Loss: 27.3758   time: 87.81s   best: 26.9604
2023-11-27 12:35:27,579:INFO:  Epoch 590/600:  train Loss: 21.6543   val Loss: 27.1035   time: 88.33s   best: 26.9604
2023-11-27 12:36:55,335:INFO:  Epoch 591/600:  train Loss: 21.6181   val Loss: 27.4666   time: 87.74s   best: 26.9604
2023-11-27 12:38:23,330:INFO:  Epoch 592/600:  train Loss: 21.9480   val Loss: 27.9502   time: 87.98s   best: 26.9604
2023-11-27 12:39:51,661:INFO:  Epoch 593/600:  train Loss: 21.6800   val Loss: 27.4339   time: 88.33s   best: 26.9604
2023-11-27 12:41:20,270:INFO:  Epoch 594/600:  train Loss: 21.8787   val Loss: 27.1731   time: 88.61s   best: 26.9604
2023-11-27 12:42:47,964:INFO:  Epoch 595/600:  train Loss: 21.5645   val Loss: 27.3015   time: 87.69s   best: 26.9604
2023-11-27 12:44:15,854:INFO:  Epoch 596/600:  train Loss: 21.5390   val Loss: 27.1915   time: 87.89s   best: 26.9604
2023-11-27 12:45:43,824:INFO:  Epoch 597/600:  train Loss: 21.7076   val Loss: 27.5113   time: 87.97s   best: 26.9604
2023-11-27 12:47:11,856:INFO:  Epoch 598/600:  train Loss: 21.8165   val Loss: 27.3825   time: 88.02s   best: 26.9604
2023-11-27 12:48:39,664:INFO:  Epoch 599/600:  train Loss: 21.4362   val Loss: 27.0018   time: 87.80s   best: 26.9604
2023-11-27 12:50:08,090:INFO:  Epoch 600/600:  train Loss: 21.4365   val Loss: 27.1456   time: 88.43s   best: 26.9604
2023-11-27 12:50:08,091:INFO:  -----> Training complete in 883m 15s   best validation loss: 26.9604
 
2023-11-28 19:36:06,507:INFO:  Starting experiment lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)
2023-11-28 19:36:06,509:INFO:  Defining the model
2023-11-28 19:36:06,603:INFO:  Reading the dataset
2023-11-28 19:41:59,873:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:41:59,893:INFO:  Epoch 1/600:  train Loss: 90.5130   val Loss: 87.8856   time: 90.50s   best: 87.8856
2023-11-28 19:43:28,593:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:43:28,612:INFO:  Epoch 2/600:  train Loss: 87.3811   val Loss: 86.1431   time: 88.70s   best: 86.1431
2023-11-28 19:44:57,136:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:44:57,155:INFO:  Epoch 3/600:  train Loss: 84.7813   val Loss: 81.7599   time: 88.51s   best: 81.7599
2023-11-28 19:46:25,686:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:46:25,705:INFO:  Epoch 4/600:  train Loss: 79.3979   val Loss: 77.4076   time: 88.53s   best: 77.4076
2023-11-28 19:47:53,944:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:47:53,963:INFO:  Epoch 5/600:  train Loss: 75.6421   val Loss: 73.5197   time: 88.23s   best: 73.5197
2023-11-28 19:49:21,836:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:49:21,856:INFO:  Epoch 6/600:  train Loss: 73.8896   val Loss: 72.1577   time: 87.87s   best: 72.1577
2023-11-28 19:50:49,809:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:50:49,828:INFO:  Epoch 7/600:  train Loss: 72.2416   val Loss: 70.4618   time: 87.94s   best: 70.4618
2023-11-28 19:52:17,749:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:52:17,768:INFO:  Epoch 8/600:  train Loss: 70.9783   val Loss: 68.9659   time: 87.90s   best: 68.9659
2023-11-28 19:53:47,110:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:53:47,129:INFO:  Epoch 9/600:  train Loss: 69.7789   val Loss: 68.1536   time: 89.33s   best: 68.1536
2023-11-28 19:55:15,604:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:55:15,623:INFO:  Epoch 10/600:  train Loss: 68.7328   val Loss: 66.9975   time: 88.47s   best: 66.9975
2023-11-28 19:56:43,972:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:56:43,992:INFO:  Epoch 11/600:  train Loss: 67.8055   val Loss: 66.7827   time: 88.34s   best: 66.7827
2023-11-28 19:58:12,411:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:58:12,431:INFO:  Epoch 12/600:  train Loss: 67.3417   val Loss: 65.9689   time: 88.40s   best: 65.9689
2023-11-28 19:59:40,745:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 19:59:40,765:INFO:  Epoch 13/600:  train Loss: 66.4358   val Loss: 65.6208   time: 88.30s   best: 65.6208
2023-11-28 20:01:09,134:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:01:09,154:INFO:  Epoch 14/600:  train Loss: 65.6937   val Loss: 65.5383   time: 88.35s   best: 65.5383
2023-11-28 20:02:37,549:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:02:37,569:INFO:  Epoch 15/600:  train Loss: 65.2270   val Loss: 65.2637   time: 88.38s   best: 65.2637
2023-11-28 20:04:05,969:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:04:05,988:INFO:  Epoch 16/600:  train Loss: 64.5901   val Loss: 63.7365   time: 88.39s   best: 63.7365
2023-11-28 20:05:34,660:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:05:34,679:INFO:  Epoch 17/600:  train Loss: 63.9380   val Loss: 63.1638   time: 88.67s   best: 63.1638
2023-11-28 20:07:03,201:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:07:03,220:INFO:  Epoch 18/600:  train Loss: 63.4528   val Loss: 62.9457   time: 88.51s   best: 62.9457
2023-11-28 20:08:31,665:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:08:31,685:INFO:  Epoch 19/600:  train Loss: 62.8136   val Loss: 61.7209   time: 88.44s   best: 61.7209
2023-11-28 20:10:00,115:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:10:00,135:INFO:  Epoch 20/600:  train Loss: 62.2525   val Loss: 61.6128   time: 88.43s   best: 61.6128
2023-11-28 20:11:28,812:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:11:28,831:INFO:  Epoch 21/600:  train Loss: 61.6347   val Loss: 61.0004   time: 88.67s   best: 61.0004
2023-11-28 20:12:57,681:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:12:57,700:INFO:  Epoch 22/600:  train Loss: 61.4331   val Loss: 60.3593   time: 88.83s   best: 60.3593
2023-11-28 20:14:26,215:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:14:26,234:INFO:  Epoch 23/600:  train Loss: 60.8406   val Loss: 60.0626   time: 88.51s   best: 60.0626
2023-11-28 20:15:54,707:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:15:54,728:INFO:  Epoch 24/600:  train Loss: 60.4257   val Loss: 59.7060   time: 88.47s   best: 59.7060
2023-11-28 20:17:23,139:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:17:23,158:INFO:  Epoch 25/600:  train Loss: 59.8831   val Loss: 58.9582   time: 88.41s   best: 58.9582
2023-11-28 20:18:51,725:INFO:  Epoch 26/600:  train Loss: 59.5395   val Loss: 59.0956   time: 88.57s   best: 58.9582
2023-11-28 20:20:20,142:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:20:20,161:INFO:  Epoch 27/600:  train Loss: 59.0189   val Loss: 58.3231   time: 88.41s   best: 58.3231
2023-11-28 20:21:48,510:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:21:48,529:INFO:  Epoch 28/600:  train Loss: 58.5546   val Loss: 57.4027   time: 88.34s   best: 57.4027
2023-11-28 20:23:17,016:INFO:  Epoch 29/600:  train Loss: 58.0998   val Loss: 57.4067   time: 88.48s   best: 57.4027
2023-11-28 20:24:45,492:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:24:45,512:INFO:  Epoch 30/600:  train Loss: 57.3719   val Loss: 56.5269   time: 88.46s   best: 56.5269
2023-11-28 20:26:13,877:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:26:13,896:INFO:  Epoch 31/600:  train Loss: 56.9055   val Loss: 56.1288   time: 88.36s   best: 56.1288
2023-11-28 20:27:42,420:INFO:  Epoch 32/600:  train Loss: 56.3290   val Loss: 56.5442   time: 88.51s   best: 56.1288
2023-11-28 20:29:10,793:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:29:10,812:INFO:  Epoch 33/600:  train Loss: 56.0321   val Loss: 55.3963   time: 88.36s   best: 55.3963
2023-11-28 20:30:39,382:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:30:39,401:INFO:  Epoch 34/600:  train Loss: 55.5718   val Loss: 54.6059   time: 88.55s   best: 54.6059
2023-11-28 20:32:08,501:INFO:  Epoch 35/600:  train Loss: 55.2537   val Loss: 55.0295   time: 89.10s   best: 54.6059
2023-11-28 20:33:36,881:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:33:36,901:INFO:  Epoch 36/600:  train Loss: 54.9907   val Loss: 53.8768   time: 88.38s   best: 53.8768
2023-11-28 20:35:05,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:35:05,314:INFO:  Epoch 37/600:  train Loss: 54.1987   val Loss: 53.4613   time: 88.38s   best: 53.4613
2023-11-28 20:36:33,944:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:36:33,979:INFO:  Epoch 38/600:  train Loss: 53.7140   val Loss: 52.7355   time: 88.61s   best: 52.7355
2023-11-28 20:38:02,824:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:38:02,843:INFO:  Epoch 39/600:  train Loss: 53.3013   val Loss: 52.4870   time: 88.84s   best: 52.4870
2023-11-28 20:39:31,140:INFO:  Epoch 40/600:  train Loss: 52.8025   val Loss: 52.9512   time: 88.28s   best: 52.4870
2023-11-28 20:40:59,419:INFO:  Epoch 41/600:  train Loss: 52.2887   val Loss: 52.9567   time: 88.28s   best: 52.4870
2023-11-28 20:42:27,774:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:42:27,793:INFO:  Epoch 42/600:  train Loss: 51.9329   val Loss: 51.7111   time: 88.34s   best: 51.7111
2023-11-28 20:43:56,121:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:43:56,141:INFO:  Epoch 43/600:  train Loss: 51.5101   val Loss: 50.6950   time: 88.32s   best: 50.6950
2023-11-28 20:45:24,894:INFO:  Epoch 44/600:  train Loss: 50.9050   val Loss: 51.5708   time: 88.75s   best: 50.6950
2023-11-28 20:46:53,948:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:46:53,968:INFO:  Epoch 45/600:  train Loss: 50.5664   val Loss: 50.0208   time: 89.05s   best: 50.0208
2023-11-28 20:48:22,547:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:48:22,575:INFO:  Epoch 46/600:  train Loss: 50.4266   val Loss: 49.6507   time: 88.57s   best: 49.6507
2023-11-28 20:49:50,960:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:49:50,979:INFO:  Epoch 47/600:  train Loss: 49.6160   val Loss: 49.3319   time: 88.37s   best: 49.3319
2023-11-28 20:51:19,490:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:51:19,510:INFO:  Epoch 48/600:  train Loss: 49.1434   val Loss: 49.2317   time: 88.50s   best: 49.2317
2023-11-28 20:52:47,872:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:52:47,892:INFO:  Epoch 49/600:  train Loss: 49.0655   val Loss: 48.2129   time: 88.33s   best: 48.2129
2023-11-28 20:54:16,434:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:54:16,453:INFO:  Epoch 50/600:  train Loss: 48.3766   val Loss: 47.9798   time: 88.53s   best: 47.9798
2023-11-28 20:55:45,330:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:55:45,349:INFO:  Epoch 51/600:  train Loss: 48.1237   val Loss: 47.5125   time: 88.87s   best: 47.5125
2023-11-28 20:57:14,054:INFO:  Epoch 52/600:  train Loss: 47.7691   val Loss: 47.8308   time: 88.70s   best: 47.5125
2023-11-28 20:58:42,516:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 20:58:42,536:INFO:  Epoch 53/600:  train Loss: 47.7159   val Loss: 46.9051   time: 88.45s   best: 46.9051
2023-11-28 21:00:10,804:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:00:10,823:INFO:  Epoch 54/600:  train Loss: 46.8074   val Loss: 46.8447   time: 88.25s   best: 46.8447
2023-11-28 21:01:39,214:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:01:39,233:INFO:  Epoch 55/600:  train Loss: 46.5730   val Loss: 46.5904   time: 88.38s   best: 46.5904
2023-11-28 21:03:07,551:INFO:  Epoch 56/600:  train Loss: 46.3867   val Loss: 47.6339   time: 88.31s   best: 46.5904
2023-11-28 21:04:35,910:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:04:35,929:INFO:  Epoch 57/600:  train Loss: 46.1202   val Loss: 46.3980   time: 88.35s   best: 46.3980
2023-11-28 21:06:04,417:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:06:04,436:INFO:  Epoch 58/600:  train Loss: 45.8106   val Loss: 46.3650   time: 88.48s   best: 46.3650
2023-11-28 21:07:32,806:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:07:32,834:INFO:  Epoch 59/600:  train Loss: 45.4663   val Loss: 46.0322   time: 88.35s   best: 46.0322
2023-11-28 21:09:01,540:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:09:01,559:INFO:  Epoch 60/600:  train Loss: 45.3330   val Loss: 45.7393   time: 88.69s   best: 45.7393
2023-11-28 21:10:30,606:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:10:30,625:INFO:  Epoch 61/600:  train Loss: 45.0251   val Loss: 45.0994   time: 89.03s   best: 45.0994
2023-11-28 21:11:58,840:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:11:58,859:INFO:  Epoch 62/600:  train Loss: 44.5192   val Loss: 45.0137   time: 88.21s   best: 45.0137
2023-11-28 21:13:27,142:INFO:  Epoch 63/600:  train Loss: 44.6052   val Loss: 45.0840   time: 88.28s   best: 45.0137
2023-11-28 21:14:55,490:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:14:55,509:INFO:  Epoch 64/600:  train Loss: 44.5242   val Loss: 44.7004   time: 88.34s   best: 44.7004
2023-11-28 21:16:24,593:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:16:24,612:INFO:  Epoch 65/600:  train Loss: 43.8870   val Loss: 44.6282   time: 89.07s   best: 44.6282
2023-11-28 21:17:52,928:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:17:52,948:INFO:  Epoch 66/600:  train Loss: 43.6527   val Loss: 44.5404   time: 88.31s   best: 44.5404
2023-11-28 21:19:21,583:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:19:21,602:INFO:  Epoch 67/600:  train Loss: 43.5172   val Loss: 44.3072   time: 88.63s   best: 44.3072
2023-11-28 21:20:50,275:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:20:50,294:INFO:  Epoch 68/600:  train Loss: 43.1019   val Loss: 44.0585   time: 88.67s   best: 44.0585
2023-11-28 21:22:18,589:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:22:18,609:INFO:  Epoch 69/600:  train Loss: 42.9421   val Loss: 43.9079   time: 88.29s   best: 43.9079
2023-11-28 21:23:47,228:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:23:47,247:INFO:  Epoch 70/600:  train Loss: 42.7887   val Loss: 43.5409   time: 88.61s   best: 43.5409
2023-11-28 21:25:15,636:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:25:15,664:INFO:  Epoch 71/600:  train Loss: 42.4527   val Loss: 43.3944   time: 88.37s   best: 43.3944
2023-11-28 21:26:44,567:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:26:44,586:INFO:  Epoch 72/600:  train Loss: 42.2951   val Loss: 43.3920   time: 88.89s   best: 43.3920
2023-11-28 21:28:13,152:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:28:13,172:INFO:  Epoch 73/600:  train Loss: 41.9935   val Loss: 43.2921   time: 88.56s   best: 43.2921
2023-11-28 21:29:41,862:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:29:41,882:INFO:  Epoch 74/600:  train Loss: 41.9093   val Loss: 43.2219   time: 88.68s   best: 43.2219
2023-11-28 21:31:10,636:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:31:10,655:INFO:  Epoch 75/600:  train Loss: 41.6411   val Loss: 42.9319   time: 88.74s   best: 42.9319
2023-11-28 21:32:39,733:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:32:39,753:INFO:  Epoch 76/600:  train Loss: 41.4853   val Loss: 42.0842   time: 89.07s   best: 42.0842
2023-11-28 21:34:08,381:INFO:  Epoch 77/600:  train Loss: 40.8939   val Loss: 42.2029   time: 88.62s   best: 42.0842
2023-11-28 21:35:36,770:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:35:36,789:INFO:  Epoch 78/600:  train Loss: 40.6513   val Loss: 41.5268   time: 88.38s   best: 41.5268
2023-11-28 21:37:05,095:INFO:  Epoch 79/600:  train Loss: 40.7584   val Loss: 43.0075   time: 88.29s   best: 41.5268
2023-11-28 21:38:34,004:INFO:  Epoch 80/600:  train Loss: 40.5457   val Loss: 41.7193   time: 88.91s   best: 41.5268
2023-11-28 21:40:02,635:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:40:02,654:INFO:  Epoch 81/600:  train Loss: 40.0190   val Loss: 41.2172   time: 88.62s   best: 41.2172
2023-11-28 21:41:30,989:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:41:31,008:INFO:  Epoch 82/600:  train Loss: 39.8521   val Loss: 40.6383   time: 88.32s   best: 40.6383
2023-11-28 21:42:59,456:INFO:  Epoch 83/600:  train Loss: 39.5964   val Loss: 41.0305   time: 88.45s   best: 40.6383
2023-11-28 21:44:28,289:INFO:  Epoch 84/600:  train Loss: 39.3618   val Loss: 41.4662   time: 88.82s   best: 40.6383
2023-11-28 21:45:56,705:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:45:56,724:INFO:  Epoch 85/600:  train Loss: 39.3526   val Loss: 40.3832   time: 88.41s   best: 40.3832
2023-11-28 21:47:24,951:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:47:24,971:INFO:  Epoch 86/600:  train Loss: 38.9917   val Loss: 39.8181   time: 88.22s   best: 39.8181
2023-11-28 21:48:53,421:INFO:  Epoch 87/600:  train Loss: 38.7119   val Loss: 40.0320   time: 88.45s   best: 39.8181
2023-11-28 21:50:21,845:INFO:  Epoch 88/600:  train Loss: 38.3796   val Loss: 40.3667   time: 88.41s   best: 39.8181
2023-11-28 21:51:50,147:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:51:50,166:INFO:  Epoch 89/600:  train Loss: 38.7955   val Loss: 39.2768   time: 88.30s   best: 39.2768
2023-11-28 21:53:18,480:INFO:  Epoch 90/600:  train Loss: 38.3631   val Loss: 40.0234   time: 88.30s   best: 39.2768
2023-11-28 21:54:47,355:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:54:47,375:INFO:  Epoch 91/600:  train Loss: 37.9613   val Loss: 39.0660   time: 88.86s   best: 39.0660
2023-11-28 21:56:15,791:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:56:15,810:INFO:  Epoch 92/600:  train Loss: 37.7305   val Loss: 38.9556   time: 88.41s   best: 38.9556
2023-11-28 21:57:44,258:INFO:  Epoch 93/600:  train Loss: 37.7512   val Loss: 39.1400   time: 88.45s   best: 38.9556
2023-11-28 21:59:12,945:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 21:59:12,964:INFO:  Epoch 94/600:  train Loss: 37.5295   val Loss: 38.5830   time: 88.68s   best: 38.5830
2023-11-28 22:00:42,033:INFO:  Epoch 95/600:  train Loss: 37.5240   val Loss: 38.8710   time: 89.06s   best: 38.5830
2023-11-28 22:02:10,508:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:02:10,527:INFO:  Epoch 96/600:  train Loss: 37.2075   val Loss: 37.9224   time: 88.46s   best: 37.9224
2023-11-28 22:03:38,736:INFO:  Epoch 97/600:  train Loss: 37.0820   val Loss: 38.4210   time: 88.20s   best: 37.9224
2023-11-28 22:05:07,210:INFO:  Epoch 98/600:  train Loss: 36.7732   val Loss: 38.6568   time: 88.46s   best: 37.9224
2023-11-28 22:06:35,660:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:06:35,679:INFO:  Epoch 99/600:  train Loss: 36.7278   val Loss: 37.8120   time: 88.43s   best: 37.8120
2023-11-28 22:08:04,143:INFO:  Epoch 100/600:  train Loss: 36.7330   val Loss: 38.2041   time: 88.45s   best: 37.8120
2023-11-28 22:09:32,448:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:09:32,467:INFO:  Epoch 101/600:  train Loss: 36.4004   val Loss: 37.4712   time: 88.30s   best: 37.4712
2023-11-28 22:11:01,179:INFO:  Epoch 102/600:  train Loss: 36.5295   val Loss: 37.9184   time: 88.70s   best: 37.4712
2023-11-28 22:12:29,751:INFO:  Epoch 103/600:  train Loss: 36.3056   val Loss: 37.6099   time: 88.56s   best: 37.4712
2023-11-28 22:13:57,910:INFO:  Epoch 104/600:  train Loss: 36.0765   val Loss: 37.8448   time: 88.15s   best: 37.4712
2023-11-28 22:15:26,143:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:15:26,162:INFO:  Epoch 105/600:  train Loss: 36.0685   val Loss: 37.1911   time: 88.22s   best: 37.1911
2023-11-28 22:16:54,334:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:16:54,353:INFO:  Epoch 106/600:  train Loss: 35.6093   val Loss: 36.8637   time: 88.16s   best: 36.8637
2023-11-28 22:18:22,629:INFO:  Epoch 107/600:  train Loss: 35.5457   val Loss: 37.3264   time: 88.26s   best: 36.8637
2023-11-28 22:19:50,805:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:19:50,824:INFO:  Epoch 108/600:  train Loss: 35.4505   val Loss: 36.4553   time: 88.16s   best: 36.4553
2023-11-28 22:21:19,032:INFO:  Epoch 109/600:  train Loss: 35.1720   val Loss: 37.1677   time: 88.21s   best: 36.4553
2023-11-28 22:22:47,293:INFO:  Epoch 110/600:  train Loss: 35.4262   val Loss: 36.7328   time: 88.25s   best: 36.4553
2023-11-28 22:24:15,934:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:24:15,953:INFO:  Epoch 111/600:  train Loss: 35.1911   val Loss: 36.0079   time: 88.63s   best: 36.0079
2023-11-28 22:25:44,168:INFO:  Epoch 112/600:  train Loss: 35.1397   val Loss: 36.5637   time: 88.20s   best: 36.0079
2023-11-28 22:27:12,469:INFO:  Epoch 113/600:  train Loss: 35.1380   val Loss: 36.1650   time: 88.30s   best: 36.0079
2023-11-28 22:28:41,349:INFO:  Epoch 114/600:  train Loss: 34.5761   val Loss: 37.0122   time: 88.87s   best: 36.0079
2023-11-28 22:30:09,811:INFO:  Epoch 115/600:  train Loss: 35.0090   val Loss: 36.5542   time: 88.45s   best: 36.0079
2023-11-28 22:31:38,043:INFO:  Epoch 116/600:  train Loss: 34.4357   val Loss: 36.3936   time: 88.23s   best: 36.0079
2023-11-28 22:33:06,473:INFO:  Epoch 117/600:  train Loss: 34.2693   val Loss: 36.6371   time: 88.42s   best: 36.0079
2023-11-28 22:34:34,833:INFO:  Epoch 118/600:  train Loss: 34.5559   val Loss: 37.2899   time: 88.36s   best: 36.0079
2023-11-28 22:36:03,600:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:36:03,619:INFO:  Epoch 119/600:  train Loss: 34.3586   val Loss: 35.3355   time: 88.75s   best: 35.3355
2023-11-28 22:37:32,148:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:37:32,167:INFO:  Epoch 120/600:  train Loss: 33.8654   val Loss: 35.2852   time: 88.52s   best: 35.2852
2023-11-28 22:39:00,893:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:39:00,912:INFO:  Epoch 121/600:  train Loss: 33.9998   val Loss: 34.7870   time: 88.71s   best: 34.7870
2023-11-28 22:40:29,242:INFO:  Epoch 122/600:  train Loss: 34.2159   val Loss: 35.1882   time: 88.32s   best: 34.7870
2023-11-28 22:41:57,518:INFO:  Epoch 123/600:  train Loss: 33.6986   val Loss: 34.8395   time: 88.26s   best: 34.7870
2023-11-28 22:43:25,818:INFO:  Epoch 124/600:  train Loss: 33.6782   val Loss: 36.1626   time: 88.29s   best: 34.7870
2023-11-28 22:44:54,231:INFO:  Epoch 125/600:  train Loss: 33.7666   val Loss: 35.2912   time: 88.41s   best: 34.7870
2023-11-28 22:46:22,813:INFO:  Epoch 126/600:  train Loss: 33.5994   val Loss: 35.2224   time: 88.57s   best: 34.7870
2023-11-28 22:47:51,188:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:47:51,207:INFO:  Epoch 127/600:  train Loss: 33.8672   val Loss: 34.5834   time: 88.36s   best: 34.5834
2023-11-28 22:49:19,860:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:49:19,880:INFO:  Epoch 128/600:  train Loss: 33.1256   val Loss: 34.5242   time: 88.64s   best: 34.5242
2023-11-28 22:50:49,034:INFO:  Epoch 129/600:  train Loss: 33.1129   val Loss: 34.7559   time: 89.15s   best: 34.5242
2023-11-28 22:52:17,574:INFO:  Epoch 130/600:  train Loss: 33.2924   val Loss: 35.3642   time: 88.53s   best: 34.5242
2023-11-28 22:53:45,781:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:53:45,809:INFO:  Epoch 131/600:  train Loss: 33.1058   val Loss: 34.3855   time: 88.20s   best: 34.3855
2023-11-28 22:55:14,538:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 22:55:14,557:INFO:  Epoch 132/600:  train Loss: 32.8966   val Loss: 34.0802   time: 88.72s   best: 34.0802
2023-11-28 22:56:42,891:INFO:  Epoch 133/600:  train Loss: 33.3350   val Loss: 35.1769   time: 88.33s   best: 34.0802
2023-11-28 22:58:11,134:INFO:  Epoch 134/600:  train Loss: 32.9018   val Loss: 34.2058   time: 88.23s   best: 34.0802
2023-11-28 22:59:39,440:INFO:  Epoch 135/600:  train Loss: 32.8682   val Loss: 34.0871   time: 88.29s   best: 34.0802
2023-11-28 23:01:07,715:INFO:  Epoch 136/600:  train Loss: 33.6063   val Loss: 34.3441   time: 88.27s   best: 34.0802
2023-11-28 23:02:36,194:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:02:36,213:INFO:  Epoch 137/600:  train Loss: 32.5840   val Loss: 34.0702   time: 88.46s   best: 34.0702
2023-11-28 23:04:04,528:INFO:  Epoch 138/600:  train Loss: 32.4519   val Loss: 34.3720   time: 88.30s   best: 34.0702
2023-11-28 23:05:32,837:INFO:  Epoch 139/600:  train Loss: 32.4645   val Loss: 35.3281   time: 88.31s   best: 34.0702
2023-11-28 23:07:01,183:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:07:01,202:INFO:  Epoch 140/600:  train Loss: 32.2025   val Loss: 33.9360   time: 88.33s   best: 33.9360
2023-11-28 23:08:29,654:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:08:29,673:INFO:  Epoch 141/600:  train Loss: 32.4503   val Loss: 33.6819   time: 88.45s   best: 33.6819
2023-11-28 23:09:58,120:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:09:58,138:INFO:  Epoch 142/600:  train Loss: 32.2265   val Loss: 33.6116   time: 88.43s   best: 33.6116
2023-11-28 23:11:26,642:INFO:  Epoch 143/600:  train Loss: 32.4764   val Loss: 33.7047   time: 88.49s   best: 33.6116
2023-11-28 23:12:55,203:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:12:55,222:INFO:  Epoch 144/600:  train Loss: 32.0952   val Loss: 33.2592   time: 88.55s   best: 33.2592
2023-11-28 23:14:24,091:INFO:  Epoch 145/600:  train Loss: 32.0803   val Loss: 33.6879   time: 88.86s   best: 33.2592
2023-11-28 23:15:52,553:INFO:  Epoch 146/600:  train Loss: 31.7834   val Loss: 33.4185   time: 88.45s   best: 33.2592
2023-11-28 23:17:20,909:INFO:  Epoch 147/600:  train Loss: 31.6735   val Loss: 33.3633   time: 88.34s   best: 33.2592
2023-11-28 23:18:49,685:INFO:  Epoch 148/600:  train Loss: 32.2839   val Loss: 33.9549   time: 88.78s   best: 33.2592
2023-11-28 23:20:18,053:INFO:  Epoch 149/600:  train Loss: 31.9260   val Loss: 33.8110   time: 88.36s   best: 33.2592
2023-11-28 23:21:46,312:INFO:  Epoch 150/600:  train Loss: 32.0774   val Loss: 33.5235   time: 88.25s   best: 33.2592
2023-11-28 23:23:14,511:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:23:14,530:INFO:  Epoch 151/600:  train Loss: 32.1586   val Loss: 33.0328   time: 88.18s   best: 33.0328
2023-11-28 23:24:43,049:INFO:  Epoch 152/600:  train Loss: 31.7846   val Loss: 33.3114   time: 88.52s   best: 33.0328
2023-11-28 23:26:11,414:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:26:11,434:INFO:  Epoch 153/600:  train Loss: 31.8480   val Loss: 32.7860   time: 88.35s   best: 32.7860
2023-11-28 23:27:39,606:INFO:  Epoch 154/600:  train Loss: 31.3541   val Loss: 33.5501   time: 88.16s   best: 32.7860
2023-11-28 23:29:07,719:INFO:  Epoch 155/600:  train Loss: 31.6519   val Loss: 33.8207   time: 88.11s   best: 32.7860
2023-11-28 23:30:36,004:INFO:  Epoch 156/600:  train Loss: 31.4366   val Loss: 32.9290   time: 88.27s   best: 32.7860
2023-11-28 23:32:04,254:INFO:  Epoch 157/600:  train Loss: 31.2326   val Loss: 32.8285   time: 88.25s   best: 32.7860
2023-11-28 23:33:32,769:INFO:  Epoch 158/600:  train Loss: 31.8227   val Loss: 33.0638   time: 88.51s   best: 32.7860
2023-11-28 23:35:01,016:INFO:  Epoch 159/600:  train Loss: 31.3217   val Loss: 33.5521   time: 88.25s   best: 32.7860
2023-11-28 23:36:29,325:INFO:  Epoch 160/600:  train Loss: 31.8645   val Loss: 33.6241   time: 88.30s   best: 32.7860
2023-11-28 23:37:58,385:INFO:  Epoch 161/600:  train Loss: 31.8508   val Loss: 35.6246   time: 89.06s   best: 32.7860
2023-11-28 23:39:26,475:INFO:  Epoch 162/600:  train Loss: 32.4261   val Loss: 33.3648   time: 88.08s   best: 32.7860
2023-11-28 23:40:54,615:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:40:54,635:INFO:  Epoch 163/600:  train Loss: 31.1280   val Loss: 32.4125   time: 88.12s   best: 32.4125
2023-11-28 23:42:23,437:INFO:  Epoch 164/600:  train Loss: 31.2935   val Loss: 32.7886   time: 88.79s   best: 32.4125
2023-11-28 23:43:51,651:INFO:  Epoch 165/600:  train Loss: 30.7697   val Loss: 32.6604   time: 88.21s   best: 32.4125
2023-11-28 23:45:19,876:INFO:  Epoch 166/600:  train Loss: 31.3703   val Loss: 35.0334   time: 88.22s   best: 32.4125
2023-11-28 23:46:48,571:INFO:  Epoch 167/600:  train Loss: 30.9239   val Loss: 34.3508   time: 88.69s   best: 32.4125
2023-11-28 23:48:16,899:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:48:16,918:INFO:  Epoch 168/600:  train Loss: 30.6713   val Loss: 32.3797   time: 88.32s   best: 32.3797
2023-11-28 23:49:45,194:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:49:45,213:INFO:  Epoch 169/600:  train Loss: 31.1267   val Loss: 32.3449   time: 88.26s   best: 32.3449
2023-11-28 23:51:13,518:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:51:13,537:INFO:  Epoch 170/600:  train Loss: 30.4586   val Loss: 32.2736   time: 88.28s   best: 32.2736
2023-11-28 23:52:41,856:INFO:  Epoch 171/600:  train Loss: 30.9711   val Loss: 35.7453   time: 88.32s   best: 32.2736
2023-11-28 23:54:10,226:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-28 23:54:10,245:INFO:  Epoch 172/600:  train Loss: 30.4057   val Loss: 31.5964   time: 88.35s   best: 31.5964
2023-11-28 23:55:38,445:INFO:  Epoch 173/600:  train Loss: 30.6560   val Loss: 32.1311   time: 88.20s   best: 31.5964
2023-11-28 23:57:06,944:INFO:  Epoch 174/600:  train Loss: 30.3536   val Loss: 31.7263   time: 88.50s   best: 31.5964
2023-11-28 23:58:35,208:INFO:  Epoch 175/600:  train Loss: 30.2279   val Loss: 32.0496   time: 88.25s   best: 31.5964
2023-11-29 00:00:03,537:INFO:  Epoch 176/600:  train Loss: 30.8860   val Loss: 32.2289   time: 88.33s   best: 31.5964
2023-11-29 00:01:31,687:INFO:  Epoch 177/600:  train Loss: 30.2599   val Loss: 31.9052   time: 88.14s   best: 31.5964
2023-11-29 00:02:59,892:INFO:  Epoch 178/600:  train Loss: 29.8963   val Loss: 31.7013   time: 88.19s   best: 31.5964
2023-11-29 00:04:28,197:INFO:  Epoch 179/600:  train Loss: 29.8847   val Loss: 32.0695   time: 88.29s   best: 31.5964
2023-11-29 00:05:56,465:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 00:05:56,485:INFO:  Epoch 180/600:  train Loss: 30.1464   val Loss: 31.5102   time: 88.26s   best: 31.5102
2023-11-29 00:07:25,044:INFO:  Epoch 181/600:  train Loss: 29.9008   val Loss: 31.8912   time: 88.55s   best: 31.5102
2023-11-29 00:08:53,679:INFO:  Epoch 182/600:  train Loss: 29.8838   val Loss: 31.6451   time: 88.62s   best: 31.5102
2023-11-29 00:10:22,037:INFO:  Epoch 183/600:  train Loss: 29.8536   val Loss: 31.5427   time: 88.35s   best: 31.5102
2023-11-29 00:11:50,126:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 00:11:50,145:INFO:  Epoch 184/600:  train Loss: 29.5377   val Loss: 31.3476   time: 88.08s   best: 31.3476
2023-11-29 00:13:18,417:INFO:  Epoch 185/600:  train Loss: 29.7666   val Loss: 31.6788   time: 88.26s   best: 31.3476
2023-11-29 00:14:47,183:INFO:  Epoch 186/600:  train Loss: 29.4249   val Loss: 33.0801   time: 88.75s   best: 31.3476
2023-11-29 00:16:15,606:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 00:16:15,625:INFO:  Epoch 187/600:  train Loss: 29.5065   val Loss: 31.1410   time: 88.42s   best: 31.1410
2023-11-29 00:17:43,743:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 00:17:43,762:INFO:  Epoch 188/600:  train Loss: 29.4333   val Loss: 30.9732   time: 88.11s   best: 30.9732
2023-11-29 00:19:12,038:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 00:19:12,066:INFO:  Epoch 189/600:  train Loss: 29.3443   val Loss: 30.9412   time: 88.26s   best: 30.9412
2023-11-29 00:20:40,391:INFO:  Epoch 190/600:  train Loss: 30.3524   val Loss: 32.2407   time: 88.31s   best: 30.9412
2023-11-29 00:22:08,848:INFO:  Epoch 191/600:  train Loss: 29.3273   val Loss: 31.0524   time: 88.46s   best: 30.9412
2023-11-29 00:23:36,980:INFO:  Epoch 192/600:  train Loss: 29.5947   val Loss: 30.9791   time: 88.12s   best: 30.9412
2023-11-29 00:25:05,144:INFO:  Epoch 193/600:  train Loss: 29.1842   val Loss: 31.0999   time: 88.16s   best: 30.9412
2023-11-29 00:26:33,280:INFO:  Epoch 194/600:  train Loss: 29.1563   val Loss: 31.0538   time: 88.14s   best: 30.9412
2023-11-29 00:28:01,574:INFO:  Epoch 195/600:  train Loss: 29.8117   val Loss: 32.6136   time: 88.29s   best: 30.9412
2023-11-29 00:29:30,125:INFO:  Epoch 196/600:  train Loss: 29.5557   val Loss: 31.3120   time: 88.55s   best: 30.9412
2023-11-29 00:30:58,891:INFO:  Epoch 197/600:  train Loss: 29.1661   val Loss: 31.0591   time: 88.75s   best: 30.9412
2023-11-29 00:32:27,471:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 00:32:27,490:INFO:  Epoch 198/600:  train Loss: 29.1222   val Loss: 30.6734   time: 88.56s   best: 30.6734
2023-11-29 00:33:55,726:INFO:  Epoch 199/600:  train Loss: 28.9135   val Loss: 31.1738   time: 88.22s   best: 30.6734
2023-11-29 00:35:24,235:INFO:  Epoch 200/600:  train Loss: 28.9994   val Loss: 31.0782   time: 88.51s   best: 30.6734
2023-11-29 00:36:52,545:INFO:  Epoch 201/600:  train Loss: 29.1441   val Loss: 33.1996   time: 88.30s   best: 30.6734
2023-11-29 00:38:20,875:INFO:  Epoch 202/600:  train Loss: 29.3764   val Loss: 31.5975   time: 88.33s   best: 30.6734
2023-11-29 00:39:50,033:INFO:  Epoch 203/600:  train Loss: 28.7653   val Loss: 30.7708   time: 89.16s   best: 30.6734
2023-11-29 00:41:18,599:INFO:  Epoch 204/600:  train Loss: 29.1076   val Loss: 31.8826   time: 88.57s   best: 30.6734
2023-11-29 00:42:46,976:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 00:42:46,995:INFO:  Epoch 205/600:  train Loss: 28.8108   val Loss: 30.5668   time: 88.36s   best: 30.5668
2023-11-29 00:44:15,472:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 00:44:15,492:INFO:  Epoch 206/600:  train Loss: 28.6317   val Loss: 30.4322   time: 88.46s   best: 30.4322
2023-11-29 00:45:43,983:INFO:  Epoch 207/600:  train Loss: 29.2122   val Loss: 30.5983   time: 88.48s   best: 30.4322
2023-11-29 00:47:12,145:INFO:  Epoch 208/600:  train Loss: 29.4334   val Loss: 31.0155   time: 88.16s   best: 30.4322
2023-11-29 00:48:40,556:INFO:  Epoch 209/600:  train Loss: 28.4909   val Loss: 31.5015   time: 88.40s   best: 30.4322
2023-11-29 00:50:08,985:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 00:50:09,004:INFO:  Epoch 210/600:  train Loss: 28.8159   val Loss: 30.4153   time: 88.41s   best: 30.4153
2023-11-29 00:51:37,435:INFO:  Epoch 211/600:  train Loss: 28.8324   val Loss: 30.5305   time: 88.42s   best: 30.4153
2023-11-29 00:53:05,696:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 00:53:05,715:INFO:  Epoch 212/600:  train Loss: 28.4674   val Loss: 30.2666   time: 88.25s   best: 30.2666
2023-11-29 00:54:33,981:INFO:  Epoch 213/600:  train Loss: 28.5427   val Loss: 30.3793   time: 88.27s   best: 30.2666
2023-11-29 00:56:02,482:INFO:  Epoch 214/600:  train Loss: 28.5830   val Loss: 31.1997   time: 88.50s   best: 30.2666
2023-11-29 00:57:30,658:INFO:  Epoch 215/600:  train Loss: 28.5274   val Loss: 30.8613   time: 88.17s   best: 30.2666
2023-11-29 00:58:58,817:INFO:  Epoch 216/600:  train Loss: 28.1964   val Loss: 30.7651   time: 88.15s   best: 30.2666
2023-11-29 01:00:27,061:INFO:  Epoch 217/600:  train Loss: 28.2536   val Loss: 30.7481   time: 88.24s   best: 30.2666
2023-11-29 01:01:55,324:INFO:  Epoch 218/600:  train Loss: 28.8052   val Loss: 31.4208   time: 88.26s   best: 30.2666
2023-11-29 01:03:23,694:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 01:03:23,713:INFO:  Epoch 219/600:  train Loss: 28.2607   val Loss: 29.9401   time: 88.37s   best: 29.9401
2023-11-29 01:04:52,458:INFO:  Epoch 220/600:  train Loss: 28.3906   val Loss: 30.5619   time: 88.74s   best: 29.9401
2023-11-29 01:06:20,691:INFO:  Epoch 221/600:  train Loss: 27.9827   val Loss: 30.4864   time: 88.22s   best: 29.9401
2023-11-29 01:07:48,915:INFO:  Epoch 222/600:  train Loss: 28.2920   val Loss: 30.6375   time: 88.21s   best: 29.9401
2023-11-29 01:09:17,496:INFO:  Epoch 223/600:  train Loss: 27.9064   val Loss: 30.0951   time: 88.57s   best: 29.9401
2023-11-29 01:10:45,677:INFO:  Epoch 224/600:  train Loss: 27.9967   val Loss: 30.5227   time: 88.17s   best: 29.9401
2023-11-29 01:12:13,924:INFO:  Epoch 225/600:  train Loss: 27.9847   val Loss: 30.4113   time: 88.25s   best: 29.9401
2023-11-29 01:13:41,985:INFO:  Epoch 226/600:  train Loss: 27.8130   val Loss: 30.4358   time: 88.06s   best: 29.9401
2023-11-29 01:15:10,264:INFO:  Epoch 227/600:  train Loss: 27.8170   val Loss: 30.2545   time: 88.28s   best: 29.9401
2023-11-29 01:16:38,869:INFO:  Epoch 228/600:  train Loss: 28.1263   val Loss: 30.8381   time: 88.60s   best: 29.9401
2023-11-29 01:18:07,127:INFO:  Epoch 229/600:  train Loss: 28.0737   val Loss: 29.9685   time: 88.25s   best: 29.9401
2023-11-29 01:19:35,555:INFO:  Epoch 230/600:  train Loss: 28.1462   val Loss: 30.2659   time: 88.43s   best: 29.9401
2023-11-29 01:21:03,708:INFO:  Epoch 231/600:  train Loss: 27.6538   val Loss: 30.0814   time: 88.14s   best: 29.9401
2023-11-29 01:22:32,063:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 01:22:32,082:INFO:  Epoch 232/600:  train Loss: 27.7141   val Loss: 29.6683   time: 88.33s   best: 29.6683
2023-11-29 01:24:00,684:INFO:  Epoch 233/600:  train Loss: 27.4918   val Loss: 29.8593   time: 88.60s   best: 29.6683
2023-11-29 01:25:29,331:INFO:  Epoch 234/600:  train Loss: 27.8420   val Loss: 30.0235   time: 88.61s   best: 29.6683
2023-11-29 01:26:58,022:INFO:  Epoch 235/600:  train Loss: 27.5154   val Loss: 30.3219   time: 88.69s   best: 29.6683
2023-11-29 01:28:26,720:INFO:  Epoch 236/600:  train Loss: 27.5151   val Loss: 29.9075   time: 88.69s   best: 29.6683
2023-11-29 01:29:55,073:INFO:  Epoch 237/600:  train Loss: 28.3072   val Loss: 31.2018   time: 88.35s   best: 29.6683
2023-11-29 01:31:23,252:INFO:  Epoch 238/600:  train Loss: 27.6975   val Loss: 30.0955   time: 88.17s   best: 29.6683
2023-11-29 01:32:51,418:INFO:  Epoch 239/600:  train Loss: 27.2594   val Loss: 29.9194   time: 88.17s   best: 29.6683
2023-11-29 01:34:19,653:INFO:  Epoch 240/600:  train Loss: 27.5178   val Loss: 29.8710   time: 88.22s   best: 29.6683
2023-11-29 01:35:47,842:INFO:  Epoch 241/600:  train Loss: 27.3324   val Loss: 31.5800   time: 88.18s   best: 29.6683
2023-11-29 01:37:15,985:INFO:  Epoch 242/600:  train Loss: 27.3354   val Loss: 30.0764   time: 88.13s   best: 29.6683
2023-11-29 01:38:44,319:INFO:  Epoch 243/600:  train Loss: 27.3757   val Loss: 30.2557   time: 88.32s   best: 29.6683
2023-11-29 01:40:12,560:INFO:  Epoch 244/600:  train Loss: 27.2524   val Loss: 29.9830   time: 88.23s   best: 29.6683
2023-11-29 01:41:40,618:INFO:  Epoch 245/600:  train Loss: 27.2597   val Loss: 29.8496   time: 88.06s   best: 29.6683
2023-11-29 01:43:08,726:INFO:  Epoch 246/600:  train Loss: 27.2389   val Loss: 29.9355   time: 88.11s   best: 29.6683
2023-11-29 01:44:36,984:INFO:  Epoch 247/600:  train Loss: 27.0647   val Loss: 29.6684   time: 88.25s   best: 29.6683
2023-11-29 01:46:05,251:INFO:  Epoch 248/600:  train Loss: 27.1537   val Loss: 29.9166   time: 88.27s   best: 29.6683
2023-11-29 01:47:33,768:INFO:  Epoch 249/600:  train Loss: 27.0408   val Loss: 30.5909   time: 88.49s   best: 29.6683
2023-11-29 01:49:02,044:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 01:49:02,063:INFO:  Epoch 250/600:  train Loss: 27.1811   val Loss: 29.3745   time: 88.26s   best: 29.3745
2023-11-29 01:50:30,209:INFO:  Epoch 251/600:  train Loss: 26.8966   val Loss: 32.8966   time: 88.15s   best: 29.3745
2023-11-29 01:51:58,271:INFO:  Epoch 252/600:  train Loss: 27.4959   val Loss: 29.6196   time: 88.05s   best: 29.3745
2023-11-29 01:53:26,560:INFO:  Epoch 253/600:  train Loss: 26.7657   val Loss: 29.6591   time: 88.29s   best: 29.3745
2023-11-29 01:54:55,129:INFO:  Epoch 254/600:  train Loss: 27.0271   val Loss: 29.4350   time: 88.56s   best: 29.3745
2023-11-29 01:56:23,170:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 01:56:23,189:INFO:  Epoch 255/600:  train Loss: 26.9582   val Loss: 29.2280   time: 88.04s   best: 29.2280
2023-11-29 01:57:51,279:INFO:  Epoch 256/600:  train Loss: 27.7729   val Loss: 29.5872   time: 88.09s   best: 29.2280
2023-11-29 01:59:19,472:INFO:  Epoch 257/600:  train Loss: 26.7695   val Loss: 29.2626   time: 88.19s   best: 29.2280
2023-11-29 02:00:47,663:INFO:  Epoch 258/600:  train Loss: 26.7385   val Loss: 30.0673   time: 88.19s   best: 29.2280
2023-11-29 02:02:16,404:INFO:  Epoch 259/600:  train Loss: 26.7397   val Loss: 29.9573   time: 88.73s   best: 29.2280
2023-11-29 02:03:44,547:INFO:  Epoch 260/600:  train Loss: 27.4380   val Loss: 29.7763   time: 88.13s   best: 29.2280
2023-11-29 02:05:12,718:INFO:  Epoch 261/600:  train Loss: 26.5940   val Loss: 29.6534   time: 88.17s   best: 29.2280
2023-11-29 02:06:40,915:INFO:  Epoch 262/600:  train Loss: 26.6116   val Loss: 29.7512   time: 88.20s   best: 29.2280
2023-11-29 02:08:08,968:INFO:  Epoch 263/600:  train Loss: 26.6549   val Loss: 29.2891   time: 88.05s   best: 29.2280
2023-11-29 02:09:37,178:INFO:  Epoch 264/600:  train Loss: 26.5382   val Loss: 29.5515   time: 88.21s   best: 29.2280
2023-11-29 02:11:05,656:INFO:  Epoch 265/600:  train Loss: 26.5405   val Loss: 29.6047   time: 88.48s   best: 29.2280
2023-11-29 02:12:33,891:INFO:  Epoch 266/600:  train Loss: 26.6042   val Loss: 29.4721   time: 88.23s   best: 29.2280
2023-11-29 02:14:02,124:INFO:  Epoch 267/600:  train Loss: 26.4556   val Loss: 29.6641   time: 88.22s   best: 29.2280
2023-11-29 02:15:30,709:INFO:  Epoch 268/600:  train Loss: 26.8157   val Loss: 29.6691   time: 88.58s   best: 29.2280
2023-11-29 02:16:58,943:INFO:  Epoch 269/600:  train Loss: 26.5618   val Loss: 32.3527   time: 88.23s   best: 29.2280
2023-11-29 02:18:27,097:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 02:18:27,116:INFO:  Epoch 270/600:  train Loss: 26.5558   val Loss: 29.1309   time: 88.15s   best: 29.1309
2023-11-29 02:19:55,280:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 02:19:55,299:INFO:  Epoch 271/600:  train Loss: 26.4369   val Loss: 28.9410   time: 88.15s   best: 28.9410
2023-11-29 02:21:23,768:INFO:  Epoch 272/600:  train Loss: 26.3111   val Loss: 29.3148   time: 88.46s   best: 28.9410
2023-11-29 02:22:52,026:INFO:  Epoch 273/600:  train Loss: 26.5167   val Loss: 29.1183   time: 88.26s   best: 28.9410
2023-11-29 02:24:20,356:INFO:  Epoch 274/600:  train Loss: 26.6290   val Loss: 29.6373   time: 88.32s   best: 28.9410
2023-11-29 02:25:48,408:INFO:  Epoch 275/600:  train Loss: 26.6445   val Loss: 29.5673   time: 88.04s   best: 28.9410
2023-11-29 02:27:17,014:INFO:  Epoch 276/600:  train Loss: 26.2757   val Loss: 29.1708   time: 88.60s   best: 28.9410
2023-11-29 02:28:45,549:INFO:  Epoch 277/600:  train Loss: 26.0685   val Loss: 29.6018   time: 88.53s   best: 28.9410
2023-11-29 02:30:13,717:INFO:  Epoch 278/600:  train Loss: 26.5606   val Loss: 30.1735   time: 88.16s   best: 28.9410
2023-11-29 02:31:41,760:INFO:  Epoch 279/600:  train Loss: 26.1233   val Loss: 30.3129   time: 88.03s   best: 28.9410
2023-11-29 02:33:09,900:INFO:  Epoch 280/600:  train Loss: 26.5341   val Loss: 29.5926   time: 88.13s   best: 28.9410
2023-11-29 02:34:38,262:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 02:34:38,281:INFO:  Epoch 281/600:  train Loss: 26.1115   val Loss: 28.7856   time: 88.35s   best: 28.7856
2023-11-29 02:36:07,151:INFO:  Epoch 282/600:  train Loss: 26.1043   val Loss: 30.4983   time: 88.86s   best: 28.7856
2023-11-29 02:37:35,751:INFO:  Epoch 283/600:  train Loss: 26.1302   val Loss: 29.0728   time: 88.60s   best: 28.7856
2023-11-29 02:39:04,379:INFO:  Epoch 284/600:  train Loss: 26.0944   val Loss: 29.3210   time: 88.63s   best: 28.7856
2023-11-29 02:40:32,753:INFO:  Epoch 285/600:  train Loss: 26.0342   val Loss: 28.9436   time: 88.36s   best: 28.7856
2023-11-29 02:42:01,037:INFO:  Epoch 286/600:  train Loss: 25.8860   val Loss: 29.3017   time: 88.27s   best: 28.7856
2023-11-29 02:43:29,378:INFO:  Epoch 287/600:  train Loss: 26.0891   val Loss: 28.7962   time: 88.33s   best: 28.7856
2023-11-29 02:44:58,317:INFO:  Epoch 288/600:  train Loss: 26.5222   val Loss: 29.0564   time: 88.94s   best: 28.7856
2023-11-29 02:46:26,794:INFO:  Epoch 289/600:  train Loss: 25.8180   val Loss: 32.3956   time: 88.47s   best: 28.7856
2023-11-29 02:47:55,023:INFO:  Epoch 290/600:  train Loss: 26.5012   val Loss: 30.9240   time: 88.23s   best: 28.7856
2023-11-29 02:49:22,951:INFO:  Epoch 291/600:  train Loss: 26.3536   val Loss: 29.1053   time: 87.93s   best: 28.7856
2023-11-29 02:50:51,623:INFO:  Epoch 292/600:  train Loss: 25.9283   val Loss: 28.8554   time: 88.67s   best: 28.7856
2023-11-29 02:52:19,834:INFO:  Epoch 293/600:  train Loss: 25.7214   val Loss: 29.7580   time: 88.19s   best: 28.7856
2023-11-29 02:53:47,962:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 02:53:47,981:INFO:  Epoch 294/600:  train Loss: 26.0098   val Loss: 28.7775   time: 88.11s   best: 28.7775
2023-11-29 02:55:16,368:INFO:  Epoch 295/600:  train Loss: 25.6704   val Loss: 29.0678   time: 88.38s   best: 28.7775
2023-11-29 02:56:44,806:INFO:  Epoch 296/600:  train Loss: 25.7080   val Loss: 29.2571   time: 88.43s   best: 28.7775
2023-11-29 02:58:13,350:INFO:  Epoch 297/600:  train Loss: 25.9604   val Loss: 29.1248   time: 88.54s   best: 28.7775
2023-11-29 02:59:41,914:INFO:  Epoch 298/600:  train Loss: 25.6648   val Loss: 28.9903   time: 88.55s   best: 28.7775
2023-11-29 03:01:10,484:INFO:  Epoch 299/600:  train Loss: 25.5433   val Loss: 29.3651   time: 88.57s   best: 28.7775
2023-11-29 03:02:38,902:INFO:  Epoch 300/600:  train Loss: 25.5248   val Loss: 29.3415   time: 88.41s   best: 28.7775
2023-11-29 03:04:07,295:INFO:  Epoch 301/600:  train Loss: 25.5850   val Loss: 29.5412   time: 88.39s   best: 28.7775
2023-11-29 03:05:35,483:INFO:  Epoch 302/600:  train Loss: 26.0014   val Loss: 29.1454   time: 88.18s   best: 28.7775
2023-11-29 03:07:03,965:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 03:07:03,984:INFO:  Epoch 303/600:  train Loss: 25.5232   val Loss: 28.7349   time: 88.47s   best: 28.7349
2023-11-29 03:08:32,594:INFO:  Epoch 304/600:  train Loss: 25.5996   val Loss: 29.0314   time: 88.61s   best: 28.7349
2023-11-29 03:10:01,270:INFO:  Epoch 305/600:  train Loss: 25.5687   val Loss: 28.9596   time: 88.66s   best: 28.7349
2023-11-29 03:11:29,499:INFO:  Epoch 306/600:  train Loss: 25.3463   val Loss: 28.9000   time: 88.22s   best: 28.7349
2023-11-29 03:12:57,523:INFO:  Epoch 307/600:  train Loss: 25.5004   val Loss: 28.9318   time: 88.01s   best: 28.7349
2023-11-29 03:14:25,691:INFO:  Epoch 308/600:  train Loss: 25.5202   val Loss: 29.1554   time: 88.16s   best: 28.7349
2023-11-29 03:15:53,837:INFO:  Epoch 309/600:  train Loss: 25.4173   val Loss: 29.0597   time: 88.15s   best: 28.7349
2023-11-29 03:17:22,025:INFO:  Epoch 310/600:  train Loss: 25.2932   val Loss: 29.0499   time: 88.19s   best: 28.7349
2023-11-29 03:18:50,241:INFO:  Epoch 311/600:  train Loss: 25.4609   val Loss: 28.8988   time: 88.21s   best: 28.7349
2023-11-29 03:20:18,362:INFO:  Epoch 312/600:  train Loss: 25.5487   val Loss: 29.1125   time: 88.12s   best: 28.7349
2023-11-29 03:21:47,189:INFO:  Epoch 313/600:  train Loss: 25.2364   val Loss: 28.8715   time: 88.81s   best: 28.7349
2023-11-29 03:23:15,524:INFO:  Epoch 314/600:  train Loss: 25.2524   val Loss: 28.9165   time: 88.32s   best: 28.7349
2023-11-29 03:24:43,604:INFO:  Epoch 315/600:  train Loss: 25.4742   val Loss: 33.0054   time: 88.08s   best: 28.7349
2023-11-29 03:26:12,029:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 03:26:12,049:INFO:  Epoch 316/600:  train Loss: 25.6541   val Loss: 28.4081   time: 88.41s   best: 28.4081
2023-11-29 03:27:40,005:INFO:  Epoch 317/600:  train Loss: 25.0737   val Loss: 28.6506   time: 87.95s   best: 28.4081
2023-11-29 03:29:08,561:INFO:  Epoch 318/600:  train Loss: 25.2992   val Loss: 28.5973   time: 88.56s   best: 28.4081
2023-11-29 03:30:36,984:INFO:  Epoch 319/600:  train Loss: 25.1212   val Loss: 28.5879   time: 88.42s   best: 28.4081
2023-11-29 03:32:05,170:INFO:  Epoch 320/600:  train Loss: 25.1709   val Loss: 28.5497   time: 88.17s   best: 28.4081
2023-11-29 03:33:33,130:INFO:  Epoch 321/600:  train Loss: 25.0613   val Loss: 28.9342   time: 87.95s   best: 28.4081
2023-11-29 03:35:01,165:INFO:  Epoch 322/600:  train Loss: 25.1891   val Loss: 29.2231   time: 88.03s   best: 28.4081
2023-11-29 03:36:29,297:INFO:  Epoch 323/600:  train Loss: 24.9598   val Loss: 28.9343   time: 88.12s   best: 28.4081
2023-11-29 03:37:57,342:INFO:  Epoch 324/600:  train Loss: 25.6128   val Loss: 32.4212   time: 88.03s   best: 28.4081
2023-11-29 03:39:25,420:INFO:  Epoch 325/600:  train Loss: 25.2323   val Loss: 28.9510   time: 88.07s   best: 28.4081
2023-11-29 03:40:53,738:INFO:  Epoch 326/600:  train Loss: 25.1274   val Loss: 31.8117   time: 88.32s   best: 28.4081
2023-11-29 03:42:22,424:INFO:  Epoch 327/600:  train Loss: 25.1426   val Loss: 28.6109   time: 88.68s   best: 28.4081
2023-11-29 03:43:50,434:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 03:43:50,454:INFO:  Epoch 328/600:  train Loss: 24.9974   val Loss: 28.2036   time: 87.99s   best: 28.2036
2023-11-29 03:45:18,524:INFO:  Epoch 329/600:  train Loss: 24.8959   val Loss: 28.6421   time: 88.06s   best: 28.2036
2023-11-29 03:46:47,141:INFO:  Epoch 330/600:  train Loss: 25.4455   val Loss: 31.2222   time: 88.62s   best: 28.2036
2023-11-29 03:48:15,522:INFO:  Epoch 331/600:  train Loss: 25.2180   val Loss: 28.8110   time: 88.37s   best: 28.2036
2023-11-29 03:49:43,666:INFO:  Epoch 332/600:  train Loss: 25.0106   val Loss: 28.6891   time: 88.13s   best: 28.2036
2023-11-29 03:51:11,818:INFO:  Epoch 333/600:  train Loss: 24.7619   val Loss: 28.9178   time: 88.15s   best: 28.2036
2023-11-29 03:52:40,159:INFO:  Epoch 334/600:  train Loss: 25.1910   val Loss: 28.6334   time: 88.33s   best: 28.2036
2023-11-29 03:54:08,410:INFO:  Epoch 335/600:  train Loss: 24.7297   val Loss: 28.6904   time: 88.24s   best: 28.2036
2023-11-29 03:55:36,378:INFO:  Epoch 336/600:  train Loss: 24.7976   val Loss: 29.0820   time: 87.97s   best: 28.2036
2023-11-29 03:57:04,828:INFO:  Epoch 337/600:  train Loss: 24.7964   val Loss: 28.8138   time: 88.44s   best: 28.2036
2023-11-29 03:58:32,958:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 03:58:32,977:INFO:  Epoch 338/600:  train Loss: 25.1622   val Loss: 28.0280   time: 88.13s   best: 28.0280
2023-11-29 04:00:01,109:INFO:  Epoch 339/600:  train Loss: 24.9377   val Loss: 28.5620   time: 88.13s   best: 28.0280
2023-11-29 04:01:29,720:INFO:  Epoch 340/600:  train Loss: 25.1297   val Loss: 28.5058   time: 88.60s   best: 28.0280
2023-11-29 04:02:58,199:INFO:  Epoch 341/600:  train Loss: 24.8295   val Loss: 28.2400   time: 88.47s   best: 28.0280
2023-11-29 04:04:26,890:INFO:  Epoch 342/600:  train Loss: 25.7477   val Loss: 28.3145   time: 88.69s   best: 28.0280
2023-11-29 04:05:55,850:INFO:  Epoch 343/600:  train Loss: 24.7364   val Loss: 28.6426   time: 88.96s   best: 28.0280
2023-11-29 04:07:23,879:INFO:  Epoch 344/600:  train Loss: 24.8276   val Loss: 28.1957   time: 88.02s   best: 28.0280
2023-11-29 04:08:52,081:INFO:  Epoch 345/600:  train Loss: 24.5465   val Loss: 28.5122   time: 88.20s   best: 28.0280
2023-11-29 04:10:20,088:INFO:  Epoch 346/600:  train Loss: 24.5586   val Loss: 30.0227   time: 87.99s   best: 28.0280
2023-11-29 04:11:48,169:INFO:  Epoch 347/600:  train Loss: 24.7326   val Loss: 28.3377   time: 88.08s   best: 28.0280
2023-11-29 04:13:16,596:INFO:  Epoch 348/600:  train Loss: 24.6064   val Loss: 28.6283   time: 88.42s   best: 28.0280
2023-11-29 04:14:44,730:INFO:  Epoch 349/600:  train Loss: 24.5829   val Loss: 28.4892   time: 88.13s   best: 28.0280
2023-11-29 04:16:12,923:INFO:  Epoch 350/600:  train Loss: 24.4687   val Loss: 28.4896   time: 88.18s   best: 28.0280
2023-11-29 04:17:40,970:INFO:  Epoch 351/600:  train Loss: 24.9171   val Loss: 31.3631   time: 88.04s   best: 28.0280
2023-11-29 04:19:09,194:INFO:  Epoch 352/600:  train Loss: 25.1084   val Loss: 28.3806   time: 88.22s   best: 28.0280
2023-11-29 04:20:37,243:INFO:  Epoch 353/600:  train Loss: 24.4806   val Loss: 28.2540   time: 88.04s   best: 28.0280
2023-11-29 04:22:05,738:INFO:  Epoch 354/600:  train Loss: 24.3609   val Loss: 28.1006   time: 88.48s   best: 28.0280
2023-11-29 04:23:33,688:INFO:  Epoch 355/600:  train Loss: 24.4081   val Loss: 28.2525   time: 87.92s   best: 28.0280
2023-11-29 04:25:01,785:INFO:  Epoch 356/600:  train Loss: 24.5736   val Loss: 28.0621   time: 88.09s   best: 28.0280
2023-11-29 04:26:30,304:INFO:  Epoch 357/600:  train Loss: 24.3826   val Loss: 28.4785   time: 88.51s   best: 28.0280
2023-11-29 04:27:58,287:INFO:  Epoch 358/600:  train Loss: 25.2844   val Loss: 29.6172   time: 87.97s   best: 28.0280
2023-11-29 04:29:26,209:INFO:  Epoch 359/600:  train Loss: 24.5433   val Loss: 28.1977   time: 87.91s   best: 28.0280
2023-11-29 04:30:54,269:INFO:  Epoch 360/600:  train Loss: 24.5552   val Loss: 28.8974   time: 88.05s   best: 28.0280
2023-11-29 04:32:22,382:INFO:  Epoch 361/600:  train Loss: 24.2701   val Loss: 28.5684   time: 88.10s   best: 28.0280
2023-11-29 04:33:50,377:INFO:  Epoch 362/600:  train Loss: 24.2370   val Loss: 28.0436   time: 87.98s   best: 28.0280
2023-11-29 04:35:18,317:INFO:  Epoch 363/600:  train Loss: 24.4157   val Loss: 28.5213   time: 87.94s   best: 28.0280
2023-11-29 04:36:46,272:INFO:  Epoch 364/600:  train Loss: 24.2323   val Loss: 28.2613   time: 87.95s   best: 28.0280
2023-11-29 04:38:14,456:INFO:  Epoch 365/600:  train Loss: 24.2435   val Loss: 28.4106   time: 88.17s   best: 28.0280
2023-11-29 04:39:42,534:INFO:  Epoch 366/600:  train Loss: 24.2418   val Loss: 28.3648   time: 88.07s   best: 28.0280
2023-11-29 04:41:10,705:INFO:  Epoch 367/600:  train Loss: 24.8032   val Loss: 28.0878   time: 88.16s   best: 28.0280
2023-11-29 04:42:39,105:INFO:  Epoch 368/600:  train Loss: 24.7603   val Loss: 29.7284   time: 88.40s   best: 28.0280
2023-11-29 04:44:07,242:INFO:  Epoch 369/600:  train Loss: 24.3042   val Loss: 28.1246   time: 88.12s   best: 28.0280
2023-11-29 04:45:35,490:INFO:  Epoch 370/600:  train Loss: 24.1866   val Loss: 28.0719   time: 88.24s   best: 28.0280
2023-11-29 04:47:03,466:INFO:  Epoch 371/600:  train Loss: 24.1154   val Loss: 28.9617   time: 87.98s   best: 28.0280
2023-11-29 04:48:31,586:INFO:  Epoch 372/600:  train Loss: 24.1456   val Loss: 28.6004   time: 88.11s   best: 28.0280
2023-11-29 04:49:59,651:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 04:49:59,670:INFO:  Epoch 373/600:  train Loss: 24.2773   val Loss: 27.9939   time: 88.05s   best: 27.9939
2023-11-29 04:51:27,824:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 04:51:27,843:INFO:  Epoch 374/600:  train Loss: 24.0008   val Loss: 27.9386   time: 88.15s   best: 27.9386
2023-11-29 04:52:55,833:INFO:  Epoch 375/600:  train Loss: 24.0517   val Loss: 28.0154   time: 87.98s   best: 27.9386
2023-11-29 04:54:24,364:INFO:  Epoch 376/600:  train Loss: 24.0355   val Loss: 28.3271   time: 88.52s   best: 27.9386
2023-11-29 04:55:52,625:INFO:  Epoch 377/600:  train Loss: 24.2408   val Loss: 28.3879   time: 88.26s   best: 27.9386
2023-11-29 04:57:21,003:INFO:  Epoch 378/600:  train Loss: 24.0102   val Loss: 28.4507   time: 88.37s   best: 27.9386
2023-11-29 04:58:49,013:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 04:58:49,032:INFO:  Epoch 379/600:  train Loss: 24.1560   val Loss: 27.8582   time: 88.00s   best: 27.8582
2023-11-29 05:00:17,034:INFO:  Epoch 380/600:  train Loss: 24.0504   val Loss: 28.3168   time: 88.00s   best: 27.8582
2023-11-29 05:01:44,945:INFO:  Epoch 381/600:  train Loss: 23.9691   val Loss: 28.3942   time: 87.91s   best: 27.8582
2023-11-29 05:03:13,136:INFO:  Epoch 382/600:  train Loss: 24.1117   val Loss: 28.2545   time: 88.19s   best: 27.8582
2023-11-29 05:04:41,078:INFO:  Epoch 383/600:  train Loss: 23.8672   val Loss: 27.9623   time: 87.93s   best: 27.8582
2023-11-29 05:06:08,926:INFO:  Epoch 384/600:  train Loss: 24.6141   val Loss: 28.6990   time: 87.84s   best: 27.8582
2023-11-29 05:07:37,241:INFO:  Epoch 385/600:  train Loss: 24.5346   val Loss: 27.9362   time: 88.31s   best: 27.8582
2023-11-29 05:09:05,401:INFO:  Epoch 386/600:  train Loss: 24.3080   val Loss: 28.5687   time: 88.16s   best: 27.8582
2023-11-29 05:10:33,810:INFO:  Epoch 387/600:  train Loss: 23.8030   val Loss: 28.1554   time: 88.41s   best: 27.8582
2023-11-29 05:12:01,691:INFO:  Epoch 388/600:  train Loss: 23.8970   val Loss: 27.9537   time: 87.88s   best: 27.8582
2023-11-29 05:13:29,795:INFO:  Epoch 389/600:  train Loss: 24.0359   val Loss: 27.9523   time: 88.10s   best: 27.8582
2023-11-29 05:14:57,788:INFO:  Epoch 390/600:  train Loss: 24.0123   val Loss: 28.7462   time: 87.99s   best: 27.8582
2023-11-29 05:16:25,725:INFO:  Epoch 391/600:  train Loss: 23.8469   val Loss: 28.0598   time: 87.94s   best: 27.8582
2023-11-29 05:17:53,624:INFO:  Epoch 392/600:  train Loss: 23.9514   val Loss: 28.0834   time: 87.90s   best: 27.8582
2023-11-29 05:19:21,640:INFO:  Epoch 393/600:  train Loss: 23.9187   val Loss: 28.0882   time: 88.00s   best: 27.8582
2023-11-29 05:20:49,652:INFO:  Epoch 394/600:  train Loss: 23.7780   val Loss: 28.3383   time: 88.01s   best: 27.8582
2023-11-29 05:22:17,546:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 05:22:17,571:INFO:  Epoch 395/600:  train Loss: 23.9765   val Loss: 27.7847   time: 87.88s   best: 27.7847
2023-11-29 05:23:45,569:INFO:  Epoch 396/600:  train Loss: 23.8299   val Loss: 28.0630   time: 87.99s   best: 27.7847
2023-11-29 05:25:13,616:INFO:  Epoch 397/600:  train Loss: 23.8022   val Loss: 27.9112   time: 88.03s   best: 27.7847
2023-11-29 05:26:41,885:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 05:26:41,904:INFO:  Epoch 398/600:  train Loss: 23.6560   val Loss: 27.4251   time: 88.26s   best: 27.4251
2023-11-29 05:28:10,191:INFO:  Epoch 399/600:  train Loss: 23.7153   val Loss: 27.8113   time: 88.28s   best: 27.4251
2023-11-29 05:29:38,737:INFO:  Epoch 400/600:  train Loss: 23.6191   val Loss: 28.3472   time: 88.55s   best: 27.4251
2023-11-29 05:31:06,556:INFO:  Epoch 401/600:  train Loss: 23.8496   val Loss: 27.8872   time: 87.82s   best: 27.4251
2023-11-29 05:32:34,506:INFO:  Epoch 402/600:  train Loss: 23.7346   val Loss: 28.0171   time: 87.95s   best: 27.4251
2023-11-29 05:34:02,529:INFO:  Epoch 403/600:  train Loss: 23.5458   val Loss: 28.0429   time: 88.02s   best: 27.4251
2023-11-29 05:35:30,493:INFO:  Epoch 404/600:  train Loss: 23.7002   val Loss: 28.3023   time: 87.96s   best: 27.4251
2023-11-29 05:36:58,461:INFO:  Epoch 405/600:  train Loss: 23.7262   val Loss: 27.8012   time: 87.97s   best: 27.4251
2023-11-29 05:38:27,070:INFO:  Epoch 406/600:  train Loss: 23.7409   val Loss: 27.4933   time: 88.60s   best: 27.4251
2023-11-29 05:39:55,590:INFO:  Epoch 407/600:  train Loss: 23.5467   val Loss: 27.5931   time: 88.51s   best: 27.4251
2023-11-29 05:41:23,862:INFO:  Epoch 408/600:  train Loss: 24.1878   val Loss: 28.6698   time: 88.27s   best: 27.4251
2023-11-29 05:42:52,784:INFO:  Epoch 409/600:  train Loss: 24.4453   val Loss: 29.4426   time: 88.92s   best: 27.4251
2023-11-29 05:44:21,119:INFO:  Epoch 410/600:  train Loss: 23.7924   val Loss: 28.0011   time: 88.33s   best: 27.4251
2023-11-29 05:45:49,004:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 05:45:49,023:INFO:  Epoch 411/600:  train Loss: 23.4598   val Loss: 27.3163   time: 87.88s   best: 27.3163
2023-11-29 05:47:17,020:INFO:  Epoch 412/600:  train Loss: 24.3327   val Loss: 27.9234   time: 88.00s   best: 27.3163
2023-11-29 05:48:45,305:INFO:  Epoch 413/600:  train Loss: 23.4101   val Loss: 27.5952   time: 88.28s   best: 27.3163
2023-11-29 05:50:13,315:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 05:50:13,334:INFO:  Epoch 414/600:  train Loss: 23.4086   val Loss: 27.2777   time: 88.01s   best: 27.2777
2023-11-29 05:51:41,136:INFO:  Epoch 415/600:  train Loss: 23.4644   val Loss: 27.7509   time: 87.79s   best: 27.2777
2023-11-29 05:53:09,008:INFO:  Epoch 416/600:  train Loss: 23.7730   val Loss: 27.8756   time: 87.86s   best: 27.2777
2023-11-29 05:54:36,962:INFO:  Epoch 417/600:  train Loss: 24.0813   val Loss: 29.5085   time: 87.95s   best: 27.2777
2023-11-29 05:56:04,785:INFO:  Epoch 418/600:  train Loss: 23.7722   val Loss: 27.6362   time: 87.81s   best: 27.2777
2023-11-29 05:57:32,722:INFO:  Epoch 419/600:  train Loss: 23.5251   val Loss: 27.3058   time: 87.93s   best: 27.2777
2023-11-29 05:59:01,084:INFO:  Epoch 420/600:  train Loss: 23.5548   val Loss: 27.5476   time: 88.35s   best: 27.2777
2023-11-29 06:00:28,998:INFO:  Epoch 421/600:  train Loss: 24.4878   val Loss: 28.0410   time: 87.90s   best: 27.2777
2023-11-29 06:01:56,926:INFO:  Epoch 422/600:  train Loss: 23.5182   val Loss: 27.7525   time: 87.93s   best: 27.2777
2023-11-29 06:03:25,214:INFO:  Epoch 423/600:  train Loss: 23.3280   val Loss: 27.3515   time: 88.29s   best: 27.2777
2023-11-29 06:04:53,457:INFO:  Epoch 424/600:  train Loss: 23.3145   val Loss: 27.4891   time: 88.24s   best: 27.2777
2023-11-29 06:06:21,506:INFO:  Epoch 425/600:  train Loss: 23.3316   val Loss: 27.4250   time: 88.04s   best: 27.2777
2023-11-29 06:07:49,418:INFO:  Epoch 426/600:  train Loss: 23.4120   val Loss: 27.8726   time: 87.90s   best: 27.2777
2023-11-29 06:09:17,439:INFO:  Epoch 427/600:  train Loss: 23.2914   val Loss: 28.0790   time: 88.02s   best: 27.2777
2023-11-29 06:10:45,312:INFO:  Epoch 428/600:  train Loss: 23.6564   val Loss: 27.7118   time: 87.87s   best: 27.2777
2023-11-29 06:12:13,515:INFO:  Epoch 429/600:  train Loss: 23.2667   val Loss: 28.0233   time: 88.20s   best: 27.2777
2023-11-29 06:13:41,950:INFO:  Epoch 430/600:  train Loss: 23.2621   val Loss: 27.6007   time: 88.43s   best: 27.2777
2023-11-29 06:15:10,028:INFO:  Epoch 431/600:  train Loss: 23.1750   val Loss: 27.7564   time: 88.07s   best: 27.2777
2023-11-29 06:16:37,869:INFO:  Epoch 432/600:  train Loss: 23.3591   val Loss: 29.0135   time: 87.83s   best: 27.2777
2023-11-29 06:18:05,893:INFO:  Epoch 433/600:  train Loss: 23.1944   val Loss: 27.9211   time: 88.01s   best: 27.2777
2023-11-29 06:19:33,854:INFO:  Epoch 434/600:  train Loss: 23.2633   val Loss: 28.1472   time: 87.96s   best: 27.2777
2023-11-29 06:21:01,665:INFO:  Epoch 435/600:  train Loss: 23.7648   val Loss: 27.4896   time: 87.80s   best: 27.2777
2023-11-29 06:22:29,482:INFO:  Epoch 436/600:  train Loss: 23.1388   val Loss: 27.6367   time: 87.82s   best: 27.2777
2023-11-29 06:23:57,622:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 06:23:57,641:INFO:  Epoch 437/600:  train Loss: 23.2585   val Loss: 27.2310   time: 88.14s   best: 27.2310
2023-11-29 06:25:25,594:INFO:  Epoch 438/600:  train Loss: 23.2022   val Loss: 27.5416   time: 87.95s   best: 27.2310
2023-11-29 06:26:53,335:INFO:  Epoch 439/600:  train Loss: 23.3855   val Loss: 27.7960   time: 87.74s   best: 27.2310
2023-11-29 06:28:21,598:INFO:  Epoch 440/600:  train Loss: 23.0754   val Loss: 27.5850   time: 88.26s   best: 27.2310
2023-11-29 06:29:49,468:INFO:  Epoch 441/600:  train Loss: 23.3737   val Loss: 27.4742   time: 87.87s   best: 27.2310
2023-11-29 06:31:17,206:INFO:  Epoch 442/600:  train Loss: 23.2401   val Loss: 27.8315   time: 87.73s   best: 27.2310
2023-11-29 06:32:45,025:INFO:  Epoch 443/600:  train Loss: 23.0660   val Loss: 27.4203   time: 87.82s   best: 27.2310
2023-11-29 06:34:13,119:INFO:  Epoch 444/600:  train Loss: 23.4104   val Loss: 27.2732   time: 88.08s   best: 27.2310
2023-11-29 06:35:41,080:INFO:  Epoch 445/600:  train Loss: 22.9900   val Loss: 27.4074   time: 87.95s   best: 27.2310
2023-11-29 06:37:09,381:INFO:  Epoch 446/600:  train Loss: 23.2556   val Loss: 27.6461   time: 88.30s   best: 27.2310
2023-11-29 06:38:37,165:INFO:  Epoch 447/600:  train Loss: 22.9577   val Loss: 27.5680   time: 87.77s   best: 27.2310
2023-11-29 06:40:05,058:INFO:  Epoch 448/600:  train Loss: 23.2231   val Loss: 27.4814   time: 87.88s   best: 27.2310
2023-11-29 06:41:32,808:INFO:  Epoch 449/600:  train Loss: 22.9763   val Loss: 27.3162   time: 87.75s   best: 27.2310
2023-11-29 06:43:00,659:INFO:  Epoch 450/600:  train Loss: 23.0331   val Loss: 27.6247   time: 87.84s   best: 27.2310
2023-11-29 06:44:28,399:INFO:  Epoch 451/600:  train Loss: 23.0373   val Loss: 27.5816   time: 87.74s   best: 27.2310
2023-11-29 06:45:56,112:INFO:  Epoch 452/600:  train Loss: 23.4852   val Loss: 28.0396   time: 87.70s   best: 27.2310
2023-11-29 06:47:23,822:INFO:  Epoch 453/600:  train Loss: 22.9426   val Loss: 27.5265   time: 87.70s   best: 27.2310
2023-11-29 06:48:52,128:INFO:  Epoch 454/600:  train Loss: 24.1146   val Loss: 27.6364   time: 88.31s   best: 27.2310
2023-11-29 06:50:19,932:INFO:  Epoch 455/600:  train Loss: 23.0679   val Loss: 27.3602   time: 87.80s   best: 27.2310
2023-11-29 06:51:47,742:INFO:  Epoch 456/600:  train Loss: 23.1760   val Loss: 27.5207   time: 87.81s   best: 27.2310
2023-11-29 06:53:15,514:INFO:  Epoch 457/600:  train Loss: 23.1079   val Loss: 27.6693   time: 87.76s   best: 27.2310
2023-11-29 06:54:43,256:INFO:  Epoch 458/600:  train Loss: 22.8367   val Loss: 27.8915   time: 87.74s   best: 27.2310
2023-11-29 06:56:10,996:INFO:  Epoch 459/600:  train Loss: 22.9519   val Loss: 27.9236   time: 87.73s   best: 27.2310
2023-11-29 06:57:38,744:INFO:  Epoch 460/600:  train Loss: 23.1512   val Loss: 27.5106   time: 87.74s   best: 27.2310
2023-11-29 06:59:06,708:INFO:  Epoch 461/600:  train Loss: 22.9482   val Loss: 27.9730   time: 87.96s   best: 27.2310
2023-11-29 07:00:34,539:INFO:  Epoch 462/600:  train Loss: 22.8622   val Loss: 27.9075   time: 87.83s   best: 27.2310
2023-11-29 07:02:02,717:INFO:  Epoch 463/600:  train Loss: 22.8446   val Loss: 27.4925   time: 88.18s   best: 27.2310
2023-11-29 07:03:30,824:INFO:  Epoch 464/600:  train Loss: 23.7968   val Loss: 30.1847   time: 88.11s   best: 27.2310
2023-11-29 07:04:58,625:INFO:  Epoch 465/600:  train Loss: 23.9068   val Loss: 27.4298   time: 87.79s   best: 27.2310
2023-11-29 07:06:26,877:INFO:  Epoch 466/600:  train Loss: 23.6387   val Loss: 28.0234   time: 88.25s   best: 27.2310
2023-11-29 07:07:55,010:INFO:  Epoch 467/600:  train Loss: 23.0986   val Loss: 27.3189   time: 88.13s   best: 27.2310
2023-11-29 07:09:22,674:INFO:  Epoch 468/600:  train Loss: 22.8675   val Loss: 27.4520   time: 87.66s   best: 27.2310
2023-11-29 07:10:50,930:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 07:10:50,949:INFO:  Epoch 469/600:  train Loss: 22.8372   val Loss: 27.1642   time: 88.25s   best: 27.1642
2023-11-29 07:12:19,095:INFO:  Epoch 470/600:  train Loss: 23.0201   val Loss: 27.9305   time: 88.13s   best: 27.1642
2023-11-29 07:13:47,352:INFO:  Epoch 471/600:  train Loss: 22.9860   val Loss: 27.5334   time: 88.26s   best: 27.1642
2023-11-29 07:15:15,723:INFO:  Epoch 472/600:  train Loss: 22.7146   val Loss: 28.0476   time: 88.37s   best: 27.1642
2023-11-29 07:16:43,909:INFO:  Epoch 473/600:  train Loss: 22.7049   val Loss: 28.0357   time: 88.19s   best: 27.1642
2023-11-29 07:18:11,646:INFO:  Epoch 474/600:  train Loss: 22.8807   val Loss: 27.3377   time: 87.72s   best: 27.1642
2023-11-29 07:19:39,765:INFO:  Epoch 475/600:  train Loss: 23.1176   val Loss: 27.4295   time: 88.11s   best: 27.1642
2023-11-29 07:21:07,608:INFO:  Epoch 476/600:  train Loss: 24.0054   val Loss: 29.4573   time: 87.84s   best: 27.1642
2023-11-29 07:22:35,688:INFO:  Epoch 477/600:  train Loss: 23.5588   val Loss: 27.5955   time: 88.07s   best: 27.1642
2023-11-29 07:24:03,630:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 07:24:03,650:INFO:  Epoch 478/600:  train Loss: 22.7858   val Loss: 27.1326   time: 87.94s   best: 27.1326
2023-11-29 07:25:31,358:INFO:  Epoch 479/600:  train Loss: 23.2832   val Loss: 28.6072   time: 87.70s   best: 27.1326
2023-11-29 07:26:59,117:INFO:  Epoch 480/600:  train Loss: 22.9190   val Loss: 27.6208   time: 87.76s   best: 27.1326
2023-11-29 07:28:26,991:INFO:  Epoch 481/600:  train Loss: 22.6459   val Loss: 27.6344   time: 87.87s   best: 27.1326
2023-11-29 07:29:54,817:INFO:  Epoch 482/600:  train Loss: 22.7503   val Loss: 27.2343   time: 87.82s   best: 27.1326
2023-11-29 07:31:22,674:INFO:  Epoch 483/600:  train Loss: 22.5706   val Loss: 27.4049   time: 87.85s   best: 27.1326
2023-11-29 07:32:50,826:INFO:  Epoch 484/600:  train Loss: 22.9171   val Loss: 27.5930   time: 88.14s   best: 27.1326
2023-11-29 07:34:18,787:INFO:  Epoch 485/600:  train Loss: 22.6667   val Loss: 28.2441   time: 87.95s   best: 27.1326
2023-11-29 07:35:46,899:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 07:35:46,920:INFO:  Epoch 486/600:  train Loss: 22.6754   val Loss: 26.9919   time: 88.11s   best: 26.9919
2023-11-29 07:37:14,852:INFO:  Epoch 487/600:  train Loss: 22.6694   val Loss: 27.6380   time: 87.93s   best: 26.9919
2023-11-29 07:38:42,344:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 07:38:42,372:INFO:  Epoch 488/600:  train Loss: 23.7363   val Loss: 26.9313   time: 87.49s   best: 26.9313
2023-11-29 07:40:09,878:INFO:  Epoch 489/600:  train Loss: 22.6588   val Loss: 29.1304   time: 87.51s   best: 26.9313
2023-11-29 07:41:37,262:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 07:41:37,281:INFO:  Epoch 490/600:  train Loss: 22.5744   val Loss: 26.8985   time: 87.38s   best: 26.8985
2023-11-29 07:43:04,612:INFO:  Epoch 491/600:  train Loss: 22.5255   val Loss: 27.3422   time: 87.33s   best: 26.8985
2023-11-29 07:44:31,950:INFO:  Epoch 492/600:  train Loss: 22.6893   val Loss: 27.0992   time: 87.33s   best: 26.8985
2023-11-29 07:45:59,294:INFO:  Epoch 493/600:  train Loss: 22.7104   val Loss: 27.4781   time: 87.34s   best: 26.8985
2023-11-29 07:47:26,659:INFO:  Epoch 494/600:  train Loss: 22.6541   val Loss: 27.2300   time: 87.36s   best: 26.8985
2023-11-29 07:48:54,011:INFO:  Epoch 495/600:  train Loss: 22.6093   val Loss: 28.9430   time: 87.35s   best: 26.8985
2023-11-29 07:50:21,391:INFO:  Epoch 496/600:  train Loss: 23.7195   val Loss: 31.1412   time: 87.38s   best: 26.8985
2023-11-29 07:51:48,768:INFO:  Epoch 497/600:  train Loss: 23.0301   val Loss: 26.9007   time: 87.36s   best: 26.8985
2023-11-29 07:53:16,287:INFO:  Epoch 498/600:  train Loss: 22.4579   val Loss: 27.2315   time: 87.51s   best: 26.8985
2023-11-29 07:54:43,608:INFO:  Epoch 499/600:  train Loss: 22.7439   val Loss: 27.4694   time: 87.31s   best: 26.8985
2023-11-29 07:56:11,072:INFO:  Epoch 500/600:  train Loss: 22.4367   val Loss: 27.1971   time: 87.46s   best: 26.8985
2023-11-29 07:57:38,647:INFO:  Epoch 501/600:  train Loss: 22.5720   val Loss: 27.1463   time: 87.57s   best: 26.8985
2023-11-29 07:59:05,916:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 07:59:05,935:INFO:  Epoch 502/600:  train Loss: 22.4530   val Loss: 26.8970   time: 87.25s   best: 26.8970
2023-11-29 08:00:33,545:INFO:  Epoch 503/600:  train Loss: 22.5694   val Loss: 27.6598   time: 87.61s   best: 26.8970
2023-11-29 08:02:01,157:INFO:  Epoch 504/600:  train Loss: 22.6331   val Loss: 27.3661   time: 87.60s   best: 26.8970
2023-11-29 08:03:29,224:INFO:  Epoch 505/600:  train Loss: 22.4703   val Loss: 27.2150   time: 88.06s   best: 26.8970
2023-11-29 08:04:56,569:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 08:04:56,588:INFO:  Epoch 506/600:  train Loss: 22.3525   val Loss: 26.8510   time: 87.33s   best: 26.8510
2023-11-29 08:06:24,305:INFO:  Epoch 507/600:  train Loss: 22.4350   val Loss: 27.0187   time: 87.72s   best: 26.8510
2023-11-29 08:07:51,859:INFO:  Epoch 508/600:  train Loss: 22.6296   val Loss: 26.9398   time: 87.54s   best: 26.8510
2023-11-29 08:09:19,324:INFO:  Epoch 509/600:  train Loss: 22.6319   val Loss: 28.5350   time: 87.45s   best: 26.8510
2023-11-29 08:10:47,044:INFO:  Epoch 510/600:  train Loss: 22.3795   val Loss: 26.9962   time: 87.72s   best: 26.8510
2023-11-29 08:12:14,270:INFO:  Epoch 511/600:  train Loss: 22.3118   val Loss: 26.9682   time: 87.23s   best: 26.8510
2023-11-29 08:13:41,548:INFO:  Epoch 512/600:  train Loss: 22.3868   val Loss: 27.2740   time: 87.28s   best: 26.8510
2023-11-29 08:15:08,884:INFO:  Epoch 513/600:  train Loss: 22.7287   val Loss: 26.9119   time: 87.32s   best: 26.8510
2023-11-29 08:16:36,622:INFO:  Epoch 514/600:  train Loss: 22.3448   val Loss: 28.9216   time: 87.73s   best: 26.8510
2023-11-29 08:18:03,879:INFO:  Epoch 515/600:  train Loss: 22.3591   val Loss: 27.3238   time: 87.24s   best: 26.8510
2023-11-29 08:19:31,149:INFO:  Epoch 516/600:  train Loss: 22.5122   val Loss: 28.7865   time: 87.26s   best: 26.8510
2023-11-29 08:20:59,197:INFO:  Epoch 517/600:  train Loss: 22.5104   val Loss: 27.2999   time: 88.05s   best: 26.8510
2023-11-29 08:22:26,993:INFO:  Epoch 518/600:  train Loss: 22.2622   val Loss: 26.9451   time: 87.80s   best: 26.8510
2023-11-29 08:23:54,479:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 08:23:54,498:INFO:  Epoch 519/600:  train Loss: 23.1903   val Loss: 26.8355   time: 87.48s   best: 26.8355
2023-11-29 08:25:21,919:INFO:  Epoch 520/600:  train Loss: 22.7925   val Loss: 27.8662   time: 87.42s   best: 26.8355
2023-11-29 08:26:49,551:INFO:  Epoch 521/600:  train Loss: 22.3649   val Loss: 26.9942   time: 87.63s   best: 26.8355
2023-11-29 08:28:17,036:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 08:28:17,055:INFO:  Epoch 522/600:  train Loss: 22.2650   val Loss: 26.8150   time: 87.48s   best: 26.8150
2023-11-29 08:29:44,371:INFO:  Epoch 523/600:  train Loss: 22.4198   val Loss: 27.5819   time: 87.30s   best: 26.8150
2023-11-29 08:31:11,796:INFO:  Epoch 524/600:  train Loss: 23.9241   val Loss: 27.2865   time: 87.42s   best: 26.8150
2023-11-29 08:32:39,527:INFO:  Epoch 525/600:  train Loss: 22.3032   val Loss: 27.6196   time: 87.73s   best: 26.8150
2023-11-29 08:34:06,952:INFO:  Epoch 526/600:  train Loss: 22.5842   val Loss: 27.2169   time: 87.41s   best: 26.8150
2023-11-29 08:35:34,232:INFO:  Epoch 527/600:  train Loss: 22.2994   val Loss: 27.0428   time: 87.27s   best: 26.8150
2023-11-29 08:37:02,007:INFO:  Epoch 528/600:  train Loss: 22.2134   val Loss: 27.2985   time: 87.77s   best: 26.8150
2023-11-29 08:38:29,767:INFO:  Epoch 529/600:  train Loss: 22.1953   val Loss: 27.2535   time: 87.75s   best: 26.8150
2023-11-29 08:39:57,037:INFO:  Epoch 530/600:  train Loss: 22.2677   val Loss: 27.6702   time: 87.27s   best: 26.8150
2023-11-29 08:41:24,218:INFO:  Epoch 531/600:  train Loss: 22.2841   val Loss: 26.9695   time: 87.17s   best: 26.8150
2023-11-29 08:42:51,441:INFO:  Epoch 532/600:  train Loss: 23.6281   val Loss: 27.8276   time: 87.22s   best: 26.8150
2023-11-29 08:44:19,153:INFO:  Epoch 533/600:  train Loss: 22.9924   val Loss: 33.6106   time: 87.71s   best: 26.8150
2023-11-29 08:45:46,147:INFO:  Epoch 534/600:  train Loss: 22.5872   val Loss: 27.2187   time: 86.98s   best: 26.8150
2023-11-29 08:47:13,361:INFO:  Epoch 535/600:  train Loss: 22.0539   val Loss: 27.3418   time: 87.20s   best: 26.8150
2023-11-29 08:48:40,479:INFO:  Epoch 536/600:  train Loss: 22.2967   val Loss: 29.1356   time: 87.12s   best: 26.8150
2023-11-29 08:50:07,658:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 08:50:07,677:INFO:  Epoch 537/600:  train Loss: 22.2191   val Loss: 26.7993   time: 87.16s   best: 26.7993
2023-11-29 08:51:34,986:INFO:  Epoch 538/600:  train Loss: 22.2515   val Loss: 26.9882   time: 87.31s   best: 26.7993
2023-11-29 08:53:02,674:INFO:  Epoch 539/600:  train Loss: 22.3754   val Loss: 26.9606   time: 87.69s   best: 26.7993
2023-11-29 08:54:29,994:INFO:  Epoch 540/600:  train Loss: 23.1371   val Loss: 27.6579   time: 87.31s   best: 26.7993
2023-11-29 08:55:57,199:INFO:  Epoch 541/600:  train Loss: 22.1704   val Loss: 27.1695   time: 87.19s   best: 26.7993
2023-11-29 08:57:24,487:INFO:  Epoch 542/600:  train Loss: 22.4793   val Loss: 26.9353   time: 87.28s   best: 26.7993
2023-11-29 08:58:51,726:INFO:  Epoch 543/600:  train Loss: 22.8482   val Loss: 26.8538   time: 87.24s   best: 26.7993
2023-11-29 09:00:18,916:INFO:  Epoch 544/600:  train Loss: 22.0998   val Loss: 27.1320   time: 87.18s   best: 26.7993
2023-11-29 09:01:46,084:INFO:  Epoch 545/600:  train Loss: 22.6608   val Loss: 27.8636   time: 87.15s   best: 26.7993
2023-11-29 09:03:13,262:INFO:  Epoch 546/600:  train Loss: 22.7257   val Loss: 28.1301   time: 87.17s   best: 26.7993
2023-11-29 09:04:40,622:INFO:  Epoch 547/600:  train Loss: 22.4222   val Loss: 28.0735   time: 87.36s   best: 26.7993
2023-11-29 09:06:07,881:INFO:  Epoch 548/600:  train Loss: 22.3581   val Loss: 27.0182   time: 87.25s   best: 26.7993
2023-11-29 09:07:35,143:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 09:07:35,161:INFO:  Epoch 549/600:  train Loss: 22.0671   val Loss: 26.5872   time: 87.25s   best: 26.5872
2023-11-29 09:09:02,484:INFO:  Epoch 550/600:  train Loss: 22.1752   val Loss: 26.7604   time: 87.31s   best: 26.5872
2023-11-29 09:10:29,769:INFO:  Epoch 551/600:  train Loss: 22.0812   val Loss: 27.3044   time: 87.27s   best: 26.5872
2023-11-29 09:11:56,975:INFO:  Epoch 552/600:  train Loss: 22.0220   val Loss: 27.0945   time: 87.19s   best: 26.5872
2023-11-29 09:13:24,211:INFO:  Epoch 553/600:  train Loss: 22.2557   val Loss: 27.4372   time: 87.24s   best: 26.5872
2023-11-29 09:14:51,611:INFO:  Epoch 554/600:  train Loss: 22.3301   val Loss: 27.8153   time: 87.39s   best: 26.5872
2023-11-29 09:16:18,980:INFO:  Epoch 555/600:  train Loss: 21.9867   val Loss: 26.9619   time: 87.36s   best: 26.5872
2023-11-29 09:17:46,154:INFO:  Epoch 556/600:  train Loss: 21.9648   val Loss: 27.0545   time: 87.17s   best: 26.5872
2023-11-29 09:19:13,393:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 09:19:13,412:INFO:  Epoch 557/600:  train Loss: 22.2353   val Loss: 26.5711   time: 87.22s   best: 26.5711
2023-11-29 09:20:41,124:INFO:  Epoch 558/600:  train Loss: 22.7991   val Loss: 28.2645   time: 87.71s   best: 26.5711
2023-11-29 09:22:08,395:INFO:  Epoch 559/600:  train Loss: 23.2962   val Loss: 26.7616   time: 87.26s   best: 26.5711
2023-11-29 09:23:35,768:INFO:  Epoch 560/600:  train Loss: 22.8400   val Loss: 27.4686   time: 87.36s   best: 26.5711
2023-11-29 09:25:03,496:INFO:  Epoch 561/600:  train Loss: 21.9263   val Loss: 27.2078   time: 87.73s   best: 26.5711
2023-11-29 09:26:30,810:INFO:  Epoch 562/600:  train Loss: 21.9226   val Loss: 26.8168   time: 87.31s   best: 26.5711
2023-11-29 09:27:58,049:INFO:  Epoch 563/600:  train Loss: 21.9873   val Loss: 26.8375   time: 87.24s   best: 26.5711
2023-11-29 09:29:25,234:INFO:  Epoch 564/600:  train Loss: 23.2213   val Loss: 27.4594   time: 87.18s   best: 26.5711
2023-11-29 09:30:52,607:INFO:  Epoch 565/600:  train Loss: 22.0869   val Loss: 26.9522   time: 87.37s   best: 26.5711
2023-11-29 09:32:19,891:INFO:  Epoch 566/600:  train Loss: 21.8642   val Loss: 26.8366   time: 87.28s   best: 26.5711
2023-11-29 09:33:47,135:INFO:  Epoch 567/600:  train Loss: 21.9940   val Loss: 26.9048   time: 87.23s   best: 26.5711
2023-11-29 09:35:14,363:INFO:  Epoch 568/600:  train Loss: 22.0197   val Loss: 26.9924   time: 87.22s   best: 26.5711
2023-11-29 09:36:41,556:INFO:  Epoch 569/600:  train Loss: 21.9575   val Loss: 26.7137   time: 87.18s   best: 26.5711
2023-11-29 09:38:09,114:INFO:  Epoch 570/600:  train Loss: 21.9150   val Loss: 27.4065   time: 87.55s   best: 26.5711
2023-11-29 09:39:36,694:INFO:  Epoch 571/600:  train Loss: 22.2010   val Loss: 27.2861   time: 87.58s   best: 26.5711
2023-11-29 09:41:03,801:INFO:  Epoch 572/600:  train Loss: 22.2135   val Loss: 27.4557   time: 87.11s   best: 26.5711
2023-11-29 09:42:30,978:INFO:  Epoch 573/600:  train Loss: 22.0405   val Loss: 26.7120   time: 87.17s   best: 26.5711
2023-11-29 09:43:58,116:INFO:  Epoch 574/600:  train Loss: 21.9073   val Loss: 27.5176   time: 87.14s   best: 26.5711
2023-11-29 09:45:25,249:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 09:45:25,268:INFO:  Epoch 575/600:  train Loss: 21.9158   val Loss: 26.5678   time: 87.12s   best: 26.5678
2023-11-29 09:46:52,354:INFO:  Epoch 576/600:  train Loss: 21.8076   val Loss: 27.6556   time: 87.07s   best: 26.5678
2023-11-29 09:48:19,858:INFO:  Epoch 577/600:  train Loss: 21.9297   val Loss: 26.7288   time: 87.50s   best: 26.5678
2023-11-29 09:49:47,229:INFO:  Epoch 578/600:  train Loss: 21.9054   val Loss: 27.7610   time: 87.36s   best: 26.5678
2023-11-29 09:51:14,642:INFO:  Epoch 579/600:  train Loss: 21.8439   val Loss: 28.5316   time: 87.41s   best: 26.5678
2023-11-29 09:52:41,877:INFO:  Epoch 580/600:  train Loss: 22.0420   val Loss: 27.2704   time: 87.22s   best: 26.5678
2023-11-29 09:54:09,070:INFO:  Epoch 581/600:  train Loss: 21.8750   val Loss: 27.1850   time: 87.18s   best: 26.5678
2023-11-29 09:55:36,591:INFO:  Epoch 582/600:  train Loss: 21.7629   val Loss: 27.6275   time: 87.51s   best: 26.5678
2023-11-29 09:57:03,755:INFO:  Epoch 583/600:  train Loss: 21.9020   val Loss: 26.7832   time: 87.15s   best: 26.5678
2023-11-29 09:58:31,102:INFO:  Epoch 584/600:  train Loss: 22.0518   val Loss: 27.4852   time: 87.33s   best: 26.5678
2023-11-29 09:59:58,287:INFO:  Epoch 585/600:  train Loss: 21.9516   val Loss: 26.8126   time: 87.17s   best: 26.5678
2023-11-29 10:01:25,505:INFO:  Epoch 586/600:  train Loss: 21.8157   val Loss: 26.8671   time: 87.22s   best: 26.5678
2023-11-29 10:02:52,703:INFO:  Epoch 587/600:  train Loss: 21.9028   val Loss: 26.8217   time: 87.20s   best: 26.5678
2023-11-29 10:04:19,915:INFO:  Epoch 588/600:  train Loss: 21.7397   val Loss: 27.2183   time: 87.20s   best: 26.5678
2023-11-29 10:05:47,025:INFO:  Epoch 589/600:  train Loss: 21.7240   val Loss: 27.3729   time: 87.10s   best: 26.5678
2023-11-29 10:07:14,385:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 10:07:14,403:INFO:  Epoch 590/600:  train Loss: 21.9659   val Loss: 26.5631   time: 87.34s   best: 26.5631
2023-11-29 10:08:41,582:INFO:  Epoch 591/600:  train Loss: 21.6845   val Loss: 26.9369   time: 87.18s   best: 26.5631
2023-11-29 10:10:08,955:INFO:  Epoch 592/600:  train Loss: 21.8493   val Loss: 26.9199   time: 87.36s   best: 26.5631
2023-11-29 10:11:36,254:INFO:  Epoch 593/600:  train Loss: 23.0768   val Loss: 28.0347   time: 87.29s   best: 26.5631
2023-11-29 10:13:03,376:INFO:  Epoch 594/600:  train Loss: 22.3391   val Loss: 27.2228   time: 87.11s   best: 26.5631
2023-11-29 10:14:30,682:INFO:  Epoch 595/600:  train Loss: 21.7927   val Loss: 27.0095   time: 87.31s   best: 26.5631
2023-11-29 10:15:58,368:INFO:  Epoch 596/600:  train Loss: 21.7143   val Loss: 27.1855   time: 87.69s   best: 26.5631
2023-11-29 10:17:25,751:INFO:  Epoch 597/600:  train Loss: 21.8107   val Loss: 26.6012   time: 87.38s   best: 26.5631
2023-11-29 10:18:53,257:INFO:  Epoch 598/600:  train Loss: 23.7980   val Loss: 30.2519   time: 87.50s   best: 26.5631
2023-11-29 10:20:20,717:INFO:  Epoch 599/600:  train Loss: 23.1699   val Loss: 27.7925   time: 87.45s   best: 26.5631
2023-11-29 10:21:47,976:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.1 dataset (0.05 dropout)_6602.pt
2023-11-29 10:21:47,995:INFO:  Epoch 600/600:  train Loss: 21.9245   val Loss: 26.4764   time: 87.24s   best: 26.4764
2023-11-29 10:21:47,996:INFO:  -----> Training complete in 881m 19s   best validation loss: 26.4764
 
2023-12-02 11:22:41,401:INFO:  Starting experiment lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2023-12-02 11:22:41,404:INFO:  Defining the model
2023-12-02 11:22:41,524:INFO:  Reading the dataset
2023-12-02 11:27:59,548:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:27:59,567:INFO:  Epoch 1/600:  train Loss: 90.7781   val Loss: 86.9617   time: 72.12s   best: 86.9617
2023-12-02 11:29:09,210:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:29:09,228:INFO:  Epoch 2/600:  train Loss: 88.6736   val Loss: 86.4079   time: 69.63s   best: 86.4079
2023-12-02 11:30:18,573:INFO:  Epoch 3/600:  train Loss: 86.7847   val Loss: 86.5273   time: 69.34s   best: 86.4079
2023-12-02 11:31:28,254:INFO:  Epoch 4/600:  train Loss: 86.4938   val Loss: 87.5482   time: 69.68s   best: 86.4079
2023-12-02 11:32:37,703:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:32:37,721:INFO:  Epoch 5/600:  train Loss: 85.1799   val Loss: 84.8959   time: 69.44s   best: 84.8959
2023-12-02 11:33:47,325:INFO:  Epoch 6/600:  train Loss: 84.7896   val Loss: 86.2976   time: 69.60s   best: 84.8959
2023-12-02 11:34:57,385:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:34:57,403:INFO:  Epoch 7/600:  train Loss: 83.3200   val Loss: 80.4139   time: 70.06s   best: 80.4139
2023-12-02 11:36:07,168:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:36:07,187:INFO:  Epoch 8/600:  train Loss: 78.7709   val Loss: 76.8896   time: 69.76s   best: 76.8896
2023-12-02 11:37:16,784:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:37:16,803:INFO:  Epoch 9/600:  train Loss: 76.4102   val Loss: 73.8937   time: 69.59s   best: 73.8937
2023-12-02 11:38:26,528:INFO:  Epoch 10/600:  train Loss: 74.6292   val Loss: 75.1055   time: 69.72s   best: 73.8937
2023-12-02 11:39:36,246:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:39:36,266:INFO:  Epoch 11/600:  train Loss: 73.5955   val Loss: 73.0933   time: 69.71s   best: 73.0933
2023-12-02 11:40:46,215:INFO:  Epoch 12/600:  train Loss: 72.7155   val Loss: 74.7167   time: 69.94s   best: 73.0933
2023-12-02 11:41:55,993:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:41:56,012:INFO:  Epoch 13/600:  train Loss: 71.9308   val Loss: 72.5743   time: 69.76s   best: 72.5743
2023-12-02 11:43:05,961:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:43:05,979:INFO:  Epoch 14/600:  train Loss: 71.0070   val Loss: 70.8687   time: 69.94s   best: 70.8687
2023-12-02 11:44:15,738:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:44:15,757:INFO:  Epoch 15/600:  train Loss: 70.2113   val Loss: 70.5643   time: 69.75s   best: 70.5643
2023-12-02 11:45:25,774:INFO:  Epoch 16/600:  train Loss: 69.5823   val Loss: 70.5853   time: 70.01s   best: 70.5643
2023-12-02 11:46:35,724:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:46:35,743:INFO:  Epoch 17/600:  train Loss: 69.2905   val Loss: 69.5017   time: 69.93s   best: 69.5017
2023-12-02 11:47:45,726:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:47:45,746:INFO:  Epoch 18/600:  train Loss: 68.6319   val Loss: 69.3706   time: 69.98s   best: 69.3706
2023-12-02 11:48:55,670:INFO:  Epoch 19/600:  train Loss: 68.2353   val Loss: 69.7837   time: 69.92s   best: 69.3706
2023-12-02 11:50:05,610:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:50:05,629:INFO:  Epoch 20/600:  train Loss: 67.6398   val Loss: 67.3440   time: 69.94s   best: 67.3440
2023-12-02 11:51:15,737:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:51:15,756:INFO:  Epoch 21/600:  train Loss: 67.5858   val Loss: 67.0008   time: 70.08s   best: 67.0008
2023-12-02 11:52:26,209:INFO:  Epoch 22/600:  train Loss: 67.1144   val Loss: 67.7213   time: 70.45s   best: 67.0008
2023-12-02 11:53:36,196:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:53:36,215:INFO:  Epoch 23/600:  train Loss: 66.6075   val Loss: 66.2775   time: 69.98s   best: 66.2775
2023-12-02 11:54:46,114:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:54:46,133:INFO:  Epoch 24/600:  train Loss: 66.4928   val Loss: 66.1051   time: 69.89s   best: 66.1051
2023-12-02 11:55:55,966:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:55:55,985:INFO:  Epoch 25/600:  train Loss: 66.1923   val Loss: 65.3450   time: 69.83s   best: 65.3450
2023-12-02 11:57:06,014:INFO:  Epoch 26/600:  train Loss: 65.5959   val Loss: 66.7445   time: 70.02s   best: 65.3450
2023-12-02 11:58:15,705:INFO:  Epoch 27/600:  train Loss: 65.3497   val Loss: 65.4194   time: 69.69s   best: 65.3450
2023-12-02 11:59:25,652:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 11:59:25,680:INFO:  Epoch 28/600:  train Loss: 64.7774   val Loss: 64.0399   time: 69.92s   best: 64.0399
2023-12-02 12:00:35,475:INFO:  Epoch 29/600:  train Loss: 64.5563   val Loss: 65.1634   time: 69.79s   best: 64.0399
2023-12-02 12:01:45,286:INFO:  Epoch 30/600:  train Loss: 64.4650   val Loss: 64.4194   time: 69.80s   best: 64.0399
2023-12-02 12:02:55,010:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:02:55,029:INFO:  Epoch 31/600:  train Loss: 64.1274   val Loss: 63.9779   time: 69.72s   best: 63.9779
2023-12-02 12:04:04,892:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:04:04,911:INFO:  Epoch 32/600:  train Loss: 63.7260   val Loss: 63.1738   time: 69.85s   best: 63.1738
2023-12-02 12:05:14,838:INFO:  Epoch 33/600:  train Loss: 63.5212   val Loss: 63.9836   time: 69.93s   best: 63.1738
2023-12-02 12:06:24,688:INFO:  Epoch 34/600:  train Loss: 63.1384   val Loss: 63.2594   time: 69.85s   best: 63.1738
2023-12-02 12:07:34,422:INFO:  Epoch 35/600:  train Loss: 62.7930   val Loss: 64.7679   time: 69.72s   best: 63.1738
2023-12-02 12:08:44,343:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:08:44,362:INFO:  Epoch 36/600:  train Loss: 62.5978   val Loss: 62.3417   time: 69.92s   best: 62.3417
2023-12-02 12:09:54,139:INFO:  Epoch 37/600:  train Loss: 62.6244   val Loss: 63.3250   time: 69.78s   best: 62.3417
2023-12-02 12:11:04,094:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:11:04,113:INFO:  Epoch 38/600:  train Loss: 62.0785   val Loss: 61.7414   time: 69.94s   best: 61.7414
2023-12-02 12:12:14,474:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:12:14,493:INFO:  Epoch 39/600:  train Loss: 61.7736   val Loss: 61.6673   time: 70.36s   best: 61.6673
2023-12-02 12:13:24,757:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:13:24,776:INFO:  Epoch 40/600:  train Loss: 61.4287   val Loss: 61.1504   time: 70.26s   best: 61.1504
2023-12-02 12:14:34,554:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:14:34,574:INFO:  Epoch 41/600:  train Loss: 61.2494   val Loss: 61.0269   time: 69.76s   best: 61.0269
2023-12-02 12:15:44,579:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:15:44,598:INFO:  Epoch 42/600:  train Loss: 61.2528   val Loss: 60.9161   time: 70.00s   best: 60.9161
2023-12-02 12:16:54,809:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:16:54,828:INFO:  Epoch 43/600:  train Loss: 60.8545   val Loss: 60.8153   time: 70.21s   best: 60.8153
2023-12-02 12:18:04,713:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:18:04,732:INFO:  Epoch 44/600:  train Loss: 60.5642   val Loss: 60.4232   time: 69.88s   best: 60.4232
2023-12-02 12:19:14,600:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:19:14,628:INFO:  Epoch 45/600:  train Loss: 60.4614   val Loss: 59.9451   time: 69.86s   best: 59.9451
2023-12-02 12:20:24,652:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:20:24,671:INFO:  Epoch 46/600:  train Loss: 60.0823   val Loss: 59.6137   time: 70.00s   best: 59.6137
2023-12-02 12:21:34,594:INFO:  Epoch 47/600:  train Loss: 59.7593   val Loss: 59.9486   time: 69.91s   best: 59.6137
2023-12-02 12:22:44,718:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:22:44,737:INFO:  Epoch 48/600:  train Loss: 59.7185   val Loss: 59.1339   time: 70.11s   best: 59.1339
2023-12-02 12:23:54,634:INFO:  Epoch 49/600:  train Loss: 59.3152   val Loss: 59.4349   time: 69.90s   best: 59.1339
2023-12-02 12:25:04,844:INFO:  Epoch 50/600:  train Loss: 59.2278   val Loss: 59.3990   time: 70.21s   best: 59.1339
2023-12-02 12:26:14,804:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:26:14,824:INFO:  Epoch 51/600:  train Loss: 58.8963   val Loss: 58.8070   time: 69.96s   best: 58.8070
2023-12-02 12:27:24,807:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:27:24,826:INFO:  Epoch 52/600:  train Loss: 58.5099   val Loss: 58.7715   time: 69.98s   best: 58.7715
2023-12-02 12:28:34,559:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:28:34,578:INFO:  Epoch 53/600:  train Loss: 58.4042   val Loss: 58.2959   time: 69.72s   best: 58.2959
2023-12-02 12:29:44,671:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:29:44,690:INFO:  Epoch 54/600:  train Loss: 58.4533   val Loss: 58.1476   time: 70.09s   best: 58.1476
2023-12-02 12:30:54,592:INFO:  Epoch 55/600:  train Loss: 57.8801   val Loss: 58.3626   time: 69.90s   best: 58.1476
2023-12-02 12:32:04,494:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:32:04,513:INFO:  Epoch 56/600:  train Loss: 57.6686   val Loss: 57.5175   time: 69.89s   best: 57.5175
2023-12-02 12:33:14,508:INFO:  Epoch 57/600:  train Loss: 57.4170   val Loss: 58.6543   time: 69.99s   best: 57.5175
2023-12-02 12:34:24,414:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:34:24,433:INFO:  Epoch 58/600:  train Loss: 57.2356   val Loss: 56.9327   time: 69.90s   best: 56.9327
2023-12-02 12:35:34,419:INFO:  Epoch 59/600:  train Loss: 56.7868   val Loss: 58.2848   time: 69.97s   best: 56.9327
2023-12-02 12:36:44,605:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:36:44,623:INFO:  Epoch 60/600:  train Loss: 56.8116   val Loss: 56.7218   time: 70.18s   best: 56.7218
2023-12-02 12:37:54,371:INFO:  Epoch 61/600:  train Loss: 56.4193   val Loss: 58.8292   time: 69.75s   best: 56.7218
2023-12-02 12:39:04,256:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:39:04,275:INFO:  Epoch 62/600:  train Loss: 56.2386   val Loss: 56.4561   time: 69.87s   best: 56.4561
2023-12-02 12:40:14,185:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:40:14,205:INFO:  Epoch 63/600:  train Loss: 55.9202   val Loss: 55.8842   time: 69.89s   best: 55.8842
2023-12-02 12:41:24,304:INFO:  Epoch 64/600:  train Loss: 55.8108   val Loss: 56.1981   time: 70.10s   best: 55.8842
2023-12-02 12:42:34,146:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:42:34,166:INFO:  Epoch 65/600:  train Loss: 55.6341   val Loss: 55.6517   time: 69.83s   best: 55.6517
2023-12-02 12:43:44,327:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:43:44,346:INFO:  Epoch 66/600:  train Loss: 55.1956   val Loss: 55.3469   time: 70.16s   best: 55.3469
2023-12-02 12:44:54,318:INFO:  Epoch 67/600:  train Loss: 55.0965   val Loss: 55.4053   time: 69.96s   best: 55.3469
2023-12-02 12:46:04,097:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:46:04,117:INFO:  Epoch 68/600:  train Loss: 54.6901   val Loss: 55.3452   time: 69.76s   best: 55.3452
2023-12-02 12:47:13,890:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:47:13,918:INFO:  Epoch 69/600:  train Loss: 54.5976   val Loss: 54.8010   time: 69.76s   best: 54.8010
2023-12-02 12:48:23,901:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:48:23,920:INFO:  Epoch 70/600:  train Loss: 54.1162   val Loss: 54.3697   time: 69.97s   best: 54.3697
2023-12-02 12:49:33,790:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:49:33,810:INFO:  Epoch 71/600:  train Loss: 54.0933   val Loss: 54.0365   time: 69.85s   best: 54.0365
2023-12-02 12:50:43,936:INFO:  Epoch 72/600:  train Loss: 53.7227   val Loss: 54.9766   time: 70.13s   best: 54.0365
2023-12-02 12:51:53,744:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:51:53,763:INFO:  Epoch 73/600:  train Loss: 53.3433   val Loss: 53.5334   time: 69.80s   best: 53.5334
2023-12-02 12:53:03,896:INFO:  Epoch 74/600:  train Loss: 53.0902   val Loss: 53.7332   time: 70.12s   best: 53.5334
2023-12-02 12:54:13,906:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:54:13,925:INFO:  Epoch 75/600:  train Loss: 52.8872   val Loss: 52.6907   time: 70.01s   best: 52.6907
2023-12-02 12:55:24,319:INFO:  Epoch 76/600:  train Loss: 52.6032   val Loss: 53.3091   time: 70.38s   best: 52.6907
2023-12-02 12:56:34,023:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:56:34,051:INFO:  Epoch 77/600:  train Loss: 52.2749   val Loss: 52.4651   time: 69.69s   best: 52.4651
2023-12-02 12:57:43,975:INFO:  Epoch 78/600:  train Loss: 52.2194   val Loss: 53.2714   time: 69.92s   best: 52.4651
2023-12-02 12:58:54,009:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 12:58:54,029:INFO:  Epoch 79/600:  train Loss: 51.6406   val Loss: 51.9604   time: 70.03s   best: 51.9604
2023-12-02 13:00:03,923:INFO:  Epoch 80/600:  train Loss: 51.3734   val Loss: 52.2621   time: 69.88s   best: 51.9604
2023-12-02 13:01:13,807:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:01:13,827:INFO:  Epoch 81/600:  train Loss: 51.1093   val Loss: 51.4493   time: 69.88s   best: 51.4493
2023-12-02 13:02:23,773:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:02:23,793:INFO:  Epoch 82/600:  train Loss: 50.8916   val Loss: 51.3188   time: 69.93s   best: 51.3188
2023-12-02 13:03:33,702:INFO:  Epoch 83/600:  train Loss: 50.3672   val Loss: 51.4748   time: 69.89s   best: 51.3188
2023-12-02 13:04:43,910:INFO:  Epoch 84/600:  train Loss: 50.6074   val Loss: 51.5562   time: 70.21s   best: 51.3188
2023-12-02 13:05:53,691:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:05:53,710:INFO:  Epoch 85/600:  train Loss: 50.3843   val Loss: 50.2885   time: 69.78s   best: 50.2885
2023-12-02 13:07:03,612:INFO:  Epoch 86/600:  train Loss: 49.7535   val Loss: 50.8455   time: 69.90s   best: 50.2885
2023-12-02 13:08:13,294:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:08:13,314:INFO:  Epoch 87/600:  train Loss: 49.4328   val Loss: 50.0340   time: 69.68s   best: 50.0340
2023-12-02 13:09:23,322:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:09:23,342:INFO:  Epoch 88/600:  train Loss: 49.3396   val Loss: 49.7826   time: 69.99s   best: 49.7826
2023-12-02 13:10:33,222:INFO:  Epoch 89/600:  train Loss: 48.9276   val Loss: 49.8444   time: 69.88s   best: 49.7826
2023-12-02 13:11:42,924:INFO:  Epoch 90/600:  train Loss: 48.7713   val Loss: 50.7030   time: 69.69s   best: 49.7826
2023-12-02 13:12:52,902:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:12:52,922:INFO:  Epoch 91/600:  train Loss: 48.7448   val Loss: 49.4346   time: 69.97s   best: 49.4346
2023-12-02 13:14:02,843:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:14:02,863:INFO:  Epoch 92/600:  train Loss: 48.2358   val Loss: 49.0775   time: 69.91s   best: 49.0775
2023-12-02 13:15:12,762:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:15:12,784:INFO:  Epoch 93/600:  train Loss: 47.8761   val Loss: 48.9393   time: 69.89s   best: 48.9393
2023-12-02 13:16:22,735:INFO:  Epoch 94/600:  train Loss: 47.7043   val Loss: 50.3401   time: 69.94s   best: 48.9393
2023-12-02 13:17:00,811:INFO:  Starting experiment lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)
2023-12-02 13:17:00,817:INFO:  Defining the model
2023-12-02 13:17:00,915:INFO:  Reading the dataset
2023-12-02 13:17:32,575:INFO:  Epoch 95/600:  train Loss: 47.6596   val Loss: 50.8313   time: 69.84s   best: 48.9393
2023-12-02 13:18:42,875:INFO:  Epoch 96/600:  train Loss: 47.7791   val Loss: 49.4656   time: 70.28s   best: 48.9393
2023-12-02 13:19:53,020:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:19:53,038:INFO:  Epoch 97/600:  train Loss: 46.9510   val Loss: 48.1176   time: 70.14s   best: 48.1176
2023-12-02 13:21:03,117:INFO:  Epoch 98/600:  train Loss: 46.9590   val Loss: 48.4608   time: 70.08s   best: 48.1176
2023-12-02 13:22:12,842:INFO:  Epoch 99/600:  train Loss: 46.6359   val Loss: 48.3609   time: 69.71s   best: 48.1176
2023-12-02 13:23:22,713:INFO:  Epoch 100/600:  train Loss: 46.8447   val Loss: 48.9046   time: 69.86s   best: 48.1176
2023-12-02 13:24:32,453:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:24:32,482:INFO:  Epoch 101/600:  train Loss: 46.4553   val Loss: 47.8882   time: 69.74s   best: 47.8882
2023-12-02 13:25:15,697:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 13:25:15,721:INFO:  Epoch 1/500:  train Loss: 81.4551   val Loss: 74.1743   time: 244.12s   best: 74.1743
2023-12-02 13:25:42,270:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:25:42,304:INFO:  Epoch 102/600:  train Loss: 45.9375   val Loss: 47.1495   time: 69.78s   best: 47.1495
2023-12-02 13:26:52,081:INFO:  Epoch 103/600:  train Loss: 45.7986   val Loss: 48.0174   time: 69.76s   best: 47.1495
2023-12-02 13:28:02,063:INFO:  Epoch 104/600:  train Loss: 45.8370   val Loss: 48.3080   time: 69.98s   best: 47.1495
2023-12-02 13:29:11,762:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:29:11,782:INFO:  Epoch 105/600:  train Loss: 45.6171   val Loss: 46.6537   time: 69.68s   best: 46.6537
2023-12-02 13:29:17,463:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 13:29:17,484:INFO:  Epoch 2/500:  train Loss: 70.3322   val Loss: 67.3848   time: 241.73s   best: 67.3848
2023-12-02 13:30:21,754:INFO:  Epoch 106/600:  train Loss: 45.1983   val Loss: 50.2842   time: 69.97s   best: 46.6537
2023-12-02 13:31:31,592:INFO:  Epoch 107/600:  train Loss: 45.1619   val Loss: 48.4400   time: 69.82s   best: 46.6537
2023-12-02 13:32:41,909:INFO:  Epoch 108/600:  train Loss: 45.1022   val Loss: 47.2503   time: 70.31s   best: 46.6537
2023-12-02 13:33:17,216:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 13:33:17,236:INFO:  Epoch 3/500:  train Loss: 66.0173   val Loss: 64.0830   time: 239.73s   best: 64.0830
2023-12-02 13:33:51,732:INFO:  Epoch 109/600:  train Loss: 44.7817   val Loss: 47.0855   time: 69.82s   best: 46.6537
2023-12-02 13:35:01,745:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:35:01,765:INFO:  Epoch 110/600:  train Loss: 44.4997   val Loss: 46.2519   time: 70.00s   best: 46.2519
2023-12-02 13:36:11,527:INFO:  Epoch 111/600:  train Loss: 44.6175   val Loss: 47.5265   time: 69.76s   best: 46.2519
2023-12-02 13:37:18,487:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 13:37:18,509:INFO:  Epoch 4/500:  train Loss: 62.7095   val Loss: 60.8783   time: 241.23s   best: 60.8783
2023-12-02 13:37:21,595:INFO:  Epoch 112/600:  train Loss: 44.7006   val Loss: 46.8523   time: 70.07s   best: 46.2519
2023-12-02 13:38:31,334:INFO:  Epoch 113/600:  train Loss: 44.3895   val Loss: 46.6506   time: 69.70s   best: 46.2519
2023-12-02 13:39:41,230:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:39:41,250:INFO:  Epoch 114/600:  train Loss: 43.9616   val Loss: 45.7516   time: 69.88s   best: 45.7516
2023-12-02 13:40:51,134:INFO:  Epoch 115/600:  train Loss: 43.7690   val Loss: 46.7965   time: 69.88s   best: 45.7516
2023-12-02 13:41:18,386:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 13:41:18,406:INFO:  Epoch 5/500:  train Loss: 60.3680   val Loss: 58.4329   time: 239.87s   best: 58.4329
2023-12-02 13:42:01,098:INFO:  Epoch 116/600:  train Loss: 43.7002   val Loss: 45.9524   time: 69.95s   best: 45.7516
2023-12-02 13:43:10,976:INFO:  Epoch 117/600:  train Loss: 43.5380   val Loss: 47.6120   time: 69.87s   best: 45.7516
2023-12-02 13:44:20,771:INFO:  Epoch 118/600:  train Loss: 43.5707   val Loss: 46.3034   time: 69.79s   best: 45.7516
2023-12-02 13:45:19,165:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 13:45:19,186:INFO:  Epoch 6/500:  train Loss: 57.7350   val Loss: 55.7920   time: 240.75s   best: 55.7920
2023-12-02 13:45:30,520:INFO:  Epoch 119/600:  train Loss: 43.8030   val Loss: 47.0417   time: 69.73s   best: 45.7516
2023-12-02 13:46:40,850:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:46:40,870:INFO:  Epoch 120/600:  train Loss: 43.4088   val Loss: 45.4837   time: 70.31s   best: 45.4837
2023-12-02 13:47:50,600:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:47:50,643:INFO:  Epoch 121/600:  train Loss: 42.9007   val Loss: 45.3245   time: 69.73s   best: 45.3245
2023-12-02 13:49:01,325:INFO:  Epoch 122/600:  train Loss: 42.7974   val Loss: 45.6914   time: 70.67s   best: 45.3245
2023-12-02 13:49:21,515:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 13:49:21,537:INFO:  Epoch 7/500:  train Loss: 54.7137   val Loss: 52.0357   time: 242.31s   best: 52.0357
2023-12-02 13:50:11,127:INFO:  Epoch 123/600:  train Loss: 42.6378   val Loss: 45.6056   time: 69.80s   best: 45.3245
2023-12-02 13:51:20,989:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:51:21,009:INFO:  Epoch 124/600:  train Loss: 42.5085   val Loss: 44.1639   time: 69.86s   best: 44.1639
2023-12-02 13:52:30,715:INFO:  Epoch 125/600:  train Loss: 42.5883   val Loss: 44.3722   time: 69.70s   best: 44.1639
2023-12-02 13:53:21,428:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 13:53:21,450:INFO:  Epoch 8/500:  train Loss: 51.4290   val Loss: 50.0392   time: 239.88s   best: 50.0392
2023-12-02 13:53:40,494:INFO:  Epoch 126/600:  train Loss: 42.5119   val Loss: 44.3527   time: 69.78s   best: 44.1639
2023-12-02 13:54:50,281:INFO:  Epoch 127/600:  train Loss: 42.3395   val Loss: 44.9756   time: 69.77s   best: 44.1639
2023-12-02 13:56:00,218:INFO:  Epoch 128/600:  train Loss: 41.9814   val Loss: 44.4386   time: 69.93s   best: 44.1639
2023-12-02 13:57:10,068:INFO:  Epoch 129/600:  train Loss: 42.2633   val Loss: 45.1594   time: 69.83s   best: 44.1639
2023-12-02 13:57:20,647:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 13:57:20,674:INFO:  Epoch 9/500:  train Loss: 48.9869   val Loss: 48.0599   time: 239.17s   best: 48.0599
2023-12-02 13:58:19,943:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 13:58:19,977:INFO:  Epoch 130/600:  train Loss: 41.8711   val Loss: 44.0252   time: 69.87s   best: 44.0252
2023-12-02 13:59:29,739:INFO:  Epoch 131/600:  train Loss: 41.9328   val Loss: 46.8601   time: 69.76s   best: 44.0252
2023-12-02 14:00:39,724:INFO:  Epoch 132/600:  train Loss: 41.9577   val Loss: 46.5563   time: 69.98s   best: 44.0252
2023-12-02 14:01:20,176:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:01:20,197:INFO:  Epoch 10/500:  train Loss: 46.9366   val Loss: 45.6622   time: 239.50s   best: 45.6622
2023-12-02 14:01:49,473:INFO:  Epoch 133/600:  train Loss: 41.6068   val Loss: 44.4963   time: 69.73s   best: 44.0252
2023-12-02 14:02:59,475:INFO:  Epoch 134/600:  train Loss: 41.3951   val Loss: 45.7849   time: 70.00s   best: 44.0252
2023-12-02 14:04:09,117:INFO:  Epoch 135/600:  train Loss: 41.4284   val Loss: 47.0364   time: 69.63s   best: 44.0252
2023-12-02 14:05:19,028:INFO:  Epoch 136/600:  train Loss: 41.4805   val Loss: 44.4792   time: 69.91s   best: 44.0252
2023-12-02 14:05:19,764:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:05:19,804:INFO:  Epoch 11/500:  train Loss: 45.0729   val Loss: 44.2959   time: 239.56s   best: 44.2959
2023-12-02 14:06:29,153:INFO:  Epoch 137/600:  train Loss: 41.1482   val Loss: 47.6037   time: 70.12s   best: 44.0252
2023-12-02 14:07:39,010:INFO:  Epoch 138/600:  train Loss: 41.4165   val Loss: 45.7413   time: 69.85s   best: 44.0252
2023-12-02 14:08:48,897:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:08:48,917:INFO:  Epoch 139/600:  train Loss: 41.0594   val Loss: 43.3046   time: 69.88s   best: 43.3046
2023-12-02 14:09:19,361:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:09:19,383:INFO:  Epoch 12/500:  train Loss: 43.4537   val Loss: 42.6378   time: 239.54s   best: 42.6378
2023-12-02 14:09:58,701:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:09:58,722:INFO:  Epoch 140/600:  train Loss: 40.6577   val Loss: 43.1250   time: 69.78s   best: 43.1250
2023-12-02 14:11:08,488:INFO:  Epoch 141/600:  train Loss: 40.4250   val Loss: 43.2743   time: 69.77s   best: 43.1250
2023-12-02 14:12:18,330:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:12:18,349:INFO:  Epoch 142/600:  train Loss: 40.3721   val Loss: 42.5392   time: 69.83s   best: 42.5392
2023-12-02 14:13:19,031:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:13:19,053:INFO:  Epoch 13/500:  train Loss: 42.0063   val Loss: 42.1609   time: 239.64s   best: 42.1609
2023-12-02 14:13:28,026:INFO:  Epoch 143/600:  train Loss: 40.4885   val Loss: 42.7157   time: 69.68s   best: 42.5392
2023-12-02 14:14:38,472:INFO:  Epoch 144/600:  train Loss: 40.4955   val Loss: 42.8089   time: 70.42s   best: 42.5392
2023-12-02 14:15:48,364:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:15:48,384:INFO:  Epoch 145/600:  train Loss: 39.9768   val Loss: 42.2949   time: 69.89s   best: 42.2949
2023-12-02 14:16:58,429:INFO:  Epoch 146/600:  train Loss: 40.0446   val Loss: 44.5124   time: 70.03s   best: 42.2949
2023-12-02 14:17:18,686:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:17:18,717:INFO:  Epoch 14/500:  train Loss: 40.7671   val Loss: 41.0801   time: 239.62s   best: 41.0801
2023-12-02 14:18:08,150:INFO:  Epoch 147/600:  train Loss: 40.3301   val Loss: 43.1826   time: 69.71s   best: 42.2949
2023-12-02 14:19:18,268:INFO:  Epoch 148/600:  train Loss: 39.7054   val Loss: 42.9138   time: 70.12s   best: 42.2949
2023-12-02 14:20:27,972:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:20:27,992:INFO:  Epoch 149/600:  train Loss: 39.4935   val Loss: 41.5284   time: 69.69s   best: 41.5284
2023-12-02 14:21:17,746:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:21:17,782:INFO:  Epoch 15/500:  train Loss: 39.4078   val Loss: 39.0323   time: 239.01s   best: 39.0323
2023-12-02 14:21:37,907:INFO:  Epoch 150/600:  train Loss: 39.3116   val Loss: 43.4767   time: 69.90s   best: 41.5284
2023-12-02 14:22:47,802:INFO:  Epoch 151/600:  train Loss: 39.7539   val Loss: 42.2700   time: 69.89s   best: 41.5284
2023-12-02 14:23:57,732:INFO:  Epoch 152/600:  train Loss: 39.2494   val Loss: 42.4908   time: 69.92s   best: 41.5284
2023-12-02 14:25:07,600:INFO:  Epoch 153/600:  train Loss: 39.1655   val Loss: 42.7869   time: 69.85s   best: 41.5284
2023-12-02 14:25:16,935:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:25:16,964:INFO:  Epoch 16/500:  train Loss: 38.4331   val Loss: 37.5773   time: 239.15s   best: 37.5773
2023-12-02 14:26:17,512:INFO:  Epoch 154/600:  train Loss: 39.2666   val Loss: 41.7406   time: 69.90s   best: 41.5284
2023-12-02 14:27:27,542:INFO:  Epoch 155/600:  train Loss: 38.9369   val Loss: 41.7304   time: 70.03s   best: 41.5284
2023-12-02 14:28:37,627:INFO:  Epoch 156/600:  train Loss: 38.9592   val Loss: 41.7798   time: 70.08s   best: 41.5284
2023-12-02 14:29:18,091:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:29:18,113:INFO:  Epoch 17/500:  train Loss: 37.5454   val Loss: 37.5385   time: 241.11s   best: 37.5385
2023-12-02 14:29:47,376:INFO:  Epoch 157/600:  train Loss: 38.8769   val Loss: 41.8584   time: 69.74s   best: 41.5284
2023-12-02 14:30:57,357:INFO:  Epoch 158/600:  train Loss: 38.6703   val Loss: 41.5563   time: 69.98s   best: 41.5284
2023-12-02 14:32:07,158:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:32:07,187:INFO:  Epoch 159/600:  train Loss: 38.8885   val Loss: 40.9849   time: 69.80s   best: 40.9849
2023-12-02 14:33:17,242:INFO:  Epoch 18/500:  train Loss: 36.6189   val Loss: 39.7946   time: 239.12s   best: 37.5385
2023-12-02 14:33:17,275:INFO:  Epoch 160/600:  train Loss: 38.3954   val Loss: 42.3295   time: 70.09s   best: 40.9849
2023-12-02 14:34:27,455:INFO:  Epoch 161/600:  train Loss: 38.5426   val Loss: 42.5309   time: 70.16s   best: 40.9849
2023-12-02 14:35:37,538:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:35:37,558:INFO:  Epoch 162/600:  train Loss: 38.4461   val Loss: 40.9625   time: 70.08s   best: 40.9625
2023-12-02 14:36:47,301:INFO:  Epoch 163/600:  train Loss: 38.4972   val Loss: 41.0291   time: 69.74s   best: 40.9625
2023-12-02 14:37:17,256:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:37:17,292:INFO:  Epoch 19/500:  train Loss: 35.9080   val Loss: 36.2206   time: 240.00s   best: 36.2206
2023-12-02 14:37:57,201:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:37:57,239:INFO:  Epoch 164/600:  train Loss: 38.0156   val Loss: 40.7089   time: 69.88s   best: 40.7089
2023-12-02 14:39:06,998:INFO:  Epoch 165/600:  train Loss: 38.5218   val Loss: 41.9501   time: 69.75s   best: 40.7089
2023-12-02 14:40:16,765:INFO:  Epoch 166/600:  train Loss: 38.3370   val Loss: 41.5691   time: 69.75s   best: 40.7089
2023-12-02 14:41:18,608:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:41:18,643:INFO:  Epoch 20/500:  train Loss: 35.1592   val Loss: 34.9343   time: 241.30s   best: 34.9343
2023-12-02 14:41:26,597:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:41:26,618:INFO:  Epoch 167/600:  train Loss: 37.7813   val Loss: 40.6781   time: 69.83s   best: 40.6781
2023-12-02 14:42:36,547:INFO:  Epoch 168/600:  train Loss: 38.1641   val Loss: 42.4624   time: 69.93s   best: 40.6781
2023-12-02 14:43:46,607:INFO:  Epoch 169/600:  train Loss: 37.9581   val Loss: 40.9055   time: 70.06s   best: 40.6781
2023-12-02 14:44:56,535:INFO:  Epoch 170/600:  train Loss: 38.0506   val Loss: 41.0453   time: 69.92s   best: 40.6781
2023-12-02 14:45:18,556:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:45:18,585:INFO:  Epoch 21/500:  train Loss: 34.7162   val Loss: 34.7236   time: 239.91s   best: 34.7236
2023-12-02 14:46:06,235:INFO:  Epoch 171/600:  train Loss: 37.6484   val Loss: 40.7702   time: 69.70s   best: 40.6781
2023-12-02 14:47:16,259:INFO:  Epoch 172/600:  train Loss: 37.4692   val Loss: 40.9908   time: 70.02s   best: 40.6781
2023-12-02 14:48:25,964:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:48:25,984:INFO:  Epoch 173/600:  train Loss: 37.4049   val Loss: 40.3102   time: 69.69s   best: 40.3102
2023-12-02 14:49:20,695:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:49:20,717:INFO:  Epoch 22/500:  train Loss: 34.0550   val Loss: 34.2354   time: 242.11s   best: 34.2354
2023-12-02 14:49:35,970:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:49:35,991:INFO:  Epoch 174/600:  train Loss: 37.3885   val Loss: 39.8729   time: 69.98s   best: 39.8729
2023-12-02 14:50:46,008:INFO:  Epoch 175/600:  train Loss: 37.4705   val Loss: 40.8229   time: 70.02s   best: 39.8729
2023-12-02 14:51:55,876:INFO:  Epoch 176/600:  train Loss: 37.1990   val Loss: 40.4495   time: 69.87s   best: 39.8729
2023-12-02 14:53:06,025:INFO:  Epoch 177/600:  train Loss: 37.3282   val Loss: 43.1689   time: 70.14s   best: 39.8729
2023-12-02 14:53:22,367:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:53:22,389:INFO:  Epoch 23/500:  train Loss: 33.6002   val Loss: 33.7319   time: 241.65s   best: 33.7319
2023-12-02 14:54:15,874:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:54:15,905:INFO:  Epoch 178/600:  train Loss: 37.2515   val Loss: 39.7653   time: 69.84s   best: 39.7653
2023-12-02 14:55:26,032:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 14:55:26,052:INFO:  Epoch 179/600:  train Loss: 37.2927   val Loss: 39.6208   time: 70.12s   best: 39.6208
2023-12-02 14:56:35,863:INFO:  Epoch 180/600:  train Loss: 37.5620   val Loss: 41.7079   time: 69.80s   best: 39.6208
2023-12-02 14:57:21,709:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 14:57:21,731:INFO:  Epoch 24/500:  train Loss: 33.1405   val Loss: 33.2946   time: 239.32s   best: 33.2946
2023-12-02 14:57:45,642:INFO:  Epoch 181/600:  train Loss: 37.5546   val Loss: 40.0150   time: 69.77s   best: 39.6208
2023-12-02 14:58:55,910:INFO:  Epoch 182/600:  train Loss: 37.0482   val Loss: 40.0504   time: 70.25s   best: 39.6208
2023-12-02 15:00:05,586:INFO:  Epoch 183/600:  train Loss: 36.8333   val Loss: 40.5656   time: 69.68s   best: 39.6208
2023-12-02 15:01:15,544:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:01:15,564:INFO:  Epoch 184/600:  train Loss: 36.5329   val Loss: 39.3449   time: 69.94s   best: 39.3449
2023-12-02 15:01:22,927:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 15:01:22,949:INFO:  Epoch 25/500:  train Loss: 32.7336   val Loss: 33.2532   time: 241.18s   best: 33.2532
2023-12-02 15:02:25,369:INFO:  Epoch 185/600:  train Loss: 37.2853   val Loss: 41.3304   time: 69.79s   best: 39.3449
2023-12-02 15:03:35,253:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:03:35,273:INFO:  Epoch 186/600:  train Loss: 36.7851   val Loss: 39.2889   time: 69.88s   best: 39.2889
2023-12-02 15:04:45,251:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:04:45,271:INFO:  Epoch 187/600:  train Loss: 36.3645   val Loss: 39.2063   time: 69.97s   best: 39.2063
2023-12-02 15:05:21,959:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 15:05:21,991:INFO:  Epoch 26/500:  train Loss: 32.1446   val Loss: 32.5636   time: 239.01s   best: 32.5636
2023-12-02 15:05:55,032:INFO:  Epoch 188/600:  train Loss: 36.5681   val Loss: 39.4340   time: 69.76s   best: 39.2063
2023-12-02 15:07:04,782:INFO:  Epoch 189/600:  train Loss: 36.4702   val Loss: 40.4616   time: 69.74s   best: 39.2063
2023-12-02 15:08:14,751:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:08:14,770:INFO:  Epoch 190/600:  train Loss: 36.4602   val Loss: 39.0668   time: 69.96s   best: 39.0668
2023-12-02 15:09:21,215:INFO:  Epoch 27/500:  train Loss: 31.9062   val Loss: 33.9746   time: 239.22s   best: 32.5636
2023-12-02 15:09:24,554:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:09:24,575:INFO:  Epoch 191/600:  train Loss: 36.2257   val Loss: 39.0189   time: 69.78s   best: 39.0189
2023-12-02 15:10:34,557:INFO:  Epoch 192/600:  train Loss: 36.1669   val Loss: 39.1641   time: 69.97s   best: 39.0189
2023-12-02 15:11:44,265:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:11:44,285:INFO:  Epoch 193/600:  train Loss: 36.0420   val Loss: 38.9144   time: 69.69s   best: 38.9144
2023-12-02 15:12:54,402:INFO:  Epoch 194/600:  train Loss: 35.8858   val Loss: 39.5549   time: 70.12s   best: 38.9144
2023-12-02 15:13:21,519:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 15:13:21,541:INFO:  Epoch 28/500:  train Loss: 31.4487   val Loss: 31.8439   time: 240.29s   best: 31.8439
2023-12-02 15:14:04,091:INFO:  Epoch 195/600:  train Loss: 36.1717   val Loss: 39.8956   time: 69.68s   best: 38.9144
2023-12-02 15:15:14,164:INFO:  Epoch 196/600:  train Loss: 36.3039   val Loss: 38.9667   time: 70.06s   best: 38.9144
2023-12-02 15:16:24,155:INFO:  Epoch 197/600:  train Loss: 35.7764   val Loss: 39.4187   time: 69.98s   best: 38.9144
2023-12-02 15:17:22,565:INFO:  Epoch 29/500:  train Loss: 31.1536   val Loss: 32.1521   time: 241.01s   best: 31.8439
2023-12-02 15:17:33,999:INFO:  Epoch 198/600:  train Loss: 36.1401   val Loss: 39.6697   time: 69.83s   best: 38.9144
2023-12-02 15:18:43,615:INFO:  Epoch 199/600:  train Loss: 35.5957   val Loss: 39.1423   time: 69.60s   best: 38.9144
2023-12-02 15:19:53,481:INFO:  Epoch 200/600:  train Loss: 36.7295   val Loss: 40.3135   time: 69.85s   best: 38.9144
2023-12-02 15:21:03,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:21:03,314:INFO:  Epoch 201/600:  train Loss: 36.3096   val Loss: 38.3652   time: 69.80s   best: 38.3652
2023-12-02 15:21:21,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 15:21:21,687:INFO:  Epoch 30/500:  train Loss: 31.0118   val Loss: 31.5719   time: 239.09s   best: 31.5719
2023-12-02 15:22:13,225:INFO:  Epoch 202/600:  train Loss: 35.8940   val Loss: 39.7134   time: 69.91s   best: 38.3652
2023-12-02 15:23:23,213:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:23:23,233:INFO:  Epoch 203/600:  train Loss: 35.7084   val Loss: 38.2491   time: 69.98s   best: 38.2491
2023-12-02 15:24:33,232:INFO:  Epoch 204/600:  train Loss: 35.4606   val Loss: 38.3181   time: 69.99s   best: 38.2491
2023-12-02 15:25:23,087:INFO:  Epoch 31/500:  train Loss: 30.5561   val Loss: 32.4520   time: 241.39s   best: 31.5719
2023-12-02 15:25:43,100:INFO:  Epoch 205/600:  train Loss: 35.3022   val Loss: 38.4055   time: 69.86s   best: 38.2491
2023-12-02 15:26:53,174:INFO:  Epoch 206/600:  train Loss: 35.4306   val Loss: 38.9302   time: 70.07s   best: 38.2491
2023-12-02 15:28:02,918:INFO:  Epoch 207/600:  train Loss: 35.5672   val Loss: 38.6551   time: 69.74s   best: 38.2491
2023-12-02 15:29:12,936:INFO:  Epoch 208/600:  train Loss: 35.3624   val Loss: 38.7052   time: 70.02s   best: 38.2491
2023-12-02 15:29:24,284:INFO:  Epoch 32/500:  train Loss: 30.4685   val Loss: 31.6764   time: 241.19s   best: 31.5719
2023-12-02 15:30:22,828:INFO:  Epoch 209/600:  train Loss: 35.1766   val Loss: 38.7618   time: 69.88s   best: 38.2491
2023-12-02 15:31:32,755:INFO:  Epoch 210/600:  train Loss: 35.5755   val Loss: 42.4287   time: 69.91s   best: 38.2491
2023-12-02 15:32:42,556:INFO:  Epoch 211/600:  train Loss: 35.7645   val Loss: 38.4796   time: 69.79s   best: 38.2491
2023-12-02 15:33:24,926:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 15:33:24,948:INFO:  Epoch 33/500:  train Loss: 30.0347   val Loss: 30.6530   time: 240.62s   best: 30.6530
2023-12-02 15:33:52,518:INFO:  Epoch 212/600:  train Loss: 34.9261   val Loss: 38.4950   time: 69.95s   best: 38.2491
2023-12-02 15:35:02,520:INFO:  Epoch 213/600:  train Loss: 35.1201   val Loss: 39.1426   time: 70.00s   best: 38.2491
2023-12-02 15:36:12,504:INFO:  Epoch 214/600:  train Loss: 35.2095   val Loss: 38.4464   time: 69.97s   best: 38.2491
2023-12-02 15:37:22,566:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:37:22,595:INFO:  Epoch 215/600:  train Loss: 34.8001   val Loss: 38.0810   time: 70.05s   best: 38.0810
2023-12-02 15:37:25,673:INFO:  Epoch 34/500:  train Loss: 29.8824   val Loss: 31.6911   time: 240.72s   best: 30.6530
2023-12-02 15:38:32,854:INFO:  Epoch 216/600:  train Loss: 35.0017   val Loss: 39.6989   time: 70.25s   best: 38.0810
2023-12-02 15:39:42,706:INFO:  Epoch 217/600:  train Loss: 34.7486   val Loss: 38.7230   time: 69.84s   best: 38.0810
2023-12-02 15:40:52,827:INFO:  Epoch 218/600:  train Loss: 35.1089   val Loss: 39.4385   time: 70.11s   best: 38.0810
2023-12-02 15:41:27,088:INFO:  Epoch 35/500:  train Loss: 29.8231   val Loss: 30.6657   time: 241.40s   best: 30.6530
2023-12-02 15:42:02,573:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:42:02,595:INFO:  Epoch 219/600:  train Loss: 34.6795   val Loss: 37.6985   time: 69.74s   best: 37.6985
2023-12-02 15:43:12,531:INFO:  Epoch 220/600:  train Loss: 34.6212   val Loss: 37.8482   time: 69.94s   best: 37.6985
2023-12-02 15:44:22,233:INFO:  Epoch 221/600:  train Loss: 34.7245   val Loss: 37.8405   time: 69.70s   best: 37.6985
2023-12-02 15:45:26,951:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 15:45:26,974:INFO:  Epoch 36/500:  train Loss: 29.3360   val Loss: 29.8892   time: 239.86s   best: 29.8892
2023-12-02 15:45:32,129:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:45:32,150:INFO:  Epoch 222/600:  train Loss: 34.4894   val Loss: 37.6252   time: 69.89s   best: 37.6252
2023-12-02 15:46:41,909:INFO:  Epoch 223/600:  train Loss: 35.0845   val Loss: 38.7654   time: 69.76s   best: 37.6252
2023-12-02 15:47:51,771:INFO:  Epoch 224/600:  train Loss: 34.6012   val Loss: 38.0390   time: 69.85s   best: 37.6252
2023-12-02 15:49:01,444:INFO:  Epoch 225/600:  train Loss: 34.5665   val Loss: 38.6119   time: 69.66s   best: 37.6252
2023-12-02 15:49:27,198:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 15:49:27,220:INFO:  Epoch 37/500:  train Loss: 29.2495   val Loss: 29.6266   time: 240.22s   best: 29.6266
2023-12-02 15:50:11,266:INFO:  Epoch 226/600:  train Loss: 34.5193   val Loss: 39.0774   time: 69.82s   best: 37.6252
2023-12-02 15:51:21,353:INFO:  Epoch 227/600:  train Loss: 34.5294   val Loss: 38.7678   time: 70.07s   best: 37.6252
2023-12-02 15:52:31,278:INFO:  Epoch 228/600:  train Loss: 34.8568   val Loss: 38.1924   time: 69.92s   best: 37.6252
2023-12-02 15:53:26,774:INFO:  Epoch 38/500:  train Loss: 28.9629   val Loss: 30.8294   time: 239.55s   best: 29.6266
2023-12-02 15:53:41,099:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 15:53:41,121:INFO:  Epoch 229/600:  train Loss: 34.2704   val Loss: 37.3185   time: 69.82s   best: 37.3185
2023-12-02 15:54:51,350:INFO:  Epoch 230/600:  train Loss: 34.1350   val Loss: 38.0575   time: 70.23s   best: 37.3185
2023-12-02 15:56:01,108:INFO:  Epoch 231/600:  train Loss: 34.1000   val Loss: 38.5884   time: 69.75s   best: 37.3185
2023-12-02 15:57:11,200:INFO:  Epoch 232/600:  train Loss: 34.3047   val Loss: 38.2347   time: 70.09s   best: 37.3185
2023-12-02 15:57:27,547:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 15:57:27,576:INFO:  Epoch 39/500:  train Loss: 28.7770   val Loss: 29.4056   time: 240.75s   best: 29.4056
2023-12-02 15:58:20,999:INFO:  Epoch 233/600:  train Loss: 34.4687   val Loss: 37.4609   time: 69.79s   best: 37.3185
2023-12-02 15:59:31,000:INFO:  Epoch 234/600:  train Loss: 34.0307   val Loss: 37.6533   time: 70.00s   best: 37.3185
2023-12-02 16:00:40,927:INFO:  Epoch 235/600:  train Loss: 35.0247   val Loss: 42.0299   time: 69.92s   best: 37.3185
2023-12-02 16:01:27,301:INFO:  Epoch 40/500:  train Loss: 28.7154   val Loss: 29.4179   time: 239.71s   best: 29.4056
2023-12-02 16:01:50,759:INFO:  Epoch 236/600:  train Loss: 35.3506   val Loss: 37.9066   time: 69.83s   best: 37.3185
2023-12-02 16:03:00,495:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 16:03:00,514:INFO:  Epoch 237/600:  train Loss: 33.8801   val Loss: 37.2436   time: 69.73s   best: 37.2436
2023-12-02 16:04:10,658:INFO:  Epoch 238/600:  train Loss: 33.9737   val Loss: 37.2675   time: 70.14s   best: 37.2436
2023-12-02 16:05:20,513:INFO:  Epoch 239/600:  train Loss: 34.0406   val Loss: 37.5245   time: 69.85s   best: 37.2436
2023-12-02 16:05:26,802:INFO:  Epoch 41/500:  train Loss: 28.4215   val Loss: 30.3409   time: 239.49s   best: 29.4056
2023-12-02 16:06:30,513:INFO:  Epoch 240/600:  train Loss: 33.6255   val Loss: 37.3614   time: 70.00s   best: 37.2436
2023-12-02 16:07:40,230:INFO:  Epoch 241/600:  train Loss: 33.7753   val Loss: 37.6829   time: 69.70s   best: 37.2436
2023-12-02 16:08:50,315:INFO:  Epoch 242/600:  train Loss: 33.7578   val Loss: 38.1048   time: 70.07s   best: 37.2436
2023-12-02 16:09:26,180:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 16:09:26,202:INFO:  Epoch 42/500:  train Loss: 28.0392   val Loss: 29.0397   time: 239.37s   best: 29.0397
2023-12-02 16:10:00,053:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 16:10:00,076:INFO:  Epoch 243/600:  train Loss: 33.7216   val Loss: 37.2266   time: 69.72s   best: 37.2266
2023-12-02 16:11:10,208:INFO:  Epoch 244/600:  train Loss: 33.6300   val Loss: 37.4755   time: 70.12s   best: 37.2266
2023-12-02 16:12:20,169:INFO:  Epoch 245/600:  train Loss: 33.6485   val Loss: 37.3919   time: 69.96s   best: 37.2266
2023-12-02 16:13:25,516:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 16:13:25,537:INFO:  Epoch 43/500:  train Loss: 27.8989   val Loss: 29.0317   time: 239.31s   best: 29.0317
2023-12-02 16:13:30,188:INFO:  Epoch 246/600:  train Loss: 33.5068   val Loss: 39.1260   time: 70.02s   best: 37.2266
2023-12-02 16:14:40,130:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 16:14:40,150:INFO:  Epoch 247/600:  train Loss: 33.9353   val Loss: 37.0188   time: 69.90s   best: 37.0188
2023-12-02 16:15:50,208:INFO:  Epoch 248/600:  train Loss: 33.5946   val Loss: 38.0823   time: 70.06s   best: 37.0188
2023-12-02 16:17:00,274:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 16:17:00,303:INFO:  Epoch 249/600:  train Loss: 33.6192   val Loss: 36.8338   time: 70.06s   best: 36.8338
2023-12-02 16:17:25,775:INFO:  Epoch 44/500:  train Loss: 27.9261   val Loss: 29.1836   time: 240.24s   best: 29.0317
2023-12-02 16:18:10,469:INFO:  Epoch 250/600:  train Loss: 33.6120   val Loss: 37.1916   time: 70.15s   best: 36.8338
2023-12-02 16:19:20,521:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 16:19:20,540:INFO:  Epoch 251/600:  train Loss: 33.6017   val Loss: 36.6173   time: 70.05s   best: 36.6173
2023-12-02 16:20:30,682:INFO:  Epoch 252/600:  train Loss: 33.3252   val Loss: 37.1243   time: 70.14s   best: 36.6173
2023-12-02 16:21:27,701:INFO:  Epoch 45/500:  train Loss: 27.6960   val Loss: 29.3054   time: 241.91s   best: 29.0317
2023-12-02 16:21:40,536:INFO:  Epoch 253/600:  train Loss: 33.2095   val Loss: 36.9715   time: 69.85s   best: 36.6173
2023-12-02 16:22:50,694:INFO:  Epoch 254/600:  train Loss: 33.4405   val Loss: 36.7996   time: 70.15s   best: 36.6173
2023-12-02 16:24:00,923:INFO:  Epoch 255/600:  train Loss: 33.2046   val Loss: 37.4328   time: 70.23s   best: 36.6173
2023-12-02 16:25:11,029:INFO:  Epoch 256/600:  train Loss: 33.9305   val Loss: 37.0596   time: 70.10s   best: 36.6173
2023-12-02 16:25:27,262:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 16:25:27,283:INFO:  Epoch 46/500:  train Loss: 27.9410   val Loss: 28.7498   time: 239.55s   best: 28.7498
2023-12-02 16:26:20,683:INFO:  Epoch 257/600:  train Loss: 33.9977   val Loss: 37.0859   time: 69.65s   best: 36.6173
2023-12-02 16:27:30,508:INFO:  Epoch 258/600:  train Loss: 33.3412   val Loss: 36.7317   time: 69.82s   best: 36.6173
2023-12-02 16:28:40,527:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 16:28:40,547:INFO:  Epoch 259/600:  train Loss: 33.1028   val Loss: 36.6001   time: 70.01s   best: 36.6001
2023-12-02 16:29:26,379:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 16:29:26,401:INFO:  Epoch 47/500:  train Loss: 27.2993   val Loss: 28.6898   time: 239.09s   best: 28.6898
2023-12-02 16:29:50,629:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 16:29:50,650:INFO:  Epoch 260/600:  train Loss: 32.9665   val Loss: 36.5847   time: 70.08s   best: 36.5847
2023-12-02 16:31:00,739:INFO:  Epoch 261/600:  train Loss: 33.0547   val Loss: 36.7344   time: 70.09s   best: 36.5847
2023-12-02 16:32:10,573:INFO:  Epoch 262/600:  train Loss: 33.0447   val Loss: 36.9730   time: 69.83s   best: 36.5847
2023-12-02 16:33:20,361:INFO:  Epoch 263/600:  train Loss: 33.1007   val Loss: 37.4386   time: 69.78s   best: 36.5847
2023-12-02 16:33:26,259:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 16:33:26,280:INFO:  Epoch 48/500:  train Loss: 27.1578   val Loss: 28.4636   time: 239.84s   best: 28.4636
2023-12-02 16:34:30,262:INFO:  Epoch 264/600:  train Loss: 33.2456   val Loss: 37.2395   time: 69.89s   best: 36.5847
2023-12-02 16:35:40,028:INFO:  Epoch 265/600:  train Loss: 32.8764   val Loss: 37.3931   time: 69.75s   best: 36.5847
2023-12-02 16:36:49,985:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 16:36:50,016:INFO:  Epoch 266/600:  train Loss: 32.9135   val Loss: 36.2450   time: 69.93s   best: 36.2450
2023-12-02 16:37:25,622:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 16:37:25,676:INFO:  Epoch 49/500:  train Loss: 26.9990   val Loss: 28.3360   time: 239.32s   best: 28.3360
2023-12-02 16:37:59,586:INFO:  Epoch 267/600:  train Loss: 32.7783   val Loss: 36.5083   time: 69.57s   best: 36.2450
2023-12-02 16:39:09,583:INFO:  Epoch 268/600:  train Loss: 32.8306   val Loss: 38.5121   time: 69.99s   best: 36.2450
2023-12-02 16:40:19,367:INFO:  Epoch 269/600:  train Loss: 32.8986   val Loss: 36.8927   time: 69.77s   best: 36.2450
2023-12-02 16:41:25,533:INFO:  Epoch 50/500:  train Loss: 26.8552   val Loss: 28.6016   time: 239.83s   best: 28.3360
2023-12-02 16:41:29,227:INFO:  Epoch 270/600:  train Loss: 32.8584   val Loss: 36.6429   time: 69.85s   best: 36.2450
2023-12-02 16:42:39,175:INFO:  Epoch 271/600:  train Loss: 33.3294   val Loss: 36.8902   time: 69.93s   best: 36.2450
2023-12-02 16:43:48,978:INFO:  Epoch 272/600:  train Loss: 32.9077   val Loss: 36.7427   time: 69.79s   best: 36.2450
2023-12-02 16:44:58,829:INFO:  Epoch 273/600:  train Loss: 32.9567   val Loss: 36.8596   time: 69.85s   best: 36.2450
2023-12-02 16:45:24,893:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 16:45:24,940:INFO:  Epoch 51/500:  train Loss: 26.7185   val Loss: 28.3346   time: 239.33s   best: 28.3346
2023-12-02 16:46:08,915:INFO:  Epoch 274/600:  train Loss: 32.5741   val Loss: 36.3095   time: 70.09s   best: 36.2450
2023-12-02 16:47:18,653:INFO:  Epoch 275/600:  train Loss: 32.3699   val Loss: 37.1437   time: 69.72s   best: 36.2450
2023-12-02 16:48:28,446:INFO:  Epoch 276/600:  train Loss: 32.5086   val Loss: 36.9337   time: 69.79s   best: 36.2450
2023-12-02 16:49:24,480:INFO:  Epoch 52/500:  train Loss: 26.6789   val Loss: 28.5410   time: 239.53s   best: 28.3346
2023-12-02 16:49:38,604:INFO:  Epoch 277/600:  train Loss: 32.8487   val Loss: 36.5890   time: 70.16s   best: 36.2450
2023-12-02 16:50:48,678:INFO:  Epoch 278/600:  train Loss: 32.3699   val Loss: 36.4149   time: 70.06s   best: 36.2450
2023-12-02 16:51:58,378:INFO:  Epoch 279/600:  train Loss: 32.3500   val Loss: 37.7849   time: 69.70s   best: 36.2450
2023-12-02 16:53:08,336:INFO:  Epoch 280/600:  train Loss: 33.3155   val Loss: 37.2540   time: 69.95s   best: 36.2450
2023-12-02 16:53:25,991:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 16:53:26,013:INFO:  Epoch 53/500:  train Loss: 26.5966   val Loss: 27.9131   time: 241.48s   best: 27.9131
2023-12-02 16:54:18,076:INFO:  Epoch 281/600:  train Loss: 32.6875   val Loss: 37.8893   time: 69.73s   best: 36.2450
2023-12-02 16:55:28,159:INFO:  Epoch 282/600:  train Loss: 32.4431   val Loss: 36.4011   time: 70.07s   best: 36.2450
2023-12-02 16:56:37,985:INFO:  Epoch 283/600:  train Loss: 32.1931   val Loss: 36.4313   time: 69.82s   best: 36.2450
2023-12-02 16:57:25,885:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 16:57:25,915:INFO:  Epoch 54/500:  train Loss: 26.4159   val Loss: 27.7355   time: 239.84s   best: 27.7355
2023-12-02 16:57:47,763:INFO:  Epoch 284/600:  train Loss: 32.8365   val Loss: 38.5197   time: 69.77s   best: 36.2450
2023-12-02 16:58:57,646:INFO:  Epoch 285/600:  train Loss: 32.4690   val Loss: 36.7467   time: 69.87s   best: 36.2450
2023-12-02 17:00:07,629:INFO:  Epoch 286/600:  train Loss: 32.4853   val Loss: 36.5599   time: 69.97s   best: 36.2450
2023-12-02 17:01:17,584:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 17:01:17,613:INFO:  Epoch 287/600:  train Loss: 32.3260   val Loss: 36.0975   time: 69.95s   best: 36.0975
2023-12-02 17:01:27,235:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 17:01:27,258:INFO:  Epoch 55/500:  train Loss: 26.2831   val Loss: 27.6398   time: 241.32s   best: 27.6398
2023-12-02 17:02:27,462:INFO:  Epoch 288/600:  train Loss: 32.4586   val Loss: 36.5895   time: 69.85s   best: 36.0975
2023-12-02 17:03:37,213:INFO:  Epoch 289/600:  train Loss: 32.1384   val Loss: 36.8759   time: 69.74s   best: 36.0975
2023-12-02 17:04:47,221:INFO:  Epoch 290/600:  train Loss: 32.0295   val Loss: 36.2803   time: 70.01s   best: 36.0975
2023-12-02 17:05:27,974:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 17:05:27,995:INFO:  Epoch 56/500:  train Loss: 26.1885   val Loss: 27.5019   time: 240.71s   best: 27.5019
2023-12-02 17:05:57,082:INFO:  Epoch 291/600:  train Loss: 32.0890   val Loss: 36.1497   time: 69.86s   best: 36.0975
2023-12-02 17:07:06,868:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 17:07:06,888:INFO:  Epoch 292/600:  train Loss: 32.0392   val Loss: 35.7870   time: 69.78s   best: 35.7870
2023-12-02 17:08:16,532:INFO:  Epoch 293/600:  train Loss: 31.9020   val Loss: 37.6131   time: 69.63s   best: 35.7870
2023-12-02 17:09:26,488:INFO:  Epoch 294/600:  train Loss: 32.0703   val Loss: 36.7640   time: 69.94s   best: 35.7870
2023-12-02 17:09:29,199:INFO:  Epoch 57/500:  train Loss: 26.0928   val Loss: 27.6692   time: 241.19s   best: 27.5019
2023-12-02 17:10:36,169:INFO:  Epoch 295/600:  train Loss: 32.0488   val Loss: 36.2507   time: 69.67s   best: 35.7870
2023-12-02 17:11:45,990:INFO:  Epoch 296/600:  train Loss: 32.2510   val Loss: 36.8060   time: 69.82s   best: 35.7870
2023-12-02 17:12:56,034:INFO:  Epoch 297/600:  train Loss: 31.9865   val Loss: 35.9327   time: 70.03s   best: 35.7870
2023-12-02 17:13:28,789:INFO:  Epoch 58/500:  train Loss: 25.8189   val Loss: 27.7007   time: 239.59s   best: 27.5019
2023-12-02 17:14:05,813:INFO:  Epoch 298/600:  train Loss: 31.8811   val Loss: 36.0089   time: 69.77s   best: 35.7870
2023-12-02 17:15:15,652:INFO:  Epoch 299/600:  train Loss: 32.6717   val Loss: 36.4795   time: 69.83s   best: 35.7870
2023-12-02 17:16:25,724:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 17:16:25,744:INFO:  Epoch 300/600:  train Loss: 32.1300   val Loss: 35.5313   time: 70.04s   best: 35.5313
2023-12-02 17:17:28,330:INFO:  Epoch 59/500:  train Loss: 25.6485   val Loss: 27.6274   time: 239.54s   best: 27.5019
2023-12-02 17:17:35,450:INFO:  Epoch 301/600:  train Loss: 31.6233   val Loss: 36.2239   time: 69.70s   best: 35.5313
2023-12-02 17:18:45,232:INFO:  Epoch 302/600:  train Loss: 31.8642   val Loss: 35.8375   time: 69.78s   best: 35.5313
2023-12-02 17:19:54,935:INFO:  Epoch 303/600:  train Loss: 31.7786   val Loss: 36.1087   time: 69.69s   best: 35.5313
2023-12-02 17:21:04,711:INFO:  Epoch 304/600:  train Loss: 31.7842   val Loss: 35.8011   time: 69.77s   best: 35.5313
2023-12-02 17:21:30,066:INFO:  Epoch 60/500:  train Loss: 25.6837   val Loss: 27.7774   time: 241.73s   best: 27.5019
2023-12-02 17:22:14,447:INFO:  Epoch 305/600:  train Loss: 31.9715   val Loss: 36.2294   time: 69.74s   best: 35.5313
2023-12-02 17:23:24,442:INFO:  Epoch 306/600:  train Loss: 31.7147   val Loss: 36.1797   time: 69.98s   best: 35.5313
2023-12-02 17:24:34,308:INFO:  Epoch 307/600:  train Loss: 31.5865   val Loss: 36.0429   time: 69.85s   best: 35.5313
2023-12-02 17:25:29,264:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 17:25:29,286:INFO:  Epoch 61/500:  train Loss: 25.4799   val Loss: 27.0150   time: 239.18s   best: 27.0150
2023-12-02 17:25:44,307:INFO:  Epoch 308/600:  train Loss: 31.9822   val Loss: 36.3819   time: 69.99s   best: 35.5313
2023-12-02 17:26:54,170:INFO:  Epoch 309/600:  train Loss: 31.9034   val Loss: 36.8634   time: 69.85s   best: 35.5313
2023-12-02 17:28:03,928:INFO:  Epoch 310/600:  train Loss: 32.1365   val Loss: 39.2130   time: 69.75s   best: 35.5313
2023-12-02 17:29:13,809:INFO:  Epoch 311/600:  train Loss: 31.7925   val Loss: 35.5334   time: 69.87s   best: 35.5313
2023-12-02 17:29:28,770:INFO:  Epoch 62/500:  train Loss: 25.7045   val Loss: 28.1106   time: 239.48s   best: 27.0150
2023-12-02 17:30:23,616:INFO:  Epoch 312/600:  train Loss: 31.5986   val Loss: 35.9803   time: 69.80s   best: 35.5313
2023-12-02 17:31:33,319:INFO:  Epoch 313/600:  train Loss: 31.5818   val Loss: 35.9267   time: 69.69s   best: 35.5313
2023-12-02 17:32:43,211:INFO:  Epoch 314/600:  train Loss: 31.3487   val Loss: 35.8284   time: 69.88s   best: 35.5313
2023-12-02 17:33:29,202:INFO:  Epoch 63/500:  train Loss: 25.2542   val Loss: 28.4256   time: 240.42s   best: 27.0150
2023-12-02 17:33:52,783:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 17:33:52,805:INFO:  Epoch 315/600:  train Loss: 31.2288   val Loss: 35.4590   time: 69.56s   best: 35.4590
2023-12-02 17:35:02,751:INFO:  Epoch 316/600:  train Loss: 31.3349   val Loss: 36.3826   time: 69.94s   best: 35.4590
2023-12-02 17:36:12,408:INFO:  Epoch 317/600:  train Loss: 31.4126   val Loss: 35.6795   time: 69.66s   best: 35.4590
2023-12-02 17:37:22,487:INFO:  Epoch 318/600:  train Loss: 31.3850   val Loss: 36.0847   time: 70.07s   best: 35.4590
2023-12-02 17:37:28,771:INFO:  Epoch 64/500:  train Loss: 25.2297   val Loss: 29.0084   time: 239.54s   best: 27.0150
2023-12-02 17:38:32,493:INFO:  Epoch 319/600:  train Loss: 31.8841   val Loss: 36.3672   time: 70.01s   best: 35.4590
2023-12-02 17:39:42,222:INFO:  Epoch 320/600:  train Loss: 31.4904   val Loss: 36.2558   time: 69.72s   best: 35.4590
2023-12-02 17:40:51,945:INFO:  Epoch 321/600:  train Loss: 31.1903   val Loss: 35.6340   time: 69.71s   best: 35.4590
2023-12-02 17:41:29,030:INFO:  Epoch 65/500:  train Loss: 25.1831   val Loss: 27.1691   time: 240.25s   best: 27.0150
2023-12-02 17:42:01,761:INFO:  Epoch 322/600:  train Loss: 31.9710   val Loss: 36.2658   time: 69.81s   best: 35.4590
2023-12-02 17:43:11,425:INFO:  Epoch 323/600:  train Loss: 31.4543   val Loss: 35.6154   time: 69.65s   best: 35.4590
2023-12-02 17:44:21,195:INFO:  Epoch 324/600:  train Loss: 31.0277   val Loss: 35.5052   time: 69.77s   best: 35.4590
2023-12-02 17:45:28,844:INFO:  Epoch 66/500:  train Loss: 25.0384   val Loss: 27.1259   time: 239.81s   best: 27.0150
2023-12-02 17:45:30,872:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 17:45:30,896:INFO:  Epoch 325/600:  train Loss: 31.1182   val Loss: 35.4250   time: 69.67s   best: 35.4250
2023-12-02 17:46:40,840:INFO:  Epoch 326/600:  train Loss: 31.1206   val Loss: 35.8174   time: 69.93s   best: 35.4250
2023-12-02 17:47:50,516:INFO:  Epoch 327/600:  train Loss: 30.9815   val Loss: 35.4290   time: 69.66s   best: 35.4250
2023-12-02 17:49:00,276:INFO:  Epoch 328/600:  train Loss: 31.2151   val Loss: 36.5177   time: 69.76s   best: 35.4250
2023-12-02 17:49:31,026:INFO:  Epoch 67/500:  train Loss: 24.9910   val Loss: 27.0748   time: 242.16s   best: 27.0150
2023-12-02 17:50:09,961:INFO:  Epoch 329/600:  train Loss: 31.0115   val Loss: 36.2234   time: 69.67s   best: 35.4250
2023-12-02 17:51:19,984:INFO:  Epoch 330/600:  train Loss: 32.0681   val Loss: 37.8207   time: 70.01s   best: 35.4250
2023-12-02 17:52:29,576:INFO:  Epoch 331/600:  train Loss: 31.3185   val Loss: 35.7848   time: 69.58s   best: 35.4250
2023-12-02 17:53:32,161:INFO:  Epoch 68/500:  train Loss: 24.7817   val Loss: 27.0335   time: 241.13s   best: 27.0150
2023-12-02 17:53:39,431:INFO:  Epoch 332/600:  train Loss: 31.0231   val Loss: 36.2525   time: 69.85s   best: 35.4250
2023-12-02 17:54:49,182:INFO:  Epoch 333/600:  train Loss: 31.1067   val Loss: 35.5654   time: 69.74s   best: 35.4250
2023-12-02 17:55:59,102:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 17:55:59,122:INFO:  Epoch 334/600:  train Loss: 30.8390   val Loss: 34.8642   time: 69.92s   best: 34.8642
2023-12-02 17:57:09,008:INFO:  Epoch 335/600:  train Loss: 30.8248   val Loss: 34.8976   time: 69.88s   best: 34.8642
2023-12-02 17:57:31,443:INFO:  Epoch 69/500:  train Loss: 24.7826   val Loss: 27.0298   time: 239.28s   best: 27.0150
2023-12-02 17:58:18,898:INFO:  Epoch 336/600:  train Loss: 31.1354   val Loss: 35.9463   time: 69.89s   best: 34.8642
2023-12-02 17:59:28,642:INFO:  Epoch 337/600:  train Loss: 31.1944   val Loss: 35.3129   time: 69.71s   best: 34.8642
2023-12-02 18:00:38,527:INFO:  Epoch 338/600:  train Loss: 31.0150   val Loss: 35.2416   time: 69.88s   best: 34.8642
2023-12-02 18:01:30,607:INFO:  Epoch 70/500:  train Loss: 24.9004   val Loss: 27.0641   time: 239.16s   best: 27.0150
2023-12-02 18:01:48,202:INFO:  Epoch 339/600:  train Loss: 30.6844   val Loss: 35.8182   time: 69.66s   best: 34.8642
2023-12-02 18:02:58,084:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 18:02:58,103:INFO:  Epoch 340/600:  train Loss: 30.9542   val Loss: 34.5033   time: 69.86s   best: 34.5033
2023-12-02 18:04:07,776:INFO:  Epoch 341/600:  train Loss: 30.6197   val Loss: 34.9675   time: 69.66s   best: 34.5033
2023-12-02 18:05:17,688:INFO:  Epoch 342/600:  train Loss: 30.6438   val Loss: 35.4663   time: 69.90s   best: 34.5033
2023-12-02 18:05:30,387:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 18:05:30,409:INFO:  Epoch 71/500:  train Loss: 24.6341   val Loss: 26.6547   time: 239.75s   best: 26.6547
2023-12-02 18:06:27,691:INFO:  Epoch 343/600:  train Loss: 30.8450   val Loss: 35.3585   time: 69.99s   best: 34.5033
2023-12-02 18:07:37,488:INFO:  Epoch 344/600:  train Loss: 30.7487   val Loss: 35.3249   time: 69.79s   best: 34.5033
2023-12-02 18:08:47,206:INFO:  Epoch 345/600:  train Loss: 30.9054   val Loss: 36.0548   time: 69.72s   best: 34.5033
2023-12-02 18:09:31,642:INFO:  Epoch 72/500:  train Loss: 24.5422   val Loss: 28.4030   time: 241.23s   best: 26.6547
2023-12-02 18:09:56,947:INFO:  Epoch 346/600:  train Loss: 31.4742   val Loss: 35.2479   time: 69.74s   best: 34.5033
2023-12-02 18:11:06,603:INFO:  Epoch 347/600:  train Loss: 30.8413   val Loss: 35.1115   time: 69.64s   best: 34.5033
2023-12-02 18:12:16,612:INFO:  Epoch 348/600:  train Loss: 30.5341   val Loss: 35.1933   time: 70.01s   best: 34.5033
2023-12-02 18:13:26,337:INFO:  Epoch 349/600:  train Loss: 30.4451   val Loss: 35.4009   time: 69.72s   best: 34.5033
2023-12-02 18:13:31,412:INFO:  Epoch 73/500:  train Loss: 25.0337   val Loss: 27.1140   time: 239.75s   best: 26.6547
2023-12-02 18:14:36,482:INFO:  Epoch 350/600:  train Loss: 30.9640   val Loss: 35.9158   time: 70.13s   best: 34.5033
2023-12-02 18:15:46,255:INFO:  Epoch 351/600:  train Loss: 30.9019   val Loss: 35.0212   time: 69.77s   best: 34.5033
2023-12-02 18:16:56,241:INFO:  Epoch 352/600:  train Loss: 30.5032   val Loss: 35.7196   time: 69.97s   best: 34.5033
2023-12-02 18:17:30,656:INFO:  Epoch 74/500:  train Loss: 24.3978   val Loss: 27.5275   time: 239.24s   best: 26.6547
2023-12-02 18:18:05,867:INFO:  Epoch 353/600:  train Loss: 30.4808   val Loss: 35.1031   time: 69.61s   best: 34.5033
2023-12-02 18:19:15,661:INFO:  Epoch 354/600:  train Loss: 31.8361   val Loss: 36.6992   time: 69.79s   best: 34.5033
2023-12-02 18:20:25,717:INFO:  Epoch 355/600:  train Loss: 31.1345   val Loss: 35.3117   time: 70.05s   best: 34.5033
2023-12-02 18:21:29,663:INFO:  Epoch 75/500:  train Loss: 24.2800   val Loss: 26.7137   time: 238.99s   best: 26.6547
2023-12-02 18:21:35,679:INFO:  Epoch 356/600:  train Loss: 31.0017   val Loss: 35.4929   time: 69.96s   best: 34.5033
2023-12-02 18:22:45,436:INFO:  Epoch 357/600:  train Loss: 30.4397   val Loss: 35.0211   time: 69.75s   best: 34.5033
2023-12-02 18:23:55,492:INFO:  Epoch 358/600:  train Loss: 30.8085   val Loss: 35.2875   time: 70.06s   best: 34.5033
2023-12-02 18:25:05,111:INFO:  Epoch 359/600:  train Loss: 30.4187   val Loss: 34.9827   time: 69.61s   best: 34.5033
2023-12-02 18:25:29,176:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 18:25:29,198:INFO:  Epoch 76/500:  train Loss: 24.3601   val Loss: 26.6282   time: 239.51s   best: 26.6282
2023-12-02 18:26:15,015:INFO:  Epoch 360/600:  train Loss: 30.2438   val Loss: 34.6324   time: 69.90s   best: 34.5033
2023-12-02 18:27:24,706:INFO:  Epoch 361/600:  train Loss: 30.5720   val Loss: 37.8592   time: 69.68s   best: 34.5033
2023-12-02 18:28:34,431:INFO:  Epoch 362/600:  train Loss: 30.9298   val Loss: 35.9991   time: 69.71s   best: 34.5033
2023-12-02 18:29:29,793:INFO:  Epoch 77/500:  train Loss: 24.1935   val Loss: 26.7229   time: 240.58s   best: 26.6282
2023-12-02 18:29:44,001:INFO:  Epoch 363/600:  train Loss: 30.9608   val Loss: 35.3452   time: 69.57s   best: 34.5033
2023-12-02 18:30:53,826:INFO:  Epoch 364/600:  train Loss: 30.2670   val Loss: 34.7662   time: 69.81s   best: 34.5033
2023-12-02 18:32:03,523:INFO:  Epoch 365/600:  train Loss: 30.1770   val Loss: 35.5229   time: 69.69s   best: 34.5033
2023-12-02 18:33:13,489:INFO:  Epoch 366/600:  train Loss: 30.3492   val Loss: 35.4743   time: 69.95s   best: 34.5033
2023-12-02 18:33:29,468:INFO:  Epoch 78/500:  train Loss: 24.1528   val Loss: 27.7406   time: 239.66s   best: 26.6282
2023-12-02 18:34:23,109:INFO:  Epoch 367/600:  train Loss: 30.4379   val Loss: 35.7823   time: 69.62s   best: 34.5033
2023-12-02 18:35:32,756:INFO:  Epoch 368/600:  train Loss: 30.2648   val Loss: 34.7193   time: 69.64s   best: 34.5033
2023-12-02 18:36:42,375:INFO:  Epoch 369/600:  train Loss: 30.9723   val Loss: 38.8780   time: 69.62s   best: 34.5033
2023-12-02 18:37:29,184:INFO:  Epoch 79/500:  train Loss: 24.0915   val Loss: 26.6865   time: 239.71s   best: 26.6282
2023-12-02 18:37:52,187:INFO:  Epoch 370/600:  train Loss: 31.1922   val Loss: 35.1041   time: 69.81s   best: 34.5033
2023-12-02 18:39:01,830:INFO:  Epoch 371/600:  train Loss: 30.2134   val Loss: 35.1642   time: 69.63s   best: 34.5033
2023-12-02 18:40:11,471:INFO:  Epoch 372/600:  train Loss: 30.4442   val Loss: 34.7926   time: 69.63s   best: 34.5033
2023-12-02 18:41:21,178:INFO:  Epoch 373/600:  train Loss: 30.0581   val Loss: 34.5704   time: 69.70s   best: 34.5033
2023-12-02 18:41:31,024:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 18:41:31,061:INFO:  Epoch 80/500:  train Loss: 24.2316   val Loss: 26.6270   time: 241.83s   best: 26.6270
2023-12-02 18:42:30,962:INFO:  Epoch 374/600:  train Loss: 30.0489   val Loss: 35.3837   time: 69.77s   best: 34.5033
2023-12-02 18:43:40,595:INFO:  Epoch 375/600:  train Loss: 29.9805   val Loss: 35.8585   time: 69.63s   best: 34.5033
2023-12-02 18:44:50,704:INFO:  Epoch 376/600:  train Loss: 30.2679   val Loss: 34.5827   time: 70.11s   best: 34.5033
2023-12-02 18:45:31,432:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 18:45:31,454:INFO:  Epoch 81/500:  train Loss: 23.9194   val Loss: 26.1662   time: 240.36s   best: 26.1662
2023-12-02 18:46:00,348:INFO:  Epoch 377/600:  train Loss: 30.2719   val Loss: 35.4722   time: 69.63s   best: 34.5033
2023-12-02 18:47:10,190:INFO:  Epoch 378/600:  train Loss: 30.4533   val Loss: 34.9399   time: 69.83s   best: 34.5033
2023-12-02 18:48:19,851:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 18:48:19,880:INFO:  Epoch 379/600:  train Loss: 30.1646   val Loss: 34.3613   time: 69.65s   best: 34.3613
2023-12-02 18:49:29,696:INFO:  Epoch 380/600:  train Loss: 29.9887   val Loss: 34.6752   time: 69.82s   best: 34.3613
2023-12-02 18:49:31,184:INFO:  Epoch 82/500:  train Loss: 23.9259   val Loss: 27.2675   time: 239.73s   best: 26.1662
2023-12-02 18:50:39,692:INFO:  Epoch 381/600:  train Loss: 29.8173   val Loss: 34.5433   time: 69.99s   best: 34.3613
2023-12-02 18:51:49,647:INFO:  Epoch 382/600:  train Loss: 29.8759   val Loss: 34.6894   time: 69.94s   best: 34.3613
2023-12-02 18:52:59,356:INFO:  Epoch 383/600:  train Loss: 29.9925   val Loss: 36.9600   time: 69.70s   best: 34.3613
2023-12-02 18:53:31,640:INFO:  Epoch 83/500:  train Loss: 23.8967   val Loss: 26.1725   time: 240.43s   best: 26.1662
2023-12-02 18:54:09,276:INFO:  Epoch 384/600:  train Loss: 30.4156   val Loss: 34.9030   time: 69.92s   best: 34.3613
2023-12-02 18:55:19,348:INFO:  Epoch 385/600:  train Loss: 29.7825   val Loss: 34.7999   time: 70.06s   best: 34.3613
2023-12-02 18:56:29,598:INFO:  Epoch 386/600:  train Loss: 29.8345   val Loss: 36.2300   time: 70.25s   best: 34.3613
2023-12-02 18:57:31,394:INFO:  Epoch 84/500:  train Loss: 23.7443   val Loss: 27.2036   time: 239.75s   best: 26.1662
2023-12-02 18:57:39,588:INFO:  Epoch 387/600:  train Loss: 29.8175   val Loss: 34.9274   time: 69.99s   best: 34.3613
2023-12-02 18:58:49,597:INFO:  Epoch 388/600:  train Loss: 30.0235   val Loss: 34.3638   time: 70.01s   best: 34.3613
2023-12-02 18:59:59,388:INFO:  Epoch 389/600:  train Loss: 30.2589   val Loss: 34.9607   time: 69.79s   best: 34.3613
2023-12-02 19:01:09,336:INFO:  Epoch 390/600:  train Loss: 30.0445   val Loss: 34.4110   time: 69.95s   best: 34.3613
2023-12-02 19:01:32,630:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 19:01:32,652:INFO:  Epoch 85/500:  train Loss: 23.6103   val Loss: 26.1235   time: 241.23s   best: 26.1235
2023-12-02 19:02:19,273:INFO:  Epoch 391/600:  train Loss: 29.7454   val Loss: 35.2140   time: 69.94s   best: 34.3613
2023-12-02 19:03:29,226:INFO:  Epoch 392/600:  train Loss: 30.0129   val Loss: 35.6908   time: 69.95s   best: 34.3613
2023-12-02 19:04:39,132:INFO:  Epoch 393/600:  train Loss: 29.7200   val Loss: 34.6052   time: 69.91s   best: 34.3613
2023-12-02 19:05:33,740:INFO:  Epoch 86/500:  train Loss: 23.5265   val Loss: 26.2725   time: 241.09s   best: 26.1235
2023-12-02 19:05:49,021:INFO:  Epoch 394/600:  train Loss: 29.8005   val Loss: 34.6099   time: 69.89s   best: 34.3613
2023-12-02 19:06:58,827:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 19:06:58,846:INFO:  Epoch 395/600:  train Loss: 29.6181   val Loss: 34.1618   time: 69.79s   best: 34.1618
2023-12-02 19:08:08,756:INFO:  Epoch 396/600:  train Loss: 29.9079   val Loss: 35.3707   time: 69.90s   best: 34.1618
2023-12-02 19:09:18,669:INFO:  Epoch 397/600:  train Loss: 29.7280   val Loss: 34.6868   time: 69.90s   best: 34.1618
2023-12-02 19:09:35,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 19:09:35,330:INFO:  Epoch 87/500:  train Loss: 23.6859   val Loss: 25.8739   time: 241.55s   best: 25.8739
2023-12-02 19:10:28,559:INFO:  Epoch 398/600:  train Loss: 29.5747   val Loss: 34.6640   time: 69.88s   best: 34.1618
2023-12-02 19:11:38,488:INFO:  Epoch 399/600:  train Loss: 29.4827   val Loss: 34.4456   time: 69.93s   best: 34.1618
2023-12-02 19:12:48,830:INFO:  Epoch 400/600:  train Loss: 29.5262   val Loss: 34.6483   time: 70.34s   best: 34.1618
2023-12-02 19:13:36,488:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 19:13:36,510:INFO:  Epoch 88/500:  train Loss: 23.7011   val Loss: 25.7624   time: 241.14s   best: 25.7624
2023-12-02 19:13:58,902:INFO:  Epoch 401/600:  train Loss: 29.5053   val Loss: 34.5040   time: 70.06s   best: 34.1618
2023-12-02 19:15:08,747:INFO:  Epoch 402/600:  train Loss: 30.7514   val Loss: 34.8074   time: 69.83s   best: 34.1618
2023-12-02 19:16:18,409:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 19:16:18,428:INFO:  Epoch 403/600:  train Loss: 29.6140   val Loss: 34.0086   time: 69.66s   best: 34.0086
2023-12-02 19:17:28,248:INFO:  Epoch 404/600:  train Loss: 29.7547   val Loss: 34.6765   time: 69.82s   best: 34.0086
2023-12-02 19:17:35,950:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 19:17:35,972:INFO:  Epoch 89/500:  train Loss: 23.4911   val Loss: 25.6498   time: 239.44s   best: 25.6498
2023-12-02 19:18:37,972:INFO:  Epoch 405/600:  train Loss: 29.5254   val Loss: 34.9281   time: 69.72s   best: 34.0086
2023-12-02 19:19:47,812:INFO:  Epoch 406/600:  train Loss: 29.4063   val Loss: 34.3015   time: 69.84s   best: 34.0086
2023-12-02 19:20:57,549:INFO:  Epoch 407/600:  train Loss: 29.3974   val Loss: 34.3414   time: 69.74s   best: 34.0086
2023-12-02 19:21:35,663:INFO:  Epoch 90/500:  train Loss: 23.3742   val Loss: 26.8776   time: 239.68s   best: 25.6498
2023-12-02 19:22:07,446:INFO:  Epoch 408/600:  train Loss: 29.9237   val Loss: 36.9247   time: 69.90s   best: 34.0086
2023-12-02 19:23:17,579:INFO:  Epoch 409/600:  train Loss: 30.5884   val Loss: 34.2422   time: 70.13s   best: 34.0086
2023-12-02 19:24:27,642:INFO:  Epoch 410/600:  train Loss: 29.2762   val Loss: 34.8039   time: 70.06s   best: 34.0086
2023-12-02 19:25:36,907:INFO:  Epoch 91/500:  train Loss: 23.2485   val Loss: 25.7032   time: 241.24s   best: 25.6498
2023-12-02 19:25:37,332:INFO:  Epoch 411/600:  train Loss: 29.2658   val Loss: 34.7468   time: 69.68s   best: 34.0086
2023-12-02 19:26:47,210:INFO:  Epoch 412/600:  train Loss: 29.5449   val Loss: 34.5203   time: 69.87s   best: 34.0086
2023-12-02 19:27:56,826:INFO:  Epoch 413/600:  train Loss: 29.3591   val Loss: 35.0422   time: 69.61s   best: 34.0086
2023-12-02 19:29:06,731:INFO:  Epoch 414/600:  train Loss: 29.4758   val Loss: 34.9162   time: 69.90s   best: 34.0086
2023-12-02 19:29:36,634:INFO:  Epoch 92/500:  train Loss: 23.1693   val Loss: 26.8639   time: 239.71s   best: 25.6498
2023-12-02 19:30:16,432:INFO:  Epoch 415/600:  train Loss: 29.3420   val Loss: 35.1032   time: 69.70s   best: 34.0086
2023-12-02 19:31:26,312:INFO:  Epoch 416/600:  train Loss: 29.2893   val Loss: 34.7162   time: 69.88s   best: 34.0086
2023-12-02 19:32:36,145:INFO:  Epoch 417/600:  train Loss: 29.2218   val Loss: 35.4364   time: 69.82s   best: 34.0086
2023-12-02 19:33:36,571:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 19:33:36,593:INFO:  Epoch 93/500:  train Loss: 23.0795   val Loss: 25.5743   time: 239.92s   best: 25.5743
2023-12-02 19:33:46,025:INFO:  Epoch 418/600:  train Loss: 29.2650   val Loss: 34.2376   time: 69.88s   best: 34.0086
2023-12-02 19:34:55,783:INFO:  Epoch 419/600:  train Loss: 29.2420   val Loss: 34.2934   time: 69.74s   best: 34.0086
2023-12-02 19:36:05,456:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 19:36:05,491:INFO:  Epoch 420/600:  train Loss: 29.1311   val Loss: 33.9703   time: 69.66s   best: 33.9703
2023-12-02 19:37:15,466:INFO:  Epoch 421/600:  train Loss: 29.3691   val Loss: 34.6876   time: 69.96s   best: 33.9703
2023-12-02 19:37:36,644:INFO:  Epoch 94/500:  train Loss: 23.1007   val Loss: 25.6475   time: 240.05s   best: 25.5743
2023-12-02 19:38:25,326:INFO:  Epoch 422/600:  train Loss: 29.1342   val Loss: 34.2813   time: 69.85s   best: 33.9703
2023-12-02 19:39:35,001:INFO:  Epoch 423/600:  train Loss: 29.0536   val Loss: 34.5678   time: 69.67s   best: 33.9703
2023-12-02 19:40:44,793:INFO:  Epoch 424/600:  train Loss: 30.6852   val Loss: 35.3801   time: 69.78s   best: 33.9703
2023-12-02 19:41:36,268:INFO:  Epoch 95/500:  train Loss: 23.2499   val Loss: 27.0500   time: 239.62s   best: 25.5743
2023-12-02 19:41:54,514:INFO:  Epoch 425/600:  train Loss: 29.2720   val Loss: 34.1008   time: 69.72s   best: 33.9703
2023-12-02 19:43:04,555:INFO:  Epoch 426/600:  train Loss: 29.0041   val Loss: 34.3040   time: 70.02s   best: 33.9703
2023-12-02 19:44:14,214:INFO:  Epoch 427/600:  train Loss: 28.9893   val Loss: 34.5278   time: 69.66s   best: 33.9703
2023-12-02 19:45:24,032:INFO:  Epoch 428/600:  train Loss: 29.2992   val Loss: 34.7522   time: 69.82s   best: 33.9703
2023-12-02 19:45:36,315:INFO:  Epoch 96/500:  train Loss: 23.1465   val Loss: 25.6642   time: 240.04s   best: 25.5743
2023-12-02 19:46:33,621:INFO:  Epoch 429/600:  train Loss: 29.0737   val Loss: 34.0294   time: 69.58s   best: 33.9703
2023-12-02 19:47:43,414:INFO:  Epoch 430/600:  train Loss: 29.1285   val Loss: 46.6130   time: 69.79s   best: 33.9703
2023-12-02 19:48:53,187:INFO:  Epoch 431/600:  train Loss: 31.7635   val Loss: 35.1898   time: 69.77s   best: 33.9703
2023-12-02 19:49:37,464:INFO:  Epoch 97/500:  train Loss: 22.9592   val Loss: 25.7070   time: 241.15s   best: 25.5743
2023-12-02 19:50:03,055:INFO:  Epoch 432/600:  train Loss: 29.2823   val Loss: 35.8483   time: 69.87s   best: 33.9703
2023-12-02 19:51:12,864:INFO:  Epoch 433/600:  train Loss: 29.1658   val Loss: 34.8207   time: 69.79s   best: 33.9703
2023-12-02 19:52:22,709:INFO:  Epoch 434/600:  train Loss: 29.0182   val Loss: 34.3147   time: 69.84s   best: 33.9703
2023-12-02 19:53:32,474:INFO:  Epoch 435/600:  train Loss: 28.8138   val Loss: 34.3392   time: 69.75s   best: 33.9703
2023-12-02 19:53:37,161:INFO:  Epoch 98/500:  train Loss: 22.8393   val Loss: 26.2101   time: 239.68s   best: 25.5743
2023-12-02 19:54:42,364:INFO:  Epoch 436/600:  train Loss: 28.8796   val Loss: 34.7833   time: 69.89s   best: 33.9703
2023-12-02 19:55:52,081:INFO:  Epoch 437/600:  train Loss: 29.1486   val Loss: 34.4507   time: 69.71s   best: 33.9703
2023-12-02 19:57:01,886:INFO:  Epoch 438/600:  train Loss: 29.0864   val Loss: 34.8501   time: 69.80s   best: 33.9703
2023-12-02 19:57:37,309:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 19:57:37,344:INFO:  Epoch 99/500:  train Loss: 23.1083   val Loss: 25.4416   time: 240.14s   best: 25.4416
2023-12-02 19:58:11,484:INFO:  Epoch 439/600:  train Loss: 29.1215   val Loss: 34.2848   time: 69.60s   best: 33.9703
2023-12-02 19:59:21,544:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 19:59:21,563:INFO:  Epoch 440/600:  train Loss: 28.8377   val Loss: 33.8463   time: 70.04s   best: 33.8463
2023-12-02 20:00:31,211:INFO:  Epoch 441/600:  train Loss: 28.9005   val Loss: 34.6471   time: 69.65s   best: 33.8463
2023-12-02 20:01:38,504:INFO:  Epoch 100/500:  train Loss: 22.9436   val Loss: 25.4520   time: 241.15s   best: 25.4416
2023-12-02 20:01:41,019:INFO:  Epoch 442/600:  train Loss: 28.7956   val Loss: 34.0684   time: 69.81s   best: 33.8463
2023-12-02 20:02:50,712:INFO:  Epoch 443/600:  train Loss: 28.8614   val Loss: 34.9792   time: 69.68s   best: 33.8463
2023-12-02 20:04:00,506:INFO:  Epoch 444/600:  train Loss: 28.8244   val Loss: 34.4551   time: 69.79s   best: 33.8463
2023-12-02 20:05:10,211:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 20:05:10,230:INFO:  Epoch 445/600:  train Loss: 28.9680   val Loss: 33.6273   time: 69.69s   best: 33.6273
2023-12-02 20:05:39,872:INFO:  Epoch 101/500:  train Loss: 22.6896   val Loss: 26.0151   time: 241.35s   best: 25.4416
2023-12-02 20:06:20,032:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 20:06:20,069:INFO:  Epoch 446/600:  train Loss: 28.7409   val Loss: 33.5849   time: 69.80s   best: 33.5849
2023-12-02 20:07:29,787:INFO:  Epoch 447/600:  train Loss: 28.6277   val Loss: 34.2976   time: 69.72s   best: 33.5849
2023-12-02 20:08:39,755:INFO:  Epoch 448/600:  train Loss: 28.7532   val Loss: 33.8527   time: 69.96s   best: 33.5849
2023-12-02 20:09:40,168:INFO:  Epoch 102/500:  train Loss: 22.5823   val Loss: 25.8410   time: 240.28s   best: 25.4416
2023-12-02 20:09:49,464:INFO:  Epoch 449/600:  train Loss: 28.5857   val Loss: 34.3326   time: 69.70s   best: 33.5849
2023-12-02 20:10:59,375:INFO:  Epoch 450/600:  train Loss: 28.7892   val Loss: 34.1332   time: 69.89s   best: 33.5849
2023-12-02 20:12:09,017:INFO:  Epoch 451/600:  train Loss: 28.6600   val Loss: 34.1245   time: 69.64s   best: 33.5849
2023-12-02 20:13:19,010:INFO:  Epoch 452/600:  train Loss: 29.5916   val Loss: 34.1644   time: 69.98s   best: 33.5849
2023-12-02 20:13:40,580:INFO:  Epoch 103/500:  train Loss: 22.7576   val Loss: 26.1403   time: 240.41s   best: 25.4416
2023-12-02 20:14:28,822:INFO:  Epoch 453/600:  train Loss: 28.8984   val Loss: 34.2429   time: 69.80s   best: 33.5849
2023-12-02 20:15:38,865:INFO:  Epoch 454/600:  train Loss: 28.4965   val Loss: 34.1657   time: 70.04s   best: 33.5849
2023-12-02 20:16:48,837:INFO:  Epoch 455/600:  train Loss: 28.5113   val Loss: 34.3344   time: 69.97s   best: 33.5849
2023-12-02 20:17:39,923:INFO:  Epoch 104/500:  train Loss: 22.6081   val Loss: 25.5932   time: 239.33s   best: 25.4416
2023-12-02 20:17:58,901:INFO:  Epoch 456/600:  train Loss: 29.0348   val Loss: 36.0902   time: 70.06s   best: 33.5849
2023-12-02 20:19:08,819:INFO:  Epoch 457/600:  train Loss: 29.1284   val Loss: 35.2915   time: 69.92s   best: 33.5849
2023-12-02 20:20:18,766:INFO:  Epoch 458/600:  train Loss: 28.6106   val Loss: 34.5223   time: 69.93s   best: 33.5849
2023-12-02 20:21:28,413:INFO:  Epoch 459/600:  train Loss: 28.5293   val Loss: 34.4599   time: 69.65s   best: 33.5849
2023-12-02 20:21:41,902:INFO:  Epoch 105/500:  train Loss: 22.7611   val Loss: 25.7415   time: 241.98s   best: 25.4416
2023-12-02 20:22:38,363:INFO:  Epoch 460/600:  train Loss: 28.4642   val Loss: 34.3164   time: 69.95s   best: 33.5849
2023-12-02 20:23:48,144:INFO:  Epoch 461/600:  train Loss: 28.8921   val Loss: 34.3552   time: 69.77s   best: 33.5849
2023-12-02 20:24:57,979:INFO:  Epoch 462/600:  train Loss: 28.5990   val Loss: 34.0177   time: 69.83s   best: 33.5849
2023-12-02 20:25:42,161:INFO:  Epoch 106/500:  train Loss: 22.7810   val Loss: 25.8278   time: 240.26s   best: 25.4416
2023-12-02 20:26:07,721:INFO:  Epoch 463/600:  train Loss: 28.3729   val Loss: 34.0128   time: 69.74s   best: 33.5849
2023-12-02 20:27:17,597:INFO:  Epoch 464/600:  train Loss: 28.4176   val Loss: 34.2874   time: 69.87s   best: 33.5849
2023-12-02 20:28:27,428:INFO:  Epoch 465/600:  train Loss: 28.3944   val Loss: 34.2080   time: 69.82s   best: 33.5849
2023-12-02 20:29:37,324:INFO:  Epoch 466/600:  train Loss: 28.5573   val Loss: 34.9649   time: 69.90s   best: 33.5849
2023-12-02 20:29:42,640:INFO:  Epoch 107/500:  train Loss: 22.5841   val Loss: 25.7605   time: 240.47s   best: 25.4416
2023-12-02 20:30:47,035:INFO:  Epoch 467/600:  train Loss: 28.7589   val Loss: 33.6233   time: 69.70s   best: 33.5849
2023-12-02 20:31:56,894:INFO:  Epoch 468/600:  train Loss: 28.4918   val Loss: 35.0170   time: 69.86s   best: 33.5849
2023-12-02 20:33:06,788:INFO:  Epoch 469/600:  train Loss: 28.4667   val Loss: 33.5987   time: 69.89s   best: 33.5849
2023-12-02 20:33:44,467:INFO:  Epoch 108/500:  train Loss: 22.5377   val Loss: 25.4749   time: 241.82s   best: 25.4416
2023-12-02 20:34:16,998:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 20:34:17,019:INFO:  Epoch 470/600:  train Loss: 28.3949   val Loss: 33.5749   time: 70.21s   best: 33.5749
2023-12-02 20:35:26,977:INFO:  Epoch 471/600:  train Loss: 28.3345   val Loss: 33.9476   time: 69.95s   best: 33.5749
2023-12-02 20:36:36,874:INFO:  Epoch 472/600:  train Loss: 28.2526   val Loss: 34.6062   time: 69.88s   best: 33.5749
2023-12-02 20:37:46,298:INFO:  Epoch 109/500:  train Loss: 22.3049   val Loss: 25.9639   time: 241.83s   best: 25.4416
2023-12-02 20:37:46,533:INFO:  Epoch 473/600:  train Loss: 28.4038   val Loss: 34.1143   time: 69.65s   best: 33.5749
2023-12-02 20:38:56,383:INFO:  Epoch 474/600:  train Loss: 28.3447   val Loss: 33.7397   time: 69.85s   best: 33.5749
2023-12-02 20:40:05,900:INFO:  Epoch 475/600:  train Loss: 28.4839   val Loss: 34.4091   time: 69.51s   best: 33.5749
2023-12-02 20:41:15,749:INFO:  Epoch 476/600:  train Loss: 28.6193   val Loss: 35.7438   time: 69.85s   best: 33.5749
2023-12-02 20:41:47,694:INFO:  Epoch 110/500:  train Loss: 22.3446   val Loss: 26.3340   time: 241.38s   best: 25.4416
2023-12-02 20:42:25,625:INFO:  Epoch 477/600:  train Loss: 28.4211   val Loss: 33.6223   time: 69.87s   best: 33.5749
2023-12-02 20:42:39,738:INFO:  Starting experiment lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2023-12-02 20:42:39,740:INFO:  Defining the model
2023-12-02 20:42:39,844:INFO:  Reading the dataset
2023-12-02 20:43:35,526:INFO:  Epoch 478/600:  train Loss: 28.1455   val Loss: 34.1924   time: 69.90s   best: 33.5749
2023-12-02 20:44:45,316:INFO:  Epoch 479/600:  train Loss: 28.3315   val Loss: 34.3069   time: 69.79s   best: 33.5749
2023-12-02 20:45:47,185:INFO:  Epoch 111/500:  train Loss: 22.3030   val Loss: 25.4803   time: 239.49s   best: 25.4416
2023-12-02 20:45:55,070:INFO:  Epoch 480/600:  train Loss: 28.1241   val Loss: 33.8390   time: 69.75s   best: 33.5749
2023-12-02 20:47:04,813:INFO:  Epoch 481/600:  train Loss: 28.3669   val Loss: 34.3710   time: 69.74s   best: 33.5749
2023-12-02 20:48:13,878:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 20:48:13,898:INFO:  Epoch 1/800:  train Loss: 90.8186   val Loss: 88.0849   time: 70.68s   best: 88.0849
2023-12-02 20:48:14,518:INFO:  Epoch 482/600:  train Loss: 28.3880   val Loss: 33.7502   time: 69.69s   best: 33.5749
2023-12-02 20:49:22,779:INFO:  Epoch 2/800:  train Loss: 88.9894   val Loss: 89.7213   time: 68.88s   best: 88.0849
2023-12-02 20:49:24,372:INFO:  Epoch 483/600:  train Loss: 28.2428   val Loss: 34.0782   time: 69.85s   best: 33.5749
2023-12-02 20:49:47,012:INFO:  Epoch 112/500:  train Loss: 22.2091   val Loss: 26.0391   time: 239.81s   best: 25.4416
2023-12-02 20:50:31,667:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 20:50:31,688:INFO:  Epoch 3/800:  train Loss: 87.6278   val Loss: 85.8057   time: 68.87s   best: 85.8057
2023-12-02 20:50:34,106:INFO:  Epoch 484/600:  train Loss: 28.6587   val Loss: 34.8302   time: 69.73s   best: 33.5749
2023-12-02 20:51:41,337:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 20:51:41,374:INFO:  Epoch 4/800:  train Loss: 85.9076   val Loss: 85.4208   time: 69.65s   best: 85.4208
2023-12-02 20:51:43,646:INFO:  Epoch 485/600:  train Loss: 28.2141   val Loss: 33.7726   time: 69.54s   best: 33.5749
2023-12-02 20:52:50,717:INFO:  Epoch 5/800:  train Loss: 85.1298   val Loss: 85.7598   time: 69.33s   best: 85.4208
2023-12-02 20:52:53,405:INFO:  Epoch 486/600:  train Loss: 29.8898   val Loss: 34.3492   time: 69.74s   best: 33.5749
2023-12-02 20:53:48,144:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 20:53:48,164:INFO:  Epoch 113/500:  train Loss: 22.2418   val Loss: 25.0922   time: 241.09s   best: 25.0922
2023-12-02 20:53:59,936:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 20:53:59,956:INFO:  Epoch 6/800:  train Loss: 84.5257   val Loss: 83.5234   time: 69.21s   best: 83.5234
2023-12-02 20:54:03,111:INFO:  Epoch 487/600:  train Loss: 28.7477   val Loss: 33.8497   time: 69.70s   best: 33.5749
2023-12-02 20:55:09,234:INFO:  Epoch 7/800:  train Loss: 83.0885   val Loss: 84.4524   time: 69.28s   best: 83.5234
2023-12-02 20:55:12,878:INFO:  Epoch 488/600:  train Loss: 28.0760   val Loss: 34.2174   time: 69.77s   best: 33.5749
2023-12-02 20:56:18,511:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 20:56:18,531:INFO:  Epoch 8/800:  train Loss: 80.3237   val Loss: 79.3597   time: 69.26s   best: 79.3597
2023-12-02 20:56:22,773:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 20:56:22,793:INFO:  Epoch 489/600:  train Loss: 28.0265   val Loss: 33.5624   time: 69.89s   best: 33.5624
2023-12-02 20:57:27,902:INFO:  Epoch 9/800:  train Loss: 77.4326   val Loss: 79.5167   time: 69.37s   best: 79.3597
2023-12-02 20:57:32,864:INFO:  Epoch 490/600:  train Loss: 28.1100   val Loss: 34.1885   time: 70.06s   best: 33.5624
2023-12-02 20:57:47,930:INFO:  Epoch 114/500:  train Loss: 22.3494   val Loss: 25.1142   time: 239.75s   best: 25.0922
2023-12-02 20:58:37,320:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 20:58:37,340:INFO:  Epoch 10/800:  train Loss: 76.7320   val Loss: 78.8034   time: 69.41s   best: 78.8034
2023-12-02 20:58:42,555:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 20:58:42,576:INFO:  Epoch 491/600:  train Loss: 28.0513   val Loss: 32.9171   time: 69.68s   best: 32.9171
2023-12-02 20:59:46,510:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 20:59:46,530:INFO:  Epoch 11/800:  train Loss: 75.1839   val Loss: 75.3772   time: 69.15s   best: 75.3772
2023-12-02 20:59:52,647:INFO:  Epoch 492/600:  train Loss: 28.3787   val Loss: 34.7126   time: 70.07s   best: 32.9171
2023-12-02 21:00:55,861:INFO:  Epoch 12/800:  train Loss: 74.1113   val Loss: 76.0387   time: 69.33s   best: 75.3772
2023-12-02 21:01:02,650:INFO:  Epoch 493/600:  train Loss: 28.1258   val Loss: 33.4905   time: 70.00s   best: 32.9171
2023-12-02 21:01:49,449:INFO:  Epoch 115/500:  train Loss: 22.4120   val Loss: 25.6893   time: 241.49s   best: 25.0922
2023-12-02 21:02:04,898:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:02:04,919:INFO:  Epoch 13/800:  train Loss: 73.3247   val Loss: 73.5289   time: 69.01s   best: 73.5289
2023-12-02 21:02:12,465:INFO:  Epoch 494/600:  train Loss: 28.3985   val Loss: 33.4842   time: 69.80s   best: 32.9171
2023-12-02 21:03:13,919:INFO:  Epoch 14/800:  train Loss: 72.4103   val Loss: 74.9067   time: 69.00s   best: 73.5289
2023-12-02 21:03:22,369:INFO:  Epoch 495/600:  train Loss: 28.0318   val Loss: 33.2716   time: 69.89s   best: 32.9171
2023-12-02 21:04:23,088:INFO:  Epoch 15/800:  train Loss: 71.6036   val Loss: 74.8564   time: 69.15s   best: 73.5289
2023-12-02 21:04:32,341:INFO:  Epoch 496/600:  train Loss: 28.1598   val Loss: 34.0763   time: 69.97s   best: 32.9171
2023-12-02 21:05:32,082:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:05:32,103:INFO:  Epoch 16/800:  train Loss: 71.0418   val Loss: 71.9647   time: 68.98s   best: 71.9647
2023-12-02 21:05:42,123:INFO:  Epoch 497/600:  train Loss: 28.0683   val Loss: 33.9893   time: 69.78s   best: 32.9171
2023-12-02 21:05:48,731:INFO:  Epoch 116/500:  train Loss: 22.1276   val Loss: 25.5417   time: 239.27s   best: 25.0922
2023-12-02 21:06:41,524:INFO:  Epoch 17/800:  train Loss: 70.2529   val Loss: 73.1924   time: 69.41s   best: 71.9647
2023-12-02 21:06:51,935:INFO:  Epoch 498/600:  train Loss: 27.9589   val Loss: 33.8803   time: 69.81s   best: 32.9171
2023-12-02 21:07:50,716:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:07:50,751:INFO:  Epoch 18/800:  train Loss: 69.8011   val Loss: 71.6447   time: 69.19s   best: 71.6447
2023-12-02 21:08:01,592:INFO:  Epoch 499/600:  train Loss: 28.3582   val Loss: 33.8099   time: 69.65s   best: 32.9171
2023-12-02 21:09:00,128:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:09:00,149:INFO:  Epoch 19/800:  train Loss: 69.1958   val Loss: 70.1430   time: 69.37s   best: 70.1430
2023-12-02 21:09:11,550:INFO:  Epoch 500/600:  train Loss: 28.5355   val Loss: 34.7189   time: 69.94s   best: 32.9171
2023-12-02 21:09:48,262:INFO:  Epoch 117/500:  train Loss: 22.3240   val Loss: 25.1932   time: 239.53s   best: 25.0922
2023-12-02 21:10:09,484:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:10:09,504:INFO:  Epoch 20/800:  train Loss: 68.6571   val Loss: 68.4306   time: 69.31s   best: 68.4306
2023-12-02 21:10:21,533:INFO:  Epoch 501/600:  train Loss: 28.3897   val Loss: 33.3916   time: 69.98s   best: 32.9171
2023-12-02 21:11:19,045:INFO:  Epoch 21/800:  train Loss: 68.1111   val Loss: 69.0972   time: 69.54s   best: 68.4306
2023-12-02 21:11:31,603:INFO:  Epoch 502/600:  train Loss: 27.8545   val Loss: 33.5485   time: 70.07s   best: 32.9171
2023-12-02 21:12:28,469:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:12:28,497:INFO:  Epoch 22/800:  train Loss: 67.6185   val Loss: 68.1452   time: 69.38s   best: 68.1452
2023-12-02 21:12:41,384:INFO:  Epoch 503/600:  train Loss: 27.8393   val Loss: 34.1651   time: 69.77s   best: 32.9171
2023-12-02 21:13:38,004:INFO:  Epoch 23/800:  train Loss: 67.3368   val Loss: 68.3213   time: 69.51s   best: 68.1452
2023-12-02 21:13:47,830:INFO:  Epoch 118/500:  train Loss: 22.3296   val Loss: 26.2637   time: 239.54s   best: 25.0922
2023-12-02 21:13:51,272:INFO:  Epoch 504/600:  train Loss: 28.4279   val Loss: 34.2226   time: 69.89s   best: 32.9171
2023-12-02 21:14:47,646:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:14:47,667:INFO:  Epoch 24/800:  train Loss: 67.2356   val Loss: 67.3341   time: 69.62s   best: 67.3341
2023-12-02 21:15:01,084:INFO:  Epoch 505/600:  train Loss: 28.0598   val Loss: 33.8685   time: 69.81s   best: 32.9171
2023-12-02 21:15:57,233:INFO:  Epoch 25/800:  train Loss: 66.7019   val Loss: 67.4827   time: 69.55s   best: 67.3341
2023-12-02 21:16:10,849:INFO:  Epoch 506/600:  train Loss: 27.8169   val Loss: 33.8174   time: 69.75s   best: 32.9171
2023-12-02 21:17:06,747:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:17:06,768:INFO:  Epoch 26/800:  train Loss: 66.3088   val Loss: 67.1147   time: 69.51s   best: 67.1147
2023-12-02 21:17:20,600:INFO:  Epoch 507/600:  train Loss: 27.7703   val Loss: 34.4508   time: 69.74s   best: 32.9171
2023-12-02 21:17:48,718:INFO:  Epoch 119/500:  train Loss: 22.2732   val Loss: 25.5238   time: 240.88s   best: 25.0922
2023-12-02 21:18:16,494:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:18:16,515:INFO:  Epoch 27/800:  train Loss: 65.9738   val Loss: 66.6076   time: 69.72s   best: 66.6076
2023-12-02 21:18:30,523:INFO:  Epoch 508/600:  train Loss: 28.0104   val Loss: 33.5893   time: 69.91s   best: 32.9171
2023-12-02 21:19:26,119:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:19:26,153:INFO:  Epoch 28/800:  train Loss: 65.4283   val Loss: 66.0056   time: 69.59s   best: 66.0056
2023-12-02 21:19:40,617:INFO:  Epoch 509/600:  train Loss: 27.8130   val Loss: 33.8197   time: 70.09s   best: 32.9171
2023-12-02 21:20:35,769:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:20:35,790:INFO:  Epoch 29/800:  train Loss: 65.2509   val Loss: 64.8482   time: 69.61s   best: 64.8482
2023-12-02 21:20:50,511:INFO:  Epoch 510/600:  train Loss: 28.1922   val Loss: 35.1151   time: 69.89s   best: 32.9171
2023-12-02 21:21:45,203:INFO:  Epoch 30/800:  train Loss: 65.0432   val Loss: 65.5917   time: 69.40s   best: 64.8482
2023-12-02 21:21:48,395:INFO:  Epoch 120/500:  train Loss: 22.0362   val Loss: 25.1987   time: 239.67s   best: 25.0922
2023-12-02 21:22:00,709:INFO:  Epoch 511/600:  train Loss: 27.8981   val Loss: 33.2020   time: 70.17s   best: 32.9171
2023-12-02 21:22:54,674:INFO:  Epoch 31/800:  train Loss: 64.6080   val Loss: 64.9371   time: 69.47s   best: 64.8482
2023-12-02 21:23:10,898:INFO:  Epoch 512/600:  train Loss: 28.9598   val Loss: 35.2772   time: 70.17s   best: 32.9171
2023-12-02 21:24:04,278:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:24:04,299:INFO:  Epoch 32/800:  train Loss: 64.1888   val Loss: 64.6874   time: 69.60s   best: 64.6874
2023-12-02 21:24:20,613:INFO:  Epoch 513/600:  train Loss: 28.6495   val Loss: 33.3915   time: 69.71s   best: 32.9171
2023-12-02 21:25:13,704:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:25:13,726:INFO:  Epoch 33/800:  train Loss: 63.9746   val Loss: 64.4870   time: 69.40s   best: 64.4870
2023-12-02 21:25:30,353:INFO:  Epoch 514/600:  train Loss: 27.8048   val Loss: 33.2182   time: 69.74s   best: 32.9171
2023-12-02 21:25:48,269:INFO:  Epoch 121/500:  train Loss: 21.9818   val Loss: 25.2257   time: 239.87s   best: 25.0922
2023-12-02 21:26:23,082:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:26:23,103:INFO:  Epoch 34/800:  train Loss: 63.9325   val Loss: 64.2587   time: 69.34s   best: 64.2587
2023-12-02 21:26:40,151:INFO:  Epoch 515/600:  train Loss: 28.3020   val Loss: 40.7972   time: 69.80s   best: 32.9171
2023-12-02 21:27:32,680:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:27:32,700:INFO:  Epoch 35/800:  train Loss: 63.6009   val Loss: 63.5522   time: 69.57s   best: 63.5522
2023-12-02 21:27:49,824:INFO:  Epoch 516/600:  train Loss: 28.3452   val Loss: 33.3979   time: 69.67s   best: 32.9171
2023-12-02 21:28:42,249:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:28:42,270:INFO:  Epoch 36/800:  train Loss: 63.1436   val Loss: 63.1329   time: 69.53s   best: 63.1329
2023-12-02 21:28:59,538:INFO:  Epoch 517/600:  train Loss: 27.9763   val Loss: 33.3668   time: 69.70s   best: 32.9171
2023-12-02 21:29:49,005:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-02 21:29:49,027:INFO:  Epoch 122/500:  train Loss: 21.9845   val Loss: 24.9069   time: 240.73s   best: 24.9069
2023-12-02 21:29:51,672:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:29:51,694:INFO:  Epoch 37/800:  train Loss: 62.9889   val Loss: 62.8646   time: 69.40s   best: 62.8646
2023-12-02 21:30:09,418:INFO:  Epoch 518/600:  train Loss: 27.6872   val Loss: 33.1848   time: 69.88s   best: 32.9171
2023-12-02 21:31:01,318:INFO:  Epoch 38/800:  train Loss: 62.6966   val Loss: 63.9846   time: 69.61s   best: 62.8646
2023-12-02 21:31:19,255:INFO:  Epoch 519/600:  train Loss: 27.5900   val Loss: 33.3539   time: 69.82s   best: 32.9171
2023-12-02 21:32:10,704:INFO:  Epoch 39/800:  train Loss: 62.4749   val Loss: 64.3789   time: 69.38s   best: 62.8646
2023-12-02 21:32:29,016:INFO:  Epoch 520/600:  train Loss: 27.7883   val Loss: 33.7411   time: 69.76s   best: 32.9171
2023-12-02 21:33:20,536:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:33:20,594:INFO:  Epoch 40/800:  train Loss: 62.0818   val Loss: 62.0474   time: 69.83s   best: 62.0474
2023-12-02 21:33:38,763:INFO:  Epoch 521/600:  train Loss: 28.4761   val Loss: 33.9102   time: 69.74s   best: 32.9171
2023-12-02 21:33:49,631:INFO:  Epoch 123/500:  train Loss: 21.9493   val Loss: 25.3794   time: 240.59s   best: 24.9069
2023-12-02 21:34:30,116:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:34:30,137:INFO:  Epoch 41/800:  train Loss: 61.7556   val Loss: 61.4939   time: 69.51s   best: 61.4939
2023-12-02 21:34:48,575:INFO:  Epoch 522/600:  train Loss: 27.6966   val Loss: 33.0488   time: 69.81s   best: 32.9171
2023-12-02 21:35:39,758:INFO:  Epoch 42/800:  train Loss: 61.4084   val Loss: 61.6675   time: 69.61s   best: 61.4939
2023-12-02 21:35:58,204:INFO:  Epoch 523/600:  train Loss: 27.5640   val Loss: 33.1793   time: 69.62s   best: 32.9171
2023-12-02 21:36:49,742:INFO:  Epoch 43/800:  train Loss: 61.4241   val Loss: 62.0820   time: 69.98s   best: 61.4939
2023-12-02 21:37:07,996:INFO:  Epoch 524/600:  train Loss: 27.7068   val Loss: 33.6016   time: 69.79s   best: 32.9171
2023-12-02 21:37:50,091:INFO:  Epoch 124/500:  train Loss: 22.0072   val Loss: 25.3908   time: 240.45s   best: 24.9069
2023-12-02 21:37:59,367:INFO:  Epoch 44/800:  train Loss: 60.9577   val Loss: 62.1186   time: 69.62s   best: 61.4939
2023-12-02 21:38:17,649:INFO:  Epoch 525/600:  train Loss: 28.0522   val Loss: 33.6807   time: 69.65s   best: 32.9171
2023-12-02 21:39:09,192:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:39:09,229:INFO:  Epoch 45/800:  train Loss: 60.9546   val Loss: 61.1303   time: 69.81s   best: 61.1303
2023-12-02 21:39:27,541:INFO:  Epoch 526/600:  train Loss: 27.7039   val Loss: 33.8099   time: 69.88s   best: 32.9171
2023-12-02 21:40:18,683:INFO:  Epoch 46/800:  train Loss: 60.6659   val Loss: 63.1153   time: 69.44s   best: 61.1303
2023-12-02 21:40:37,454:INFO:  Epoch 527/600:  train Loss: 27.6927   val Loss: 33.9197   time: 69.91s   best: 32.9171
2023-12-02 21:41:28,095:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:41:28,116:INFO:  Epoch 47/800:  train Loss: 60.5668   val Loss: 60.4421   time: 69.41s   best: 60.4421
2023-12-02 21:41:47,300:INFO:  Epoch 528/600:  train Loss: 27.6344   val Loss: 33.5734   time: 69.83s   best: 32.9171
2023-12-02 21:41:51,718:INFO:  Epoch 125/500:  train Loss: 21.9596   val Loss: 25.8442   time: 241.61s   best: 24.9069
2023-12-02 21:42:37,503:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:42:37,536:INFO:  Epoch 48/800:  train Loss: 59.9823   val Loss: 60.2761   time: 69.36s   best: 60.2761
2023-12-02 21:42:57,033:INFO:  Epoch 529/600:  train Loss: 27.5635   val Loss: 34.0572   time: 69.73s   best: 32.9171
2023-12-02 21:43:46,999:INFO:  Epoch 49/800:  train Loss: 59.9430   val Loss: 60.8712   time: 69.45s   best: 60.2761
2023-12-02 21:44:06,765:INFO:  Epoch 530/600:  train Loss: 27.4925   val Loss: 33.0371   time: 69.73s   best: 32.9171
2023-12-02 21:44:56,585:INFO:  Epoch 50/800:  train Loss: 59.6129   val Loss: 62.1571   time: 69.57s   best: 60.2761
2023-12-02 21:45:16,539:INFO:  Epoch 531/600:  train Loss: 27.6991   val Loss: 34.2454   time: 69.77s   best: 32.9171
2023-12-02 21:45:51,168:INFO:  Epoch 126/500:  train Loss: 21.8209   val Loss: 25.2292   time: 239.45s   best: 24.9069
2023-12-02 21:46:06,094:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:46:06,115:INFO:  Epoch 51/800:  train Loss: 59.7016   val Loss: 59.5024   time: 69.50s   best: 59.5024
2023-12-02 21:46:26,296:INFO:  Epoch 532/600:  train Loss: 27.6990   val Loss: 33.0806   time: 69.74s   best: 32.9171
2023-12-02 21:47:16,076:INFO:  Epoch 52/800:  train Loss: 58.9905   val Loss: 60.0576   time: 69.95s   best: 59.5024
2023-12-02 21:47:36,034:INFO:  Epoch 533/600:  train Loss: 27.4237   val Loss: 33.0114   time: 69.73s   best: 32.9171
2023-12-02 21:48:25,841:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:48:25,862:INFO:  Epoch 53/800:  train Loss: 58.8495   val Loss: 59.1080   time: 69.76s   best: 59.1080
2023-12-02 21:48:45,926:INFO:  Epoch 534/600:  train Loss: 27.4417   val Loss: 33.2156   time: 69.88s   best: 32.9171
2023-12-02 21:49:35,372:INFO:  Epoch 54/800:  train Loss: 58.8171   val Loss: 59.6155   time: 69.51s   best: 59.1080
2023-12-02 21:49:51,549:INFO:  Epoch 127/500:  train Loss: 21.8603   val Loss: 25.3891   time: 240.38s   best: 24.9069
2023-12-02 21:49:55,744:INFO:  Epoch 535/600:  train Loss: 27.5520   val Loss: 36.3278   time: 69.81s   best: 32.9171
2023-12-02 21:50:45,182:INFO:  Epoch 55/800:  train Loss: 58.8981   val Loss: 59.1444   time: 69.79s   best: 59.1080
2023-12-02 21:51:05,546:INFO:  Epoch 536/600:  train Loss: 27.7073   val Loss: 33.8436   time: 69.77s   best: 32.9171
2023-12-02 21:51:54,705:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:51:54,737:INFO:  Epoch 56/800:  train Loss: 58.3081   val Loss: 58.8611   time: 69.50s   best: 58.8611
2023-12-02 21:52:15,196:INFO:  Epoch 537/600:  train Loss: 27.4317   val Loss: 33.4545   time: 69.64s   best: 32.9171
2023-12-02 21:53:04,260:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:53:04,282:INFO:  Epoch 57/800:  train Loss: 57.9005   val Loss: 58.0825   time: 69.51s   best: 58.0825
2023-12-02 21:53:25,185:INFO:  Epoch 538/600:  train Loss: 27.4157   val Loss: 34.0044   time: 69.99s   best: 32.9171
2023-12-02 21:53:52,889:INFO:  Epoch 128/500:  train Loss: 21.7982   val Loss: 25.1499   time: 241.33s   best: 24.9069
2023-12-02 21:54:13,873:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:54:13,927:INFO:  Epoch 58/800:  train Loss: 57.6240   val Loss: 58.0067   time: 69.58s   best: 58.0067
2023-12-02 21:54:34,925:INFO:  Epoch 539/600:  train Loss: 27.5358   val Loss: 34.5887   time: 69.74s   best: 32.9171
2023-12-02 21:55:23,355:INFO:  Epoch 59/800:  train Loss: 57.4673   val Loss: 59.1713   time: 69.43s   best: 58.0067
2023-12-02 21:55:44,914:INFO:  Epoch 540/600:  train Loss: 27.7038   val Loss: 34.8741   time: 69.98s   best: 32.9171
2023-12-02 21:56:32,718:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:56:32,739:INFO:  Epoch 60/800:  train Loss: 57.0438   val Loss: 57.4985   time: 69.36s   best: 57.4985
2023-12-02 21:56:54,646:INFO:  Epoch 541/600:  train Loss: 28.6129   val Loss: 34.3240   time: 69.73s   best: 32.9171
2023-12-02 21:57:42,404:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 21:57:42,425:INFO:  Epoch 61/800:  train Loss: 56.8851   val Loss: 57.3817   time: 69.66s   best: 57.3817
2023-12-02 21:57:52,594:INFO:  Epoch 129/500:  train Loss: 21.8049   val Loss: 25.5318   time: 239.70s   best: 24.9069
2023-12-02 21:58:04,487:INFO:  Epoch 542/600:  train Loss: 27.4753   val Loss: 33.8576   time: 69.83s   best: 32.9171
2023-12-02 21:58:51,903:INFO:  Epoch 62/800:  train Loss: 56.6795   val Loss: 58.7762   time: 69.48s   best: 57.3817
2023-12-02 21:59:14,493:INFO:  Epoch 543/600:  train Loss: 27.3489   val Loss: 33.0104   time: 70.00s   best: 32.9171
2023-12-02 22:00:01,325:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:00:01,359:INFO:  Epoch 63/800:  train Loss: 56.5008   val Loss: 56.8413   time: 69.40s   best: 56.8413
2023-12-02 22:00:24,690:INFO:  Epoch 544/600:  train Loss: 27.4993   val Loss: 33.5817   time: 70.19s   best: 32.9171
2023-12-02 22:01:11,102:INFO:  Epoch 64/800:  train Loss: 56.4078   val Loss: 56.9579   time: 69.73s   best: 56.8413
2023-12-02 22:01:34,507:INFO:  Epoch 545/600:  train Loss: 27.6540   val Loss: 34.0135   time: 69.81s   best: 32.9171
2023-12-02 22:01:53,719:INFO:  Epoch 130/500:  train Loss: 21.7049   val Loss: 25.5088   time: 241.11s   best: 24.9069
2023-12-02 22:02:20,690:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:02:20,711:INFO:  Epoch 65/800:  train Loss: 55.9904   val Loss: 56.3697   time: 69.58s   best: 56.3697
2023-12-02 22:02:44,423:INFO:  Epoch 546/600:  train Loss: 28.0539   val Loss: 35.5513   time: 69.91s   best: 32.9171
2023-12-02 22:03:30,300:INFO:  Epoch 66/800:  train Loss: 55.7219   val Loss: 56.5789   time: 69.58s   best: 56.3697
2023-12-02 22:03:54,231:INFO:  Epoch 547/600:  train Loss: 27.6965   val Loss: 33.2692   time: 69.81s   best: 32.9171
2023-12-02 22:04:39,856:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:04:39,877:INFO:  Epoch 67/800:  train Loss: 55.6335   val Loss: 56.1566   time: 69.54s   best: 56.1566
2023-12-02 22:05:04,134:INFO:  Epoch 548/600:  train Loss: 27.4940   val Loss: 35.4677   time: 69.90s   best: 32.9171
2023-12-02 22:05:49,281:INFO:  Epoch 68/800:  train Loss: 55.3772   val Loss: 56.4861   time: 69.40s   best: 56.1566
2023-12-02 22:05:54,594:INFO:  Epoch 131/500:  train Loss: 21.8714   val Loss: 25.1365   time: 240.87s   best: 24.9069
2023-12-02 22:06:13,820:INFO:  Epoch 549/600:  train Loss: 27.3863   val Loss: 33.7318   time: 69.68s   best: 32.9171
2023-12-02 22:06:58,929:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:06:58,951:INFO:  Epoch 69/800:  train Loss: 55.1172   val Loss: 55.8480   time: 69.63s   best: 55.8480
2023-12-02 22:07:23,674:INFO:  Epoch 550/600:  train Loss: 27.2914   val Loss: 33.3445   time: 69.85s   best: 32.9171
2023-12-02 22:08:08,475:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:08:08,497:INFO:  Epoch 70/800:  train Loss: 54.7573   val Loss: 55.7590   time: 69.52s   best: 55.7590
2023-12-02 22:08:33,391:INFO:  Epoch 551/600:  train Loss: 27.2217   val Loss: 33.6668   time: 69.70s   best: 32.9171
2023-12-02 22:09:18,696:INFO:  Epoch 71/800:  train Loss: 54.9376   val Loss: 60.1208   time: 70.19s   best: 55.7590
2023-12-02 22:09:43,221:INFO:  Epoch 552/600:  train Loss: 27.2602   val Loss: 33.2365   time: 69.83s   best: 32.9171
2023-12-02 22:09:54,527:INFO:  Epoch 132/500:  train Loss: 21.8117   val Loss: 26.2268   time: 239.92s   best: 24.9069
2023-12-02 22:10:28,340:INFO:  Epoch 72/800:  train Loss: 55.0551   val Loss: 56.0651   time: 69.64s   best: 55.7590
2023-12-02 22:10:52,884:INFO:  Epoch 553/600:  train Loss: 27.2674   val Loss: 33.3255   time: 69.66s   best: 32.9171
2023-12-02 22:11:38,388:INFO:  Epoch 73/800:  train Loss: 54.3402   val Loss: 55.7728   time: 70.03s   best: 55.7590
2023-12-02 22:12:02,705:INFO:  Epoch 554/600:  train Loss: 27.3116   val Loss: 33.8547   time: 69.82s   best: 32.9171
2023-12-02 22:12:48,417:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:12:48,438:INFO:  Epoch 74/800:  train Loss: 54.2138   val Loss: 55.5638   time: 70.01s   best: 55.5638
2023-12-02 22:13:12,383:INFO:  Epoch 555/600:  train Loss: 27.3892   val Loss: 33.4907   time: 69.68s   best: 32.9171
2023-12-02 22:13:55,738:INFO:  Epoch 133/500:  train Loss: 21.6247   val Loss: 24.9737   time: 241.21s   best: 24.9069
2023-12-02 22:13:58,316:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:13:58,339:INFO:  Epoch 75/800:  train Loss: 53.8642   val Loss: 54.6351   time: 69.86s   best: 54.6351
2023-12-02 22:14:22,186:INFO:  Epoch 556/600:  train Loss: 27.3290   val Loss: 32.9187   time: 69.78s   best: 32.9171
2023-12-02 22:15:08,111:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:15:08,132:INFO:  Epoch 76/800:  train Loss: 53.8188   val Loss: 54.5129   time: 69.77s   best: 54.5129
2023-12-02 22:15:31,975:INFO:  Epoch 557/600:  train Loss: 27.3298   val Loss: 33.4073   time: 69.79s   best: 32.9171
2023-12-02 22:16:17,908:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:16:17,930:INFO:  Epoch 77/800:  train Loss: 53.4560   val Loss: 54.3094   time: 69.77s   best: 54.3094
2023-12-02 22:16:41,816:INFO:  Epoch 558/600:  train Loss: 27.6690   val Loss: 33.5787   time: 69.84s   best: 32.9171
2023-12-02 22:17:27,394:INFO:  Epoch 78/800:  train Loss: 53.2612   val Loss: 54.5177   time: 69.46s   best: 54.3094
2023-12-02 22:17:51,426:INFO:  Epoch 559/600:  train Loss: 27.1492   val Loss: 33.4458   time: 69.61s   best: 32.9171
2023-12-02 22:17:55,379:INFO:  Epoch 134/500:  train Loss: 21.6133   val Loss: 25.2566   time: 239.63s   best: 24.9069
2023-12-02 22:18:36,799:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:18:36,820:INFO:  Epoch 79/800:  train Loss: 53.1108   val Loss: 54.0283   time: 69.39s   best: 54.0283
2023-12-02 22:19:01,204:INFO:  Epoch 560/600:  train Loss: 28.1849   val Loss: 35.7175   time: 69.76s   best: 32.9171
2023-12-02 22:19:46,122:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:19:46,144:INFO:  Epoch 80/800:  train Loss: 53.0395   val Loss: 53.7795   time: 69.30s   best: 53.7795
2023-12-02 22:20:11,116:INFO:  Epoch 561/600:  train Loss: 27.5870   val Loss: 33.9812   time: 69.91s   best: 32.9171
2023-12-02 22:20:55,694:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:20:55,715:INFO:  Epoch 81/800:  train Loss: 52.5152   val Loss: 53.1056   time: 69.53s   best: 53.1056
2023-12-02 22:21:20,898:INFO:  Epoch 562/600:  train Loss: 27.1845   val Loss: 32.9535   time: 69.76s   best: 32.9171
2023-12-02 22:21:56,140:INFO:  Epoch 135/500:  train Loss: 21.4988   val Loss: 25.2243   time: 240.75s   best: 24.9069
2023-12-02 22:22:05,437:INFO:  Epoch 82/800:  train Loss: 52.6925   val Loss: 53.9808   time: 69.72s   best: 53.1056
2023-12-02 22:22:30,469:INFO:  Epoch 563/600:  train Loss: 27.2784   val Loss: 34.6993   time: 69.56s   best: 32.9171
2023-12-02 22:23:14,914:INFO:  Epoch 83/800:  train Loss: 52.2217   val Loss: 53.1397   time: 69.47s   best: 53.1056
2023-12-02 22:23:40,248:INFO:  Epoch 564/600:  train Loss: 27.2690   val Loss: 33.3009   time: 69.77s   best: 32.9171
2023-12-02 22:24:24,212:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:24:24,233:INFO:  Epoch 84/800:  train Loss: 51.7920   val Loss: 52.6910   time: 69.29s   best: 52.6910
2023-12-02 22:24:49,984:INFO:  Epoch 565/600:  train Loss: 27.0682   val Loss: 33.0889   time: 69.72s   best: 32.9171
2023-12-02 22:25:33,711:INFO:  Epoch 85/800:  train Loss: 51.5475   val Loss: 53.3366   time: 69.48s   best: 52.6910
2023-12-02 22:25:57,103:INFO:  Epoch 136/500:  train Loss: 21.8649   val Loss: 25.2421   time: 240.96s   best: 24.9069
2023-12-02 22:25:59,769:INFO:  Epoch 566/600:  train Loss: 27.5507   val Loss: 34.6763   time: 69.78s   best: 32.9171
2023-12-02 22:26:42,968:INFO:  Epoch 86/800:  train Loss: 51.4079   val Loss: 53.7991   time: 69.24s   best: 52.6910
2023-12-02 22:27:09,571:INFO:  Epoch 567/600:  train Loss: 27.3856   val Loss: 34.6386   time: 69.79s   best: 32.9171
2023-12-02 22:27:52,275:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:27:52,306:INFO:  Epoch 87/800:  train Loss: 51.4147   val Loss: 52.6877   time: 69.30s   best: 52.6877
2023-12-02 22:28:19,288:INFO:  Epoch 568/600:  train Loss: 27.1316   val Loss: 33.2424   time: 69.70s   best: 32.9171
2023-12-02 22:29:01,761:INFO:  Epoch 88/800:  train Loss: 50.9181   val Loss: 53.4439   time: 69.44s   best: 52.6877
2023-12-02 22:29:29,110:INFO:  Epoch 569/600:  train Loss: 28.0627   val Loss: 34.2822   time: 69.81s   best: 32.9171
2023-12-02 22:29:58,407:INFO:  Epoch 137/500:  train Loss: 21.4945   val Loss: 26.0378   time: 241.29s   best: 24.9069
2023-12-02 22:30:11,055:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:30:11,076:INFO:  Epoch 89/800:  train Loss: 51.5053   val Loss: 52.6546   time: 69.28s   best: 52.6546
2023-12-02 22:30:39,034:INFO:  Epoch 570/600:  train Loss: 27.3966   val Loss: 33.8044   time: 69.92s   best: 32.9171
2023-12-02 22:31:20,451:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:31:20,472:INFO:  Epoch 90/800:  train Loss: 50.7370   val Loss: 51.4017   time: 69.35s   best: 51.4017
2023-12-02 22:31:48,606:INFO:  Epoch 571/600:  train Loss: 27.0218   val Loss: 33.7661   time: 69.57s   best: 32.9171
2023-12-02 22:32:29,817:INFO:  Epoch 91/800:  train Loss: 50.3305   val Loss: 51.9594   time: 69.34s   best: 51.4017
2023-12-02 22:32:58,324:INFO:  Epoch 572/600:  train Loss: 27.1962   val Loss: 33.3349   time: 69.72s   best: 32.9171
2023-12-02 22:33:39,068:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:33:39,089:INFO:  Epoch 92/800:  train Loss: 50.0513   val Loss: 51.3384   time: 69.24s   best: 51.3384
2023-12-02 22:33:57,514:INFO:  Epoch 138/500:  train Loss: 21.4886   val Loss: 25.1923   time: 239.10s   best: 24.9069
2023-12-02 22:34:08,260:INFO:  Epoch 573/600:  train Loss: 27.1017   val Loss: 33.2821   time: 69.93s   best: 32.9171
2023-12-02 22:34:48,545:INFO:  Epoch 93/800:  train Loss: 49.7017   val Loss: 51.5938   time: 69.46s   best: 51.3384
2023-12-02 22:35:18,086:INFO:  Epoch 574/600:  train Loss: 27.0474   val Loss: 33.8883   time: 69.82s   best: 32.9171
2023-12-02 22:35:57,956:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:35:57,977:INFO:  Epoch 94/800:  train Loss: 49.5119   val Loss: 50.8458   time: 69.39s   best: 50.8458
2023-12-02 22:36:27,764:INFO:  Epoch 575/600:  train Loss: 26.9834   val Loss: 33.3945   time: 69.66s   best: 32.9171
2023-12-02 22:37:07,548:INFO:  Epoch 95/800:  train Loss: 49.2674   val Loss: 50.9105   time: 69.56s   best: 50.8458
2023-12-02 22:37:37,485:INFO:  Epoch 576/600:  train Loss: 27.1013   val Loss: 32.9931   time: 69.72s   best: 32.9171
2023-12-02 22:37:57,173:INFO:  Epoch 139/500:  train Loss: 21.3710   val Loss: 25.0961   time: 239.66s   best: 24.9069
2023-12-02 22:38:16,919:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:38:16,940:INFO:  Epoch 96/800:  train Loss: 49.0473   val Loss: 49.8522   time: 69.36s   best: 49.8522
2023-12-02 22:38:47,037:INFO:  Epoch 577/600:  train Loss: 26.9689   val Loss: 34.2944   time: 69.55s   best: 32.9171
2023-12-02 22:39:26,421:INFO:  Epoch 97/800:  train Loss: 48.7616   val Loss: 51.5074   time: 69.47s   best: 49.8522
2023-12-02 22:39:56,803:INFO:  Epoch 578/600:  train Loss: 28.1556   val Loss: 33.4425   time: 69.76s   best: 32.9171
2023-12-02 22:40:35,916:INFO:  Epoch 98/800:  train Loss: 48.6148   val Loss: 50.0110   time: 69.49s   best: 49.8522
2023-12-02 22:41:06,420:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 22:41:06,439:INFO:  Epoch 579/600:  train Loss: 27.3124   val Loss: 32.8507   time: 69.61s   best: 32.8507
2023-12-02 22:41:45,527:INFO:  Epoch 99/800:  train Loss: 48.4324   val Loss: 49.9269   time: 69.61s   best: 49.8522
2023-12-02 22:41:58,320:INFO:  Epoch 140/500:  train Loss: 21.8421   val Loss: 25.3550   time: 241.13s   best: 24.9069
2023-12-02 22:42:16,172:INFO:  Epoch 580/600:  train Loss: 27.1438   val Loss: 33.3782   time: 69.72s   best: 32.8507
2023-12-02 22:42:55,138:INFO:  Epoch 100/800:  train Loss: 48.1201   val Loss: 50.7178   time: 69.60s   best: 49.8522
2023-12-02 22:43:26,021:INFO:  Epoch 581/600:  train Loss: 27.0543   val Loss: 33.4961   time: 69.85s   best: 32.8507
2023-12-02 22:44:04,568:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:44:04,588:INFO:  Epoch 101/800:  train Loss: 48.0446   val Loss: 49.5590   time: 69.42s   best: 49.5590
2023-12-02 22:44:35,900:INFO:  Epoch 582/600:  train Loss: 26.9751   val Loss: 33.5738   time: 69.86s   best: 32.8507
2023-12-02 22:45:14,098:INFO:  Epoch 102/800:  train Loss: 48.1917   val Loss: 49.7430   time: 69.50s   best: 49.5590
2023-12-02 22:45:45,524:INFO:  Epoch 583/600:  train Loss: 26.7941   val Loss: 33.2970   time: 69.62s   best: 32.8507
2023-12-02 22:45:58,907:INFO:  Epoch 141/500:  train Loss: 21.6867   val Loss: 25.6180   time: 240.58s   best: 24.9069
2023-12-02 22:46:23,356:INFO:  Epoch 103/800:  train Loss: 47.6282   val Loss: 49.8597   time: 69.24s   best: 49.5590
2023-12-02 22:46:55,129:INFO:  Epoch 584/600:  train Loss: 26.9612   val Loss: 33.1289   time: 69.59s   best: 32.8507
2023-12-02 22:47:32,674:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:47:32,694:INFO:  Epoch 104/800:  train Loss: 47.2063   val Loss: 48.8442   time: 69.31s   best: 48.8442
2023-12-02 22:48:04,871:INFO:  Epoch 585/600:  train Loss: 27.3713   val Loss: 32.8896   time: 69.73s   best: 32.8507
2023-12-02 22:48:42,057:INFO:  Epoch 105/800:  train Loss: 47.0725   val Loss: 48.9184   time: 69.36s   best: 48.8442
2023-12-02 22:49:14,482:INFO:  Epoch 586/600:  train Loss: 27.0383   val Loss: 33.1044   time: 69.60s   best: 32.8507
2023-12-02 22:49:51,399:INFO:  Epoch 106/800:  train Loss: 46.9809   val Loss: 49.0091   time: 69.34s   best: 48.8442
2023-12-02 22:49:59,338:INFO:  Epoch 142/500:  train Loss: 21.3172   val Loss: 25.6628   time: 240.41s   best: 24.9069
2023-12-02 22:50:24,023:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_c9c4.pt
2023-12-02 22:50:24,044:INFO:  Epoch 587/600:  train Loss: 26.8157   val Loss: 32.6679   time: 69.52s   best: 32.6679
2023-12-02 22:51:00,809:INFO:  Epoch 107/800:  train Loss: 46.5803   val Loss: 50.1430   time: 69.40s   best: 48.8442
2023-12-02 22:51:33,649:INFO:  Epoch 588/600:  train Loss: 26.9928   val Loss: 33.2360   time: 69.60s   best: 32.6679
2023-12-02 22:52:10,059:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:52:10,085:INFO:  Epoch 108/800:  train Loss: 46.5268   val Loss: 48.7339   time: 69.23s   best: 48.7339
2023-12-02 22:52:43,110:INFO:  Epoch 589/600:  train Loss: 26.8654   val Loss: 32.9151   time: 69.46s   best: 32.6679
2023-12-02 22:53:19,437:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:53:19,460:INFO:  Epoch 109/800:  train Loss: 46.1107   val Loss: 47.8448   time: 69.34s   best: 47.8448
2023-12-02 22:53:52,595:INFO:  Epoch 590/600:  train Loss: 26.7913   val Loss: 33.0538   time: 69.48s   best: 32.6679
2023-12-02 22:54:00,509:INFO:  Epoch 143/500:  train Loss: 21.6443   val Loss: 25.4708   time: 241.16s   best: 24.9069
2023-12-02 22:54:28,740:INFO:  Epoch 110/800:  train Loss: 46.4464   val Loss: 51.1693   time: 69.28s   best: 47.8448
2023-12-02 22:55:02,041:INFO:  Epoch 591/600:  train Loss: 27.0538   val Loss: 33.3138   time: 69.44s   best: 32.6679
2023-12-02 22:55:38,033:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 22:55:38,054:INFO:  Epoch 111/800:  train Loss: 46.2754   val Loss: 47.7681   time: 69.29s   best: 47.7681
2023-12-02 22:56:12,017:INFO:  Epoch 592/600:  train Loss: 27.3663   val Loss: 33.3994   time: 69.97s   best: 32.6679
2023-12-02 22:56:47,486:INFO:  Epoch 112/800:  train Loss: 45.6044   val Loss: 48.2175   time: 69.42s   best: 47.7681
2023-12-02 22:57:21,620:INFO:  Epoch 593/600:  train Loss: 26.7832   val Loss: 33.6525   time: 69.59s   best: 32.6679
2023-12-02 22:57:56,750:INFO:  Epoch 113/800:  train Loss: 46.0316   val Loss: 48.6308   time: 69.26s   best: 47.7681
2023-12-02 22:58:00,224:INFO:  Epoch 144/500:  train Loss: 21.4092   val Loss: 25.9755   time: 239.71s   best: 24.9069
2023-12-02 22:58:31,297:INFO:  Epoch 594/600:  train Loss: 27.0356   val Loss: 33.2248   time: 69.66s   best: 32.6679
2023-12-02 22:59:06,080:INFO:  Epoch 114/800:  train Loss: 45.7395   val Loss: 48.7402   time: 69.31s   best: 47.7681
2023-12-02 22:59:40,661:INFO:  Epoch 595/600:  train Loss: 27.0815   val Loss: 34.2734   time: 69.35s   best: 32.6679
2023-12-02 23:00:15,308:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 23:00:15,329:INFO:  Epoch 115/800:  train Loss: 45.2212   val Loss: 46.9832   time: 69.21s   best: 46.9832
2023-12-02 23:00:50,296:INFO:  Epoch 596/600:  train Loss: 26.8876   val Loss: 33.1605   time: 69.63s   best: 32.6679
2023-12-02 23:01:24,688:INFO:  Epoch 116/800:  train Loss: 44.9629   val Loss: 47.6893   time: 69.36s   best: 46.9832
2023-12-02 23:02:00,181:INFO:  Epoch 597/600:  train Loss: 26.6683   val Loss: 33.0207   time: 69.88s   best: 32.6679
2023-12-02 23:02:01,902:INFO:  Epoch 145/500:  train Loss: 21.7334   val Loss: 27.5007   time: 241.66s   best: 24.9069
2023-12-02 23:02:33,988:INFO:  Epoch 117/800:  train Loss: 45.0798   val Loss: 47.4884   time: 69.30s   best: 46.9832
2023-12-02 23:03:09,767:INFO:  Epoch 598/600:  train Loss: 28.5240   val Loss: 38.2983   time: 69.58s   best: 32.6679
2023-12-02 23:03:43,171:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 23:03:43,192:INFO:  Epoch 118/800:  train Loss: 44.7227   val Loss: 46.9427   time: 69.18s   best: 46.9427
2023-12-02 23:04:19,338:INFO:  Epoch 599/600:  train Loss: 29.0119   val Loss: 33.0304   time: 69.57s   best: 32.6679
2023-12-02 23:04:52,650:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 23:04:52,671:INFO:  Epoch 119/800:  train Loss: 44.5486   val Loss: 46.2313   time: 69.45s   best: 46.2313
2023-12-02 23:05:28,912:INFO:  Epoch 600/600:  train Loss: 27.0010   val Loss: 33.0788   time: 69.56s   best: 32.6679
2023-12-02 23:05:28,914:INFO:  -----> Training complete in 698m 41s   best validation loss: 32.6679
 
2023-12-02 23:06:01,115:INFO:  Epoch 146/500:  train Loss: 21.2467   val Loss: 25.1847   time: 239.21s   best: 24.9069
2023-12-02 23:06:01,915:INFO:  Epoch 120/800:  train Loss: 44.1300   val Loss: 47.1792   time: 69.23s   best: 46.2313
2023-12-02 23:07:11,262:INFO:  Epoch 121/800:  train Loss: 44.4714   val Loss: 46.3406   time: 69.34s   best: 46.2313
2023-12-02 23:08:20,452:INFO:  Epoch 122/800:  train Loss: 43.8941   val Loss: 47.5713   time: 69.19s   best: 46.2313
2023-12-02 23:09:29,832:INFO:  Epoch 123/800:  train Loss: 44.0109   val Loss: 46.8620   time: 69.37s   best: 46.2313
2023-12-02 23:10:00,534:INFO:  Epoch 147/500:  train Loss: 21.2301   val Loss: 25.3023   time: 239.41s   best: 24.9069
2023-12-02 23:10:39,138:INFO:  Epoch 124/800:  train Loss: 43.5324   val Loss: 46.5059   time: 69.29s   best: 46.2313
2023-12-02 23:11:48,490:INFO:  Epoch 125/800:  train Loss: 43.4964   val Loss: 47.8541   time: 69.32s   best: 46.2313
2023-12-02 23:12:57,938:INFO:  Epoch 126/800:  train Loss: 43.7466   val Loss: 48.9313   time: 69.44s   best: 46.2313
2023-12-02 23:14:00,146:INFO:  Epoch 148/500:  train Loss: 21.2047   val Loss: 25.1765   time: 239.60s   best: 24.9069
2023-12-02 23:14:07,188:INFO:  Epoch 127/800:  train Loss: 43.8639   val Loss: 46.6354   time: 69.24s   best: 46.2313
2023-12-02 23:15:16,762:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 23:15:16,782:INFO:  Epoch 128/800:  train Loss: 43.2305   val Loss: 45.2394   time: 69.57s   best: 45.2394
2023-12-02 23:16:25,977:INFO:  Epoch 129/800:  train Loss: 42.9505   val Loss: 46.6906   time: 69.19s   best: 45.2394
2023-12-02 23:17:35,239:INFO:  Epoch 130/800:  train Loss: 42.8469   val Loss: 46.2769   time: 69.26s   best: 45.2394
2023-12-02 23:17:59,400:INFO:  Epoch 149/500:  train Loss: 22.0134   val Loss: 25.8763   time: 239.25s   best: 24.9069
2023-12-02 23:18:44,490:INFO:  Epoch 131/800:  train Loss: 42.8578   val Loss: 47.1300   time: 69.25s   best: 45.2394
2023-12-02 23:19:53,622:INFO:  Epoch 132/800:  train Loss: 42.7675   val Loss: 46.0059   time: 69.13s   best: 45.2394
2023-12-02 23:21:02,931:INFO:  Epoch 133/800:  train Loss: 42.3171   val Loss: 45.3943   time: 69.31s   best: 45.2394
2023-12-02 23:21:58,429:INFO:  Epoch 150/500:  train Loss: 21.1633   val Loss: 25.5538   time: 239.01s   best: 24.9069
2023-12-02 23:22:12,084:INFO:  Epoch 134/800:  train Loss: 42.1854   val Loss: 46.2454   time: 69.15s   best: 45.2394
2023-12-02 23:23:21,354:INFO:  Epoch 135/800:  train Loss: 42.0479   val Loss: 46.0255   time: 69.27s   best: 45.2394
2023-12-02 23:24:30,564:INFO:  Epoch 136/800:  train Loss: 43.1455   val Loss: 46.1412   time: 69.21s   best: 45.2394
2023-12-02 23:25:39,822:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 23:25:39,841:INFO:  Epoch 137/800:  train Loss: 42.0020   val Loss: 44.5505   time: 69.25s   best: 44.5505
2023-12-02 23:25:59,774:INFO:  Epoch 151/500:  train Loss: 21.4497   val Loss: 25.7412   time: 241.32s   best: 24.9069
2023-12-02 23:26:49,122:INFO:  Epoch 138/800:  train Loss: 41.6571   val Loss: 44.8180   time: 69.27s   best: 44.5505
2023-12-02 23:27:58,379:INFO:  Epoch 139/800:  train Loss: 41.7877   val Loss: 44.9640   time: 69.25s   best: 44.5505
2023-12-02 23:29:07,632:INFO:  Epoch 140/800:  train Loss: 41.3831   val Loss: 46.2565   time: 69.24s   best: 44.5505
2023-12-02 23:29:58,932:INFO:  Epoch 152/500:  train Loss: 21.0298   val Loss: 25.3080   time: 239.16s   best: 24.9069
2023-12-02 23:30:17,127:INFO:  Epoch 141/800:  train Loss: 41.5152   val Loss: 45.1915   time: 69.48s   best: 44.5505
2023-12-02 23:31:26,374:INFO:  Epoch 142/800:  train Loss: 41.2993   val Loss: 44.6231   time: 69.22s   best: 44.5505
2023-12-02 23:32:35,659:INFO:  Epoch 143/800:  train Loss: 41.1321   val Loss: 45.1478   time: 69.28s   best: 44.5505
2023-12-02 23:33:44,968:INFO:  Epoch 144/800:  train Loss: 41.4008   val Loss: 45.0753   time: 69.30s   best: 44.5505
2023-12-02 23:33:59,137:INFO:  Epoch 153/500:  train Loss: 21.1297   val Loss: 24.9099   time: 240.18s   best: 24.9069
2023-12-02 23:34:54,304:INFO:  Epoch 145/800:  train Loss: 40.9808   val Loss: 44.9253   time: 69.34s   best: 44.5505
2023-12-02 23:36:03,727:INFO:  Epoch 146/800:  train Loss: 40.8655   val Loss: 45.1113   time: 69.41s   best: 44.5505
2023-12-02 23:37:13,228:INFO:  Epoch 147/800:  train Loss: 41.1694   val Loss: 45.8914   time: 69.50s   best: 44.5505
2023-12-02 23:37:58,508:INFO:  Epoch 154/500:  train Loss: 21.0051   val Loss: 24.9946   time: 239.32s   best: 24.9069
2023-12-02 23:38:22,771:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 23:38:22,792:INFO:  Epoch 148/800:  train Loss: 40.8534   val Loss: 43.9801   time: 69.54s   best: 43.9801
2023-12-02 23:39:32,166:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 23:39:32,185:INFO:  Epoch 149/800:  train Loss: 40.4882   val Loss: 43.4280   time: 69.36s   best: 43.4280
2023-12-02 23:40:41,689:INFO:  Epoch 150/800:  train Loss: 40.5201   val Loss: 44.2233   time: 69.50s   best: 43.4280
2023-12-02 23:41:51,331:INFO:  Epoch 151/800:  train Loss: 40.4209   val Loss: 43.7564   time: 69.63s   best: 43.4280
2023-12-02 23:41:59,566:INFO:  Epoch 155/500:  train Loss: 21.0014   val Loss: 25.1636   time: 241.05s   best: 24.9069
2023-12-02 23:43:00,805:INFO:  Epoch 152/800:  train Loss: 40.1456   val Loss: 43.9034   time: 69.47s   best: 43.4280
2023-12-02 23:44:10,149:INFO:  Epoch 153/800:  train Loss: 40.3165   val Loss: 43.5637   time: 69.34s   best: 43.4280
2023-12-02 23:45:19,406:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 23:45:19,426:INFO:  Epoch 154/800:  train Loss: 40.0917   val Loss: 42.6738   time: 69.25s   best: 42.6738
2023-12-02 23:45:58,843:INFO:  Epoch 156/500:  train Loss: 21.2322   val Loss: 25.4415   time: 239.25s   best: 24.9069
2023-12-02 23:46:28,708:INFO:  Epoch 155/800:  train Loss: 39.9072   val Loss: 43.2259   time: 69.28s   best: 42.6738
2023-12-02 23:47:37,891:INFO:  Epoch 156/800:  train Loss: 39.6446   val Loss: 43.1524   time: 69.17s   best: 42.6738
2023-12-02 23:48:47,259:INFO:  Epoch 157/800:  train Loss: 40.2732   val Loss: 44.2684   time: 69.36s   best: 42.6738
2023-12-02 23:49:56,640:INFO:  Epoch 158/800:  train Loss: 39.6843   val Loss: 42.9391   time: 69.38s   best: 42.6738
2023-12-02 23:49:58,178:INFO:  Epoch 157/500:  train Loss: 21.0079   val Loss: 25.2189   time: 239.33s   best: 24.9069
2023-12-02 23:51:06,158:INFO:  Epoch 159/800:  train Loss: 39.4270   val Loss: 43.4346   time: 69.52s   best: 42.6738
2023-12-02 23:52:15,324:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 23:52:15,342:INFO:  Epoch 160/800:  train Loss: 39.3388   val Loss: 42.5616   time: 69.15s   best: 42.5616
2023-12-02 23:53:25,101:INFO:  Epoch 161/800:  train Loss: 39.3313   val Loss: 43.2292   time: 69.75s   best: 42.5616
2023-12-02 23:53:57,308:INFO:  Epoch 158/500:  train Loss: 20.9188   val Loss: 25.0493   time: 239.13s   best: 24.9069
2023-12-02 23:54:34,284:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-02 23:54:34,305:INFO:  Epoch 162/800:  train Loss: 39.2076   val Loss: 42.3133   time: 69.18s   best: 42.3133
2023-12-02 23:55:43,983:INFO:  Epoch 163/800:  train Loss: 39.3545   val Loss: 43.1236   time: 69.68s   best: 42.3133
2023-12-02 23:56:53,247:INFO:  Epoch 164/800:  train Loss: 39.1060   val Loss: 42.9057   time: 69.26s   best: 42.3133
2023-12-02 23:57:57,936:INFO:  Epoch 159/500:  train Loss: 20.8574   val Loss: 25.2218   time: 240.63s   best: 24.9069
2023-12-02 23:58:02,472:INFO:  Epoch 165/800:  train Loss: 39.2736   val Loss: 42.5439   time: 69.21s   best: 42.3133
2023-12-02 23:59:11,827:INFO:  Epoch 166/800:  train Loss: 39.0094   val Loss: 42.6160   time: 69.34s   best: 42.3133
2023-12-03 00:00:21,062:INFO:  Epoch 167/800:  train Loss: 38.8927   val Loss: 43.5614   time: 69.22s   best: 42.3133
2023-12-03 00:01:30,269:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:01:30,295:INFO:  Epoch 168/800:  train Loss: 38.8618   val Loss: 42.0432   time: 69.20s   best: 42.0432
2023-12-03 00:01:57,902:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 00:01:57,922:INFO:  Epoch 160/500:  train Loss: 20.8699   val Loss: 24.8812   time: 239.94s   best: 24.8812
2023-12-03 00:02:39,710:INFO:  Epoch 169/800:  train Loss: 38.6029   val Loss: 42.9127   time: 69.40s   best: 42.0432
2023-12-03 00:03:48,972:INFO:  Epoch 170/800:  train Loss: 38.6752   val Loss: 42.8751   time: 69.26s   best: 42.0432
2023-12-03 00:04:58,436:INFO:  Epoch 171/800:  train Loss: 38.5764   val Loss: 42.5955   time: 69.46s   best: 42.0432
2023-12-03 00:05:59,309:INFO:  Epoch 161/500:  train Loss: 20.7971   val Loss: 25.4444   time: 241.39s   best: 24.8812
2023-12-03 00:06:07,634:INFO:  Epoch 172/800:  train Loss: 38.6424   val Loss: 42.3556   time: 69.20s   best: 42.0432
2023-12-03 00:07:17,147:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:07:17,166:INFO:  Epoch 173/800:  train Loss: 38.3744   val Loss: 41.6840   time: 69.51s   best: 41.6840
2023-12-03 00:08:26,275:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:08:26,294:INFO:  Epoch 174/800:  train Loss: 38.1751   val Loss: 41.6158   time: 69.09s   best: 41.6158
2023-12-03 00:09:35,598:INFO:  Epoch 175/800:  train Loss: 38.0851   val Loss: 42.1815   time: 69.30s   best: 41.6158
2023-12-03 00:09:58,827:INFO:  Epoch 162/500:  train Loss: 20.9636   val Loss: 25.1299   time: 239.50s   best: 24.8812
2023-12-03 00:10:44,993:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:10:45,013:INFO:  Epoch 176/800:  train Loss: 38.1628   val Loss: 41.2502   time: 69.38s   best: 41.2502
2023-12-03 00:11:54,581:INFO:  Epoch 177/800:  train Loss: 38.0899   val Loss: 41.9162   time: 69.56s   best: 41.2502
2023-12-03 00:13:03,698:INFO:  Epoch 178/800:  train Loss: 38.2184   val Loss: 41.7265   time: 69.12s   best: 41.2502
2023-12-03 00:13:57,364:INFO:  Epoch 163/500:  train Loss: 20.7970   val Loss: 24.9358   time: 238.53s   best: 24.8812
2023-12-03 00:14:13,036:INFO:  Epoch 179/800:  train Loss: 37.7969   val Loss: 41.5424   time: 69.33s   best: 41.2502
2023-12-03 00:15:22,291:INFO:  Epoch 180/800:  train Loss: 37.9306   val Loss: 41.3592   time: 69.24s   best: 41.2502
2023-12-03 00:16:31,908:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:16:31,927:INFO:  Epoch 181/800:  train Loss: 37.7162   val Loss: 41.2337   time: 69.61s   best: 41.2337
2023-12-03 00:17:41,122:INFO:  Epoch 182/800:  train Loss: 37.5741   val Loss: 41.7432   time: 69.18s   best: 41.2337
2023-12-03 00:17:56,558:INFO:  Epoch 164/500:  train Loss: 20.8623   val Loss: 24.9970   time: 239.19s   best: 24.8812
2023-12-03 00:18:50,419:INFO:  Epoch 183/800:  train Loss: 37.5713   val Loss: 42.0868   time: 69.29s   best: 41.2337
2023-12-03 00:19:59,627:INFO:  Epoch 184/800:  train Loss: 37.6708   val Loss: 41.5539   time: 69.20s   best: 41.2337
2023-12-03 00:21:09,010:INFO:  Epoch 185/800:  train Loss: 37.6803   val Loss: 41.7223   time: 69.38s   best: 41.2337
2023-12-03 00:21:57,826:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 00:21:57,847:INFO:  Epoch 165/500:  train Loss: 20.8527   val Loss: 24.8631   time: 241.25s   best: 24.8631
2023-12-03 00:22:18,252:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:22:18,274:INFO:  Epoch 186/800:  train Loss: 37.3645   val Loss: 41.2022   time: 69.23s   best: 41.2022
2023-12-03 00:23:27,542:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:23:27,564:INFO:  Epoch 187/800:  train Loss: 37.4973   val Loss: 40.7654   time: 69.26s   best: 40.7654
2023-12-03 00:24:36,845:INFO:  Epoch 188/800:  train Loss: 37.2958   val Loss: 41.0321   time: 69.28s   best: 40.7654
2023-12-03 00:25:46,086:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:25:46,105:INFO:  Epoch 189/800:  train Loss: 37.0746   val Loss: 40.2644   time: 69.24s   best: 40.2644
2023-12-03 00:25:57,159:INFO:  Epoch 166/500:  train Loss: 20.7303   val Loss: 25.0836   time: 239.30s   best: 24.8631
2023-12-03 00:26:55,357:INFO:  Epoch 190/800:  train Loss: 37.1818   val Loss: 41.1684   time: 69.24s   best: 40.2644
2023-12-03 00:28:04,525:INFO:  Epoch 191/800:  train Loss: 37.0382   val Loss: 40.4024   time: 69.17s   best: 40.2644
2023-12-03 00:29:13,922:INFO:  Epoch 192/800:  train Loss: 36.8849   val Loss: 40.7840   time: 69.40s   best: 40.2644
2023-12-03 00:29:56,568:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 00:29:56,589:INFO:  Epoch 167/500:  train Loss: 20.7424   val Loss: 24.8349   time: 239.38s   best: 24.8349
2023-12-03 00:30:23,080:INFO:  Epoch 193/800:  train Loss: 36.9254   val Loss: 40.6018   time: 69.15s   best: 40.2644
2023-12-03 00:31:32,271:INFO:  Epoch 194/800:  train Loss: 37.2786   val Loss: 41.0046   time: 69.18s   best: 40.2644
2023-12-03 00:32:41,494:INFO:  Epoch 195/800:  train Loss: 36.7049   val Loss: 40.4246   time: 69.21s   best: 40.2644
2023-12-03 00:33:51,300:INFO:  Epoch 196/800:  train Loss: 36.5728   val Loss: 40.9302   time: 69.80s   best: 40.2644
2023-12-03 00:33:57,572:INFO:  Epoch 168/500:  train Loss: 20.9583   val Loss: 25.1667   time: 240.97s   best: 24.8349
2023-12-03 00:35:00,824:INFO:  Epoch 197/800:  train Loss: 37.0285   val Loss: 41.0359   time: 69.52s   best: 40.2644
2023-12-03 00:36:10,027:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:36:10,046:INFO:  Epoch 198/800:  train Loss: 36.5361   val Loss: 40.1599   time: 69.20s   best: 40.1599
2023-12-03 00:37:19,271:INFO:  Epoch 199/800:  train Loss: 36.4252   val Loss: 40.8785   time: 69.22s   best: 40.1599
2023-12-03 00:37:56,909:INFO:  Epoch 169/500:  train Loss: 20.6801   val Loss: 24.8613   time: 239.32s   best: 24.8349
2023-12-03 00:38:29,098:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:38:29,119:INFO:  Epoch 200/800:  train Loss: 36.4836   val Loss: 39.9135   time: 69.81s   best: 39.9135
2023-12-03 00:39:38,566:INFO:  Epoch 201/800:  train Loss: 36.4648   val Loss: 40.2282   time: 69.45s   best: 39.9135
2023-12-03 00:40:47,769:INFO:  Epoch 202/800:  train Loss: 36.4388   val Loss: 40.2520   time: 69.20s   best: 39.9135
2023-12-03 00:41:56,933:INFO:  Epoch 203/800:  train Loss: 36.6681   val Loss: 40.8966   time: 69.16s   best: 39.9135
2023-12-03 00:41:57,934:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 00:41:57,955:INFO:  Epoch 170/500:  train Loss: 20.6472   val Loss: 24.6014   time: 241.01s   best: 24.6014
2023-12-03 00:43:06,179:INFO:  Epoch 204/800:  train Loss: 36.4508   val Loss: 40.3697   time: 69.24s   best: 39.9135
2023-12-03 00:44:15,433:INFO:  Epoch 205/800:  train Loss: 36.4667   val Loss: 41.3441   time: 69.24s   best: 39.9135
2023-12-03 00:45:24,630:INFO:  Epoch 206/800:  train Loss: 36.6418   val Loss: 40.0079   time: 69.20s   best: 39.9135
2023-12-03 00:45:57,559:INFO:  Epoch 171/500:  train Loss: 20.5673   val Loss: 24.9511   time: 239.59s   best: 24.6014
2023-12-03 00:46:33,832:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:46:33,861:INFO:  Epoch 207/800:  train Loss: 36.1629   val Loss: 39.7820   time: 69.19s   best: 39.7820
2023-12-03 00:47:43,277:INFO:  Epoch 208/800:  train Loss: 36.1923   val Loss: 40.0660   time: 69.40s   best: 39.7820
2023-12-03 00:48:52,688:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:48:52,707:INFO:  Epoch 209/800:  train Loss: 35.9134   val Loss: 39.0921   time: 69.41s   best: 39.0921
2023-12-03 00:49:57,416:INFO:  Epoch 172/500:  train Loss: 20.6934   val Loss: 25.3297   time: 239.85s   best: 24.6014
2023-12-03 00:50:01,936:INFO:  Epoch 210/800:  train Loss: 36.1563   val Loss: 39.6803   time: 69.23s   best: 39.0921
2023-12-03 00:51:11,886:INFO:  Epoch 211/800:  train Loss: 35.8438   val Loss: 39.7952   time: 69.95s   best: 39.0921
2023-12-03 00:52:21,274:INFO:  Epoch 212/800:  train Loss: 35.6468   val Loss: 39.3656   time: 69.39s   best: 39.0921
2023-12-03 00:53:30,511:INFO:  Epoch 213/800:  train Loss: 35.9532   val Loss: 39.6309   time: 69.24s   best: 39.0921
2023-12-03 00:53:57,693:INFO:  Epoch 173/500:  train Loss: 20.5940   val Loss: 25.4553   time: 240.27s   best: 24.6014
2023-12-03 00:54:39,892:INFO:  Epoch 214/800:  train Loss: 35.5311   val Loss: 39.7764   time: 69.38s   best: 39.0921
2023-12-03 00:55:48,989:INFO:  Epoch 215/800:  train Loss: 35.9397   val Loss: 40.1573   time: 69.09s   best: 39.0921
2023-12-03 00:56:58,518:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 00:56:58,538:INFO:  Epoch 216/800:  train Loss: 35.6673   val Loss: 38.9829   time: 69.52s   best: 38.9829
2023-12-03 00:57:58,671:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 00:57:58,702:INFO:  Epoch 174/500:  train Loss: 20.7526   val Loss: 24.4469   time: 240.97s   best: 24.4469
2023-12-03 00:58:07,804:INFO:  Epoch 217/800:  train Loss: 35.3820   val Loss: 40.8859   time: 69.27s   best: 38.9829
2023-12-03 00:59:17,078:INFO:  Epoch 218/800:  train Loss: 35.5815   val Loss: 39.7436   time: 69.27s   best: 38.9829
2023-12-03 01:00:26,379:INFO:  Epoch 219/800:  train Loss: 35.8056   val Loss: 40.5797   time: 69.30s   best: 38.9829
2023-12-03 01:01:35,655:INFO:  Epoch 220/800:  train Loss: 35.5668   val Loss: 39.3533   time: 69.26s   best: 38.9829
2023-12-03 01:01:59,874:INFO:  Epoch 175/500:  train Loss: 20.6330   val Loss: 24.4958   time: 241.17s   best: 24.4469
2023-12-03 01:02:45,098:INFO:  Epoch 221/800:  train Loss: 35.3121   val Loss: 39.3698   time: 69.44s   best: 38.9829
2023-12-03 01:03:54,743:INFO:  Epoch 222/800:  train Loss: 35.9213   val Loss: 39.6701   time: 69.63s   best: 38.9829
2023-12-03 01:05:04,225:INFO:  Epoch 223/800:  train Loss: 35.2623   val Loss: 39.8711   time: 69.48s   best: 38.9829
2023-12-03 01:05:59,004:INFO:  Epoch 176/500:  train Loss: 20.6096   val Loss: 24.6613   time: 239.13s   best: 24.4469
2023-12-03 01:06:13,513:INFO:  Epoch 224/800:  train Loss: 35.0305   val Loss: 39.2811   time: 69.28s   best: 38.9829
2023-12-03 01:07:22,873:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:07:22,892:INFO:  Epoch 225/800:  train Loss: 35.1397   val Loss: 38.9025   time: 69.35s   best: 38.9025
2023-12-03 01:08:32,317:INFO:  Epoch 226/800:  train Loss: 35.4384   val Loss: 39.5865   time: 69.41s   best: 38.9025
2023-12-03 01:09:41,711:INFO:  Epoch 227/800:  train Loss: 35.3589   val Loss: 39.0016   time: 69.39s   best: 38.9025
2023-12-03 01:09:58,164:INFO:  Epoch 177/500:  train Loss: 20.5982   val Loss: 24.7672   time: 239.14s   best: 24.4469
2023-12-03 01:10:50,994:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:10:51,015:INFO:  Epoch 228/800:  train Loss: 34.8929   val Loss: 38.8838   time: 69.28s   best: 38.8838
2023-12-03 01:12:00,219:INFO:  Epoch 229/800:  train Loss: 35.0311   val Loss: 39.0634   time: 69.20s   best: 38.8838
2023-12-03 01:13:09,418:INFO:  Epoch 230/800:  train Loss: 35.3620   val Loss: 41.8711   time: 69.19s   best: 38.8838
2023-12-03 01:13:57,840:INFO:  Epoch 178/500:  train Loss: 21.0052   val Loss: 25.0394   time: 239.65s   best: 24.4469
2023-12-03 01:14:18,623:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:14:18,651:INFO:  Epoch 231/800:  train Loss: 35.6224   val Loss: 38.6925   time: 69.20s   best: 38.6925
2023-12-03 01:15:27,884:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:15:27,903:INFO:  Epoch 232/800:  train Loss: 34.7349   val Loss: 38.5700   time: 69.22s   best: 38.5700
2023-12-03 01:16:37,414:INFO:  Epoch 233/800:  train Loss: 34.9277   val Loss: 39.8263   time: 69.51s   best: 38.5700
2023-12-03 01:17:46,689:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:17:46,708:INFO:  Epoch 234/800:  train Loss: 34.9666   val Loss: 38.5091   time: 69.27s   best: 38.5091
2023-12-03 01:17:58,940:INFO:  Epoch 179/500:  train Loss: 20.5135   val Loss: 25.0710   time: 241.09s   best: 24.4469
2023-12-03 01:18:56,151:INFO:  Epoch 235/800:  train Loss: 34.6448   val Loss: 39.6388   time: 69.43s   best: 38.5091
2023-12-03 01:20:05,433:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:20:05,452:INFO:  Epoch 236/800:  train Loss: 34.9037   val Loss: 38.4976   time: 69.28s   best: 38.4976
2023-12-03 01:21:15,163:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:21:15,182:INFO:  Epoch 237/800:  train Loss: 34.6180   val Loss: 38.4497   time: 69.70s   best: 38.4497
2023-12-03 01:22:00,045:INFO:  Epoch 180/500:  train Loss: 20.5761   val Loss: 25.0730   time: 241.09s   best: 24.4469
2023-12-03 01:22:24,660:INFO:  Epoch 238/800:  train Loss: 34.7173   val Loss: 39.3814   time: 69.48s   best: 38.4497
2023-12-03 01:23:34,219:INFO:  Epoch 239/800:  train Loss: 35.2453   val Loss: 40.3179   time: 69.55s   best: 38.4497
2023-12-03 01:24:43,805:INFO:  Epoch 240/800:  train Loss: 35.0032   val Loss: 38.6734   time: 69.58s   best: 38.4497
2023-12-03 01:25:53,255:INFO:  Epoch 241/800:  train Loss: 34.5535   val Loss: 39.4711   time: 69.44s   best: 38.4497
2023-12-03 01:26:01,518:INFO:  Epoch 181/500:  train Loss: 20.4479   val Loss: 24.7070   time: 241.47s   best: 24.4469
2023-12-03 01:27:02,426:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:27:02,447:INFO:  Epoch 242/800:  train Loss: 34.5272   val Loss: 38.4161   time: 69.17s   best: 38.4161
2023-12-03 01:28:11,836:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:28:11,856:INFO:  Epoch 243/800:  train Loss: 34.2733   val Loss: 38.0773   time: 69.39s   best: 38.0773
2023-12-03 01:29:21,159:INFO:  Epoch 244/800:  train Loss: 34.2935   val Loss: 38.4647   time: 69.30s   best: 38.0773
2023-12-03 01:30:01,703:INFO:  Epoch 182/500:  train Loss: 20.5464   val Loss: 25.8319   time: 240.17s   best: 24.4469
2023-12-03 01:30:30,482:INFO:  Epoch 245/800:  train Loss: 34.5211   val Loss: 39.2872   time: 69.32s   best: 38.0773
2023-12-03 01:31:39,763:INFO:  Epoch 246/800:  train Loss: 34.3700   val Loss: 38.6123   time: 69.27s   best: 38.0773
2023-12-03 01:32:49,106:INFO:  Epoch 247/800:  train Loss: 34.1744   val Loss: 38.1915   time: 69.33s   best: 38.0773
2023-12-03 01:33:58,434:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:33:58,455:INFO:  Epoch 248/800:  train Loss: 34.2445   val Loss: 37.9073   time: 69.31s   best: 37.9073
2023-12-03 01:34:03,406:INFO:  Epoch 183/500:  train Loss: 20.3506   val Loss: 24.6334   time: 241.70s   best: 24.4469
2023-12-03 01:35:08,027:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:35:08,058:INFO:  Epoch 249/800:  train Loss: 34.0067   val Loss: 37.3912   time: 69.55s   best: 37.3912
2023-12-03 01:36:17,592:INFO:  Epoch 250/800:  train Loss: 34.4167   val Loss: 39.9575   time: 69.53s   best: 37.3912
2023-12-03 01:37:27,044:INFO:  Epoch 251/800:  train Loss: 34.3309   val Loss: 37.9904   time: 69.44s   best: 37.3912
2023-12-03 01:38:02,910:INFO:  Epoch 184/500:  train Loss: 20.4853   val Loss: 24.5511   time: 239.49s   best: 24.4469
2023-12-03 01:38:36,491:INFO:  Epoch 252/800:  train Loss: 34.6528   val Loss: 40.2674   time: 69.45s   best: 37.3912
2023-12-03 01:39:45,801:INFO:  Epoch 253/800:  train Loss: 34.6771   val Loss: 37.5120   time: 69.31s   best: 37.3912
2023-12-03 01:40:55,131:INFO:  Epoch 254/800:  train Loss: 34.0622   val Loss: 37.9154   time: 69.33s   best: 37.3912
2023-12-03 01:42:02,238:INFO:  Epoch 185/500:  train Loss: 20.9708   val Loss: 25.3735   time: 239.31s   best: 24.4469
2023-12-03 01:42:04,424:INFO:  Epoch 255/800:  train Loss: 33.9583   val Loss: 38.0398   time: 69.28s   best: 37.3912
2023-12-03 01:43:13,701:INFO:  Epoch 256/800:  train Loss: 34.1746   val Loss: 37.4277   time: 69.26s   best: 37.3912
2023-12-03 01:44:22,878:INFO:  Epoch 257/800:  train Loss: 33.9857   val Loss: 37.8234   time: 69.18s   best: 37.3912
2023-12-03 01:45:32,177:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:45:32,196:INFO:  Epoch 258/800:  train Loss: 33.6964   val Loss: 37.3264   time: 69.29s   best: 37.3264
2023-12-03 01:46:01,525:INFO:  Epoch 186/500:  train Loss: 20.5552   val Loss: 24.8036   time: 239.27s   best: 24.4469
2023-12-03 01:46:41,606:INFO:  Epoch 259/800:  train Loss: 33.6885   val Loss: 37.5838   time: 69.40s   best: 37.3264
2023-12-03 01:47:50,826:INFO:  Epoch 260/800:  train Loss: 33.8347   val Loss: 38.5138   time: 69.22s   best: 37.3264
2023-12-03 01:49:00,148:INFO:  Epoch 261/800:  train Loss: 34.0249   val Loss: 37.8703   time: 69.32s   best: 37.3264
2023-12-03 01:50:01,089:INFO:  Epoch 187/500:  train Loss: 20.3674   val Loss: 25.2215   time: 239.55s   best: 24.4469
2023-12-03 01:50:09,404:INFO:  Epoch 262/800:  train Loss: 33.6410   val Loss: 37.8712   time: 69.25s   best: 37.3264
2023-12-03 01:51:18,906:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:51:18,925:INFO:  Epoch 263/800:  train Loss: 33.8016   val Loss: 37.2741   time: 69.50s   best: 37.2741
2023-12-03 01:52:28,165:INFO:  Epoch 264/800:  train Loss: 33.5850   val Loss: 37.7304   time: 69.24s   best: 37.2741
2023-12-03 01:53:37,473:INFO:  Epoch 265/800:  train Loss: 34.2698   val Loss: 42.8851   time: 69.31s   best: 37.2741
2023-12-03 01:54:02,420:INFO:  Epoch 188/500:  train Loss: 20.5033   val Loss: 25.0031   time: 241.31s   best: 24.4469
2023-12-03 01:54:46,799:INFO:  Epoch 266/800:  train Loss: 34.4401   val Loss: 37.8071   time: 69.31s   best: 37.2741
2023-12-03 01:55:55,977:INFO:  Epoch 267/800:  train Loss: 34.5021   val Loss: 38.4237   time: 69.18s   best: 37.2741
2023-12-03 01:57:05,512:INFO:  Epoch 268/800:  train Loss: 33.6696   val Loss: 37.6036   time: 69.52s   best: 37.2741
2023-12-03 01:58:03,048:INFO:  Epoch 189/500:  train Loss: 20.2677   val Loss: 25.3553   time: 240.62s   best: 24.4469
2023-12-03 01:58:14,819:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:58:14,839:INFO:  Epoch 269/800:  train Loss: 33.3700   val Loss: 37.1298   time: 69.30s   best: 37.1298
2023-12-03 01:59:24,114:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 01:59:24,133:INFO:  Epoch 270/800:  train Loss: 33.3728   val Loss: 37.0966   time: 69.27s   best: 37.0966
2023-12-03 02:00:33,472:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 02:00:33,491:INFO:  Epoch 271/800:  train Loss: 33.3249   val Loss: 36.9730   time: 69.33s   best: 36.9730
2023-12-03 02:01:42,905:INFO:  Epoch 272/800:  train Loss: 33.3348   val Loss: 37.0646   time: 69.41s   best: 36.9730
2023-12-03 02:02:05,043:INFO:  Epoch 190/500:  train Loss: 20.3723   val Loss: 24.7826   time: 241.98s   best: 24.4469
2023-12-03 02:02:52,376:INFO:  Epoch 273/800:  train Loss: 33.2426   val Loss: 37.3812   time: 69.46s   best: 36.9730
2023-12-03 02:04:01,573:INFO:  Epoch 274/800:  train Loss: 33.2555   val Loss: 37.1519   time: 69.19s   best: 36.9730
2023-12-03 02:05:11,006:INFO:  Epoch 275/800:  train Loss: 33.2121   val Loss: 37.5955   time: 69.43s   best: 36.9730
2023-12-03 02:06:06,796:INFO:  Epoch 191/500:  train Loss: 20.2461   val Loss: 25.5073   time: 241.75s   best: 24.4469
2023-12-03 02:06:20,302:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 02:06:20,322:INFO:  Epoch 276/800:  train Loss: 33.7536   val Loss: 36.9625   time: 69.29s   best: 36.9625
2023-12-03 02:07:29,688:INFO:  Epoch 277/800:  train Loss: 33.1685   val Loss: 37.1533   time: 69.36s   best: 36.9625
2023-12-03 02:08:39,048:INFO:  Epoch 278/800:  train Loss: 35.7812   val Loss: 37.0739   time: 69.36s   best: 36.9625
2023-12-03 02:09:48,578:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 02:09:48,597:INFO:  Epoch 279/800:  train Loss: 33.0372   val Loss: 36.8001   time: 69.52s   best: 36.8001
2023-12-03 02:10:07,923:INFO:  Epoch 192/500:  train Loss: 20.2422   val Loss: 24.6338   time: 241.11s   best: 24.4469
2023-12-03 02:10:58,352:INFO:  Epoch 280/800:  train Loss: 33.3227   val Loss: 37.7602   time: 69.75s   best: 36.8001
2023-12-03 02:12:08,154:INFO:  Epoch 281/800:  train Loss: 33.0515   val Loss: 37.3773   time: 69.79s   best: 36.8001
2023-12-03 02:13:17,795:INFO:  Epoch 282/800:  train Loss: 32.8562   val Loss: 36.9151   time: 69.63s   best: 36.8001
2023-12-03 02:14:07,199:INFO:  Epoch 193/500:  train Loss: 20.2345   val Loss: 24.6161   time: 239.27s   best: 24.4469
2023-12-03 02:14:27,151:INFO:  Epoch 283/800:  train Loss: 32.8153   val Loss: 36.8582   time: 69.34s   best: 36.8001
2023-12-03 02:15:36,397:INFO:  Epoch 284/800:  train Loss: 32.8663   val Loss: 36.9013   time: 69.24s   best: 36.8001
2023-12-03 02:16:46,064:INFO:  Epoch 285/800:  train Loss: 32.9987   val Loss: 37.1479   time: 69.65s   best: 36.8001
2023-12-03 02:17:55,503:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 02:17:55,522:INFO:  Epoch 286/800:  train Loss: 33.3294   val Loss: 36.7622   time: 69.42s   best: 36.7622
2023-12-03 02:18:06,950:INFO:  Epoch 194/500:  train Loss: 20.3602   val Loss: 24.8581   time: 239.75s   best: 24.4469
2023-12-03 02:19:04,949:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 02:19:04,972:INFO:  Epoch 287/800:  train Loss: 32.8912   val Loss: 36.4537   time: 69.42s   best: 36.4537
2023-12-03 02:20:14,192:INFO:  Epoch 288/800:  train Loss: 32.8223   val Loss: 36.7916   time: 69.22s   best: 36.4537
2023-12-03 02:21:23,567:INFO:  Epoch 289/800:  train Loss: 32.6947   val Loss: 37.0421   time: 69.36s   best: 36.4537
2023-12-03 02:22:06,450:INFO:  Epoch 195/500:  train Loss: 20.2304   val Loss: 24.7883   time: 239.49s   best: 24.4469
2023-12-03 02:22:32,846:INFO:  Epoch 290/800:  train Loss: 32.8966   val Loss: 36.7756   time: 69.27s   best: 36.4537
2023-12-03 02:23:42,224:INFO:  Epoch 291/800:  train Loss: 32.9834   val Loss: 37.2384   time: 69.36s   best: 36.4537
2023-12-03 02:24:51,728:INFO:  Epoch 292/800:  train Loss: 32.7215   val Loss: 36.5967   time: 69.49s   best: 36.4537
2023-12-03 02:26:00,965:INFO:  Epoch 293/800:  train Loss: 32.5048   val Loss: 36.6338   time: 69.22s   best: 36.4537
2023-12-03 02:26:07,184:INFO:  Epoch 196/500:  train Loss: 20.2410   val Loss: 25.1696   time: 240.73s   best: 24.4469
2023-12-03 02:27:10,602:INFO:  Epoch 294/800:  train Loss: 33.1009   val Loss: 37.2529   time: 69.64s   best: 36.4537
2023-12-03 02:28:19,863:INFO:  Epoch 295/800:  train Loss: 32.7001   val Loss: 36.7091   time: 69.26s   best: 36.4537
2023-12-03 02:29:29,298:INFO:  Epoch 296/800:  train Loss: 32.6372   val Loss: 37.1158   time: 69.43s   best: 36.4537
2023-12-03 02:30:07,898:INFO:  Epoch 197/500:  train Loss: 20.2060   val Loss: 25.9582   time: 240.71s   best: 24.4469
2023-12-03 02:30:38,707:INFO:  Epoch 297/800:  train Loss: 33.2677   val Loss: 38.7150   time: 69.41s   best: 36.4537
2023-12-03 02:31:47,984:INFO:  Epoch 298/800:  train Loss: 33.1552   val Loss: 36.6741   time: 69.27s   best: 36.4537
2023-12-03 02:32:57,386:INFO:  Epoch 299/800:  train Loss: 32.4328   val Loss: 36.7971   time: 69.39s   best: 36.4537
2023-12-03 02:34:06,620:INFO:  Epoch 300/800:  train Loss: 32.4964   val Loss: 36.6099   time: 69.22s   best: 36.4537
2023-12-03 02:34:07,648:INFO:  Epoch 198/500:  train Loss: 20.3129   val Loss: 25.1613   time: 239.75s   best: 24.4469
2023-12-03 02:35:15,902:INFO:  Epoch 301/800:  train Loss: 32.3499   val Loss: 36.9824   time: 69.27s   best: 36.4537
2023-12-03 02:36:25,102:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 02:36:25,121:INFO:  Epoch 302/800:  train Loss: 32.3119   val Loss: 36.2360   time: 69.19s   best: 36.2360
2023-12-03 02:37:34,357:INFO:  Epoch 303/800:  train Loss: 32.3183   val Loss: 36.2567   time: 69.24s   best: 36.2360
2023-12-03 02:38:07,561:INFO:  Epoch 199/500:  train Loss: 20.0986   val Loss: 24.7804   time: 239.89s   best: 24.4469
2023-12-03 02:38:43,671:INFO:  Epoch 304/800:  train Loss: 32.3065   val Loss: 36.6913   time: 69.30s   best: 36.2360
2023-12-03 02:39:52,948:INFO:  Epoch 305/800:  train Loss: 33.2859   val Loss: 38.4130   time: 69.26s   best: 36.2360
2023-12-03 02:41:02,233:INFO:  Epoch 306/800:  train Loss: 33.2104   val Loss: 38.7271   time: 69.28s   best: 36.2360
2023-12-03 02:42:09,324:INFO:  Epoch 200/500:  train Loss: 20.5872   val Loss: 24.9645   time: 241.76s   best: 24.4469
2023-12-03 02:42:11,542:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 02:42:11,564:INFO:  Epoch 307/800:  train Loss: 32.7559   val Loss: 36.1563   time: 69.30s   best: 36.1563
2023-12-03 02:43:21,216:INFO:  Epoch 308/800:  train Loss: 32.0863   val Loss: 36.3539   time: 69.65s   best: 36.1563
2023-12-03 02:44:30,559:INFO:  Epoch 309/800:  train Loss: 32.6497   val Loss: 37.0688   time: 69.34s   best: 36.1563
2023-12-03 02:45:39,805:INFO:  Epoch 310/800:  train Loss: 32.1992   val Loss: 36.1633   time: 69.23s   best: 36.1563
2023-12-03 02:46:09,614:INFO:  Epoch 201/500:  train Loss: 20.3405   val Loss: 24.5648   time: 240.29s   best: 24.4469
2023-12-03 02:46:49,176:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 02:46:49,230:INFO:  Epoch 311/800:  train Loss: 32.2601   val Loss: 36.0576   time: 69.37s   best: 36.0576
2023-12-03 02:47:58,402:INFO:  Epoch 312/800:  train Loss: 32.0753   val Loss: 36.0716   time: 69.16s   best: 36.0576
2023-12-03 02:49:07,625:INFO:  Epoch 313/800:  train Loss: 32.5742   val Loss: 40.1477   time: 69.22s   best: 36.0576
2023-12-03 02:50:10,715:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 02:50:10,735:INFO:  Epoch 202/500:  train Loss: 20.3013   val Loss: 24.0526   time: 241.09s   best: 24.0526
2023-12-03 02:50:16,804:INFO:  Epoch 314/800:  train Loss: 32.5250   val Loss: 36.3129   time: 69.18s   best: 36.0576
2023-12-03 02:51:26,183:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 02:51:26,202:INFO:  Epoch 315/800:  train Loss: 31.8318   val Loss: 35.5648   time: 69.36s   best: 35.5648
2023-12-03 02:52:35,620:INFO:  Epoch 316/800:  train Loss: 31.8517   val Loss: 36.6723   time: 69.41s   best: 35.5648
2023-12-03 02:53:45,109:INFO:  Epoch 317/800:  train Loss: 32.0134   val Loss: 35.9632   time: 69.49s   best: 35.5648
2023-12-03 02:54:11,108:INFO:  Epoch 203/500:  train Loss: 20.2324   val Loss: 25.2428   time: 240.37s   best: 24.0526
2023-12-03 02:54:54,540:INFO:  Epoch 318/800:  train Loss: 32.3144   val Loss: 37.1886   time: 69.43s   best: 35.5648
2023-12-03 02:56:03,814:INFO:  Epoch 319/800:  train Loss: 32.3828   val Loss: 36.2724   time: 69.27s   best: 35.5648
2023-12-03 02:57:13,103:INFO:  Epoch 320/800:  train Loss: 32.1563   val Loss: 36.0056   time: 69.29s   best: 35.5648
2023-12-03 02:58:10,583:INFO:  Epoch 204/500:  train Loss: 20.0898   val Loss: 24.8740   time: 239.46s   best: 24.0526
2023-12-03 02:58:22,320:INFO:  Epoch 321/800:  train Loss: 32.0382   val Loss: 36.5130   time: 69.22s   best: 35.5648
2023-12-03 02:59:31,524:INFO:  Epoch 322/800:  train Loss: 31.6664   val Loss: 35.9083   time: 69.20s   best: 35.5648
2023-12-03 03:00:40,872:INFO:  Epoch 323/800:  train Loss: 32.3247   val Loss: 38.4271   time: 69.34s   best: 35.5648
2023-12-03 03:01:49,948:INFO:  Epoch 324/800:  train Loss: 32.6691   val Loss: 38.2453   time: 69.05s   best: 35.5648
2023-12-03 03:02:11,871:INFO:  Epoch 205/500:  train Loss: 20.0591   val Loss: 24.9764   time: 241.27s   best: 24.0526
2023-12-03 03:02:59,220:INFO:  Epoch 325/800:  train Loss: 32.1430   val Loss: 36.6958   time: 69.27s   best: 35.5648
2023-12-03 03:04:08,436:INFO:  Epoch 326/800:  train Loss: 31.7566   val Loss: 38.7362   time: 69.21s   best: 35.5648
2023-12-03 03:05:17,797:INFO:  Epoch 327/800:  train Loss: 31.9381   val Loss: 35.9121   time: 69.35s   best: 35.5648
2023-12-03 03:06:11,452:INFO:  Epoch 206/500:  train Loss: 20.0566   val Loss: 24.8803   time: 239.57s   best: 24.0526
2023-12-03 03:06:27,022:INFO:  Epoch 328/800:  train Loss: 31.6450   val Loss: 36.1135   time: 69.21s   best: 35.5648
2023-12-03 03:07:36,272:INFO:  Epoch 329/800:  train Loss: 31.7247   val Loss: 36.0481   time: 69.25s   best: 35.5648
2023-12-03 03:08:45,459:INFO:  Epoch 330/800:  train Loss: 31.9015   val Loss: 36.3998   time: 69.19s   best: 35.5648
2023-12-03 03:09:54,766:INFO:  Epoch 331/800:  train Loss: 31.7128   val Loss: 35.6544   time: 69.29s   best: 35.5648
2023-12-03 03:10:11,345:INFO:  Epoch 207/500:  train Loss: 20.1618   val Loss: 25.1568   time: 239.88s   best: 24.0526
2023-12-03 03:11:04,056:INFO:  Epoch 332/800:  train Loss: 31.5630   val Loss: 35.7870   time: 69.28s   best: 35.5648
2023-12-03 03:12:13,474:INFO:  Epoch 333/800:  train Loss: 31.8403   val Loss: 35.8432   time: 69.42s   best: 35.5648
2023-12-03 03:13:23,093:INFO:  Epoch 334/800:  train Loss: 31.7527   val Loss: 36.0100   time: 69.62s   best: 35.5648
2023-12-03 03:14:11,573:INFO:  Epoch 208/500:  train Loss: 20.1966   val Loss: 24.8904   time: 240.22s   best: 24.0526
2023-12-03 03:14:32,447:INFO:  Epoch 335/800:  train Loss: 31.6325   val Loss: 35.8245   time: 69.35s   best: 35.5648
2023-12-03 03:15:41,679:INFO:  Epoch 336/800:  train Loss: 31.4864   val Loss: 36.3442   time: 69.23s   best: 35.5648
2023-12-03 03:16:51,266:INFO:  Epoch 337/800:  train Loss: 31.7895   val Loss: 35.8403   time: 69.57s   best: 35.5648
2023-12-03 03:18:00,532:INFO:  Epoch 338/800:  train Loss: 31.4665   val Loss: 35.7536   time: 69.25s   best: 35.5648
2023-12-03 03:18:12,835:INFO:  Epoch 209/500:  train Loss: 20.0083   val Loss: 24.7440   time: 241.26s   best: 24.0526
2023-12-03 03:19:09,915:INFO:  Epoch 339/800:  train Loss: 31.3325   val Loss: 35.6609   time: 69.38s   best: 35.5648
2023-12-03 03:20:19,176:INFO:  Epoch 340/800:  train Loss: 31.3688   val Loss: 35.5667   time: 69.25s   best: 35.5648
2023-12-03 03:21:28,562:INFO:  Epoch 341/800:  train Loss: 31.3927   val Loss: 35.8045   time: 69.39s   best: 35.5648
2023-12-03 03:22:13,090:INFO:  Epoch 210/500:  train Loss: 19.9515   val Loss: 24.6284   time: 240.25s   best: 24.0526
2023-12-03 03:22:37,880:INFO:  Epoch 342/800:  train Loss: 31.3444   val Loss: 36.1683   time: 69.32s   best: 35.5648
2023-12-03 03:23:47,218:INFO:  Epoch 343/800:  train Loss: 31.3475   val Loss: 36.3550   time: 69.34s   best: 35.5648
2023-12-03 03:24:56,594:INFO:  Epoch 344/800:  train Loss: 31.3626   val Loss: 36.2884   time: 69.37s   best: 35.5648
2023-12-03 03:26:05,925:INFO:  Epoch 345/800:  train Loss: 31.2150   val Loss: 35.9003   time: 69.32s   best: 35.5648
2023-12-03 03:26:14,806:INFO:  Epoch 211/500:  train Loss: 20.5161   val Loss: 24.4335   time: 241.71s   best: 24.0526
2023-12-03 03:27:15,294:INFO:  Epoch 346/800:  train Loss: 32.1088   val Loss: 36.5600   time: 69.36s   best: 35.5648
2023-12-03 03:28:24,561:INFO:  Epoch 347/800:  train Loss: 31.3986   val Loss: 35.7354   time: 69.25s   best: 35.5648
2023-12-03 03:29:33,850:INFO:  Epoch 348/800:  train Loss: 31.3008   val Loss: 36.4855   time: 69.28s   best: 35.5648
2023-12-03 03:30:14,439:INFO:  Epoch 212/500:  train Loss: 20.0980   val Loss: 24.5256   time: 239.62s   best: 24.0526
2023-12-03 03:30:43,362:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 03:30:43,384:INFO:  Epoch 349/800:  train Loss: 31.2681   val Loss: 35.3580   time: 69.50s   best: 35.3580
2023-12-03 03:31:52,635:INFO:  Epoch 350/800:  train Loss: 31.6138   val Loss: 36.8830   time: 69.25s   best: 35.3580
2023-12-03 03:33:02,085:INFO:  Epoch 351/800:  train Loss: 31.2132   val Loss: 35.6763   time: 69.45s   best: 35.3580
2023-12-03 03:34:11,245:INFO:  Epoch 352/800:  train Loss: 31.1332   val Loss: 35.9510   time: 69.15s   best: 35.3580
2023-12-03 03:34:15,858:INFO:  Epoch 213/500:  train Loss: 19.9455   val Loss: 26.5936   time: 241.42s   best: 24.0526
2023-12-03 03:35:20,562:INFO:  Epoch 353/800:  train Loss: 31.2410   val Loss: 35.6446   time: 69.32s   best: 35.3580
2023-12-03 03:36:29,801:INFO:  Epoch 354/800:  train Loss: 31.0989   val Loss: 35.4513   time: 69.24s   best: 35.3580
2023-12-03 03:37:39,040:INFO:  Epoch 355/800:  train Loss: 31.0693   val Loss: 36.1558   time: 69.24s   best: 35.3580
2023-12-03 03:38:17,739:INFO:  Epoch 214/500:  train Loss: 20.2694   val Loss: 25.1311   time: 241.88s   best: 24.0526
2023-12-03 03:38:48,488:INFO:  Epoch 356/800:  train Loss: 31.3501   val Loss: 35.6447   time: 69.44s   best: 35.3580
2023-12-03 03:39:57,758:INFO:  Epoch 357/800:  train Loss: 31.2474   val Loss: 35.9155   time: 69.27s   best: 35.3580
2023-12-03 03:41:07,374:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 03:41:07,393:INFO:  Epoch 358/800:  train Loss: 30.9980   val Loss: 35.2110   time: 69.61s   best: 35.2110
2023-12-03 03:42:16,846:INFO:  Epoch 359/800:  train Loss: 31.2849   val Loss: 36.8507   time: 69.45s   best: 35.2110
2023-12-03 03:42:19,379:INFO:  Epoch 215/500:  train Loss: 19.9502   val Loss: 24.6437   time: 241.62s   best: 24.0526
2023-12-03 03:43:26,148:INFO:  Epoch 360/800:  train Loss: 31.1319   val Loss: 35.7609   time: 69.30s   best: 35.2110
2023-12-03 03:44:35,569:INFO:  Epoch 361/800:  train Loss: 30.9349   val Loss: 35.6460   time: 69.42s   best: 35.2110
2023-12-03 03:45:45,296:INFO:  Epoch 362/800:  train Loss: 30.8925   val Loss: 35.9719   time: 69.72s   best: 35.2110
2023-12-03 03:46:18,736:INFO:  Epoch 216/500:  train Loss: 19.8795   val Loss: 24.8192   time: 239.34s   best: 24.0526
2023-12-03 03:46:54,687:INFO:  Epoch 363/800:  train Loss: 30.7613   val Loss: 35.4720   time: 69.38s   best: 35.2110
2023-12-03 03:48:03,895:INFO:  Epoch 364/800:  train Loss: 30.9199   val Loss: 35.3493   time: 69.20s   best: 35.2110
2023-12-03 03:49:13,303:INFO:  Epoch 365/800:  train Loss: 30.8630   val Loss: 35.5517   time: 69.41s   best: 35.2110
2023-12-03 03:50:19,321:INFO:  Epoch 217/500:  train Loss: 20.1646   val Loss: 26.3930   time: 240.58s   best: 24.0526
2023-12-03 03:50:22,593:INFO:  Epoch 366/800:  train Loss: 30.7122   val Loss: 35.5754   time: 69.27s   best: 35.2110
2023-12-03 03:51:31,863:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 03:51:31,882:INFO:  Epoch 367/800:  train Loss: 31.3497   val Loss: 35.2042   time: 69.25s   best: 35.2042
2023-12-03 03:52:41,330:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 03:52:41,350:INFO:  Epoch 368/800:  train Loss: 30.6852   val Loss: 35.1645   time: 69.44s   best: 35.1645
2023-12-03 03:53:50,812:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 03:53:50,831:INFO:  Epoch 369/800:  train Loss: 30.9485   val Loss: 35.1052   time: 69.45s   best: 35.1052
2023-12-03 03:54:20,664:INFO:  Epoch 218/500:  train Loss: 20.1802   val Loss: 26.3429   time: 241.34s   best: 24.0526
2023-12-03 03:55:00,262:INFO:  Epoch 370/800:  train Loss: 30.9703   val Loss: 36.1211   time: 69.43s   best: 35.1052
2023-12-03 03:56:09,530:INFO:  Epoch 371/800:  train Loss: 30.6587   val Loss: 35.5522   time: 69.27s   best: 35.1052
2023-12-03 03:57:18,940:INFO:  Epoch 372/800:  train Loss: 30.5753   val Loss: 35.3416   time: 69.41s   best: 35.1052
2023-12-03 03:58:21,155:INFO:  Epoch 219/500:  train Loss: 20.1266   val Loss: 24.7475   time: 240.49s   best: 24.0526
2023-12-03 03:58:28,222:INFO:  Epoch 373/800:  train Loss: 30.6156   val Loss: 35.5246   time: 69.28s   best: 35.1052
2023-12-03 03:59:37,492:INFO:  Epoch 374/800:  train Loss: 30.5518   val Loss: 35.1111   time: 69.27s   best: 35.1052
2023-12-03 04:00:46,762:INFO:  Epoch 375/800:  train Loss: 30.7334   val Loss: 36.0929   time: 69.27s   best: 35.1052
2023-12-03 04:01:55,894:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 04:01:55,913:INFO:  Epoch 376/800:  train Loss: 30.9899   val Loss: 34.7221   time: 69.13s   best: 34.7221
2023-12-03 04:02:20,582:INFO:  Epoch 220/500:  train Loss: 20.0250   val Loss: 25.6144   time: 239.42s   best: 24.0526
2023-12-03 04:03:05,330:INFO:  Epoch 377/800:  train Loss: 30.8071   val Loss: 39.1049   time: 69.42s   best: 34.7221
2023-12-03 04:04:14,565:INFO:  Epoch 378/800:  train Loss: 30.8590   val Loss: 35.1737   time: 69.23s   best: 34.7221
2023-12-03 04:05:23,890:INFO:  Epoch 379/800:  train Loss: 30.3851   val Loss: 35.3037   time: 69.31s   best: 34.7221
2023-12-03 04:06:20,041:INFO:  Epoch 221/500:  train Loss: 19.8408   val Loss: 24.8179   time: 239.44s   best: 24.0526
2023-12-03 04:06:33,131:INFO:  Epoch 380/800:  train Loss: 30.4949   val Loss: 35.1190   time: 69.24s   best: 34.7221
2023-12-03 04:07:42,529:INFO:  Epoch 381/800:  train Loss: 30.5357   val Loss: 35.1006   time: 69.39s   best: 34.7221
2023-12-03 04:08:51,700:INFO:  Epoch 382/800:  train Loss: 30.5241   val Loss: 35.8955   time: 69.16s   best: 34.7221
2023-12-03 04:10:00,967:INFO:  Epoch 383/800:  train Loss: 31.2440   val Loss: 36.1754   time: 69.27s   best: 34.7221
2023-12-03 04:10:20,056:INFO:  Epoch 222/500:  train Loss: 19.8845   val Loss: 24.9568   time: 240.00s   best: 24.0526
2023-12-03 04:11:10,295:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 04:11:10,316:INFO:  Epoch 384/800:  train Loss: 30.5153   val Loss: 34.6509   time: 69.32s   best: 34.6509
2023-12-03 04:12:19,759:INFO:  Epoch 385/800:  train Loss: 30.4329   val Loss: 35.2981   time: 69.43s   best: 34.6509
2023-12-03 04:13:29,266:INFO:  Epoch 386/800:  train Loss: 30.5900   val Loss: 35.2400   time: 69.51s   best: 34.6509
2023-12-03 04:14:21,552:INFO:  Epoch 223/500:  train Loss: 19.8979   val Loss: 24.7599   time: 241.48s   best: 24.0526
2023-12-03 04:14:38,663:INFO:  Epoch 387/800:  train Loss: 30.4083   val Loss: 35.1095   time: 69.38s   best: 34.6509
2023-12-03 04:15:47,988:INFO:  Epoch 388/800:  train Loss: 30.2724   val Loss: 34.8951   time: 69.32s   best: 34.6509
2023-12-03 04:16:57,373:INFO:  Epoch 389/800:  train Loss: 30.2659   val Loss: 35.0103   time: 69.38s   best: 34.6509
2023-12-03 04:18:06,712:INFO:  Epoch 390/800:  train Loss: 30.2231   val Loss: 35.1442   time: 69.34s   best: 34.6509
2023-12-03 04:18:21,819:INFO:  Epoch 224/500:  train Loss: 19.8071   val Loss: 25.7924   time: 240.26s   best: 24.0526
2023-12-03 04:19:16,059:INFO:  Epoch 391/800:  train Loss: 30.5949   val Loss: 35.4965   time: 69.34s   best: 34.6509
2023-12-03 04:20:25,302:INFO:  Epoch 392/800:  train Loss: 30.6053   val Loss: 35.8568   time: 69.23s   best: 34.6509
2023-12-03 04:21:34,857:INFO:  Epoch 393/800:  train Loss: 30.3803   val Loss: 36.0924   time: 69.55s   best: 34.6509
2023-12-03 04:22:23,143:INFO:  Epoch 225/500:  train Loss: 19.7685   val Loss: 24.6369   time: 241.31s   best: 24.0526
2023-12-03 04:22:44,239:INFO:  Epoch 394/800:  train Loss: 30.2533   val Loss: 35.0130   time: 69.38s   best: 34.6509
2023-12-03 04:23:53,509:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 04:23:53,529:INFO:  Epoch 395/800:  train Loss: 30.1675   val Loss: 34.2002   time: 69.25s   best: 34.2002
2023-12-03 04:25:02,980:INFO:  Epoch 396/800:  train Loss: 30.2668   val Loss: 35.5167   time: 69.44s   best: 34.2002
2023-12-03 04:26:12,393:INFO:  Epoch 397/800:  train Loss: 30.0831   val Loss: 35.1613   time: 69.41s   best: 34.2002
2023-12-03 04:26:24,466:INFO:  Epoch 226/500:  train Loss: 19.8724   val Loss: 25.3406   time: 241.32s   best: 24.0526
2023-12-03 04:27:21,613:INFO:  Epoch 398/800:  train Loss: 30.3256   val Loss: 35.0033   time: 69.21s   best: 34.2002
2023-12-03 04:28:30,947:INFO:  Epoch 399/800:  train Loss: 30.0807   val Loss: 35.3112   time: 69.33s   best: 34.2002
2023-12-03 04:29:40,142:INFO:  Epoch 400/800:  train Loss: 30.6549   val Loss: 35.4177   time: 69.19s   best: 34.2002
2023-12-03 04:30:26,051:INFO:  Epoch 227/500:  train Loss: 19.9273   val Loss: 24.5883   time: 241.57s   best: 24.0526
2023-12-03 04:30:49,608:INFO:  Epoch 401/800:  train Loss: 30.4543   val Loss: 35.0816   time: 69.46s   best: 34.2002
2023-12-03 04:31:58,705:INFO:  Epoch 402/800:  train Loss: 30.1869   val Loss: 34.8842   time: 69.09s   best: 34.2002
2023-12-03 04:33:07,934:INFO:  Epoch 403/800:  train Loss: 30.0129   val Loss: 35.3639   time: 69.23s   best: 34.2002
2023-12-03 04:34:17,116:INFO:  Epoch 404/800:  train Loss: 30.2309   val Loss: 34.8518   time: 69.18s   best: 34.2002
2023-12-03 04:34:25,555:INFO:  Epoch 228/500:  train Loss: 19.7386   val Loss: 25.5368   time: 239.50s   best: 24.0526
2023-12-03 04:35:26,529:INFO:  Epoch 405/800:  train Loss: 30.0616   val Loss: 34.7553   time: 69.40s   best: 34.2002
2023-12-03 04:36:35,860:INFO:  Epoch 406/800:  train Loss: 29.9572   val Loss: 34.9501   time: 69.33s   best: 34.2002
2023-12-03 04:37:45,280:INFO:  Epoch 407/800:  train Loss: 29.8566   val Loss: 35.1255   time: 69.42s   best: 34.2002
2023-12-03 04:38:27,027:INFO:  Epoch 229/500:  train Loss: 19.8216   val Loss: 24.2255   time: 241.47s   best: 24.0526
2023-12-03 04:38:54,678:INFO:  Epoch 408/800:  train Loss: 30.1747   val Loss: 34.9638   time: 69.39s   best: 34.2002
2023-12-03 04:40:04,032:INFO:  Epoch 409/800:  train Loss: 29.8394   val Loss: 35.0815   time: 69.35s   best: 34.2002
2023-12-03 04:41:13,208:INFO:  Epoch 410/800:  train Loss: 29.8270   val Loss: 34.9278   time: 69.16s   best: 34.2002
2023-12-03 04:42:22,390:INFO:  Epoch 411/800:  train Loss: 30.0746   val Loss: 35.2652   time: 69.18s   best: 34.2002
2023-12-03 04:42:26,199:INFO:  Epoch 230/500:  train Loss: 19.7950   val Loss: 24.5901   time: 239.15s   best: 24.0526
2023-12-03 04:43:31,813:INFO:  Epoch 412/800:  train Loss: 29.8409   val Loss: 35.0152   time: 69.42s   best: 34.2002
2023-12-03 04:44:41,137:INFO:  Epoch 413/800:  train Loss: 29.8113   val Loss: 35.3177   time: 69.32s   best: 34.2002
2023-12-03 04:45:50,476:INFO:  Epoch 414/800:  train Loss: 30.3216   val Loss: 36.4118   time: 69.33s   best: 34.2002
2023-12-03 04:46:25,849:INFO:  Epoch 231/500:  train Loss: 19.8024   val Loss: 24.3667   time: 239.65s   best: 24.0526
2023-12-03 04:46:59,889:INFO:  Epoch 415/800:  train Loss: 30.9818   val Loss: 34.9394   time: 69.40s   best: 34.2002
2023-12-03 04:48:09,271:INFO:  Epoch 416/800:  train Loss: 29.9848   val Loss: 34.8364   time: 69.37s   best: 34.2002
2023-12-03 04:49:18,688:INFO:  Epoch 417/800:  train Loss: 30.0007   val Loss: 34.8892   time: 69.42s   best: 34.2002
2023-12-03 04:50:27,440:INFO:  Epoch 232/500:  train Loss: 19.6954   val Loss: 24.7584   time: 241.59s   best: 24.0526
2023-12-03 04:50:27,801:INFO:  Epoch 418/800:  train Loss: 29.8108   val Loss: 34.9937   time: 69.11s   best: 34.2002
2023-12-03 04:51:37,031:INFO:  Epoch 419/800:  train Loss: 29.6747   val Loss: 34.6884   time: 69.23s   best: 34.2002
2023-12-03 04:52:46,317:INFO:  Epoch 420/800:  train Loss: 29.5948   val Loss: 34.9404   time: 69.28s   best: 34.2002
2023-12-03 04:53:55,761:INFO:  Epoch 421/800:  train Loss: 30.1376   val Loss: 34.8191   time: 69.44s   best: 34.2002
2023-12-03 04:54:27,466:INFO:  Epoch 233/500:  train Loss: 19.7351   val Loss: 29.1432   time: 240.01s   best: 24.0526
2023-12-03 04:55:05,022:INFO:  Epoch 422/800:  train Loss: 29.5575   val Loss: 35.0869   time: 69.26s   best: 34.2002
2023-12-03 04:56:14,449:INFO:  Epoch 423/800:  train Loss: 30.3856   val Loss: 36.7393   time: 69.43s   best: 34.2002
2023-12-03 04:57:23,697:INFO:  Epoch 424/800:  train Loss: 30.4995   val Loss: 35.7886   time: 69.24s   best: 34.2002
2023-12-03 04:58:27,160:INFO:  Epoch 234/500:  train Loss: 19.9508   val Loss: 25.1426   time: 239.68s   best: 24.0526
2023-12-03 04:58:32,918:INFO:  Epoch 425/800:  train Loss: 30.0550   val Loss: 34.4560   time: 69.22s   best: 34.2002
2023-12-03 04:59:42,568:INFO:  Epoch 426/800:  train Loss: 29.5304   val Loss: 34.6626   time: 69.64s   best: 34.2002
2023-12-03 05:00:52,193:INFO:  Epoch 427/800:  train Loss: 29.5242   val Loss: 34.9523   time: 69.62s   best: 34.2002
2023-12-03 05:02:01,723:INFO:  Epoch 428/800:  train Loss: 29.9456   val Loss: 35.2691   time: 69.52s   best: 34.2002
2023-12-03 05:02:27,719:INFO:  Epoch 235/500:  train Loss: 19.7452   val Loss: 24.3426   time: 240.55s   best: 24.0526
2023-12-03 05:03:11,334:INFO:  Epoch 429/800:  train Loss: 29.8209   val Loss: 34.9512   time: 69.60s   best: 34.2002
2023-12-03 05:04:20,764:INFO:  Epoch 430/800:  train Loss: 29.8769   val Loss: 35.1726   time: 69.42s   best: 34.2002
2023-12-03 05:05:30,398:INFO:  Epoch 431/800:  train Loss: 30.0253   val Loss: 34.5186   time: 69.63s   best: 34.2002
2023-12-03 05:06:28,994:INFO:  Epoch 236/500:  train Loss: 19.6085   val Loss: 24.5145   time: 241.25s   best: 24.0526
2023-12-03 05:06:39,916:INFO:  Epoch 432/800:  train Loss: 29.6283   val Loss: 36.0685   time: 69.52s   best: 34.2002
2023-12-03 05:07:49,347:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 05:07:49,366:INFO:  Epoch 433/800:  train Loss: 29.5470   val Loss: 34.0328   time: 69.42s   best: 34.0328
2023-12-03 05:08:58,973:INFO:  Epoch 434/800:  train Loss: 29.4256   val Loss: 34.6713   time: 69.59s   best: 34.0328
2023-12-03 05:10:08,474:INFO:  Epoch 435/800:  train Loss: 29.5825   val Loss: 34.3933   time: 69.49s   best: 34.0328
2023-12-03 05:10:30,392:INFO:  Epoch 237/500:  train Loss: 19.6325   val Loss: 24.8980   time: 241.39s   best: 24.0526
2023-12-03 05:11:18,028:INFO:  Epoch 436/800:  train Loss: 29.6526   val Loss: 35.1652   time: 69.55s   best: 34.0328
2023-12-03 05:12:27,517:INFO:  Epoch 437/800:  train Loss: 29.4462   val Loss: 34.3377   time: 69.48s   best: 34.0328
2023-12-03 05:13:37,069:INFO:  Epoch 438/800:  train Loss: 30.3192   val Loss: 35.5575   time: 69.54s   best: 34.0328
2023-12-03 05:14:31,648:INFO:  Epoch 238/500:  train Loss: 20.2376   val Loss: 24.5581   time: 241.25s   best: 24.0526
2023-12-03 05:14:46,635:INFO:  Epoch 439/800:  train Loss: 29.5818   val Loss: 34.2643   time: 69.56s   best: 34.0328
2023-12-03 05:15:56,107:INFO:  Epoch 440/800:  train Loss: 29.4113   val Loss: 34.8426   time: 69.46s   best: 34.0328
2023-12-03 05:17:05,791:INFO:  Epoch 441/800:  train Loss: 29.9528   val Loss: 34.9897   time: 69.67s   best: 34.0328
2023-12-03 05:18:15,194:INFO:  Epoch 442/800:  train Loss: 29.3511   val Loss: 34.3685   time: 69.39s   best: 34.0328
2023-12-03 05:18:31,352:INFO:  Epoch 239/500:  train Loss: 19.7609   val Loss: 25.2322   time: 239.70s   best: 24.0526
2023-12-03 05:19:24,698:INFO:  Epoch 443/800:  train Loss: 29.5782   val Loss: 34.1885   time: 69.50s   best: 34.0328
2023-12-03 05:20:34,187:INFO:  Epoch 444/800:  train Loss: 29.3089   val Loss: 35.1964   time: 69.47s   best: 34.0328
2023-12-03 05:21:43,637:INFO:  Epoch 445/800:  train Loss: 29.3590   val Loss: 34.8818   time: 69.45s   best: 34.0328
2023-12-03 05:22:32,502:INFO:  Epoch 240/500:  train Loss: 19.6001   val Loss: 25.0274   time: 241.13s   best: 24.0526
2023-12-03 05:22:53,155:INFO:  Epoch 446/800:  train Loss: 29.2382   val Loss: 35.1482   time: 69.51s   best: 34.0328
2023-12-03 05:24:02,607:INFO:  Epoch 447/800:  train Loss: 29.3804   val Loss: 34.3823   time: 69.45s   best: 34.0328
2023-12-03 05:25:12,154:INFO:  Epoch 448/800:  train Loss: 29.2261   val Loss: 34.3687   time: 69.55s   best: 34.0328
2023-12-03 05:26:21,717:INFO:  Epoch 449/800:  train Loss: 29.3325   val Loss: 35.0012   time: 69.55s   best: 34.0328
2023-12-03 05:26:32,257:INFO:  Epoch 241/500:  train Loss: 19.6008   val Loss: 25.1103   time: 239.75s   best: 24.0526
2023-12-03 05:27:31,195:INFO:  Epoch 450/800:  train Loss: 30.1681   val Loss: 42.9057   time: 69.48s   best: 34.0328
2023-12-03 05:28:40,696:INFO:  Epoch 451/800:  train Loss: 31.0243   val Loss: 34.6411   time: 69.49s   best: 34.0328
2023-12-03 05:29:50,234:INFO:  Epoch 452/800:  train Loss: 29.3331   val Loss: 34.1701   time: 69.53s   best: 34.0328
2023-12-03 05:30:32,351:INFO:  Epoch 242/500:  train Loss: 19.8308   val Loss: 24.5313   time: 240.09s   best: 24.0526
2023-12-03 05:30:59,831:INFO:  Epoch 453/800:  train Loss: 29.1103   val Loss: 34.3546   time: 69.60s   best: 34.0328
2023-12-03 05:32:09,313:INFO:  Epoch 454/800:  train Loss: 29.0395   val Loss: 34.4496   time: 69.48s   best: 34.0328
2023-12-03 05:33:18,807:INFO:  Epoch 455/800:  train Loss: 29.0711   val Loss: 34.6124   time: 69.48s   best: 34.0328
2023-12-03 05:34:28,231:INFO:  Epoch 456/800:  train Loss: 29.4101   val Loss: 34.2117   time: 69.41s   best: 34.0328
2023-12-03 05:34:33,860:INFO:  Epoch 243/500:  train Loss: 20.1022   val Loss: 24.8753   time: 241.51s   best: 24.0526
2023-12-03 05:35:37,792:INFO:  Epoch 457/800:  train Loss: 29.0481   val Loss: 34.5304   time: 69.56s   best: 34.0328
2023-12-03 05:36:47,395:INFO:  Epoch 458/800:  train Loss: 29.0468   val Loss: 34.6259   time: 69.60s   best: 34.0328
2023-12-03 05:37:56,885:INFO:  Epoch 459/800:  train Loss: 29.1553   val Loss: 34.6027   time: 69.49s   best: 34.0328
2023-12-03 05:38:33,767:INFO:  Epoch 244/500:  train Loss: 19.6333   val Loss: 24.6277   time: 239.90s   best: 24.0526
2023-12-03 05:39:06,363:INFO:  Epoch 460/800:  train Loss: 29.0380   val Loss: 34.4878   time: 69.48s   best: 34.0328
2023-12-03 05:40:15,894:INFO:  Epoch 461/800:  train Loss: 29.3179   val Loss: 35.7405   time: 69.51s   best: 34.0328
2023-12-03 05:41:25,395:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 05:41:25,414:INFO:  Epoch 462/800:  train Loss: 29.0153   val Loss: 33.8921   time: 69.50s   best: 33.8921
2023-12-03 05:42:34,974:INFO:  Epoch 245/500:  train Loss: 19.6182   val Loss: 24.7716   time: 241.18s   best: 24.0526
2023-12-03 05:42:35,002:INFO:  Epoch 463/800:  train Loss: 29.1926   val Loss: 35.3751   time: 69.58s   best: 33.8921
2023-12-03 05:43:44,446:INFO:  Epoch 464/800:  train Loss: 28.8107   val Loss: 34.2567   time: 69.43s   best: 33.8921
2023-12-03 05:44:53,951:INFO:  Epoch 465/800:  train Loss: 28.7966   val Loss: 34.0612   time: 69.50s   best: 33.8921
2023-12-03 05:46:03,428:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 05:46:03,447:INFO:  Epoch 466/800:  train Loss: 28.9609   val Loss: 33.8181   time: 69.46s   best: 33.8181
2023-12-03 05:46:36,471:INFO:  Epoch 246/500:  train Loss: 19.5723   val Loss: 24.3264   time: 241.49s   best: 24.0526
2023-12-03 05:47:13,163:INFO:  Epoch 467/800:  train Loss: 28.8504   val Loss: 34.4117   time: 69.72s   best: 33.8181
2023-12-03 05:48:22,727:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 05:48:22,746:INFO:  Epoch 468/800:  train Loss: 28.5097   val Loss: 33.3615   time: 69.55s   best: 33.3615
2023-12-03 05:49:32,635:INFO:  Epoch 469/800:  train Loss: 28.5289   val Loss: 33.6180   time: 69.89s   best: 33.3615
2023-12-03 05:50:36,296:INFO:  Epoch 247/500:  train Loss: 19.6855   val Loss: 24.7490   time: 239.81s   best: 24.0526
2023-12-03 05:50:41,919:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 05:50:41,940:INFO:  Epoch 470/800:  train Loss: 28.7351   val Loss: 33.3225   time: 69.28s   best: 33.3225
2023-12-03 05:51:51,181:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 05:51:51,200:INFO:  Epoch 471/800:  train Loss: 28.1626   val Loss: 33.2832   time: 69.23s   best: 33.2832
2023-12-03 05:53:00,839:INFO:  Epoch 472/800:  train Loss: 28.3149   val Loss: 33.4838   time: 69.64s   best: 33.2832
2023-12-03 05:54:10,285:INFO:  Epoch 473/800:  train Loss: 28.2178   val Loss: 34.0834   time: 69.45s   best: 33.2832
2023-12-03 05:54:37,678:INFO:  Epoch 248/500:  train Loss: 19.9041   val Loss: 24.6664   time: 241.37s   best: 24.0526
2023-12-03 05:55:19,680:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 05:55:19,701:INFO:  Epoch 474/800:  train Loss: 28.1007   val Loss: 33.1889   time: 69.39s   best: 33.1889
2023-12-03 05:56:28,871:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 05:56:28,890:INFO:  Epoch 475/800:  train Loss: 28.8819   val Loss: 33.0358   time: 69.16s   best: 33.0358
2023-12-03 05:57:38,078:INFO:  Epoch 476/800:  train Loss: 27.9500   val Loss: 33.1011   time: 69.19s   best: 33.0358
2023-12-03 05:58:38,693:INFO:  Epoch 249/500:  train Loss: 19.5233   val Loss: 24.9987   time: 241.00s   best: 24.0526
2023-12-03 05:58:47,460:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 05:58:47,480:INFO:  Epoch 477/800:  train Loss: 28.0498   val Loss: 32.9838   time: 69.38s   best: 32.9838
2023-12-03 05:59:56,600:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 05:59:56,619:INFO:  Epoch 478/800:  train Loss: 28.0356   val Loss: 32.6312   time: 69.12s   best: 32.6312
2023-12-03 06:01:05,882:INFO:  Epoch 479/800:  train Loss: 28.7450   val Loss: 35.3615   time: 69.26s   best: 32.6312
2023-12-03 06:02:14,924:INFO:  Epoch 480/800:  train Loss: 28.7396   val Loss: 33.0235   time: 69.03s   best: 32.6312
2023-12-03 06:02:38,547:INFO:  Epoch 250/500:  train Loss: 19.6942   val Loss: 25.0609   time: 239.85s   best: 24.0526
2023-12-03 06:03:24,192:INFO:  Epoch 481/800:  train Loss: 28.6131   val Loss: 33.0600   time: 69.26s   best: 32.6312
2023-12-03 06:04:33,306:INFO:  Epoch 482/800:  train Loss: 27.7364   val Loss: 32.9015   time: 69.10s   best: 32.6312
2023-12-03 06:05:42,488:INFO:  Epoch 483/800:  train Loss: 27.8002   val Loss: 33.0421   time: 69.17s   best: 32.6312
2023-12-03 06:06:41,008:INFO:  Epoch 251/500:  train Loss: 19.5973   val Loss: 25.2138   time: 242.46s   best: 24.0526
2023-12-03 06:06:51,866:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 06:06:51,887:INFO:  Epoch 484/800:  train Loss: 27.8814   val Loss: 32.5396   time: 69.37s   best: 32.5396
2023-12-03 06:08:01,092:INFO:  Epoch 485/800:  train Loss: 27.6458   val Loss: 32.5580   time: 69.19s   best: 32.5396
2023-12-03 06:09:10,363:INFO:  Epoch 486/800:  train Loss: 27.7698   val Loss: 32.8304   time: 69.27s   best: 32.5396
2023-12-03 06:10:19,746:INFO:  Epoch 487/800:  train Loss: 27.6183   val Loss: 32.9150   time: 69.38s   best: 32.5396
2023-12-03 06:10:42,585:INFO:  Epoch 252/500:  train Loss: 19.4981   val Loss: 24.7346   time: 241.57s   best: 24.0526
2023-12-03 06:11:29,025:INFO:  Epoch 488/800:  train Loss: 27.9564   val Loss: 32.9213   time: 69.28s   best: 32.5396
2023-12-03 06:12:38,233:INFO:  Epoch 489/800:  train Loss: 27.5425   val Loss: 32.8302   time: 69.20s   best: 32.5396
2023-12-03 06:13:47,196:INFO:  Epoch 490/800:  train Loss: 28.0532   val Loss: 34.7301   time: 68.95s   best: 32.5396
2023-12-03 06:14:43,747:INFO:  Epoch 253/500:  train Loss: 19.4397   val Loss: 24.4704   time: 241.13s   best: 24.0526
2023-12-03 06:14:56,431:INFO:  Epoch 491/800:  train Loss: 27.8899   val Loss: 32.8955   time: 69.22s   best: 32.5396
2023-12-03 06:16:05,535:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 06:16:05,552:INFO:  Epoch 492/800:  train Loss: 27.5874   val Loss: 32.4582   time: 69.09s   best: 32.4582
2023-12-03 06:17:15,251:INFO:  Epoch 493/800:  train Loss: 27.5020   val Loss: 32.7341   time: 69.70s   best: 32.4582
2023-12-03 06:18:24,724:INFO:  Epoch 494/800:  train Loss: 28.2248   val Loss: 39.3666   time: 69.47s   best: 32.4582
2023-12-03 06:18:43,243:INFO:  Epoch 254/500:  train Loss: 19.9389   val Loss: 24.4942   time: 239.49s   best: 24.0526
2023-12-03 06:19:33,856:INFO:  Epoch 495/800:  train Loss: 28.4125   val Loss: 32.8250   time: 69.13s   best: 32.4582
2023-12-03 06:20:42,980:INFO:  Epoch 496/800:  train Loss: 27.3680   val Loss: 32.5763   time: 69.12s   best: 32.4582
2023-12-03 06:21:52,042:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 06:21:52,070:INFO:  Epoch 497/800:  train Loss: 27.4767   val Loss: 32.2338   time: 69.05s   best: 32.2338
2023-12-03 06:22:42,998:INFO:  Epoch 255/500:  train Loss: 19.5973   val Loss: 24.5840   time: 239.75s   best: 24.0526
2023-12-03 06:23:01,204:INFO:  Epoch 498/800:  train Loss: 27.2794   val Loss: 32.8688   time: 69.12s   best: 32.2338
2023-12-03 06:24:10,333:INFO:  Epoch 499/800:  train Loss: 27.7423   val Loss: 32.9886   time: 69.11s   best: 32.2338
2023-12-03 06:25:19,407:INFO:  Epoch 500/800:  train Loss: 27.2581   val Loss: 32.2828   time: 69.07s   best: 32.2338
2023-12-03 06:26:28,474:INFO:  Epoch 501/800:  train Loss: 27.5415   val Loss: 32.6096   time: 69.07s   best: 32.2338
2023-12-03 06:26:42,606:INFO:  Epoch 256/500:  train Loss: 19.6431   val Loss: 24.5441   time: 239.60s   best: 24.0526
2023-12-03 06:27:37,523:INFO:  Epoch 502/800:  train Loss: 27.7846   val Loss: 34.5048   time: 69.04s   best: 32.2338
2023-12-03 06:28:46,681:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 06:28:46,700:INFO:  Epoch 503/800:  train Loss: 27.5485   val Loss: 32.0984   time: 69.14s   best: 32.0984
2023-12-03 06:29:55,901:INFO:  Epoch 504/800:  train Loss: 27.3473   val Loss: 32.5995   time: 69.18s   best: 32.0984
2023-12-03 06:30:42,985:INFO:  Epoch 257/500:  train Loss: 19.5930   val Loss: 26.3612   time: 240.36s   best: 24.0526
2023-12-03 06:31:05,196:INFO:  Epoch 505/800:  train Loss: 27.1702   val Loss: 32.1117   time: 69.29s   best: 32.0984
2023-12-03 06:32:14,193:INFO:  Epoch 506/800:  train Loss: 27.2484   val Loss: 32.1136   time: 68.99s   best: 32.0984
2023-12-03 06:33:23,302:INFO:  Epoch 507/800:  train Loss: 27.2865   val Loss: 32.3193   time: 69.11s   best: 32.0984
2023-12-03 06:34:32,644:INFO:  Epoch 508/800:  train Loss: 27.1391   val Loss: 32.4673   time: 69.34s   best: 32.0984
2023-12-03 06:34:44,364:INFO:  Epoch 258/500:  train Loss: 19.5374   val Loss: 24.1641   time: 241.36s   best: 24.0526
2023-12-03 06:35:41,784:INFO:  Epoch 509/800:  train Loss: 27.4590   val Loss: 32.4701   time: 69.14s   best: 32.0984
2023-12-03 06:36:50,911:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 06:36:50,928:INFO:  Epoch 510/800:  train Loss: 27.2211   val Loss: 32.0969   time: 69.12s   best: 32.0969
2023-12-03 06:38:00,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 06:38:00,108:INFO:  Epoch 511/800:  train Loss: 27.2004   val Loss: 32.0221   time: 69.14s   best: 32.0221
2023-12-03 06:38:45,562:INFO:  Epoch 259/500:  train Loss: 19.6428   val Loss: 25.0096   time: 241.20s   best: 24.0526
2023-12-03 06:39:09,457:INFO:  Epoch 512/800:  train Loss: 27.0118   val Loss: 32.2203   time: 69.34s   best: 32.0221
2023-12-03 06:40:18,946:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 06:40:18,965:INFO:  Epoch 513/800:  train Loss: 27.0496   val Loss: 32.0014   time: 69.46s   best: 32.0014
2023-12-03 06:41:28,327:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 06:41:28,346:INFO:  Epoch 514/800:  train Loss: 27.3396   val Loss: 31.9022   time: 69.36s   best: 31.9022
2023-12-03 06:42:37,368:INFO:  Epoch 515/800:  train Loss: 27.4847   val Loss: 32.9819   time: 69.02s   best: 31.9022
2023-12-03 06:42:44,863:INFO:  Epoch 260/500:  train Loss: 19.3691   val Loss: 24.2937   time: 239.30s   best: 24.0526
2023-12-03 06:43:46,378:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 06:43:46,399:INFO:  Epoch 516/800:  train Loss: 27.1344   val Loss: 31.7952   time: 68.99s   best: 31.7952
2023-12-03 06:44:55,563:INFO:  Epoch 517/800:  train Loss: 26.9087   val Loss: 32.1214   time: 69.16s   best: 31.7952
2023-12-03 06:46:04,582:INFO:  Epoch 518/800:  train Loss: 27.0131   val Loss: 32.5587   time: 69.02s   best: 31.7952
2023-12-03 06:46:44,644:INFO:  Epoch 261/500:  train Loss: 19.5251   val Loss: 24.9799   time: 239.78s   best: 24.0526
2023-12-03 06:47:13,640:INFO:  Epoch 519/800:  train Loss: 26.7932   val Loss: 32.0976   time: 69.06s   best: 31.7952
2023-12-03 06:48:22,771:INFO:  Epoch 520/800:  train Loss: 27.8431   val Loss: 37.0976   time: 69.11s   best: 31.7952
2023-12-03 06:49:31,932:INFO:  Epoch 521/800:  train Loss: 28.0722   val Loss: 32.3646   time: 69.16s   best: 31.7952
2023-12-03 06:50:40,913:INFO:  Epoch 522/800:  train Loss: 26.9867   val Loss: 32.1122   time: 68.98s   best: 31.7952
2023-12-03 06:50:44,144:INFO:  Epoch 262/500:  train Loss: 19.3442   val Loss: 24.5755   time: 239.49s   best: 24.0526
2023-12-03 06:51:50,239:INFO:  Epoch 523/800:  train Loss: 27.0167   val Loss: 32.0907   time: 69.31s   best: 31.7952
2023-12-03 06:52:59,467:INFO:  Epoch 524/800:  train Loss: 26.8844   val Loss: 32.0556   time: 69.23s   best: 31.7952
2023-12-03 06:54:08,555:INFO:  Epoch 525/800:  train Loss: 26.8640   val Loss: 32.6177   time: 69.06s   best: 31.7952
2023-12-03 06:54:45,583:INFO:  Epoch 263/500:  train Loss: 19.4370   val Loss: 24.4633   time: 241.42s   best: 24.0526
2023-12-03 06:55:17,829:INFO:  Epoch 526/800:  train Loss: 26.8680   val Loss: 32.2429   time: 69.27s   best: 31.7952
2023-12-03 06:56:27,029:INFO:  Epoch 527/800:  train Loss: 26.9762   val Loss: 32.3020   time: 69.19s   best: 31.7952
2023-12-03 06:57:36,040:INFO:  Epoch 528/800:  train Loss: 27.0984   val Loss: 33.0490   time: 69.00s   best: 31.7952
2023-12-03 06:58:45,181:INFO:  Epoch 529/800:  train Loss: 26.7495   val Loss: 32.0649   time: 69.13s   best: 31.7952
2023-12-03 06:58:47,636:INFO:  Epoch 264/500:  train Loss: 19.4516   val Loss: 24.6343   time: 242.05s   best: 24.0526
2023-12-03 06:59:54,229:INFO:  Epoch 530/800:  train Loss: 26.9112   val Loss: 32.1622   time: 69.05s   best: 31.7952
2023-12-03 07:01:03,375:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 07:01:03,392:INFO:  Epoch 531/800:  train Loss: 26.7409   val Loss: 31.7139   time: 69.13s   best: 31.7139
2023-12-03 07:02:12,450:INFO:  Epoch 532/800:  train Loss: 26.6449   val Loss: 31.9774   time: 69.06s   best: 31.7139
2023-12-03 07:02:48,772:INFO:  Epoch 265/500:  train Loss: 19.5873   val Loss: 24.9482   time: 241.12s   best: 24.0526
2023-12-03 07:03:21,673:INFO:  Epoch 533/800:  train Loss: 26.8023   val Loss: 31.8772   time: 69.21s   best: 31.7139
2023-12-03 07:04:30,934:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 07:04:30,975:INFO:  Epoch 534/800:  train Loss: 26.6625   val Loss: 31.5675   time: 69.25s   best: 31.5675
2023-12-03 07:05:40,014:INFO:  Epoch 535/800:  train Loss: 26.8568   val Loss: 32.0109   time: 69.04s   best: 31.5675
2023-12-03 07:06:48,557:INFO:  Epoch 266/500:  train Loss: 19.4523   val Loss: 24.2925   time: 239.76s   best: 24.0526
2023-12-03 07:06:49,007:INFO:  Epoch 536/800:  train Loss: 26.7944   val Loss: 32.1242   time: 68.99s   best: 31.5675
2023-12-03 07:07:58,071:INFO:  Epoch 537/800:  train Loss: 27.0646   val Loss: 32.5026   time: 69.06s   best: 31.5675
2023-12-03 07:09:07,080:INFO:  Epoch 538/800:  train Loss: 26.6082   val Loss: 31.9790   time: 69.01s   best: 31.5675
2023-12-03 07:10:16,076:INFO:  Epoch 539/800:  train Loss: 26.9348   val Loss: 31.7704   time: 68.98s   best: 31.5675
2023-12-03 07:10:48,561:INFO:  Epoch 267/500:  train Loss: 19.4138   val Loss: 24.2199   time: 240.00s   best: 24.0526
2023-12-03 07:11:25,071:INFO:  Epoch 540/800:  train Loss: 26.8278   val Loss: 31.9038   time: 68.99s   best: 31.5675
2023-12-03 07:12:34,121:INFO:  Epoch 541/800:  train Loss: 26.5090   val Loss: 32.4649   time: 69.03s   best: 31.5675
2023-12-03 07:13:43,201:INFO:  Epoch 542/800:  train Loss: 26.5018   val Loss: 31.8733   time: 69.07s   best: 31.5675
2023-12-03 07:14:49,765:INFO:  Epoch 268/500:  train Loss: 19.5079   val Loss: 24.4974   time: 241.19s   best: 24.0526
2023-12-03 07:14:52,396:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 07:14:52,434:INFO:  Epoch 543/800:  train Loss: 26.4827   val Loss: 31.4372   time: 69.18s   best: 31.4372
2023-12-03 07:16:01,680:INFO:  Epoch 544/800:  train Loss: 26.4771   val Loss: 31.7093   time: 69.23s   best: 31.4372
2023-12-03 07:17:10,887:INFO:  Epoch 545/800:  train Loss: 26.3826   val Loss: 31.5943   time: 69.21s   best: 31.4372
2023-12-03 07:18:19,966:INFO:  Epoch 546/800:  train Loss: 26.4046   val Loss: 31.8899   time: 69.08s   best: 31.4372
2023-12-03 07:18:50,746:INFO:  Epoch 269/500:  train Loss: 19.4953   val Loss: 24.4761   time: 240.96s   best: 24.0526
2023-12-03 07:19:29,230:INFO:  Epoch 547/800:  train Loss: 26.5408   val Loss: 31.8997   time: 69.26s   best: 31.4372
2023-12-03 07:20:38,421:INFO:  Epoch 548/800:  train Loss: 26.4527   val Loss: 32.1985   time: 69.19s   best: 31.4372
2023-12-03 07:21:47,508:INFO:  Epoch 549/800:  train Loss: 26.9770   val Loss: 31.6908   time: 69.09s   best: 31.4372
2023-12-03 07:22:52,077:INFO:  Epoch 270/500:  train Loss: 19.4408   val Loss: 29.5664   time: 241.33s   best: 24.0526
2023-12-03 07:22:56,449:INFO:  Epoch 550/800:  train Loss: 26.2573   val Loss: 31.7539   time: 68.94s   best: 31.4372
2023-12-03 07:24:05,755:INFO:  Epoch 551/800:  train Loss: 26.4968   val Loss: 33.2220   time: 69.28s   best: 31.4372
2023-12-03 07:25:15,304:INFO:  Epoch 552/800:  train Loss: 26.8520   val Loss: 32.2893   time: 69.54s   best: 31.4372
2023-12-03 07:26:24,676:INFO:  Epoch 553/800:  train Loss: 26.3417   val Loss: 31.4971   time: 69.37s   best: 31.4372
2023-12-03 07:26:51,167:INFO:  Epoch 271/500:  train Loss: 19.5966   val Loss: 24.5930   time: 239.08s   best: 24.0526
2023-12-03 07:27:33,861:INFO:  Epoch 554/800:  train Loss: 26.1687   val Loss: 31.6166   time: 69.18s   best: 31.4372
2023-12-03 07:28:43,212:INFO:  Epoch 555/800:  train Loss: 26.3573   val Loss: 31.9287   time: 69.35s   best: 31.4372
2023-12-03 07:29:52,434:INFO:  Epoch 556/800:  train Loss: 26.3719   val Loss: 32.3166   time: 69.22s   best: 31.4372
2023-12-03 07:30:50,499:INFO:  Epoch 272/500:  train Loss: 19.2990   val Loss: 24.5346   time: 239.32s   best: 24.0526
2023-12-03 07:31:01,762:INFO:  Epoch 557/800:  train Loss: 26.2210   val Loss: 31.7535   time: 69.33s   best: 31.4372
2023-12-03 07:32:10,988:INFO:  Epoch 558/800:  train Loss: 26.1173   val Loss: 31.5376   time: 69.22s   best: 31.4372
2023-12-03 07:33:20,383:INFO:  Epoch 559/800:  train Loss: 27.2550   val Loss: 33.1452   time: 69.38s   best: 31.4372
2023-12-03 07:34:29,487:INFO:  Epoch 560/800:  train Loss: 28.0548   val Loss: 32.3101   time: 69.09s   best: 31.4372
2023-12-03 07:34:49,956:INFO:  Epoch 273/500:  train Loss: 19.7093   val Loss: 29.5783   time: 239.45s   best: 24.0526
2023-12-03 07:35:38,748:INFO:  Epoch 561/800:  train Loss: 26.7270   val Loss: 31.7361   time: 69.25s   best: 31.4372
2023-12-03 07:36:48,126:INFO:  Epoch 562/800:  train Loss: 26.1224   val Loss: 32.0022   time: 69.37s   best: 31.4372
2023-12-03 07:37:57,494:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 07:37:57,524:INFO:  Epoch 563/800:  train Loss: 26.1428   val Loss: 31.1129   time: 69.35s   best: 31.1129
2023-12-03 07:38:52,466:INFO:  Epoch 274/500:  train Loss: 19.3977   val Loss: 24.5272   time: 242.50s   best: 24.0526
2023-12-03 07:39:06,620:INFO:  Epoch 564/800:  train Loss: 26.1027   val Loss: 31.2223   time: 69.10s   best: 31.1129
2023-12-03 07:40:15,652:INFO:  Epoch 565/800:  train Loss: 26.0290   val Loss: 31.5252   time: 69.02s   best: 31.1129
2023-12-03 07:41:25,000:INFO:  Epoch 566/800:  train Loss: 26.0066   val Loss: 32.0064   time: 69.34s   best: 31.1129
2023-12-03 07:42:34,131:INFO:  Epoch 567/800:  train Loss: 25.9909   val Loss: 32.1187   time: 69.12s   best: 31.1129
2023-12-03 07:42:52,649:INFO:  Epoch 275/500:  train Loss: 19.3003   val Loss: 25.0544   time: 240.18s   best: 24.0526
2023-12-03 07:43:43,097:INFO:  Epoch 568/800:  train Loss: 26.2950   val Loss: 32.0079   time: 68.96s   best: 31.1129
2023-12-03 07:44:52,294:INFO:  Epoch 569/800:  train Loss: 26.6982   val Loss: 33.2415   time: 69.17s   best: 31.1129
2023-12-03 07:46:01,378:INFO:  Epoch 570/800:  train Loss: 26.7132   val Loss: 31.7622   time: 69.08s   best: 31.1129
2023-12-03 07:46:53,521:INFO:  Epoch 276/500:  train Loss: 19.3064   val Loss: 24.4146   time: 240.86s   best: 24.0526
2023-12-03 07:47:10,726:INFO:  Epoch 571/800:  train Loss: 26.0256   val Loss: 31.4080   time: 69.35s   best: 31.1129
2023-12-03 07:48:19,738:INFO:  Epoch 572/800:  train Loss: 26.1164   val Loss: 31.3116   time: 69.01s   best: 31.1129
2023-12-03 07:49:28,841:INFO:  Epoch 573/800:  train Loss: 25.8893   val Loss: 31.8969   time: 69.10s   best: 31.1129
2023-12-03 07:50:38,161:INFO:  Epoch 574/800:  train Loss: 25.9293   val Loss: 31.5586   time: 69.31s   best: 31.1129
2023-12-03 07:50:53,442:INFO:  Epoch 277/500:  train Loss: 19.2469   val Loss: 24.1337   time: 239.92s   best: 24.0526
2023-12-03 07:51:47,433:INFO:  Epoch 575/800:  train Loss: 25.8051   val Loss: 31.5199   time: 69.26s   best: 31.1129
2023-12-03 07:52:56,572:INFO:  Epoch 576/800:  train Loss: 25.8871   val Loss: 31.1139   time: 69.13s   best: 31.1129
2023-12-03 07:54:05,616:INFO:  Epoch 577/800:  train Loss: 26.1325   val Loss: 31.6309   time: 69.04s   best: 31.1129
2023-12-03 07:54:54,328:INFO:  Epoch 278/500:  train Loss: 19.4294   val Loss: 24.5695   time: 240.87s   best: 24.0526
2023-12-03 07:55:14,612:INFO:  Epoch 578/800:  train Loss: 26.7348   val Loss: 31.4213   time: 69.00s   best: 31.1129
2023-12-03 07:56:23,716:INFO:  Epoch 579/800:  train Loss: 25.9100   val Loss: 31.2588   time: 69.09s   best: 31.1129
2023-12-03 07:57:33,281:INFO:  Epoch 580/800:  train Loss: 26.0100   val Loss: 31.4188   time: 69.56s   best: 31.1129
2023-12-03 07:58:42,866:INFO:  Epoch 581/800:  train Loss: 27.4503   val Loss: 31.4205   time: 69.58s   best: 31.1129
2023-12-03 07:58:54,624:INFO:  Epoch 279/500:  train Loss: 19.3141   val Loss: 24.2070   time: 240.29s   best: 24.0526
2023-12-03 07:59:52,563:INFO:  Epoch 582/800:  train Loss: 25.9578   val Loss: 31.8811   time: 69.70s   best: 31.1129
2023-12-03 08:01:02,343:INFO:  Epoch 583/800:  train Loss: 25.8240   val Loss: 31.4836   time: 69.77s   best: 31.1129
2023-12-03 08:02:12,015:INFO:  Epoch 584/800:  train Loss: 26.6811   val Loss: 31.9465   time: 69.66s   best: 31.1129
2023-12-03 08:02:55,701:INFO:  Epoch 280/500:  train Loss: 19.1999   val Loss: 24.2667   time: 241.07s   best: 24.0526
2023-12-03 08:03:21,546:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 08:03:21,567:INFO:  Epoch 585/800:  train Loss: 25.9403   val Loss: 30.8822   time: 69.51s   best: 30.8822
2023-12-03 08:04:30,670:INFO:  Epoch 586/800:  train Loss: 25.6131   val Loss: 31.8275   time: 69.09s   best: 30.8822
2023-12-03 08:05:39,733:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 08:05:39,752:INFO:  Epoch 587/800:  train Loss: 25.9086   val Loss: 30.7835   time: 69.05s   best: 30.7835
2023-12-03 08:06:48,897:INFO:  Epoch 588/800:  train Loss: 25.9514   val Loss: 32.1701   time: 69.15s   best: 30.7835
2023-12-03 08:06:55,135:INFO:  Epoch 281/500:  train Loss: 19.3471   val Loss: 24.3185   time: 239.43s   best: 24.0526
2023-12-03 08:07:57,855:INFO:  Epoch 589/800:  train Loss: 25.7100   val Loss: 31.1421   time: 68.95s   best: 30.7835
2023-12-03 08:09:06,780:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 08:09:06,799:INFO:  Epoch 590/800:  train Loss: 25.6349   val Loss: 30.6536   time: 68.91s   best: 30.6536
2023-12-03 08:10:15,958:INFO:  Epoch 591/800:  train Loss: 25.8594   val Loss: 32.4070   time: 69.16s   best: 30.6536
2023-12-03 08:10:56,892:INFO:  Epoch 282/500:  train Loss: 19.6222   val Loss: 24.5893   time: 241.75s   best: 24.0526
2023-12-03 08:11:24,771:INFO:  Epoch 592/800:  train Loss: 25.7804   val Loss: 31.2123   time: 68.81s   best: 30.6536
2023-12-03 08:12:33,615:INFO:  Epoch 593/800:  train Loss: 25.6194   val Loss: 31.2363   time: 68.83s   best: 30.6536
2023-12-03 08:13:42,398:INFO:  Epoch 594/800:  train Loss: 25.7218   val Loss: 31.1312   time: 68.77s   best: 30.6536
2023-12-03 08:14:51,252:INFO:  Epoch 595/800:  train Loss: 26.1252   val Loss: 31.2308   time: 68.85s   best: 30.6536
2023-12-03 08:14:57,919:INFO:  Epoch 283/500:  train Loss: 19.2106   val Loss: 24.4069   time: 241.00s   best: 24.0526
2023-12-03 08:16:00,117:INFO:  Epoch 596/800:  train Loss: 25.6751   val Loss: 31.6201   time: 68.86s   best: 30.6536
2023-12-03 08:17:09,013:INFO:  Epoch 597/800:  train Loss: 25.5280   val Loss: 31.5645   time: 68.88s   best: 30.6536
2023-12-03 08:18:17,859:INFO:  Epoch 598/800:  train Loss: 25.5066   val Loss: 31.4309   time: 68.85s   best: 30.6536
2023-12-03 08:18:57,450:INFO:  Epoch 284/500:  train Loss: 19.2020   val Loss: 24.2426   time: 239.52s   best: 24.0526
2023-12-03 08:19:26,797:INFO:  Epoch 599/800:  train Loss: 25.5038   val Loss: 32.0297   time: 68.93s   best: 30.6536
2023-12-03 08:20:36,095:INFO:  Epoch 600/800:  train Loss: 25.5196   val Loss: 31.1869   time: 69.30s   best: 30.6536
2023-12-03 08:21:45,569:INFO:  Epoch 601/800:  train Loss: 25.4560   val Loss: 31.7621   time: 69.46s   best: 30.6536
2023-12-03 08:22:55,035:INFO:  Epoch 602/800:  train Loss: 25.6358   val Loss: 31.5622   time: 69.47s   best: 30.6536
2023-12-03 08:22:57,176:INFO:  Epoch 285/500:  train Loss: 19.2243   val Loss: 24.9677   time: 239.71s   best: 24.0526
2023-12-03 08:24:04,419:INFO:  Epoch 603/800:  train Loss: 25.7037   val Loss: 32.8995   time: 69.37s   best: 30.6536
2023-12-03 08:25:13,845:INFO:  Epoch 604/800:  train Loss: 25.8655   val Loss: 31.5440   time: 69.41s   best: 30.6536
2023-12-03 08:26:23,100:INFO:  Epoch 605/800:  train Loss: 25.5692   val Loss: 31.2857   time: 69.24s   best: 30.6536
2023-12-03 08:26:57,222:INFO:  Epoch 286/500:  train Loss: 19.4927   val Loss: 24.6117   time: 240.04s   best: 24.0526
2023-12-03 08:27:32,530:INFO:  Epoch 606/800:  train Loss: 25.9314   val Loss: 32.5608   time: 69.42s   best: 30.6536
2023-12-03 08:28:41,915:INFO:  Epoch 607/800:  train Loss: 25.4463   val Loss: 31.0549   time: 69.35s   best: 30.6536
2023-12-03 08:29:51,274:INFO:  Epoch 608/800:  train Loss: 25.2933   val Loss: 31.3012   time: 69.36s   best: 30.6536
2023-12-03 08:30:58,821:INFO:  Epoch 287/500:  train Loss: 19.4226   val Loss: 24.5800   time: 241.58s   best: 24.0526
2023-12-03 08:31:00,455:INFO:  Epoch 609/800:  train Loss: 25.3696   val Loss: 31.2592   time: 69.18s   best: 30.6536
2023-12-03 08:32:09,601:INFO:  Epoch 610/800:  train Loss: 25.3799   val Loss: 31.4137   time: 69.13s   best: 30.6536
2023-12-03 08:33:18,834:INFO:  Epoch 611/800:  train Loss: 25.3642   val Loss: 31.5170   time: 69.23s   best: 30.6536
2023-12-03 08:34:28,032:INFO:  Epoch 612/800:  train Loss: 25.9295   val Loss: 33.0196   time: 69.19s   best: 30.6536
2023-12-03 08:34:58,862:INFO:  Epoch 288/500:  train Loss: 19.1497   val Loss: 24.5979   time: 240.02s   best: 24.0526
2023-12-03 08:35:37,108:INFO:  Epoch 613/800:  train Loss: 25.5359   val Loss: 31.1079   time: 69.06s   best: 30.6536
2023-12-03 08:36:45,995:INFO:  Epoch 614/800:  train Loss: 25.8104   val Loss: 32.4881   time: 68.88s   best: 30.6536
2023-12-03 08:37:55,242:INFO:  Epoch 615/800:  train Loss: 25.3675   val Loss: 31.8157   time: 69.24s   best: 30.6536
2023-12-03 08:39:00,353:INFO:  Epoch 289/500:  train Loss: 19.2182   val Loss: 25.2349   time: 241.49s   best: 24.0526
2023-12-03 08:39:04,178:INFO:  Epoch 616/800:  train Loss: 25.3707   val Loss: 31.5330   time: 68.93s   best: 30.6536
2023-12-03 08:40:13,324:INFO:  Epoch 617/800:  train Loss: 25.2456   val Loss: 31.1758   time: 69.14s   best: 30.6536
2023-12-03 08:41:22,430:INFO:  Epoch 618/800:  train Loss: 25.2306   val Loss: 30.9817   time: 69.11s   best: 30.6536
2023-12-03 08:42:31,326:INFO:  Epoch 619/800:  train Loss: 25.1735   val Loss: 31.3238   time: 68.88s   best: 30.6536
2023-12-03 08:42:59,913:INFO:  Epoch 290/500:  train Loss: 19.2307   val Loss: 24.3797   time: 239.54s   best: 24.0526
2023-12-03 08:43:40,131:INFO:  Epoch 620/800:  train Loss: 25.4962   val Loss: 31.2077   time: 68.80s   best: 30.6536
2023-12-03 08:44:49,041:INFO:  Epoch 621/800:  train Loss: 25.1993   val Loss: 31.4086   time: 68.91s   best: 30.6536
2023-12-03 08:45:57,837:INFO:  Epoch 622/800:  train Loss: 25.0639   val Loss: 30.7860   time: 68.78s   best: 30.6536
2023-12-03 08:47:00,677:INFO:  Epoch 291/500:  train Loss: 19.1841   val Loss: 24.1957   time: 240.75s   best: 24.0526
2023-12-03 08:47:06,683:INFO:  Epoch 623/800:  train Loss: 25.3730   val Loss: 31.2741   time: 68.85s   best: 30.6536
2023-12-03 08:48:15,961:INFO:  Epoch 624/800:  train Loss: 26.2912   val Loss: 31.1631   time: 69.28s   best: 30.6536
2023-12-03 08:49:25,282:INFO:  Epoch 625/800:  train Loss: 25.3424   val Loss: 30.7833   time: 69.31s   best: 30.6536
2023-12-03 08:50:34,058:INFO:  Epoch 626/800:  train Loss: 25.0432   val Loss: 30.8544   time: 68.76s   best: 30.6536
2023-12-03 08:51:00,449:INFO:  Epoch 292/500:  train Loss: 19.1934   val Loss: 24.8996   time: 239.77s   best: 24.0526
2023-12-03 08:51:42,982:INFO:  Epoch 627/800:  train Loss: 24.9943   val Loss: 30.9862   time: 68.92s   best: 30.6536
2023-12-03 08:52:52,007:INFO:  Epoch 628/800:  train Loss: 25.2134   val Loss: 31.7869   time: 69.01s   best: 30.6536
2023-12-03 08:54:01,132:INFO:  Epoch 629/800:  train Loss: 25.1444   val Loss: 31.3888   time: 69.12s   best: 30.6536
2023-12-03 08:55:01,961:INFO:  Epoch 293/500:  train Loss: 20.2964   val Loss: 24.3539   time: 241.51s   best: 24.0526
2023-12-03 08:55:10,032:INFO:  Epoch 630/800:  train Loss: 25.1176   val Loss: 30.9562   time: 68.89s   best: 30.6536
2023-12-03 08:56:18,896:INFO:  Epoch 631/800:  train Loss: 25.5398   val Loss: 39.5614   time: 68.86s   best: 30.6536
2023-12-03 08:57:27,865:INFO:  Epoch 632/800:  train Loss: 26.0231   val Loss: 31.1658   time: 68.97s   best: 30.6536
2023-12-03 08:58:36,898:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 08:58:36,918:INFO:  Epoch 633/800:  train Loss: 25.1578   val Loss: 30.6308   time: 69.03s   best: 30.6308
2023-12-03 08:59:03,620:INFO:  Epoch 294/500:  train Loss: 19.1055   val Loss: 24.2112   time: 241.64s   best: 24.0526
2023-12-03 08:59:45,770:INFO:  Epoch 634/800:  train Loss: 24.8976   val Loss: 31.3835   time: 68.84s   best: 30.6308
2023-12-03 09:00:54,733:INFO:  Epoch 635/800:  train Loss: 25.1366   val Loss: 31.3706   time: 68.95s   best: 30.6308
2023-12-03 09:02:03,482:INFO:  Epoch 636/800:  train Loss: 24.9184   val Loss: 31.2088   time: 68.75s   best: 30.6308
2023-12-03 09:03:05,019:INFO:  Epoch 295/500:  train Loss: 19.2257   val Loss: 24.5721   time: 241.39s   best: 24.0526
2023-12-03 09:03:12,363:INFO:  Epoch 637/800:  train Loss: 25.4220   val Loss: 31.5093   time: 68.88s   best: 30.6308
2023-12-03 09:04:21,155:INFO:  Epoch 638/800:  train Loss: 25.1773   val Loss: 31.1718   time: 68.79s   best: 30.6308
2023-12-03 09:05:29,984:INFO:  Epoch 639/800:  train Loss: 25.0656   val Loss: 31.1924   time: 68.81s   best: 30.6308
2023-12-03 09:06:38,906:INFO:  Epoch 640/800:  train Loss: 25.1590   val Loss: 30.7801   time: 68.91s   best: 30.6308
2023-12-03 09:07:06,477:INFO:  Epoch 296/500:  train Loss: 19.3330   val Loss: 24.1140   time: 241.44s   best: 24.0526
2023-12-03 09:07:47,707:INFO:  Epoch 641/800:  train Loss: 24.8503   val Loss: 30.7271   time: 68.80s   best: 30.6308
2023-12-03 09:08:56,489:INFO:  Epoch 642/800:  train Loss: 24.9121   val Loss: 31.4790   time: 68.78s   best: 30.6308
2023-12-03 09:10:05,361:INFO:  Epoch 643/800:  train Loss: 25.0016   val Loss: 31.1778   time: 68.87s   best: 30.6308
2023-12-03 09:11:07,519:INFO:  Epoch 297/500:  train Loss: 19.2210   val Loss: 24.6267   time: 241.04s   best: 24.0526
2023-12-03 09:11:14,356:INFO:  Epoch 644/800:  train Loss: 24.8606   val Loss: 30.9318   time: 68.99s   best: 30.6308
2023-12-03 09:12:23,277:INFO:  Epoch 645/800:  train Loss: 25.0194   val Loss: 31.1600   time: 68.92s   best: 30.6308
2023-12-03 09:13:32,151:INFO:  Epoch 646/800:  train Loss: 25.4753   val Loss: 30.6915   time: 68.87s   best: 30.6308
2023-12-03 09:14:40,975:INFO:  Epoch 647/800:  train Loss: 25.0348   val Loss: 31.9668   time: 68.82s   best: 30.6308
2023-12-03 09:15:08,513:INFO:  Epoch 298/500:  train Loss: 19.0637   val Loss: 24.6835   time: 240.99s   best: 24.0526
2023-12-03 09:15:49,770:INFO:  Epoch 648/800:  train Loss: 25.1137   val Loss: 31.2267   time: 68.79s   best: 30.6308
2023-12-03 09:16:58,575:INFO:  Epoch 649/800:  train Loss: 24.8023   val Loss: 31.1841   time: 68.79s   best: 30.6308
2023-12-03 09:18:07,518:INFO:  Epoch 650/800:  train Loss: 24.7327   val Loss: 31.0768   time: 68.93s   best: 30.6308
2023-12-03 09:19:09,344:INFO:  Epoch 299/500:  train Loss: 19.3602   val Loss: 24.9624   time: 240.83s   best: 24.0526
2023-12-03 09:19:16,411:INFO:  Epoch 651/800:  train Loss: 24.8006   val Loss: 30.8857   time: 68.88s   best: 30.6308
2023-12-03 09:20:25,374:INFO:  Epoch 652/800:  train Loss: 25.1303   val Loss: 32.4390   time: 68.95s   best: 30.6308
2023-12-03 09:21:34,225:INFO:  Epoch 653/800:  train Loss: 24.9063   val Loss: 31.2546   time: 68.84s   best: 30.6308
2023-12-03 09:22:42,934:INFO:  Epoch 654/800:  train Loss: 25.0725   val Loss: 31.2350   time: 68.71s   best: 30.6308
2023-12-03 09:23:09,298:INFO:  Epoch 300/500:  train Loss: 19.0476   val Loss: 24.4228   time: 239.93s   best: 24.0526
2023-12-03 09:23:51,713:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 09:23:51,734:INFO:  Epoch 655/800:  train Loss: 24.7760   val Loss: 30.3236   time: 68.78s   best: 30.3236
2023-12-03 09:25:00,618:INFO:  Epoch 656/800:  train Loss: 24.7187   val Loss: 30.8602   time: 68.87s   best: 30.3236
2023-12-03 09:26:09,760:INFO:  Epoch 657/800:  train Loss: 24.6335   val Loss: 30.5159   time: 69.14s   best: 30.3236
2023-12-03 09:27:10,717:INFO:  Epoch 301/500:  train Loss: 19.1228   val Loss: 24.4965   time: 241.42s   best: 24.0526
2023-12-03 09:27:18,678:INFO:  Epoch 658/800:  train Loss: 24.9688   val Loss: 31.9462   time: 68.91s   best: 30.3236
2023-12-03 09:28:27,571:INFO:  Epoch 659/800:  train Loss: 24.6898   val Loss: 31.0245   time: 68.88s   best: 30.3236
2023-12-03 09:29:36,414:INFO:  Epoch 660/800:  train Loss: 25.6466   val Loss: 32.3392   time: 68.84s   best: 30.3236
2023-12-03 09:30:45,375:INFO:  Epoch 661/800:  train Loss: 24.9044   val Loss: 31.4902   time: 68.96s   best: 30.3236
2023-12-03 09:31:12,310:INFO:  Epoch 302/500:  train Loss: 19.2137   val Loss: 24.5472   time: 241.59s   best: 24.0526
2023-12-03 09:31:54,313:INFO:  Epoch 662/800:  train Loss: 24.5974   val Loss: 30.7250   time: 68.94s   best: 30.3236
2023-12-03 09:33:03,164:INFO:  Epoch 663/800:  train Loss: 24.5736   val Loss: 30.5892   time: 68.85s   best: 30.3236
2023-12-03 09:34:11,960:INFO:  Epoch 664/800:  train Loss: 24.9517   val Loss: 31.4116   time: 68.79s   best: 30.3236
2023-12-03 09:35:12,320:INFO:  Epoch 303/500:  train Loss: 19.0277   val Loss: 24.7453   time: 239.99s   best: 24.0526
2023-12-03 09:35:20,855:INFO:  Epoch 665/800:  train Loss: 24.5837   val Loss: 30.6203   time: 68.88s   best: 30.3236
2023-12-03 09:36:30,008:INFO:  Epoch 666/800:  train Loss: 24.6112   val Loss: 31.4218   time: 69.15s   best: 30.3236
2023-12-03 09:37:39,115:INFO:  Epoch 667/800:  train Loss: 24.7296   val Loss: 31.2129   time: 69.09s   best: 30.3236
2023-12-03 09:38:47,897:INFO:  Epoch 668/800:  train Loss: 24.6196   val Loss: 30.7007   time: 68.78s   best: 30.3236
2023-12-03 09:39:12,286:INFO:  Epoch 304/500:  train Loss: 19.1912   val Loss: 25.8422   time: 239.96s   best: 24.0526
2023-12-03 09:39:56,750:INFO:  Epoch 669/800:  train Loss: 24.6360   val Loss: 30.7218   time: 68.84s   best: 30.3236
2023-12-03 09:41:05,638:INFO:  Epoch 670/800:  train Loss: 25.2948   val Loss: 32.6423   time: 68.87s   best: 30.3236
2023-12-03 09:42:14,664:INFO:  Epoch 671/800:  train Loss: 24.9874   val Loss: 30.6767   time: 69.02s   best: 30.3236
2023-12-03 09:43:12,273:INFO:  Epoch 305/500:  train Loss: 19.0140   val Loss: 24.1205   time: 239.97s   best: 24.0526
2023-12-03 09:43:23,470:INFO:  Epoch 672/800:  train Loss: 25.1410   val Loss: 31.0924   time: 68.80s   best: 30.3236
2023-12-03 09:44:32,298:INFO:  Epoch 673/800:  train Loss: 24.7169   val Loss: 30.4378   time: 68.81s   best: 30.3236
2023-12-03 09:45:41,122:INFO:  Epoch 674/800:  train Loss: 24.4512   val Loss: 31.0624   time: 68.81s   best: 30.3236
2023-12-03 09:46:50,098:INFO:  Epoch 675/800:  train Loss: 24.4565   val Loss: 30.9033   time: 68.96s   best: 30.3236
2023-12-03 09:47:13,920:INFO:  Epoch 306/500:  train Loss: 19.0886   val Loss: 25.1003   time: 241.62s   best: 24.0526
2023-12-03 09:47:58,928:INFO:  Epoch 676/800:  train Loss: 24.5340   val Loss: 30.5303   time: 68.82s   best: 30.3236
2023-12-03 09:49:07,811:INFO:  Epoch 677/800:  train Loss: 24.4734   val Loss: 30.5819   time: 68.87s   best: 30.3236
2023-12-03 09:50:16,575:INFO:  Epoch 678/800:  train Loss: 24.4622   val Loss: 31.1427   time: 68.75s   best: 30.3236
2023-12-03 09:51:15,504:INFO:  Epoch 307/500:  train Loss: 19.0752   val Loss: 24.7651   time: 241.58s   best: 24.0526
2023-12-03 09:51:25,412:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 09:51:25,433:INFO:  Epoch 679/800:  train Loss: 24.3743   val Loss: 30.2376   time: 68.83s   best: 30.2376
2023-12-03 09:52:34,240:INFO:  Epoch 680/800:  train Loss: 24.9453   val Loss: 31.6051   time: 68.80s   best: 30.2376
2023-12-03 09:53:43,261:INFO:  Epoch 681/800:  train Loss: 24.7567   val Loss: 30.7815   time: 69.02s   best: 30.2376
2023-12-03 09:54:51,977:INFO:  Epoch 682/800:  train Loss: 24.5082   val Loss: 31.0774   time: 68.70s   best: 30.2376
2023-12-03 09:55:14,929:INFO:  Epoch 308/500:  train Loss: 19.4678   val Loss: 24.4127   time: 239.40s   best: 24.0526
2023-12-03 09:56:00,738:INFO:  Epoch 683/800:  train Loss: 24.4133   val Loss: 31.0601   time: 68.76s   best: 30.2376
2023-12-03 09:57:09,638:INFO:  Epoch 684/800:  train Loss: 24.4637   val Loss: 30.8878   time: 68.90s   best: 30.2376
2023-12-03 09:58:18,477:INFO:  Epoch 685/800:  train Loss: 24.4300   val Loss: 30.5108   time: 68.83s   best: 30.2376
2023-12-03 09:59:14,370:INFO:  Epoch 309/500:  train Loss: 19.0642   val Loss: 27.6932   time: 239.44s   best: 24.0526
2023-12-03 09:59:27,378:INFO:  Epoch 686/800:  train Loss: 24.2569   val Loss: 31.5921   time: 68.90s   best: 30.2376
2023-12-03 10:00:36,615:INFO:  Epoch 687/800:  train Loss: 24.9213   val Loss: 32.7986   time: 69.23s   best: 30.2376
2023-12-03 10:01:45,771:INFO:  Epoch 688/800:  train Loss: 24.5618   val Loss: 30.9698   time: 69.15s   best: 30.2376
2023-12-03 10:02:54,988:INFO:  Epoch 689/800:  train Loss: 24.2513   val Loss: 31.4510   time: 69.21s   best: 30.2376
2023-12-03 10:03:14,341:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 10:03:14,361:INFO:  Epoch 310/500:  train Loss: 18.9803   val Loss: 23.9888   time: 239.95s   best: 23.9888
2023-12-03 10:04:04,247:INFO:  Epoch 690/800:  train Loss: 24.2355   val Loss: 30.9881   time: 69.26s   best: 30.2376
2023-12-03 10:05:13,195:INFO:  Epoch 691/800:  train Loss: 24.4423   val Loss: 30.6761   time: 68.93s   best: 30.2376
2023-12-03 10:06:21,980:INFO:  Epoch 692/800:  train Loss: 24.3489   val Loss: 30.8261   time: 68.77s   best: 30.2376
2023-12-03 10:07:15,201:INFO:  Epoch 311/500:  train Loss: 19.0226   val Loss: 24.9484   time: 240.83s   best: 23.9888
2023-12-03 10:07:30,702:INFO:  Epoch 693/800:  train Loss: 24.4743   val Loss: 30.6138   time: 68.71s   best: 30.2376
2023-12-03 10:08:39,449:INFO:  Epoch 694/800:  train Loss: 24.6111   val Loss: 31.1832   time: 68.74s   best: 30.2376
2023-12-03 10:09:48,536:INFO:  Epoch 695/800:  train Loss: 24.4693   val Loss: 31.9115   time: 69.09s   best: 30.2376
2023-12-03 10:10:57,528:INFO:  Epoch 696/800:  train Loss: 25.2510   val Loss: 34.1367   time: 68.99s   best: 30.2376
2023-12-03 10:11:16,763:INFO:  Epoch 312/500:  train Loss: 18.9232   val Loss: 24.7861   time: 241.54s   best: 23.9888
2023-12-03 10:12:06,556:INFO:  Epoch 697/800:  train Loss: 25.5362   val Loss: 30.9597   time: 69.03s   best: 30.2376
2023-12-03 10:13:15,491:INFO:  Epoch 698/800:  train Loss: 24.7845   val Loss: 30.6814   time: 68.92s   best: 30.2376
2023-12-03 10:14:24,334:INFO:  Epoch 699/800:  train Loss: 24.4174   val Loss: 30.9841   time: 68.83s   best: 30.2376
2023-12-03 10:15:16,055:INFO:  Epoch 313/500:  train Loss: 19.1268   val Loss: 25.1470   time: 239.29s   best: 23.9888
2023-12-03 10:15:33,160:INFO:  Epoch 700/800:  train Loss: 24.6600   val Loss: 31.1542   time: 68.81s   best: 30.2376
2023-12-03 10:16:42,004:INFO:  Epoch 701/800:  train Loss: 24.2626   val Loss: 30.6111   time: 68.84s   best: 30.2376
2023-12-03 10:17:51,078:INFO:  Epoch 702/800:  train Loss: 24.1906   val Loss: 30.5379   time: 69.05s   best: 30.2376
2023-12-03 10:19:00,016:INFO:  Epoch 703/800:  train Loss: 24.1576   val Loss: 30.5884   time: 68.94s   best: 30.2376
2023-12-03 10:19:15,367:INFO:  Epoch 314/500:  train Loss: 19.0537   val Loss: 24.9070   time: 239.30s   best: 23.9888
2023-12-03 10:20:08,875:INFO:  Epoch 704/800:  train Loss: 24.1376   val Loss: 30.7651   time: 68.86s   best: 30.2376
2023-12-03 10:21:17,674:INFO:  Epoch 705/800:  train Loss: 24.1683   val Loss: 30.9070   time: 68.80s   best: 30.2376
2023-12-03 10:22:26,430:INFO:  Epoch 706/800:  train Loss: 24.2817   val Loss: 30.7950   time: 68.76s   best: 30.2376
2023-12-03 10:23:16,182:INFO:  Epoch 315/500:  train Loss: 18.9668   val Loss: 24.1066   time: 240.81s   best: 23.9888
2023-12-03 10:23:35,289:INFO:  Epoch 707/800:  train Loss: 24.1774   val Loss: 30.4000   time: 68.85s   best: 30.2376
2023-12-03 10:24:44,034:INFO:  Epoch 708/800:  train Loss: 24.3763   val Loss: 30.9552   time: 68.74s   best: 30.2376
2023-12-03 10:25:52,727:INFO:  Epoch 709/800:  train Loss: 24.0730   val Loss: 30.6503   time: 68.69s   best: 30.2376
2023-12-03 10:27:01,414:INFO:  Epoch 710/800:  train Loss: 24.2895   val Loss: 30.8798   time: 68.69s   best: 30.2376
2023-12-03 10:27:15,915:INFO:  Epoch 316/500:  train Loss: 18.9019   val Loss: 24.4530   time: 239.73s   best: 23.9888
2023-12-03 10:28:10,223:INFO:  Epoch 711/800:  train Loss: 24.2117   val Loss: 30.5003   time: 68.81s   best: 30.2376
2023-12-03 10:29:18,952:INFO:  Epoch 712/800:  train Loss: 24.3825   val Loss: 31.1595   time: 68.73s   best: 30.2376
2023-12-03 10:30:27,752:INFO:  Epoch 713/800:  train Loss: 25.7375   val Loss: 30.6540   time: 68.80s   best: 30.2376
2023-12-03 10:31:15,806:INFO:  Epoch 317/500:  train Loss: 19.1154   val Loss: 24.5988   time: 239.88s   best: 23.9888
2023-12-03 10:31:36,941:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 10:31:36,962:INFO:  Epoch 714/800:  train Loss: 24.0411   val Loss: 30.1662   time: 69.18s   best: 30.1662
2023-12-03 10:32:45,943:INFO:  Epoch 715/800:  train Loss: 23.9342   val Loss: 30.9099   time: 68.98s   best: 30.1662
2023-12-03 10:33:54,709:INFO:  Epoch 716/800:  train Loss: 24.1459   val Loss: 31.3278   time: 68.77s   best: 30.1662
2023-12-03 10:35:03,846:INFO:  Epoch 717/800:  train Loss: 24.0292   val Loss: 31.1935   time: 69.13s   best: 30.1662
2023-12-03 10:35:16,923:INFO:  Epoch 318/500:  train Loss: 19.0853   val Loss: 24.5501   time: 241.11s   best: 23.9888
2023-12-03 10:36:12,902:INFO:  Epoch 718/800:  train Loss: 24.2076   val Loss: 30.6488   time: 69.05s   best: 30.1662
2023-12-03 10:37:21,851:INFO:  Epoch 719/800:  train Loss: 24.0982   val Loss: 30.3994   time: 68.95s   best: 30.1662
2023-12-03 10:38:30,611:INFO:  Epoch 720/800:  train Loss: 23.9876   val Loss: 30.7146   time: 68.76s   best: 30.1662
2023-12-03 10:39:17,950:INFO:  Epoch 319/500:  train Loss: 19.1016   val Loss: 24.6686   time: 241.02s   best: 23.9888
2023-12-03 10:39:39,562:INFO:  Epoch 721/800:  train Loss: 24.0450   val Loss: 30.9434   time: 68.94s   best: 30.1662
2023-12-03 10:40:48,320:INFO:  Epoch 722/800:  train Loss: 24.0851   val Loss: 31.0984   time: 68.76s   best: 30.1662
2023-12-03 10:41:57,127:INFO:  Epoch 723/800:  train Loss: 24.0365   val Loss: 31.0633   time: 68.81s   best: 30.1662
2023-12-03 10:43:05,907:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 10:43:05,926:INFO:  Epoch 724/800:  train Loss: 23.8186   val Loss: 30.1543   time: 68.77s   best: 30.1543
2023-12-03 10:43:17,126:INFO:  Epoch 320/500:  train Loss: 19.0204   val Loss: 24.6470   time: 239.16s   best: 23.9888
2023-12-03 10:44:14,736:INFO:  Epoch 725/800:  train Loss: 24.2830   val Loss: 31.5769   time: 68.81s   best: 30.1543
2023-12-03 10:45:23,580:INFO:  Epoch 726/800:  train Loss: 24.0396   val Loss: 30.3594   time: 68.83s   best: 30.1543
2023-12-03 10:46:32,475:INFO:  Epoch 727/800:  train Loss: 23.8717   val Loss: 30.8315   time: 68.88s   best: 30.1543
2023-12-03 10:47:17,606:INFO:  Epoch 321/500:  train Loss: 18.8815   val Loss: 24.2556   time: 240.47s   best: 23.9888
2023-12-03 10:47:41,272:INFO:  Epoch 728/800:  train Loss: 24.1062   val Loss: 30.8738   time: 68.80s   best: 30.1543
2023-12-03 10:48:50,205:INFO:  Epoch 729/800:  train Loss: 23.9443   val Loss: 30.5884   time: 68.91s   best: 30.1543
2023-12-03 10:49:58,936:INFO:  Epoch 730/800:  train Loss: 23.8616   val Loss: 30.3628   time: 68.73s   best: 30.1543
2023-12-03 10:51:07,760:INFO:  Epoch 731/800:  train Loss: 24.0608   val Loss: 30.6586   time: 68.81s   best: 30.1543
2023-12-03 10:51:19,268:INFO:  Epoch 322/500:  train Loss: 19.2094   val Loss: 24.4519   time: 241.65s   best: 23.9888
2023-12-03 10:52:16,708:INFO:  Epoch 732/800:  train Loss: 23.8129   val Loss: 30.5682   time: 68.94s   best: 30.1543
2023-12-03 10:53:25,570:INFO:  Epoch 733/800:  train Loss: 23.9025   val Loss: 31.2404   time: 68.86s   best: 30.1543
2023-12-03 10:54:34,527:INFO:  Epoch 734/800:  train Loss: 23.7279   val Loss: 30.6740   time: 68.95s   best: 30.1543
2023-12-03 10:55:19,559:INFO:  Epoch 323/500:  train Loss: 18.9335   val Loss: 24.7387   time: 240.28s   best: 23.9888
2023-12-03 10:55:43,662:INFO:  Epoch 735/800:  train Loss: 24.0181   val Loss: 30.9717   time: 69.13s   best: 30.1543
2023-12-03 10:56:52,527:INFO:  Epoch 736/800:  train Loss: 23.8894   val Loss: 30.8629   time: 68.85s   best: 30.1543
2023-12-03 10:58:01,300:INFO:  Epoch 737/800:  train Loss: 23.8364   val Loss: 30.6773   time: 68.77s   best: 30.1543
2023-12-03 10:59:10,128:INFO:  Epoch 738/800:  train Loss: 23.8297   val Loss: 30.9097   time: 68.82s   best: 30.1543
2023-12-03 10:59:18,984:INFO:  Epoch 324/500:  train Loss: 18.9029   val Loss: 24.3493   time: 239.42s   best: 23.9888
2023-12-03 11:00:18,957:INFO:  Epoch 739/800:  train Loss: 23.8857   val Loss: 31.0441   time: 68.82s   best: 30.1543
2023-12-03 11:01:27,828:INFO:  Epoch 740/800:  train Loss: 23.8233   val Loss: 31.0166   time: 68.87s   best: 30.1543
2023-12-03 11:02:36,685:INFO:  Epoch 741/800:  train Loss: 23.6558   val Loss: 30.9889   time: 68.86s   best: 30.1543
2023-12-03 11:03:18,689:INFO:  Epoch 325/500:  train Loss: 18.9372   val Loss: 24.3350   time: 239.70s   best: 23.9888
2023-12-03 11:03:45,623:INFO:  Epoch 742/800:  train Loss: 24.5063   val Loss: 31.8614   time: 68.94s   best: 30.1543
2023-12-03 11:04:54,648:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 11:04:54,677:INFO:  Epoch 743/800:  train Loss: 24.3793   val Loss: 30.1337   time: 69.01s   best: 30.1337
2023-12-03 11:06:03,473:INFO:  Epoch 744/800:  train Loss: 23.6862   val Loss: 30.2824   time: 68.80s   best: 30.1337
2023-12-03 11:07:12,447:INFO:  Epoch 745/800:  train Loss: 23.7340   val Loss: 31.1483   time: 68.97s   best: 30.1337
2023-12-03 11:07:18,091:INFO:  Epoch 326/500:  train Loss: 19.0315   val Loss: 24.2559   time: 239.39s   best: 23.9888
2023-12-03 11:08:21,392:INFO:  Epoch 746/800:  train Loss: 23.6194   val Loss: 30.6469   time: 68.94s   best: 30.1337
2023-12-03 11:09:30,193:INFO:  Epoch 747/800:  train Loss: 23.8501   val Loss: 30.3363   time: 68.80s   best: 30.1337
2023-12-03 11:10:39,397:INFO:  Epoch 748/800:  train Loss: 23.6383   val Loss: 30.4553   time: 69.19s   best: 30.1337
2023-12-03 11:11:19,868:INFO:  Epoch 327/500:  train Loss: 19.6221   val Loss: 24.3849   time: 241.77s   best: 23.9888
2023-12-03 11:11:48,476:INFO:  Epoch 749/800:  train Loss: 23.6368   val Loss: 30.4938   time: 69.07s   best: 30.1337
2023-12-03 11:12:57,591:INFO:  Epoch 750/800:  train Loss: 24.3423   val Loss: 30.9912   time: 69.10s   best: 30.1337
2023-12-03 11:14:06,689:INFO:  Epoch 751/800:  train Loss: 23.8001   val Loss: 30.7034   time: 69.10s   best: 30.1337
2023-12-03 11:15:15,615:INFO:  Epoch 752/800:  train Loss: 23.5489   val Loss: 30.7854   time: 68.91s   best: 30.1337
2023-12-03 11:15:21,406:INFO:  Epoch 328/500:  train Loss: 18.9539   val Loss: 24.6096   time: 241.52s   best: 23.9888
2023-12-03 11:16:24,519:INFO:  Epoch 753/800:  train Loss: 23.5719   val Loss: 30.1919   time: 68.90s   best: 30.1337
2023-12-03 11:17:33,356:INFO:  Epoch 754/800:  train Loss: 24.0496   val Loss: 31.0964   time: 68.82s   best: 30.1337
2023-12-03 11:18:42,135:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_91d4.pt
2023-12-03 11:18:42,153:INFO:  Epoch 755/800:  train Loss: 23.6745   val Loss: 29.9769   time: 68.77s   best: 29.9769
2023-12-03 11:19:22,389:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 11:19:22,410:INFO:  Epoch 329/500:  train Loss: 19.0436   val Loss: 23.9352   time: 240.96s   best: 23.9352
2023-12-03 11:19:51,112:INFO:  Epoch 756/800:  train Loss: 24.4436   val Loss: 30.8250   time: 68.95s   best: 29.9769
2023-12-03 11:20:59,956:INFO:  Epoch 757/800:  train Loss: 23.6614   val Loss: 30.7257   time: 68.83s   best: 29.9769
2023-12-03 11:22:08,703:INFO:  Epoch 758/800:  train Loss: 24.0018   val Loss: 32.6734   time: 68.74s   best: 29.9769
2023-12-03 11:23:17,575:INFO:  Epoch 759/800:  train Loss: 24.4393   val Loss: 30.1421   time: 68.87s   best: 29.9769
2023-12-03 11:23:22,670:INFO:  Epoch 330/500:  train Loss: 18.7882   val Loss: 24.9334   time: 240.25s   best: 23.9352
2023-12-03 11:24:26,420:INFO:  Epoch 760/800:  train Loss: 23.5458   val Loss: 30.6855   time: 68.83s   best: 29.9769
2023-12-03 11:25:35,429:INFO:  Epoch 761/800:  train Loss: 23.4881   val Loss: 30.7021   time: 69.00s   best: 29.9769
2023-12-03 11:26:44,500:INFO:  Epoch 762/800:  train Loss: 23.9641   val Loss: 31.8151   time: 69.07s   best: 29.9769
2023-12-03 11:27:21,940:INFO:  Epoch 331/500:  train Loss: 18.9234   val Loss: 24.9597   time: 239.27s   best: 23.9352
2023-12-03 11:27:53,372:INFO:  Epoch 763/800:  train Loss: 23.7033   val Loss: 30.4945   time: 68.85s   best: 29.9769
2023-12-03 11:29:02,395:INFO:  Epoch 764/800:  train Loss: 24.0915   val Loss: 31.2621   time: 69.02s   best: 29.9769
2023-12-03 11:30:11,303:INFO:  Epoch 765/800:  train Loss: 23.7802   val Loss: 30.4973   time: 68.91s   best: 29.9769
2023-12-03 11:31:20,282:INFO:  Epoch 766/800:  train Loss: 23.5831   val Loss: 30.7559   time: 68.97s   best: 29.9769
2023-12-03 11:31:21,762:INFO:  Epoch 332/500:  train Loss: 18.9244   val Loss: 24.7528   time: 239.82s   best: 23.9352
2023-12-03 11:32:29,266:INFO:  Epoch 767/800:  train Loss: 23.9157   val Loss: 31.0975   time: 68.98s   best: 29.9769
2023-12-03 11:33:38,056:INFO:  Epoch 768/800:  train Loss: 23.5002   val Loss: 30.3794   time: 68.78s   best: 29.9769
2023-12-03 11:34:47,057:INFO:  Epoch 769/800:  train Loss: 23.5676   val Loss: 30.4939   time: 69.00s   best: 29.9769
2023-12-03 11:35:21,207:INFO:  Epoch 333/500:  train Loss: 18.8075   val Loss: 24.0977   time: 239.43s   best: 23.9352
2023-12-03 11:35:56,082:INFO:  Epoch 770/800:  train Loss: 23.6393   val Loss: 30.1698   time: 69.02s   best: 29.9769
2023-12-03 11:37:05,100:INFO:  Epoch 771/800:  train Loss: 23.4013   val Loss: 31.2703   time: 69.01s   best: 29.9769
2023-12-03 11:38:13,853:INFO:  Epoch 772/800:  train Loss: 23.6956   val Loss: 30.3156   time: 68.74s   best: 29.9769
2023-12-03 11:39:21,232:INFO:  Epoch 334/500:  train Loss: 19.7793   val Loss: 24.1896   time: 240.01s   best: 23.9352
2023-12-03 11:39:23,077:INFO:  Epoch 773/800:  train Loss: 23.4993   val Loss: 30.6616   time: 69.22s   best: 29.9769
2023-12-03 11:40:31,875:INFO:  Epoch 774/800:  train Loss: 23.4803   val Loss: 30.6042   time: 68.79s   best: 29.9769
2023-12-03 11:41:40,695:INFO:  Epoch 775/800:  train Loss: 23.4832   val Loss: 30.0138   time: 68.81s   best: 29.9769
2023-12-03 11:42:49,506:INFO:  Epoch 776/800:  train Loss: 23.7205   val Loss: 30.4959   time: 68.80s   best: 29.9769
2023-12-03 11:43:20,950:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 11:43:20,973:INFO:  Epoch 335/500:  train Loss: 18.8579   val Loss: 23.8627   time: 239.70s   best: 23.8627
2023-12-03 11:43:58,301:INFO:  Epoch 777/800:  train Loss: 23.7426   val Loss: 30.9763   time: 68.78s   best: 29.9769
2023-12-03 11:45:07,161:INFO:  Epoch 778/800:  train Loss: 24.1814   val Loss: 30.1477   time: 68.84s   best: 29.9769
2023-12-03 11:46:15,996:INFO:  Epoch 779/800:  train Loss: 23.6011   val Loss: 31.4451   time: 68.82s   best: 29.9769
2023-12-03 11:47:20,754:INFO:  Epoch 336/500:  train Loss: 18.8252   val Loss: 24.4582   time: 239.77s   best: 23.8627
2023-12-03 11:47:24,946:INFO:  Epoch 780/800:  train Loss: 23.3975   val Loss: 30.2769   time: 68.95s   best: 29.9769
2023-12-03 11:48:33,829:INFO:  Epoch 781/800:  train Loss: 23.3300   val Loss: 30.0991   time: 68.87s   best: 29.9769
2023-12-03 11:49:42,658:INFO:  Epoch 782/800:  train Loss: 23.5688   val Loss: 30.5176   time: 68.83s   best: 29.9769
2023-12-03 11:50:51,441:INFO:  Epoch 783/800:  train Loss: 23.3189   val Loss: 30.3889   time: 68.76s   best: 29.9769
2023-12-03 11:51:20,576:INFO:  Epoch 337/500:  train Loss: 18.8543   val Loss: 24.4311   time: 239.81s   best: 23.8627
2023-12-03 11:52:00,307:INFO:  Epoch 784/800:  train Loss: 23.2625   val Loss: 30.5396   time: 68.87s   best: 29.9769
2023-12-03 11:53:09,343:INFO:  Epoch 785/800:  train Loss: 23.3500   val Loss: 30.5717   time: 69.02s   best: 29.9769
2023-12-03 11:54:18,323:INFO:  Epoch 786/800:  train Loss: 24.3240   val Loss: 31.1365   time: 68.97s   best: 29.9769
2023-12-03 11:55:22,220:INFO:  Epoch 338/500:  train Loss: 18.7408   val Loss: 24.6606   time: 241.63s   best: 23.8627
2023-12-03 11:55:27,248:INFO:  Epoch 787/800:  train Loss: 23.4150   val Loss: 30.1200   time: 68.91s   best: 29.9769
2023-12-03 11:56:36,290:INFO:  Epoch 788/800:  train Loss: 26.3811   val Loss: 31.8209   time: 69.03s   best: 29.9769
2023-12-03 11:57:45,342:INFO:  Epoch 789/800:  train Loss: 24.2017   val Loss: 30.2859   time: 69.05s   best: 29.9769
2023-12-03 11:58:54,289:INFO:  Epoch 790/800:  train Loss: 23.5441   val Loss: 30.7492   time: 68.94s   best: 29.9769
2023-12-03 11:59:21,927:INFO:  Epoch 339/500:  train Loss: 18.7847   val Loss: 24.7357   time: 239.68s   best: 23.8627
2023-12-03 12:00:03,163:INFO:  Epoch 791/800:  train Loss: 23.3397   val Loss: 30.9144   time: 68.87s   best: 29.9769
2023-12-03 12:01:11,952:INFO:  Epoch 792/800:  train Loss: 23.2498   val Loss: 30.3093   time: 68.77s   best: 29.9769
2023-12-03 12:02:20,768:INFO:  Epoch 793/800:  train Loss: 23.4704   val Loss: 30.2715   time: 68.82s   best: 29.9769
2023-12-03 12:03:21,030:INFO:  Epoch 340/500:  train Loss: 19.1969   val Loss: 24.2481   time: 239.09s   best: 23.8627
2023-12-03 12:03:29,644:INFO:  Epoch 794/800:  train Loss: 23.2645   val Loss: 30.6312   time: 68.88s   best: 29.9769
2023-12-03 12:04:38,432:INFO:  Epoch 795/800:  train Loss: 23.4419   val Loss: 30.4497   time: 68.78s   best: 29.9769
2023-12-03 12:05:47,282:INFO:  Epoch 796/800:  train Loss: 23.6522   val Loss: 30.4390   time: 68.85s   best: 29.9769
2023-12-03 12:06:56,189:INFO:  Epoch 797/800:  train Loss: 23.2061   val Loss: 30.5968   time: 68.91s   best: 29.9769
2023-12-03 12:07:20,609:INFO:  Epoch 341/500:  train Loss: 18.7714   val Loss: 24.1119   time: 239.56s   best: 23.8627
2023-12-03 12:08:05,009:INFO:  Epoch 798/800:  train Loss: 23.9064   val Loss: 30.7189   time: 68.82s   best: 29.9769
2023-12-03 12:09:13,978:INFO:  Epoch 799/800:  train Loss: 23.6875   val Loss: 31.3438   time: 68.97s   best: 29.9769
2023-12-03 12:10:22,720:INFO:  Epoch 800/800:  train Loss: 23.2557   val Loss: 30.2769   time: 68.73s   best: 29.9769
2023-12-03 12:10:22,720:INFO:  -----> Training complete in 923m 20s   best validation loss: 29.9769
 
2023-12-03 12:11:21,402:INFO:  Epoch 342/500:  train Loss: 18.8547   val Loss: 24.1919   time: 240.77s   best: 23.8627
2023-12-03 12:15:21,111:INFO:  Epoch 343/500:  train Loss: 18.7637   val Loss: 24.9658   time: 239.71s   best: 23.8627
2023-12-03 12:19:20,076:INFO:  Epoch 344/500:  train Loss: 18.7266   val Loss: 24.8486   time: 238.96s   best: 23.8627
2023-12-03 12:23:19,483:INFO:  Epoch 345/500:  train Loss: 18.7215   val Loss: 24.5502   time: 239.39s   best: 23.8627
2023-12-03 12:27:18,945:INFO:  Epoch 346/500:  train Loss: 18.7385   val Loss: 24.7359   time: 239.46s   best: 23.8627
2023-12-03 12:31:20,427:INFO:  Epoch 347/500:  train Loss: 18.6301   val Loss: 24.2923   time: 241.46s   best: 23.8627
2023-12-03 12:35:19,819:INFO:  Epoch 348/500:  train Loss: 18.8294   val Loss: 25.4135   time: 239.38s   best: 23.8627
2023-12-03 12:39:19,231:INFO:  Epoch 349/500:  train Loss: 18.9080   val Loss: 24.7610   time: 239.40s   best: 23.8627
2023-12-03 12:43:20,923:INFO:  Epoch 350/500:  train Loss: 18.7639   val Loss: 24.7794   time: 241.68s   best: 23.8627
2023-12-03 12:47:22,627:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 12:47:22,645:INFO:  Epoch 351/500:  train Loss: 19.0573   val Loss: 23.7493   time: 241.69s   best: 23.7493
2023-12-03 12:51:23,607:INFO:  Epoch 352/500:  train Loss: 18.7619   val Loss: 24.5560   time: 240.96s   best: 23.7493
2023-12-03 12:55:23,519:INFO:  Epoch 353/500:  train Loss: 18.9228   val Loss: 24.2905   time: 239.91s   best: 23.7493
2023-12-03 12:59:23,558:INFO:  Epoch 354/500:  train Loss: 18.6865   val Loss: 25.0377   time: 240.03s   best: 23.7493
2023-12-03 13:03:25,495:INFO:  Epoch 355/500:  train Loss: 18.6216   val Loss: 24.8112   time: 241.94s   best: 23.7493
2023-12-03 13:07:26,899:INFO:  Epoch 356/500:  train Loss: 18.7925   val Loss: 24.6427   time: 241.40s   best: 23.7493
2023-12-03 13:11:27,524:INFO:  Epoch 357/500:  train Loss: 18.7330   val Loss: 23.8290   time: 240.61s   best: 23.7493
2023-12-03 13:15:27,214:INFO:  Epoch 358/500:  train Loss: 18.7814   val Loss: 27.9388   time: 239.68s   best: 23.7493
2023-12-03 13:19:26,961:INFO:  Epoch 359/500:  train Loss: 18.8469   val Loss: 24.3470   time: 239.74s   best: 23.7493
2023-12-03 13:23:27,401:INFO:  Epoch 360/500:  train Loss: 18.6134   val Loss: 24.2962   time: 240.43s   best: 23.7493
2023-12-03 13:27:29,168:INFO:  Epoch 361/500:  train Loss: 18.6000   val Loss: 24.5625   time: 241.76s   best: 23.7493
2023-12-03 13:31:29,221:INFO:  Epoch 362/500:  train Loss: 18.6883   val Loss: 24.7830   time: 240.05s   best: 23.7493
2023-12-03 13:35:28,642:INFO:  Epoch 363/500:  train Loss: 18.6451   val Loss: 24.4807   time: 239.41s   best: 23.7493
2023-12-03 13:39:29,602:INFO:  Epoch 364/500:  train Loss: 18.5615   val Loss: 24.3419   time: 240.95s   best: 23.7493
2023-12-03 13:43:30,813:INFO:  Epoch 365/500:  train Loss: 18.7478   val Loss: 24.8317   time: 241.20s   best: 23.7493
2023-12-03 13:47:32,164:INFO:  Epoch 366/500:  train Loss: 18.6812   val Loss: 24.5932   time: 241.35s   best: 23.7493
2023-12-03 13:51:33,931:INFO:  Epoch 367/500:  train Loss: 18.5314   val Loss: 24.0982   time: 241.77s   best: 23.7493
2023-12-03 13:55:33,025:INFO:  Epoch 368/500:  train Loss: 18.8032   val Loss: 24.2687   time: 239.08s   best: 23.7493
2023-12-03 13:59:32,085:INFO:  Epoch 369/500:  train Loss: 18.5568   val Loss: 24.6964   time: 239.05s   best: 23.7493
2023-12-03 14:03:31,321:INFO:  Epoch 370/500:  train Loss: 18.6288   val Loss: 24.2825   time: 239.22s   best: 23.7493
2023-12-03 14:07:30,539:INFO:  Epoch 371/500:  train Loss: 18.7258   val Loss: 24.4643   time: 239.22s   best: 23.7493
2023-12-03 14:11:31,173:INFO:  Epoch 372/500:  train Loss: 18.4758   val Loss: 24.8394   time: 240.62s   best: 23.7493
2023-12-03 14:15:32,025:INFO:  Epoch 373/500:  train Loss: 18.6910   val Loss: 24.4547   time: 240.85s   best: 23.7493
2023-12-03 14:19:32,928:INFO:  Epoch 374/500:  train Loss: 18.6178   val Loss: 24.3528   time: 240.90s   best: 23.7493
2023-12-03 14:23:32,110:INFO:  Epoch 375/500:  train Loss: 18.5405   val Loss: 23.9677   time: 239.18s   best: 23.7493
2023-12-03 14:27:31,850:INFO:  Epoch 376/500:  train Loss: 18.6143   val Loss: 24.2354   time: 239.74s   best: 23.7493
2023-12-03 14:31:32,671:INFO:  Epoch 377/500:  train Loss: 18.4802   val Loss: 24.1798   time: 240.81s   best: 23.7493
2023-12-03 14:35:34,356:INFO:  Epoch 378/500:  train Loss: 18.5365   val Loss: 23.9578   time: 241.68s   best: 23.7493
2023-12-03 14:39:33,776:INFO:  Epoch 379/500:  train Loss: 18.6955   val Loss: 28.0895   time: 239.41s   best: 23.7493
2023-12-03 14:43:33,589:INFO:  Epoch 380/500:  train Loss: 18.6741   val Loss: 24.8317   time: 239.80s   best: 23.7493
2023-12-03 14:47:32,864:INFO:  Epoch 381/500:  train Loss: 18.4831   val Loss: 25.0988   time: 239.26s   best: 23.7493
2023-12-03 14:51:32,628:INFO:  Epoch 382/500:  train Loss: 18.5601   val Loss: 24.2036   time: 239.76s   best: 23.7493
2023-12-03 14:55:31,989:INFO:  Epoch 383/500:  train Loss: 18.7845   val Loss: 24.5302   time: 239.36s   best: 23.7493
2023-12-03 14:59:31,355:INFO:  Epoch 384/500:  train Loss: 18.5670   val Loss: 24.2788   time: 239.37s   best: 23.7493
2023-12-03 15:03:32,634:INFO:  Epoch 385/500:  train Loss: 18.7935   val Loss: 25.2641   time: 241.28s   best: 23.7493
2023-12-03 15:07:32,485:INFO:  Epoch 386/500:  train Loss: 18.6654   val Loss: 24.5894   time: 239.84s   best: 23.7493
2023-12-03 15:11:33,938:INFO:  Epoch 387/500:  train Loss: 18.4554   val Loss: 24.5586   time: 241.45s   best: 23.7493
2023-12-03 15:15:34,111:INFO:  Epoch 388/500:  train Loss: 18.9854   val Loss: 24.1947   time: 240.16s   best: 23.7493
2023-12-03 15:19:35,527:INFO:  Epoch 389/500:  train Loss: 18.7930   val Loss: 25.0169   time: 241.40s   best: 23.7493
2023-12-03 15:23:37,218:INFO:  Epoch 390/500:  train Loss: 18.3949   val Loss: 24.3959   time: 241.68s   best: 23.7493
2023-12-03 15:27:37,754:INFO:  Epoch 391/500:  train Loss: 18.6063   val Loss: 24.6367   time: 240.52s   best: 23.7493
2023-12-03 15:31:40,321:INFO:  Epoch 392/500:  train Loss: 18.5356   val Loss: 24.3961   time: 242.57s   best: 23.7493
2023-12-03 15:35:41,034:INFO:  Epoch 393/500:  train Loss: 18.4609   val Loss: 24.3116   time: 240.70s   best: 23.7493
2023-12-03 15:39:41,150:INFO:  Epoch 394/500:  train Loss: 18.4706   val Loss: 24.1986   time: 240.11s   best: 23.7493
2023-12-03 15:43:40,629:INFO:  Epoch 395/500:  train Loss: 18.6208   val Loss: 24.4844   time: 239.48s   best: 23.7493
2023-12-03 15:47:42,621:INFO:  Epoch 396/500:  train Loss: 18.8726   val Loss: 24.1555   time: 241.99s   best: 23.7493
2023-12-03 15:51:44,388:INFO:  Epoch 397/500:  train Loss: 18.5685   val Loss: 24.6536   time: 241.77s   best: 23.7493
2023-12-03 15:55:44,508:INFO:  Epoch 398/500:  train Loss: 18.6688   val Loss: 24.3229   time: 240.12s   best: 23.7493
2023-12-03 15:59:43,473:INFO:  Epoch 399/500:  train Loss: 18.6220   val Loss: 24.1890   time: 238.96s   best: 23.7493
2023-12-03 16:03:44,458:INFO:  Epoch 400/500:  train Loss: 18.5198   val Loss: 27.4823   time: 240.97s   best: 23.7493
2023-12-03 16:07:43,518:INFO:  Epoch 401/500:  train Loss: 18.5537   val Loss: 23.9696   time: 239.05s   best: 23.7493
2023-12-03 16:11:44,047:INFO:  Epoch 402/500:  train Loss: 18.5137   val Loss: 24.8420   time: 240.52s   best: 23.7493
2023-12-03 16:15:43,318:INFO:  Epoch 403/500:  train Loss: 18.5582   val Loss: 24.6011   time: 239.27s   best: 23.7493
2023-12-03 16:19:44,709:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 16:19:44,726:INFO:  Epoch 404/500:  train Loss: 18.3436   val Loss: 23.6854   time: 241.39s   best: 23.6854
2023-12-03 16:23:43,605:INFO:  Epoch 405/500:  train Loss: 18.4967   val Loss: 24.8068   time: 238.87s   best: 23.6854
2023-12-03 16:27:42,656:INFO:  Epoch 406/500:  train Loss: 18.5230   val Loss: 24.4334   time: 239.04s   best: 23.6854
2023-12-03 16:31:41,614:INFO:  Epoch 407/500:  train Loss: 18.3825   val Loss: 24.2962   time: 238.96s   best: 23.6854
2023-12-03 16:35:41,781:INFO:  Epoch 408/500:  train Loss: 18.6078   val Loss: 25.9092   time: 240.15s   best: 23.6854
2023-12-03 16:39:40,446:INFO:  Epoch 409/500:  train Loss: 18.5297   val Loss: 25.1562   time: 238.66s   best: 23.6854
2023-12-03 16:43:40,672:INFO:  Epoch 410/500:  train Loss: 18.4680   val Loss: 24.3448   time: 240.22s   best: 23.6854
2023-12-03 16:47:39,359:INFO:  Epoch 411/500:  train Loss: 18.4111   val Loss: 24.5654   time: 238.68s   best: 23.6854
2023-12-03 16:51:38,513:INFO:  Epoch 412/500:  train Loss: 18.4330   val Loss: 25.4062   time: 239.14s   best: 23.6854
2023-12-03 16:55:38,858:INFO:  Epoch 413/500:  train Loss: 18.5006   val Loss: 24.3179   time: 240.33s   best: 23.6854
2023-12-03 16:59:37,446:INFO:  Epoch 414/500:  train Loss: 18.5301   val Loss: 24.5472   time: 238.58s   best: 23.6854
2023-12-03 17:03:37,384:INFO:  Epoch 415/500:  train Loss: 18.4066   val Loss: 24.6389   time: 239.94s   best: 23.6854
2023-12-03 17:07:35,960:INFO:  Epoch 416/500:  train Loss: 18.4837   val Loss: 24.2186   time: 238.58s   best: 23.6854
2023-12-03 17:11:35,428:INFO:  Epoch 417/500:  train Loss: 18.5412   val Loss: 24.2767   time: 239.46s   best: 23.6854
2023-12-03 17:15:36,024:INFO:  Epoch 418/500:  train Loss: 18.3643   val Loss: 23.9876   time: 240.57s   best: 23.6854
2023-12-03 17:19:35,186:INFO:  Epoch 419/500:  train Loss: 18.3040   val Loss: 25.1801   time: 239.16s   best: 23.6854
2023-12-03 17:23:36,086:INFO:  Epoch 420/500:  train Loss: 18.4728   val Loss: 24.0820   time: 240.89s   best: 23.6854
2023-12-03 17:27:35,650:INFO:  Epoch 421/500:  train Loss: 18.7791   val Loss: 23.9996   time: 239.55s   best: 23.6854
2023-12-03 17:31:35,039:INFO:  Epoch 422/500:  train Loss: 18.2642   val Loss: 24.1360   time: 239.38s   best: 23.6854
2023-12-03 17:35:36,294:INFO:  Epoch 423/500:  train Loss: 18.3368   val Loss: 23.8865   time: 241.25s   best: 23.6854
2023-12-03 17:39:36,657:INFO:  Epoch 424/500:  train Loss: 18.3533   val Loss: 24.0588   time: 240.36s   best: 23.6854
2023-12-03 17:43:36,963:INFO:  Epoch 425/500:  train Loss: 18.3469   val Loss: 24.3144   time: 240.30s   best: 23.6854
2023-12-03 17:47:37,796:INFO:  Epoch 426/500:  train Loss: 18.2393   val Loss: 24.4171   time: 240.83s   best: 23.6854
2023-12-03 17:51:36,917:INFO:  Epoch 427/500:  train Loss: 18.6646   val Loss: 24.0056   time: 239.12s   best: 23.6854
2023-12-03 17:55:36,939:INFO:  Epoch 428/500:  train Loss: 18.2339   val Loss: 25.1007   time: 240.01s   best: 23.6854
2023-12-03 17:59:37,656:INFO:  Epoch 429/500:  train Loss: 18.3995   val Loss: 24.3069   time: 240.72s   best: 23.6854
2023-12-03 18:03:38,143:INFO:  Epoch 430/500:  train Loss: 18.2282   val Loss: 24.2925   time: 240.49s   best: 23.6854
2023-12-03 18:07:37,476:INFO:  Epoch 431/500:  train Loss: 18.4305   val Loss: 26.2636   time: 239.32s   best: 23.6854
2023-12-03 18:11:38,720:INFO:  Epoch 432/500:  train Loss: 18.4310   val Loss: 24.3828   time: 241.24s   best: 23.6854
2023-12-03 18:15:39,984:INFO:  Epoch 433/500:  train Loss: 18.3200   val Loss: 25.3015   time: 241.25s   best: 23.6854
2023-12-03 18:19:40,023:INFO:  Epoch 434/500:  train Loss: 18.2632   val Loss: 25.1660   time: 240.03s   best: 23.6854
2023-12-03 18:23:39,215:INFO:  Epoch 435/500:  train Loss: 18.2479   val Loss: 24.4549   time: 239.18s   best: 23.6854
2023-12-03 18:27:40,806:INFO:  Epoch 436/500:  train Loss: 18.2931   val Loss: 25.7376   time: 241.58s   best: 23.6854
2023-12-03 18:31:40,710:INFO:  Epoch 437/500:  train Loss: 18.1560   val Loss: 24.1796   time: 239.90s   best: 23.6854
2023-12-03 18:35:40,243:INFO:  Epoch 438/500:  train Loss: 18.4065   val Loss: 24.6868   time: 239.52s   best: 23.6854
2023-12-03 18:39:39,377:INFO:  Epoch 439/500:  train Loss: 18.9354   val Loss: 24.9207   time: 239.12s   best: 23.6854
2023-12-03 18:43:39,648:INFO:  Epoch 440/500:  train Loss: 18.4894   val Loss: 24.1929   time: 240.27s   best: 23.6854
2023-12-03 18:47:38,658:INFO:  Epoch 441/500:  train Loss: 18.3695   val Loss: 23.7274   time: 239.01s   best: 23.6854
2023-12-03 18:51:40,509:INFO:  Epoch 442/500:  train Loss: 18.3907   val Loss: 24.1207   time: 241.85s   best: 23.6854
2023-12-03 18:55:39,476:INFO:  Epoch 443/500:  train Loss: 18.2504   val Loss: 28.2131   time: 238.97s   best: 23.6854
2023-12-03 18:59:38,790:INFO:  Epoch 444/500:  train Loss: 18.4290   val Loss: 24.4995   time: 239.30s   best: 23.6854
2023-12-03 19:03:37,828:INFO:  Epoch 445/500:  train Loss: 18.2747   val Loss: 24.5164   time: 239.04s   best: 23.6854
2023-12-03 19:07:36,729:INFO:  Epoch 446/500:  train Loss: 18.2397   val Loss: 25.0557   time: 238.89s   best: 23.6854
2023-12-03 19:11:38,115:INFO:  Epoch 447/500:  train Loss: 18.3542   val Loss: 25.9513   time: 241.37s   best: 23.6854
2023-12-03 19:15:38,928:INFO:  Epoch 448/500:  train Loss: 18.2559   val Loss: 24.1362   time: 240.81s   best: 23.6854
2023-12-03 19:19:37,991:INFO:  Epoch 449/500:  train Loss: 18.2299   val Loss: 24.4949   time: 239.06s   best: 23.6854
2023-12-03 19:23:39,380:INFO:  Epoch 450/500:  train Loss: 18.1271   val Loss: 24.0597   time: 241.39s   best: 23.6854
2023-12-03 19:27:39,677:INFO:  Epoch 451/500:  train Loss: 18.3329   val Loss: 24.0850   time: 240.29s   best: 23.6854
2023-12-03 19:31:38,719:INFO:  Epoch 452/500:  train Loss: 18.3353   val Loss: 24.2682   time: 239.04s   best: 23.6854
2023-12-03 19:35:37,858:INFO:  Epoch 453/500:  train Loss: 18.1682   val Loss: 24.7606   time: 239.13s   best: 23.6854
2023-12-03 19:39:37,612:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_lstm autoencoder perm50 with 0.5 dataset (0.05 dropout)_b166.pt
2023-12-03 19:39:37,632:INFO:  Epoch 454/500:  train Loss: 18.2567   val Loss: 23.6319   time: 239.73s   best: 23.6319
2023-12-03 19:43:37,037:INFO:  Epoch 455/500:  train Loss: 18.3812   val Loss: 24.0762   time: 239.40s   best: 23.6319
2023-12-03 19:47:36,304:INFO:  Epoch 456/500:  train Loss: 18.5189   val Loss: 24.2290   time: 239.25s   best: 23.6319
2023-12-03 19:51:37,163:INFO:  Epoch 457/500:  train Loss: 18.2224   val Loss: 24.0531   time: 240.86s   best: 23.6319
2023-12-03 19:55:36,058:INFO:  Epoch 458/500:  train Loss: 18.1908   val Loss: 23.8412   time: 238.89s   best: 23.6319
2023-12-03 19:59:35,268:INFO:  Epoch 459/500:  train Loss: 18.2923   val Loss: 24.0679   time: 239.21s   best: 23.6319
2023-12-03 20:03:35,223:INFO:  Epoch 460/500:  train Loss: 18.1890   val Loss: 24.5623   time: 239.95s   best: 23.6319
2023-12-03 20:07:36,734:INFO:  Epoch 461/500:  train Loss: 18.1444   val Loss: 24.1978   time: 241.50s   best: 23.6319
2023-12-03 20:11:38,484:INFO:  Epoch 462/500:  train Loss: 18.1680   val Loss: 24.3198   time: 241.74s   best: 23.6319
2023-12-03 20:15:39,106:INFO:  Epoch 463/500:  train Loss: 18.4479   val Loss: 25.1565   time: 240.62s   best: 23.6319
2023-12-03 20:19:38,284:INFO:  Epoch 464/500:  train Loss: 18.4245   val Loss: 24.2515   time: 239.17s   best: 23.6319
2023-12-03 20:23:37,524:INFO:  Epoch 465/500:  train Loss: 18.3118   val Loss: 24.5746   time: 239.23s   best: 23.6319
2023-12-03 20:27:38,530:INFO:  Epoch 466/500:  train Loss: 18.2888   val Loss: 24.2938   time: 240.99s   best: 23.6319
2023-12-03 20:31:37,677:INFO:  Epoch 467/500:  train Loss: 18.1178   val Loss: 24.3253   time: 239.15s   best: 23.6319
2023-12-03 20:35:36,999:INFO:  Epoch 468/500:  train Loss: 18.2633   val Loss: 23.8154   time: 239.31s   best: 23.6319
2023-12-03 20:39:36,926:INFO:  Epoch 469/500:  train Loss: 18.3762   val Loss: 23.8729   time: 239.92s   best: 23.6319
2023-12-03 20:43:35,641:INFO:  Epoch 470/500:  train Loss: 18.2189   val Loss: 24.5171   time: 238.71s   best: 23.6319
2023-12-03 20:47:34,346:INFO:  Epoch 471/500:  train Loss: 18.5108   val Loss: 24.5470   time: 238.69s   best: 23.6319
2023-12-03 20:51:35,084:INFO:  Epoch 472/500:  train Loss: 18.0434   val Loss: 24.4062   time: 240.73s   best: 23.6319
2023-12-03 20:55:34,163:INFO:  Epoch 473/500:  train Loss: 18.1022   val Loss: 24.4679   time: 239.08s   best: 23.6319
2023-12-03 20:59:32,971:INFO:  Epoch 474/500:  train Loss: 18.1103   val Loss: 23.8482   time: 238.80s   best: 23.6319
2023-12-03 21:03:33,278:INFO:  Epoch 475/500:  train Loss: 18.1951   val Loss: 24.0544   time: 240.30s   best: 23.6319
2023-12-03 21:07:33,751:INFO:  Epoch 476/500:  train Loss: 18.1593   val Loss: 24.5815   time: 240.47s   best: 23.6319
2023-12-03 21:11:32,764:INFO:  Epoch 477/500:  train Loss: 18.1080   val Loss: 24.4928   time: 239.00s   best: 23.6319
2023-12-03 21:15:31,627:INFO:  Epoch 478/500:  train Loss: 18.0697   val Loss: 24.0117   time: 238.85s   best: 23.6319
2023-12-03 21:19:31,313:INFO:  Epoch 479/500:  train Loss: 18.2540   val Loss: 24.1532   time: 239.67s   best: 23.6319
2023-12-03 21:23:30,702:INFO:  Epoch 480/500:  train Loss: 18.0080   val Loss: 24.9594   time: 239.39s   best: 23.6319
2023-12-03 21:27:29,626:INFO:  Epoch 481/500:  train Loss: 18.1908   val Loss: 24.0660   time: 238.92s   best: 23.6319
2023-12-03 21:31:29,110:INFO:  Epoch 482/500:  train Loss: 18.0788   val Loss: 24.2762   time: 239.48s   best: 23.6319
2023-12-03 21:35:28,091:INFO:  Epoch 483/500:  train Loss: 18.2267   val Loss: 23.8748   time: 238.97s   best: 23.6319
2023-12-03 21:39:29,073:INFO:  Epoch 484/500:  train Loss: 18.4065   val Loss: 23.9569   time: 240.98s   best: 23.6319
2023-12-03 21:43:28,187:INFO:  Epoch 485/500:  train Loss: 18.2627   val Loss: 24.0490   time: 239.10s   best: 23.6319
2023-12-03 21:47:27,720:INFO:  Epoch 486/500:  train Loss: 17.9497   val Loss: 24.0963   time: 239.53s   best: 23.6319
2023-12-03 21:51:27,052:INFO:  Epoch 487/500:  train Loss: 18.0919   val Loss: 24.1814   time: 239.32s   best: 23.6319
2023-12-03 21:55:25,600:INFO:  Epoch 488/500:  train Loss: 18.1321   val Loss: 24.5925   time: 238.54s   best: 23.6319
2023-12-03 21:59:25,424:INFO:  Epoch 489/500:  train Loss: 18.4544   val Loss: 23.9911   time: 239.81s   best: 23.6319
2023-12-03 22:03:25,713:INFO:  Epoch 490/500:  train Loss: 18.0546   val Loss: 24.5690   time: 240.29s   best: 23.6319
2023-12-03 22:07:24,684:INFO:  Epoch 491/500:  train Loss: 18.1810   val Loss: 24.0392   time: 238.96s   best: 23.6319
2023-12-03 22:11:25,060:INFO:  Epoch 492/500:  train Loss: 18.0425   val Loss: 24.4879   time: 240.36s   best: 23.6319
2023-12-03 22:15:25,828:INFO:  Epoch 493/500:  train Loss: 17.9591   val Loss: 23.9929   time: 240.75s   best: 23.6319
2023-12-03 22:19:25,271:INFO:  Epoch 494/500:  train Loss: 18.0165   val Loss: 24.7172   time: 239.44s   best: 23.6319
2023-12-03 22:23:24,638:INFO:  Epoch 495/500:  train Loss: 18.0350   val Loss: 24.4675   time: 239.35s   best: 23.6319
2023-12-03 22:27:25,077:INFO:  Epoch 496/500:  train Loss: 18.1040   val Loss: 28.4444   time: 240.44s   best: 23.6319
2023-12-03 22:31:25,842:INFO:  Epoch 497/500:  train Loss: 18.0884   val Loss: 24.0067   time: 240.75s   best: 23.6319
2023-12-03 22:35:24,661:INFO:  Epoch 498/500:  train Loss: 18.0110   val Loss: 23.8684   time: 238.82s   best: 23.6319
2023-12-03 22:39:24,682:INFO:  Epoch 499/500:  train Loss: 18.0986   val Loss: 24.4977   time: 240.01s   best: 23.6319
2023-12-03 22:43:23,629:INFO:  Epoch 500/500:  train Loss: 17.9800   val Loss: 24.0053   time: 238.95s   best: 23.6319
2023-12-03 22:43:23,630:INFO:  -----> Training complete in 2002m 12s   best validation loss: 23.6319
 
2024-09-01 21:40:49,501:INFO:  Starting experiment lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 21:40:49,503:INFO:  Defining the model
2024-09-01 21:51:14,647:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 21:51:14,648:INFO:  Defining the model
2024-09-01 21:52:45,592:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 21:52:45,592:INFO:  Defining the model
2024-09-01 21:52:46,144:INFO:  Reading the dataset
2024-09-01 21:54:54,412:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 21:54:54,414:INFO:  Defining the model
2024-09-01 21:54:54,886:INFO:  Reading the dataset
2024-09-01 21:55:53,665:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 21:55:53,666:INFO:  Defining the model
2024-09-01 21:55:54,142:INFO:  Reading the dataset
2024-09-01 22:04:54,049:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 22:04:54,049:INFO:  Defining the model
2024-09-01 22:04:54,553:INFO:  Reading the dataset
2024-09-01 22:06:06,485:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 22:06:06,486:INFO:  Defining the model
2024-09-01 22:06:06,949:INFO:  Reading the dataset
2024-09-01 22:09:10,823:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 22:09:10,824:INFO:  Defining the model
2024-09-01 22:09:11,291:INFO:  Reading the dataset
2024-09-01 22:11:16,987:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 22:11:16,988:INFO:  Defining the model
2024-09-01 22:11:17,479:INFO:  Reading the dataset
2024-09-01 22:13:25,037:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 22:13:25,037:INFO:  Defining the model
2024-09-01 22:13:25,539:INFO:  Reading the dataset
2024-09-01 22:27:31,686:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 22:27:31,686:INFO:  Defining the model
2024-09-01 22:27:32,165:INFO:  Reading the dataset
2024-09-01 22:27:38,800:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:38,822:INFO:  Epoch 1/650:  train Loss: 99.6870   val Loss: 99.7720   time: 0.96s   best: 99.7720
2024-09-01 22:27:39,083:INFO:  Epoch 2/650:  train Loss: 98.9916   val Loss: 100.0553   time: 0.26s   best: 99.7720
2024-09-01 22:27:39,329:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:39,373:INFO:  Epoch 3/650:  train Loss: 98.8889   val Loss: 97.7168   time: 0.24s   best: 97.7168
2024-09-01 22:27:39,618:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:39,642:INFO:  Epoch 4/650:  train Loss: 98.6646   val Loss: 93.8268   time: 0.24s   best: 93.8268
2024-09-01 22:27:39,923:INFO:  Epoch 5/650:  train Loss: 96.3128   val Loss: 94.2582   time: 0.28s   best: 93.8268
2024-09-01 22:27:40,160:INFO:  Epoch 6/650:  train Loss: 95.4745   val Loss: 94.8370   time: 0.24s   best: 93.8268
2024-09-01 22:27:40,432:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:40,455:INFO:  Epoch 7/650:  train Loss: 94.6565   val Loss: 93.8229   time: 0.27s   best: 93.8229
2024-09-01 22:27:40,699:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:40,722:INFO:  Epoch 8/650:  train Loss: 93.7476   val Loss: 89.9857   time: 0.24s   best: 89.9857
2024-09-01 22:27:41,088:INFO:  Epoch 9/650:  train Loss: 93.1212   val Loss: 93.4533   time: 0.37s   best: 89.9857
2024-09-01 22:27:41,325:INFO:  Epoch 10/650:  train Loss: 93.0541   val Loss: 93.4936   time: 0.24s   best: 89.9857
2024-09-01 22:27:41,588:INFO:  Epoch 11/650:  train Loss: 93.2733   val Loss: 92.9668   time: 0.26s   best: 89.9857
2024-09-01 22:27:41,825:INFO:  Epoch 12/650:  train Loss: 92.6509   val Loss: 93.4126   time: 0.24s   best: 89.9857
2024-09-01 22:27:42,128:INFO:  Epoch 13/650:  train Loss: 93.6936   val Loss: 93.1874   time: 0.30s   best: 89.9857
2024-09-01 22:27:42,365:INFO:  Epoch 14/650:  train Loss: 92.9464   val Loss: 90.4943   time: 0.24s   best: 89.9857
2024-09-01 22:27:42,630:INFO:  Epoch 15/650:  train Loss: 92.5545   val Loss: 93.1904   time: 0.26s   best: 89.9857
2024-09-01 22:27:42,866:INFO:  Epoch 16/650:  train Loss: 92.4589   val Loss: 93.1670   time: 0.24s   best: 89.9857
2024-09-01 22:27:43,130:INFO:  Epoch 17/650:  train Loss: 92.9204   val Loss: 90.0451   time: 0.26s   best: 89.9857
2024-09-01 22:27:43,368:INFO:  Epoch 18/650:  train Loss: 92.5193   val Loss: 90.5314   time: 0.24s   best: 89.9857
2024-09-01 22:27:43,632:INFO:  Epoch 19/650:  train Loss: 92.6820   val Loss: 92.2137   time: 0.26s   best: 89.9857
2024-09-01 22:27:43,868:INFO:  Epoch 20/650:  train Loss: 92.2820   val Loss: 92.6106   time: 0.24s   best: 89.9857
2024-09-01 22:27:44,172:INFO:  Epoch 21/650:  train Loss: 92.3878   val Loss: 92.3717   time: 0.30s   best: 89.9857
2024-09-01 22:27:44,409:INFO:  Epoch 22/650:  train Loss: 92.9052   val Loss: 91.9253   time: 0.24s   best: 89.9857
2024-09-01 22:27:44,685:INFO:  Epoch 23/650:  train Loss: 91.7654   val Loss: 91.2145   time: 0.27s   best: 89.9857
2024-09-01 22:27:44,936:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:44,967:INFO:  Epoch 24/650:  train Loss: 91.2758   val Loss: 87.6944   time: 0.24s   best: 87.6944
2024-09-01 22:27:45,229:INFO:  Epoch 25/650:  train Loss: 90.0742   val Loss: 89.4734   time: 0.26s   best: 87.6944
2024-09-01 22:27:45,483:INFO:  Epoch 26/650:  train Loss: 90.0098   val Loss: 89.8946   time: 0.25s   best: 87.6944
2024-09-01 22:27:45,743:INFO:  Epoch 27/650:  train Loss: 89.2735   val Loss: 88.8479   time: 0.26s   best: 87.6944
2024-09-01 22:27:46,034:INFO:  Epoch 28/650:  train Loss: 89.0474   val Loss: 88.9793   time: 0.29s   best: 87.6944
2024-09-01 22:27:46,294:INFO:  Epoch 29/650:  train Loss: 89.1390   val Loss: 88.4977   time: 0.26s   best: 87.6944
2024-09-01 22:27:46,543:INFO:  Epoch 30/650:  train Loss: 89.1801   val Loss: 88.5144   time: 0.25s   best: 87.6944
2024-09-01 22:27:46,820:INFO:  Epoch 31/650:  train Loss: 88.8705   val Loss: 88.2180   time: 0.28s   best: 87.6944
2024-09-01 22:27:47,063:INFO:  Epoch 32/650:  train Loss: 88.5660   val Loss: 88.3232   time: 0.24s   best: 87.6944
2024-09-01 22:27:47,312:INFO:  Epoch 33/650:  train Loss: 88.7777   val Loss: 87.9838   time: 0.25s   best: 87.6944
2024-09-01 22:27:47,555:INFO:  Epoch 34/650:  train Loss: 88.6726   val Loss: 88.1277   time: 0.24s   best: 87.6944
2024-09-01 22:27:47,805:INFO:  Epoch 35/650:  train Loss: 88.5142   val Loss: 88.2914   time: 0.25s   best: 87.6944
2024-09-01 22:27:48,095:INFO:  Epoch 36/650:  train Loss: 88.4825   val Loss: 87.8490   time: 0.29s   best: 87.6944
2024-09-01 22:27:48,350:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:48,373:INFO:  Epoch 37/650:  train Loss: 87.8620   val Loss: 87.5022   time: 0.25s   best: 87.5022
2024-09-01 22:27:48,620:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:48,643:INFO:  Epoch 38/650:  train Loss: 88.0654   val Loss: 87.5007   time: 0.24s   best: 87.5007
2024-09-01 22:27:48,898:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:48,921:INFO:  Epoch 39/650:  train Loss: 88.0057   val Loss: 87.1752   time: 0.25s   best: 87.1752
2024-09-01 22:27:49,167:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:49,210:INFO:  Epoch 40/650:  train Loss: 87.3326   val Loss: 86.2019   time: 0.24s   best: 86.2019
2024-09-01 22:27:49,506:INFO:  Epoch 41/650:  train Loss: 86.6760   val Loss: 86.2135   time: 0.30s   best: 86.2019
2024-09-01 22:27:49,752:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:49,775:INFO:  Epoch 42/650:  train Loss: 86.4984   val Loss: 85.8515   time: 0.24s   best: 85.8515
2024-09-01 22:27:50,180:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:50,203:INFO:  Epoch 43/650:  train Loss: 86.1448   val Loss: 84.8046   time: 0.40s   best: 84.8046
2024-09-01 22:27:50,440:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:50,463:INFO:  Epoch 44/650:  train Loss: 85.3549   val Loss: 84.0915   time: 0.23s   best: 84.0915
2024-09-01 22:27:50,727:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:50,751:INFO:  Epoch 45/650:  train Loss: 84.4628   val Loss: 83.5371   time: 0.26s   best: 83.5371
2024-09-01 22:27:50,989:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:51,025:INFO:  Epoch 46/650:  train Loss: 84.0251   val Loss: 82.6236   time: 0.23s   best: 82.6236
2024-09-01 22:27:51,284:INFO:  Epoch 47/650:  train Loss: 83.5826   val Loss: 82.7273   time: 0.26s   best: 82.6236
2024-09-01 22:27:51,522:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:51,563:INFO:  Epoch 48/650:  train Loss: 83.1503   val Loss: 81.8163   time: 0.23s   best: 81.8163
2024-09-01 22:27:51,805:INFO:  Epoch 49/650:  train Loss: 83.7350   val Loss: 82.3519   time: 0.24s   best: 81.8163
2024-09-01 22:27:52,096:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:52,126:INFO:  Epoch 50/650:  train Loss: 82.9454   val Loss: 81.7634   time: 0.28s   best: 81.7634
2024-09-01 22:27:52,396:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:52,420:INFO:  Epoch 51/650:  train Loss: 81.9414   val Loss: 80.9204   time: 0.27s   best: 80.9204
2024-09-01 22:27:52,689:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:52,713:INFO:  Epoch 52/650:  train Loss: 82.1392   val Loss: 80.4573   time: 0.26s   best: 80.4573
2024-09-01 22:27:52,946:INFO:  Epoch 53/650:  train Loss: 81.4038   val Loss: 81.3866   time: 0.23s   best: 80.4573
2024-09-01 22:27:53,216:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:53,240:INFO:  Epoch 54/650:  train Loss: 81.6002   val Loss: 79.7567   time: 0.26s   best: 79.7567
2024-09-01 22:27:53,473:INFO:  Epoch 55/650:  train Loss: 80.5143   val Loss: 79.8361   time: 0.23s   best: 79.7567
2024-09-01 22:27:53,738:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:53,762:INFO:  Epoch 56/650:  train Loss: 80.4306   val Loss: 79.0923   time: 0.26s   best: 79.0923
2024-09-01 22:27:53,997:INFO:  Epoch 57/650:  train Loss: 80.0876   val Loss: 79.1747   time: 0.23s   best: 79.0923
2024-09-01 22:27:54,331:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:54,355:INFO:  Epoch 58/650:  train Loss: 80.2905   val Loss: 78.3438   time: 0.33s   best: 78.3438
2024-09-01 22:27:54,587:INFO:  Epoch 59/650:  train Loss: 80.0255   val Loss: 79.6808   time: 0.23s   best: 78.3438
2024-09-01 22:27:54,841:INFO:  Epoch 60/650:  train Loss: 81.9349   val Loss: 80.2595   time: 0.25s   best: 78.3438
2024-09-01 22:27:55,073:INFO:  Epoch 61/650:  train Loss: 80.0378   val Loss: 79.5746   time: 0.23s   best: 78.3438
2024-09-01 22:27:55,332:INFO:  Epoch 62/650:  train Loss: 79.8294   val Loss: 78.8171   time: 0.26s   best: 78.3438
2024-09-01 22:27:55,568:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:55,591:INFO:  Epoch 63/650:  train Loss: 79.5264   val Loss: 78.1136   time: 0.23s   best: 78.1136
2024-09-01 22:27:55,945:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:55,975:INFO:  Epoch 64/650:  train Loss: 78.4784   val Loss: 77.9085   time: 0.35s   best: 77.9085
2024-09-01 22:27:56,218:INFO:  Epoch 65/650:  train Loss: 79.3163   val Loss: 78.5124   time: 0.24s   best: 77.9085
2024-09-01 22:27:56,501:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:56,524:INFO:  Epoch 66/650:  train Loss: 78.9077   val Loss: 77.0158   time: 0.28s   best: 77.0158
2024-09-01 22:27:56,779:INFO:  Epoch 67/650:  train Loss: 77.7741   val Loss: 77.7198   time: 0.25s   best: 77.0158
2024-09-01 22:27:57,078:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:57,103:INFO:  Epoch 68/650:  train Loss: 78.4210   val Loss: 76.9591   time: 0.29s   best: 76.9591
2024-09-01 22:27:57,367:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:57,390:INFO:  Epoch 69/650:  train Loss: 78.1108   val Loss: 76.0430   time: 0.26s   best: 76.0430
2024-09-01 22:27:57,629:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:57,653:INFO:  Epoch 70/650:  train Loss: 77.4672   val Loss: 75.3720   time: 0.23s   best: 75.3720
2024-09-01 22:27:57,908:INFO:  Epoch 71/650:  train Loss: 76.4255   val Loss: 75.6737   time: 0.25s   best: 75.3720
2024-09-01 22:27:58,346:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:58,369:INFO:  Epoch 72/650:  train Loss: 76.4318   val Loss: 75.2242   time: 0.43s   best: 75.2242
2024-09-01 22:27:58,637:INFO:  Epoch 73/650:  train Loss: 77.7778   val Loss: 77.5106   time: 0.26s   best: 75.2242
2024-09-01 22:27:59,007:INFO:  Epoch 74/650:  train Loss: 78.1952   val Loss: 76.4343   time: 0.37s   best: 75.2242
2024-09-01 22:27:59,247:INFO:  Epoch 75/650:  train Loss: 77.4599   val Loss: 75.2362   time: 0.24s   best: 75.2242
2024-09-01 22:27:59,532:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:59,556:INFO:  Epoch 76/650:  train Loss: 76.6940   val Loss: 74.2737   time: 0.28s   best: 74.2737
2024-09-01 22:27:59,809:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:27:59,833:INFO:  Epoch 77/650:  train Loss: 75.0743   val Loss: 74.0069   time: 0.25s   best: 74.0069
2024-09-01 22:28:00,170:INFO:  Epoch 78/650:  train Loss: 75.5199   val Loss: 74.3392   time: 0.33s   best: 74.0069
2024-09-01 22:28:00,471:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:00,495:INFO:  Epoch 79/650:  train Loss: 75.5436   val Loss: 73.3215   time: 0.30s   best: 73.3215
2024-09-01 22:28:00,753:INFO:  Epoch 80/650:  train Loss: 75.3786   val Loss: 75.5515   time: 0.26s   best: 73.3215
2024-09-01 22:28:01,000:INFO:  Epoch 81/650:  train Loss: 77.0231   val Loss: 74.5872   time: 0.25s   best: 73.3215
2024-09-01 22:28:01,262:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:01,285:INFO:  Epoch 82/650:  train Loss: 76.2088   val Loss: 73.3155   time: 0.26s   best: 73.3155
2024-09-01 22:28:01,543:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:01,580:INFO:  Epoch 83/650:  train Loss: 75.2050   val Loss: 72.4029   time: 0.25s   best: 72.4029
2024-09-01 22:28:01,847:INFO:  Epoch 84/650:  train Loss: 73.7000   val Loss: 72.7589   time: 0.27s   best: 72.4029
2024-09-01 22:28:02,106:INFO:  Epoch 85/650:  train Loss: 74.9469   val Loss: 73.6354   time: 0.26s   best: 72.4029
2024-09-01 22:28:02,357:INFO:  Epoch 86/650:  train Loss: 75.2761   val Loss: 73.1186   time: 0.25s   best: 72.4029
2024-09-01 22:28:02,664:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:02,687:INFO:  Epoch 87/650:  train Loss: 74.2553   val Loss: 72.1043   time: 0.30s   best: 72.1043
2024-09-01 22:28:02,937:INFO:  Epoch 88/650:  train Loss: 74.2178   val Loss: 72.6059   time: 0.25s   best: 72.1043
2024-09-01 22:28:03,194:INFO:  Epoch 89/650:  train Loss: 73.7413   val Loss: 72.6770   time: 0.26s   best: 72.1043
2024-09-01 22:28:03,453:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:03,476:INFO:  Epoch 90/650:  train Loss: 73.7527   val Loss: 71.9090   time: 0.24s   best: 71.9090
2024-09-01 22:28:03,817:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:03,848:INFO:  Epoch 91/650:  train Loss: 73.1602   val Loss: 71.7108   time: 0.32s   best: 71.7108
2024-09-01 22:28:04,095:INFO:  Epoch 92/650:  train Loss: 73.5532   val Loss: 71.7558   time: 0.24s   best: 71.7108
2024-09-01 22:28:04,375:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:04,398:INFO:  Epoch 93/650:  train Loss: 73.1756   val Loss: 70.6812   time: 0.27s   best: 70.6812
2024-09-01 22:28:04,767:INFO:  Epoch 94/650:  train Loss: 72.7218   val Loss: 71.8466   time: 0.36s   best: 70.6812
2024-09-01 22:28:05,021:INFO:  Epoch 95/650:  train Loss: 73.3857   val Loss: 71.1180   time: 0.25s   best: 70.6812
2024-09-01 22:28:05,288:INFO:  Epoch 96/650:  train Loss: 74.1023   val Loss: 72.1205   time: 0.26s   best: 70.6812
2024-09-01 22:28:05,539:INFO:  Epoch 97/650:  train Loss: 73.5315   val Loss: 72.7454   time: 0.25s   best: 70.6812
2024-09-01 22:28:05,838:INFO:  Epoch 98/650:  train Loss: 73.7540   val Loss: 71.6225   time: 0.30s   best: 70.6812
2024-09-01 22:28:06,093:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:06,116:INFO:  Epoch 99/650:  train Loss: 73.0439   val Loss: 70.5674   time: 0.25s   best: 70.5674
2024-09-01 22:28:06,375:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:06,408:INFO:  Epoch 100/650:  train Loss: 71.6471   val Loss: 70.3789   time: 0.25s   best: 70.3789
2024-09-01 22:28:06,680:INFO:  Epoch 101/650:  train Loss: 72.1509   val Loss: 70.8925   time: 0.27s   best: 70.3789
2024-09-01 22:28:06,951:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:06,974:INFO:  Epoch 102/650:  train Loss: 72.5480   val Loss: 70.2256   time: 0.26s   best: 70.2256
2024-09-01 22:28:07,214:INFO:  Epoch 103/650:  train Loss: 72.4279   val Loss: 70.3436   time: 0.24s   best: 70.2256
2024-09-01 22:28:07,479:INFO:  Epoch 104/650:  train Loss: 71.4812   val Loss: 70.2612   time: 0.26s   best: 70.2256
2024-09-01 22:28:07,718:INFO:  Epoch 105/650:  train Loss: 71.9916   val Loss: 70.6527   time: 0.23s   best: 70.2256
2024-09-01 22:28:07,985:INFO:  Epoch 106/650:  train Loss: 71.9392   val Loss: 70.6983   time: 0.26s   best: 70.2256
2024-09-01 22:28:08,230:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:08,253:INFO:  Epoch 107/650:  train Loss: 71.6700   val Loss: 69.2240   time: 0.24s   best: 69.2240
2024-09-01 22:28:08,527:INFO:  Epoch 108/650:  train Loss: 71.7342   val Loss: 69.6361   time: 0.27s   best: 69.2240
2024-09-01 22:28:08,800:INFO:  Epoch 109/650:  train Loss: 71.9273   val Loss: 70.3847   time: 0.27s   best: 69.2240
2024-09-01 22:28:09,164:INFO:  Epoch 110/650:  train Loss: 71.5006   val Loss: 70.1887   time: 0.36s   best: 69.2240
2024-09-01 22:28:09,394:INFO:  Epoch 111/650:  train Loss: 71.0877   val Loss: 69.3953   time: 0.23s   best: 69.2240
2024-09-01 22:28:09,654:INFO:  Epoch 112/650:  train Loss: 71.8769   val Loss: 71.2116   time: 0.26s   best: 69.2240
2024-09-01 22:28:09,884:INFO:  Epoch 113/650:  train Loss: 72.0303   val Loss: 69.8637   time: 0.23s   best: 69.2240
2024-09-01 22:28:10,155:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:10,178:INFO:  Epoch 114/650:  train Loss: 71.3950   val Loss: 69.0529   time: 0.26s   best: 69.0529
2024-09-01 22:28:10,409:INFO:  Epoch 115/650:  train Loss: 71.9807   val Loss: 69.9625   time: 0.23s   best: 69.0529
2024-09-01 22:28:10,690:INFO:  Epoch 116/650:  train Loss: 71.4462   val Loss: 70.0848   time: 0.28s   best: 69.0529
2024-09-01 22:28:10,935:INFO:  Epoch 117/650:  train Loss: 71.1173   val Loss: 69.8649   time: 0.24s   best: 69.0529
2024-09-01 22:28:11,189:INFO:  Epoch 118/650:  train Loss: 71.6431   val Loss: 71.2682   time: 0.25s   best: 69.0529
2024-09-01 22:28:11,439:INFO:  Epoch 119/650:  train Loss: 72.1206   val Loss: 70.6290   time: 0.25s   best: 69.0529
2024-09-01 22:28:11,704:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:11,728:INFO:  Epoch 120/650:  train Loss: 70.9575   val Loss: 68.9283   time: 0.26s   best: 68.9283
2024-09-01 22:28:12,068:INFO:  Epoch 121/650:  train Loss: 71.3823   val Loss: 69.0145   time: 0.34s   best: 68.9283
2024-09-01 22:28:12,334:INFO:  Epoch 122/650:  train Loss: 70.7520   val Loss: 69.5269   time: 0.27s   best: 68.9283
2024-09-01 22:28:12,594:INFO:  Epoch 123/650:  train Loss: 70.6848   val Loss: 69.5404   time: 0.26s   best: 68.9283
2024-09-01 22:28:12,922:INFO:  Epoch 124/650:  train Loss: 71.2003   val Loss: 70.4909   time: 0.33s   best: 68.9283
2024-09-01 22:28:13,197:INFO:  Epoch 125/650:  train Loss: 71.3683   val Loss: 69.7101   time: 0.27s   best: 68.9283
2024-09-01 22:28:13,456:INFO:  Epoch 126/650:  train Loss: 70.7622   val Loss: 69.2178   time: 0.26s   best: 68.9283
2024-09-01 22:28:13,739:INFO:  Epoch 127/650:  train Loss: 70.5006   val Loss: 69.0941   time: 0.28s   best: 68.9283
2024-09-01 22:28:13,991:INFO:  Epoch 128/650:  train Loss: 70.3315   val Loss: 70.5083   time: 0.25s   best: 68.9283
2024-09-01 22:28:14,271:INFO:  Epoch 129/650:  train Loss: 71.3401   val Loss: 70.7044   time: 0.28s   best: 68.9283
2024-09-01 22:28:14,523:INFO:  Epoch 130/650:  train Loss: 71.1575   val Loss: 68.9662   time: 0.25s   best: 68.9283
2024-09-01 22:28:14,851:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:14,874:INFO:  Epoch 131/650:  train Loss: 71.0036   val Loss: 68.4106   time: 0.32s   best: 68.4106
2024-09-01 22:28:15,134:INFO:  Epoch 132/650:  train Loss: 70.4022   val Loss: 70.0289   time: 0.26s   best: 68.4106
2024-09-01 22:28:15,400:INFO:  Epoch 133/650:  train Loss: 71.3325   val Loss: 68.6675   time: 0.27s   best: 68.4106
2024-09-01 22:28:15,666:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:15,690:INFO:  Epoch 134/650:  train Loss: 70.4928   val Loss: 68.0400   time: 0.26s   best: 68.0400
2024-09-01 22:28:16,045:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:16,095:INFO:  Epoch 135/650:  train Loss: 69.9789   val Loss: 67.7129   time: 0.35s   best: 67.7129
2024-09-01 22:28:16,373:INFO:  Epoch 136/650:  train Loss: 69.4605   val Loss: 68.0629   time: 0.28s   best: 67.7129
2024-09-01 22:28:16,648:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:16,671:INFO:  Epoch 137/650:  train Loss: 69.4023   val Loss: 67.3680   time: 0.27s   best: 67.3680
2024-09-01 22:28:16,979:INFO:  Epoch 138/650:  train Loss: 69.1712   val Loss: 67.6816   time: 0.30s   best: 67.3680
2024-09-01 22:28:17,243:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:17,266:INFO:  Epoch 139/650:  train Loss: 69.3480   val Loss: 66.8252   time: 0.26s   best: 66.8252
2024-09-01 22:28:17,609:INFO:  Epoch 140/650:  train Loss: 68.4732   val Loss: 67.2071   time: 0.34s   best: 66.8252
2024-09-01 22:28:17,899:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:17,922:INFO:  Epoch 141/650:  train Loss: 69.2075   val Loss: 66.4310   time: 0.28s   best: 66.4310
2024-09-01 22:28:18,218:INFO:  Epoch 142/650:  train Loss: 68.9934   val Loss: 68.3862   time: 0.29s   best: 66.4310
2024-09-01 22:28:18,654:INFO:  Epoch 143/650:  train Loss: 69.5574   val Loss: 67.9972   time: 0.43s   best: 66.4310
2024-09-01 22:28:18,939:INFO:  Epoch 144/650:  train Loss: 69.4352   val Loss: 67.8979   time: 0.28s   best: 66.4310
2024-09-01 22:28:19,230:INFO:  Epoch 145/650:  train Loss: 69.6860   val Loss: 66.9702   time: 0.29s   best: 66.4310
2024-09-01 22:28:19,490:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:19,519:INFO:  Epoch 146/650:  train Loss: 69.2048   val Loss: 66.4020   time: 0.26s   best: 66.4020
2024-09-01 22:28:19,793:INFO:  Epoch 147/650:  train Loss: 68.8628   val Loss: 67.4293   time: 0.26s   best: 66.4020
2024-09-01 22:28:20,068:INFO:  Epoch 148/650:  train Loss: 69.1029   val Loss: 67.4095   time: 0.27s   best: 66.4020
2024-09-01 22:28:20,361:INFO:  Epoch 149/650:  train Loss: 69.0027   val Loss: 67.4422   time: 0.29s   best: 66.4020
2024-09-01 22:28:20,635:INFO:  Epoch 150/650:  train Loss: 68.8337   val Loss: 67.2341   time: 0.27s   best: 66.4020
2024-09-01 22:28:20,900:INFO:  Epoch 151/650:  train Loss: 68.6538   val Loss: 66.5835   time: 0.26s   best: 66.4020
2024-09-01 22:28:21,218:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:21,241:INFO:  Epoch 152/650:  train Loss: 68.3814   val Loss: 66.1789   time: 0.31s   best: 66.1789
2024-09-01 22:28:21,498:INFO:  Epoch 153/650:  train Loss: 68.8205   val Loss: 67.3195   time: 0.26s   best: 66.1789
2024-09-01 22:28:21,781:INFO:  Epoch 154/650:  train Loss: 68.5775   val Loss: 66.5613   time: 0.28s   best: 66.1789
2024-09-01 22:28:22,038:INFO:  Epoch 155/650:  train Loss: 68.4442   val Loss: 66.2763   time: 0.26s   best: 66.1789
2024-09-01 22:28:22,351:INFO:  Epoch 156/650:  train Loss: 68.6357   val Loss: 67.2967   time: 0.31s   best: 66.1789
2024-09-01 22:28:22,607:INFO:  Epoch 157/650:  train Loss: 67.9654   val Loss: 66.2806   time: 0.26s   best: 66.1789
2024-09-01 22:28:22,891:INFO:  Epoch 158/650:  train Loss: 69.0152   val Loss: 67.7568   time: 0.28s   best: 66.1789
2024-09-01 22:28:23,194:INFO:  Epoch 159/650:  train Loss: 69.0768   val Loss: 67.0375   time: 0.30s   best: 66.1789
2024-09-01 22:28:23,456:INFO:  Epoch 160/650:  train Loss: 68.7910   val Loss: 66.4423   time: 0.26s   best: 66.1789
2024-09-01 22:28:23,726:INFO:  Epoch 161/650:  train Loss: 67.9682   val Loss: 66.7459   time: 0.27s   best: 66.1789
2024-09-01 22:28:23,990:INFO:  Epoch 162/650:  train Loss: 68.5352   val Loss: 66.5106   time: 0.26s   best: 66.1789
2024-09-01 22:28:24,271:INFO:  Epoch 163/650:  train Loss: 68.5337   val Loss: 67.2279   time: 0.28s   best: 66.1789
2024-09-01 22:28:24,528:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:24,551:INFO:  Epoch 164/650:  train Loss: 68.6341   val Loss: 65.2066   time: 0.25s   best: 65.2066
2024-09-01 22:28:24,831:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:24,854:INFO:  Epoch 165/650:  train Loss: 68.3582   val Loss: 65.0632   time: 0.27s   best: 65.0632
2024-09-01 22:28:25,139:INFO:  Epoch 166/650:  train Loss: 68.1871   val Loss: 66.3213   time: 0.28s   best: 65.0632
2024-09-01 22:28:25,417:INFO:  Epoch 167/650:  train Loss: 67.6762   val Loss: 65.7313   time: 0.28s   best: 65.0632
2024-09-01 22:28:25,670:INFO:  Epoch 168/650:  train Loss: 68.0749   val Loss: 67.2286   time: 0.25s   best: 65.0632
2024-09-01 22:28:25,950:INFO:  Epoch 169/650:  train Loss: 68.2575   val Loss: 65.5611   time: 0.28s   best: 65.0632
2024-09-01 22:28:26,204:INFO:  Epoch 170/650:  train Loss: 67.7423   val Loss: 65.1173   time: 0.25s   best: 65.0632
2024-09-01 22:28:26,508:INFO:  Epoch 171/650:  train Loss: 67.5822   val Loss: 66.1838   time: 0.30s   best: 65.0632
2024-09-01 22:28:26,765:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:26,788:INFO:  Epoch 172/650:  train Loss: 67.1275   val Loss: 64.8613   time: 0.25s   best: 64.8613
2024-09-01 22:28:27,122:INFO:  Epoch 173/650:  train Loss: 66.9051   val Loss: 65.2223   time: 0.33s   best: 64.8613
2024-09-01 22:28:27,396:INFO:  Epoch 174/650:  train Loss: 66.6005   val Loss: 65.6531   time: 0.27s   best: 64.8613
2024-09-01 22:28:27,654:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:27,677:INFO:  Epoch 175/650:  train Loss: 66.8096   val Loss: 64.6681   time: 0.25s   best: 64.6681
2024-09-01 22:28:27,951:INFO:  Epoch 176/650:  train Loss: 66.3651   val Loss: 65.8717   time: 0.27s   best: 64.6681
2024-09-01 22:28:28,205:INFO:  Epoch 177/650:  train Loss: 67.3775   val Loss: 65.7607   time: 0.25s   best: 64.6681
2024-09-01 22:28:28,487:INFO:  Epoch 178/650:  train Loss: 67.1531   val Loss: 65.8350   time: 0.28s   best: 64.6681
2024-09-01 22:28:28,888:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:28,924:INFO:  Epoch 179/650:  train Loss: 67.6193   val Loss: 64.6235   time: 0.28s   best: 64.6235
2024-09-01 22:28:29,227:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:29,251:INFO:  Epoch 180/650:  train Loss: 66.6282   val Loss: 64.4952   time: 0.30s   best: 64.4952
2024-09-01 22:28:29,527:INFO:  Epoch 181/650:  train Loss: 66.1031   val Loss: 64.9705   time: 0.28s   best: 64.4952
2024-09-01 22:28:29,779:INFO:  Epoch 182/650:  train Loss: 67.0567   val Loss: 65.6240   time: 0.25s   best: 64.4952
2024-09-01 22:28:30,062:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:30,085:INFO:  Epoch 183/650:  train Loss: 66.7403   val Loss: 64.1986   time: 0.28s   best: 64.1986
2024-09-01 22:28:30,337:INFO:  Epoch 184/650:  train Loss: 66.7262   val Loss: 65.1966   time: 0.25s   best: 64.1986
2024-09-01 22:28:30,641:INFO:  Epoch 185/650:  train Loss: 66.7185   val Loss: 64.7484   time: 0.30s   best: 64.1986
2024-09-01 22:28:30,893:INFO:  Epoch 186/650:  train Loss: 66.9390   val Loss: 66.8410   time: 0.25s   best: 64.1986
2024-09-01 22:28:31,261:INFO:  Epoch 187/650:  train Loss: 67.9298   val Loss: 65.6038   time: 0.37s   best: 64.1986
2024-09-01 22:28:31,540:INFO:  Epoch 188/650:  train Loss: 68.3846   val Loss: 66.2557   time: 0.28s   best: 64.1986
2024-09-01 22:28:31,799:INFO:  Epoch 189/650:  train Loss: 67.3088   val Loss: 64.5797   time: 0.26s   best: 64.1986
2024-09-01 22:28:32,082:INFO:  Epoch 190/650:  train Loss: 66.4215   val Loss: 65.0528   time: 0.28s   best: 64.1986
2024-09-01 22:28:32,346:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:32,370:INFO:  Epoch 191/650:  train Loss: 66.0876   val Loss: 63.8779   time: 0.25s   best: 63.8779
2024-09-01 22:28:32,683:INFO:  Epoch 192/650:  train Loss: 66.1543   val Loss: 64.5619   time: 0.31s   best: 63.8779
2024-09-01 22:28:32,944:INFO:  Epoch 193/650:  train Loss: 65.7466   val Loss: 64.5513   time: 0.25s   best: 63.8779
2024-09-01 22:28:33,303:INFO:  Epoch 194/650:  train Loss: 66.2808   val Loss: 64.8865   time: 0.35s   best: 63.8779
2024-09-01 22:28:33,563:INFO:  Epoch 195/650:  train Loss: 66.9256   val Loss: 65.0219   time: 0.26s   best: 63.8779
2024-09-01 22:28:33,841:INFO:  Epoch 196/650:  train Loss: 66.6620   val Loss: 65.9351   time: 0.28s   best: 63.8779
2024-09-01 22:28:34,124:INFO:  Epoch 197/650:  train Loss: 67.5895   val Loss: 67.7146   time: 0.28s   best: 63.8779
2024-09-01 22:28:34,383:INFO:  Epoch 198/650:  train Loss: 67.4511   val Loss: 64.0195   time: 0.25s   best: 63.8779
2024-09-01 22:28:34,666:INFO:  Epoch 199/650:  train Loss: 66.1531   val Loss: 65.5496   time: 0.28s   best: 63.8779
2024-09-01 22:28:34,955:INFO:  Epoch 200/650:  train Loss: 67.2776   val Loss: 64.8188   time: 0.28s   best: 63.8779
2024-09-01 22:28:35,274:INFO:  Epoch 201/650:  train Loss: 66.4645   val Loss: 64.1923   time: 0.32s   best: 63.8779
2024-09-01 22:28:35,536:INFO:  Epoch 202/650:  train Loss: 66.3436   val Loss: 65.3659   time: 0.26s   best: 63.8779
2024-09-01 22:28:35,806:INFO:  Epoch 203/650:  train Loss: 66.2190   val Loss: 64.5308   time: 0.27s   best: 63.8779
2024-09-01 22:28:36,068:INFO:  Epoch 204/650:  train Loss: 65.7825   val Loss: 64.2551   time: 0.26s   best: 63.8779
2024-09-01 22:28:36,344:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:36,367:INFO:  Epoch 205/650:  train Loss: 65.4814   val Loss: 62.9005   time: 0.27s   best: 62.9005
2024-09-01 22:28:36,628:INFO:  Epoch 206/650:  train Loss: 65.3745   val Loss: 63.3105   time: 0.26s   best: 62.9005
2024-09-01 22:28:36,919:INFO:  Epoch 207/650:  train Loss: 65.2228   val Loss: 63.2355   time: 0.29s   best: 62.9005
2024-09-01 22:28:37,180:INFO:  Epoch 208/650:  train Loss: 65.1553   val Loss: 63.9797   time: 0.26s   best: 62.9005
2024-09-01 22:28:37,492:INFO:  Epoch 209/650:  train Loss: 66.1451   val Loss: 63.9010   time: 0.31s   best: 62.9005
2024-09-01 22:28:37,768:INFO:  Epoch 210/650:  train Loss: 65.6138   val Loss: 64.1183   time: 0.27s   best: 62.9005
2024-09-01 22:28:38,021:INFO:  Epoch 211/650:  train Loss: 65.2557   val Loss: 63.1958   time: 0.25s   best: 62.9005
2024-09-01 22:28:38,300:INFO:  Epoch 212/650:  train Loss: 65.4661   val Loss: 63.7512   time: 0.28s   best: 62.9005
2024-09-01 22:28:38,563:INFO:  Epoch 213/650:  train Loss: 64.9649   val Loss: 64.1754   time: 0.26s   best: 62.9005
2024-09-01 22:28:38,962:INFO:  Epoch 214/650:  train Loss: 65.0487   val Loss: 63.1880   time: 0.40s   best: 62.9005
2024-09-01 22:28:39,245:INFO:  Epoch 215/650:  train Loss: 65.5675   val Loss: 64.8795   time: 0.28s   best: 62.9005
2024-09-01 22:28:39,547:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:39,570:INFO:  Epoch 216/650:  train Loss: 65.2736   val Loss: 62.7109   time: 0.30s   best: 62.7109
2024-09-01 22:28:39,831:INFO:  Epoch 217/650:  train Loss: 65.6581   val Loss: 63.9540   time: 0.26s   best: 62.7109
2024-09-01 22:28:40,115:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:40,138:INFO:  Epoch 218/650:  train Loss: 65.1483   val Loss: 62.5741   time: 0.28s   best: 62.5741
2024-09-01 22:28:40,393:INFO:  Epoch 219/650:  train Loss: 64.8533   val Loss: 63.3110   time: 0.25s   best: 62.5741
2024-09-01 22:28:40,677:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:40,700:INFO:  Epoch 220/650:  train Loss: 64.7990   val Loss: 62.1423   time: 0.28s   best: 62.1423
2024-09-01 22:28:41,044:INFO:  Epoch 221/650:  train Loss: 64.6764   val Loss: 64.4443   time: 0.34s   best: 62.1423
2024-09-01 22:28:41,303:INFO:  Epoch 222/650:  train Loss: 65.0762   val Loss: 63.0897   time: 0.26s   best: 62.1423
2024-09-01 22:28:41,604:INFO:  Epoch 223/650:  train Loss: 64.3685   val Loss: 62.9032   time: 0.30s   best: 62.1423
2024-09-01 22:28:41,865:INFO:  Epoch 224/650:  train Loss: 65.1503   val Loss: 63.9895   time: 0.26s   best: 62.1423
2024-09-01 22:28:42,134:INFO:  Epoch 225/650:  train Loss: 65.1001   val Loss: 62.4424   time: 0.27s   best: 62.1423
2024-09-01 22:28:42,394:INFO:  Epoch 226/650:  train Loss: 65.1831   val Loss: 65.6900   time: 0.26s   best: 62.1423
2024-09-01 22:28:42,669:INFO:  Epoch 227/650:  train Loss: 65.7397   val Loss: 63.2794   time: 0.27s   best: 62.1423
2024-09-01 22:28:42,922:INFO:  Epoch 228/650:  train Loss: 65.8749   val Loss: 65.1419   time: 0.25s   best: 62.1423
2024-09-01 22:28:43,205:INFO:  Epoch 229/650:  train Loss: 65.4632   val Loss: 65.3387   time: 0.28s   best: 62.1423
2024-09-01 22:28:43,481:INFO:  Epoch 230/650:  train Loss: 66.4128   val Loss: 64.5143   time: 0.28s   best: 62.1423
2024-09-01 22:28:43,772:INFO:  Epoch 231/650:  train Loss: 66.4411   val Loss: 64.6959   time: 0.29s   best: 62.1423
2024-09-01 22:28:44,028:INFO:  Epoch 232/650:  train Loss: 66.2949   val Loss: 64.6135   time: 0.25s   best: 62.1423
2024-09-01 22:28:44,373:INFO:  Epoch 233/650:  train Loss: 65.5615   val Loss: 65.2827   time: 0.34s   best: 62.1423
2024-09-01 22:28:44,632:INFO:  Epoch 234/650:  train Loss: 65.0596   val Loss: 62.3584   time: 0.26s   best: 62.1423
2024-09-01 22:28:44,922:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:44,945:INFO:  Epoch 235/650:  train Loss: 64.0077   val Loss: 61.3694   time: 0.28s   best: 61.3694
2024-09-01 22:28:45,246:INFO:  Epoch 236/650:  train Loss: 63.7949   val Loss: 62.8521   time: 0.30s   best: 61.3694
2024-09-01 22:28:45,505:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:45,570:INFO:  Epoch 237/650:  train Loss: 64.2150   val Loss: 61.1231   time: 0.25s   best: 61.1231
2024-09-01 22:28:45,845:INFO:  Epoch 238/650:  train Loss: 63.6680   val Loss: 62.1179   time: 0.27s   best: 61.1231
2024-09-01 22:28:46,097:INFO:  Epoch 239/650:  train Loss: 63.7447   val Loss: 63.4620   time: 0.25s   best: 61.1231
2024-09-01 22:28:46,376:INFO:  Epoch 240/650:  train Loss: 64.1894   val Loss: 61.2745   time: 0.28s   best: 61.1231
2024-09-01 22:28:46,628:INFO:  Epoch 241/650:  train Loss: 64.4472   val Loss: 62.9137   time: 0.25s   best: 61.1231
2024-09-01 22:28:46,911:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:46,935:INFO:  Epoch 242/650:  train Loss: 64.0205   val Loss: 60.5980   time: 0.28s   best: 60.5980
2024-09-01 22:28:47,187:INFO:  Epoch 243/650:  train Loss: 63.5239   val Loss: 60.6195   time: 0.25s   best: 60.5980
2024-09-01 22:28:47,464:INFO:  Epoch 244/650:  train Loss: 63.0427   val Loss: 61.2355   time: 0.28s   best: 60.5980
2024-09-01 22:28:47,826:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:47,850:INFO:  Epoch 245/650:  train Loss: 63.5282   val Loss: 60.1331   time: 0.36s   best: 60.1331
2024-09-01 22:28:48,110:INFO:  Epoch 246/650:  train Loss: 63.1090   val Loss: 62.4817   time: 0.26s   best: 60.1331
2024-09-01 22:28:48,423:INFO:  Epoch 247/650:  train Loss: 63.8223   val Loss: 62.5929   time: 0.31s   best: 60.1331
2024-09-01 22:28:48,679:INFO:  Epoch 248/650:  train Loss: 64.3980   val Loss: 61.9503   time: 0.26s   best: 60.1331
2024-09-01 22:28:49,068:INFO:  Epoch 249/650:  train Loss: 64.0039   val Loss: 61.7843   time: 0.39s   best: 60.1331
2024-09-01 22:28:49,343:INFO:  Epoch 250/650:  train Loss: 64.0368   val Loss: 60.3931   time: 0.27s   best: 60.1331
2024-09-01 22:28:49,613:INFO:  Epoch 251/650:  train Loss: 63.7990   val Loss: 62.0323   time: 0.27s   best: 60.1331
2024-09-01 22:28:49,909:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:49,932:INFO:  Epoch 252/650:  train Loss: 63.2123   val Loss: 59.9404   time: 0.29s   best: 59.9404
2024-09-01 22:28:50,203:INFO:  Epoch 253/650:  train Loss: 63.2418   val Loss: 62.3337   time: 0.27s   best: 59.9404
2024-09-01 22:28:50,464:INFO:  Epoch 254/650:  train Loss: 63.7060   val Loss: 61.7593   time: 0.26s   best: 59.9404
2024-09-01 22:28:50,734:INFO:  Epoch 255/650:  train Loss: 62.7125   val Loss: 60.8589   time: 0.27s   best: 59.9404
2024-09-01 22:28:50,994:INFO:  Epoch 256/650:  train Loss: 63.9156   val Loss: 60.0091   time: 0.26s   best: 59.9404
2024-09-01 22:28:51,264:INFO:  Epoch 257/650:  train Loss: 62.1786   val Loss: 60.9719   time: 0.27s   best: 59.9404
2024-09-01 22:28:51,526:INFO:  Epoch 258/650:  train Loss: 63.1429   val Loss: 60.7134   time: 0.26s   best: 59.9404
2024-09-01 22:28:51,841:INFO:  Epoch 259/650:  train Loss: 62.4656   val Loss: 61.5033   time: 0.31s   best: 59.9404
2024-09-01 22:28:52,106:INFO:  Epoch 260/650:  train Loss: 62.8852   val Loss: 61.2699   time: 0.26s   best: 59.9404
2024-09-01 22:28:52,367:INFO:  Epoch 261/650:  train Loss: 63.9293   val Loss: 61.2258   time: 0.26s   best: 59.9404
2024-09-01 22:28:52,641:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:52,665:INFO:  Epoch 262/650:  train Loss: 62.7720   val Loss: 59.9209   time: 0.27s   best: 59.9209
2024-09-01 22:28:52,937:INFO:  Epoch 263/650:  train Loss: 63.5478   val Loss: 61.8300   time: 0.27s   best: 59.9209
2024-09-01 22:28:53,206:INFO:  Epoch 264/650:  train Loss: 62.9911   val Loss: 60.4273   time: 0.27s   best: 59.9209
2024-09-01 22:28:53,492:INFO:  Epoch 265/650:  train Loss: 63.3430   val Loss: 62.7641   time: 0.29s   best: 59.9209
2024-09-01 22:28:53,783:INFO:  Epoch 266/650:  train Loss: 65.1468   val Loss: 60.5980   time: 0.29s   best: 59.9209
2024-09-01 22:28:54,058:INFO:  Epoch 267/650:  train Loss: 62.8877   val Loss: 61.4809   time: 0.27s   best: 59.9209
2024-09-01 22:28:54,398:INFO:  Epoch 268/650:  train Loss: 63.2695   val Loss: 63.3328   time: 0.34s   best: 59.9209
2024-09-01 22:28:54,650:INFO:  Epoch 269/650:  train Loss: 63.2130   val Loss: 60.2222   time: 0.25s   best: 59.9209
2024-09-01 22:28:54,934:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:54,957:INFO:  Epoch 270/650:  train Loss: 62.6138   val Loss: 59.6701   time: 0.28s   best: 59.6701
2024-09-01 22:28:55,226:INFO:  Epoch 271/650:  train Loss: 61.9246   val Loss: 61.7595   time: 0.27s   best: 59.6701
2024-09-01 22:28:55,523:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:55,546:INFO:  Epoch 272/650:  train Loss: 62.5843   val Loss: 58.3207   time: 0.29s   best: 58.3207
2024-09-01 22:28:55,851:INFO:  Epoch 273/650:  train Loss: 62.7212   val Loss: 63.8088   time: 0.30s   best: 58.3207
2024-09-01 22:28:56,113:INFO:  Epoch 274/650:  train Loss: 64.3029   val Loss: 61.7440   time: 0.26s   best: 58.3207
2024-09-01 22:28:56,380:INFO:  Epoch 275/650:  train Loss: 61.9766   val Loss: 59.3424   time: 0.27s   best: 58.3207
2024-09-01 22:28:56,643:INFO:  Epoch 276/650:  train Loss: 61.4332   val Loss: 62.1698   time: 0.26s   best: 58.3207
2024-09-01 22:28:56,914:INFO:  Epoch 277/650:  train Loss: 62.2953   val Loss: 58.7928   time: 0.27s   best: 58.3207
2024-09-01 22:28:57,176:INFO:  Epoch 278/650:  train Loss: 62.0876   val Loss: 60.7809   time: 0.26s   best: 58.3207
2024-09-01 22:28:57,456:INFO:  Epoch 279/650:  train Loss: 61.8734   val Loss: 59.5002   time: 0.28s   best: 58.3207
2024-09-01 22:28:57,708:INFO:  Epoch 280/650:  train Loss: 62.5972   val Loss: 59.1458   time: 0.25s   best: 58.3207
2024-09-01 22:28:58,020:INFO:  Epoch 281/650:  train Loss: 61.5000   val Loss: 64.0666   time: 0.31s   best: 58.3207
2024-09-01 22:28:58,278:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:28:58,301:INFO:  Epoch 282/650:  train Loss: 62.9166   val Loss: 58.1607   time: 0.25s   best: 58.1607
2024-09-01 22:28:58,660:INFO:  Epoch 283/650:  train Loss: 62.7553   val Loss: 62.4503   time: 0.36s   best: 58.1607
2024-09-01 22:28:59,047:INFO:  Epoch 284/650:  train Loss: 62.0151   val Loss: 59.1589   time: 0.39s   best: 58.1607
2024-09-01 22:28:59,300:INFO:  Epoch 285/650:  train Loss: 61.0069   val Loss: 58.6426   time: 0.25s   best: 58.1607
2024-09-01 22:28:59,628:INFO:  Epoch 286/650:  train Loss: 61.1505   val Loss: 59.9143   time: 0.33s   best: 58.1607
2024-09-01 22:28:59,881:INFO:  Epoch 287/650:  train Loss: 61.6902   val Loss: 59.9252   time: 0.25s   best: 58.1607
2024-09-01 22:29:00,191:INFO:  Epoch 288/650:  train Loss: 61.8828   val Loss: 59.1749   time: 0.31s   best: 58.1607
2024-09-01 22:29:00,445:INFO:  Epoch 289/650:  train Loss: 61.3322   val Loss: 58.9747   time: 0.25s   best: 58.1607
2024-09-01 22:29:00,724:INFO:  Epoch 290/650:  train Loss: 60.5569   val Loss: 60.1199   time: 0.28s   best: 58.1607
2024-09-01 22:29:00,981:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:01,005:INFO:  Epoch 291/650:  train Loss: 61.1094   val Loss: 57.6579   time: 0.25s   best: 57.6579
2024-09-01 22:29:01,284:INFO:  Epoch 292/650:  train Loss: 61.1179   val Loss: 60.0107   time: 0.28s   best: 57.6579
2024-09-01 22:29:01,545:INFO:  Epoch 293/650:  train Loss: 60.7258   val Loss: 58.0138   time: 0.26s   best: 57.6579
2024-09-01 22:29:01,815:INFO:  Epoch 294/650:  train Loss: 60.8948   val Loss: 59.2799   time: 0.27s   best: 57.6579
2024-09-01 22:29:02,126:INFO:  Epoch 295/650:  train Loss: 61.4276   val Loss: 58.3212   time: 0.31s   best: 57.6579
2024-09-01 22:29:02,381:INFO:  Epoch 296/650:  train Loss: 61.9868   val Loss: 61.7754   time: 0.25s   best: 57.6579
2024-09-01 22:29:02,661:INFO:  Epoch 297/650:  train Loss: 61.9575   val Loss: 59.8336   time: 0.28s   best: 57.6579
2024-09-01 22:29:02,914:INFO:  Epoch 298/650:  train Loss: 62.8099   val Loss: 61.2589   time: 0.25s   best: 57.6579
2024-09-01 22:29:03,193:INFO:  Epoch 299/650:  train Loss: 62.9851   val Loss: 61.2085   time: 0.28s   best: 57.6579
2024-09-01 22:29:03,522:INFO:  Epoch 300/650:  train Loss: 62.5028   val Loss: 59.8491   time: 0.33s   best: 57.6579
2024-09-01 22:29:03,818:INFO:  Epoch 301/650:  train Loss: 61.3781   val Loss: 58.0219   time: 0.30s   best: 57.6579
2024-09-01 22:29:04,116:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:04,139:INFO:  Epoch 302/650:  train Loss: 60.7016   val Loss: 56.7995   time: 0.29s   best: 56.7995
2024-09-01 22:29:04,411:INFO:  Epoch 303/650:  train Loss: 60.1600   val Loss: 57.6643   time: 0.27s   best: 56.7995
2024-09-01 22:29:04,670:INFO:  Epoch 304/650:  train Loss: 60.3873   val Loss: 61.6713   time: 0.26s   best: 56.7995
2024-09-01 22:29:04,939:INFO:  Epoch 305/650:  train Loss: 62.0347   val Loss: 60.2509   time: 0.27s   best: 56.7995
2024-09-01 22:29:05,201:INFO:  Epoch 306/650:  train Loss: 63.4188   val Loss: 64.8400   time: 0.26s   best: 56.7995
2024-09-01 22:29:05,526:INFO:  Epoch 307/650:  train Loss: 63.2543   val Loss: 59.1339   time: 0.32s   best: 56.7995
2024-09-01 22:29:05,817:INFO:  Epoch 308/650:  train Loss: 62.6505   val Loss: 63.0289   time: 0.29s   best: 56.7995
2024-09-01 22:29:06,165:INFO:  Epoch 309/650:  train Loss: 62.6907   val Loss: 59.5729   time: 0.35s   best: 56.7995
2024-09-01 22:29:06,432:INFO:  Epoch 310/650:  train Loss: 61.4732   val Loss: 59.9059   time: 0.27s   best: 56.7995
2024-09-01 22:29:06,697:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:06,720:INFO:  Epoch 311/650:  train Loss: 60.4585   val Loss: 56.7248   time: 0.26s   best: 56.7248
2024-09-01 22:29:06,991:INFO:  Epoch 312/650:  train Loss: 59.6188   val Loss: 58.2986   time: 0.27s   best: 56.7248
2024-09-01 22:29:07,251:INFO:  Epoch 313/650:  train Loss: 60.0546   val Loss: 56.8418   time: 0.26s   best: 56.7248
2024-09-01 22:29:07,522:INFO:  Epoch 314/650:  train Loss: 60.1920   val Loss: 59.9423   time: 0.27s   best: 56.7248
2024-09-01 22:29:07,783:INFO:  Epoch 315/650:  train Loss: 60.1437   val Loss: 57.2459   time: 0.26s   best: 56.7248
2024-09-01 22:29:08,054:INFO:  Epoch 316/650:  train Loss: 59.9923   val Loss: 57.5447   time: 0.27s   best: 56.7248
2024-09-01 22:29:08,350:INFO:  Epoch 317/650:  train Loss: 59.9267   val Loss: 58.7936   time: 0.30s   best: 56.7248
2024-09-01 22:29:08,619:INFO:  Epoch 318/650:  train Loss: 59.9813   val Loss: 59.2446   time: 0.27s   best: 56.7248
2024-09-01 22:29:08,880:INFO:  Epoch 319/650:  train Loss: 61.6796   val Loss: 59.5139   time: 0.26s   best: 56.7248
2024-09-01 22:29:09,148:INFO:  Epoch 320/650:  train Loss: 61.5085   val Loss: 59.3585   time: 0.27s   best: 56.7248
2024-09-01 22:29:09,535:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:09,558:INFO:  Epoch 321/650:  train Loss: 61.0635   val Loss: 56.2638   time: 0.38s   best: 56.2638
2024-09-01 22:29:09,823:INFO:  Epoch 322/650:  train Loss: 59.5956   val Loss: 58.7241   time: 0.26s   best: 56.2638
2024-09-01 22:29:10,098:INFO:  Epoch 323/650:  train Loss: 60.5780   val Loss: 56.6173   time: 0.27s   best: 56.2638
2024-09-01 22:29:10,390:INFO:  Epoch 324/650:  train Loss: 60.4245   val Loss: 59.4215   time: 0.29s   best: 56.2638
2024-09-01 22:29:10,660:INFO:  Epoch 325/650:  train Loss: 59.5493   val Loss: 61.7316   time: 0.27s   best: 56.2638
2024-09-01 22:29:10,922:INFO:  Epoch 326/650:  train Loss: 63.4696   val Loss: 59.8925   time: 0.26s   best: 56.2638
2024-09-01 22:29:11,193:INFO:  Epoch 327/650:  train Loss: 60.0299   val Loss: 60.0654   time: 0.27s   best: 56.2638
2024-09-01 22:29:11,456:INFO:  Epoch 328/650:  train Loss: 61.8043   val Loss: 61.2718   time: 0.26s   best: 56.2638
2024-09-01 22:29:11,726:INFO:  Epoch 329/650:  train Loss: 60.1081   val Loss: 58.3830   time: 0.27s   best: 56.2638
2024-09-01 22:29:11,989:INFO:  Epoch 330/650:  train Loss: 59.8209   val Loss: 58.8875   time: 0.26s   best: 56.2638
2024-09-01 22:29:12,259:INFO:  Epoch 331/650:  train Loss: 61.2162   val Loss: 59.0017   time: 0.27s   best: 56.2638
2024-09-01 22:29:12,582:INFO:  Epoch 332/650:  train Loss: 60.9007   val Loss: 60.4418   time: 0.32s   best: 56.2638
2024-09-01 22:29:12,840:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:12,873:INFO:  Epoch 333/650:  train Loss: 59.9929   val Loss: 56.2445   time: 0.25s   best: 56.2445
2024-09-01 22:29:13,147:INFO:  Epoch 334/650:  train Loss: 59.7313   val Loss: 58.0105   time: 0.27s   best: 56.2445
2024-09-01 22:29:13,420:INFO:  Epoch 335/650:  train Loss: 59.9858   val Loss: 56.7275   time: 0.27s   best: 56.2445
2024-09-01 22:29:13,688:INFO:  Epoch 336/650:  train Loss: 60.0047   val Loss: 57.9205   time: 0.27s   best: 56.2445
2024-09-01 22:29:13,948:INFO:  Epoch 337/650:  train Loss: 58.9819   val Loss: 56.6452   time: 0.26s   best: 56.2445
2024-09-01 22:29:14,227:INFO:  Epoch 338/650:  train Loss: 60.0684   val Loss: 56.5755   time: 0.28s   best: 56.2445
2024-09-01 22:29:14,529:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:14,552:INFO:  Epoch 339/650:  train Loss: 57.6623   val Loss: 54.9485   time: 0.30s   best: 54.9485
2024-09-01 22:29:14,837:INFO:  Epoch 340/650:  train Loss: 57.7121   val Loss: 55.1601   time: 0.28s   best: 54.9485
2024-09-01 22:29:15,106:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:15,130:INFO:  Epoch 341/650:  train Loss: 58.2464   val Loss: 54.7138   time: 0.26s   best: 54.7138
2024-09-01 22:29:15,419:INFO:  Epoch 342/650:  train Loss: 57.9095   val Loss: 55.6385   time: 0.29s   best: 54.7138
2024-09-01 22:29:15,692:INFO:  Epoch 343/650:  train Loss: 57.7214   val Loss: 54.7245   time: 0.27s   best: 54.7138
2024-09-01 22:29:15,955:INFO:  Epoch 344/650:  train Loss: 57.3537   val Loss: 54.9930   time: 0.26s   best: 54.7138
2024-09-01 22:29:16,258:INFO:  Epoch 345/650:  train Loss: 57.5599   val Loss: 55.2904   time: 0.30s   best: 54.7138
2024-09-01 22:29:16,560:INFO:  Epoch 346/650:  train Loss: 57.1911   val Loss: 55.8900   time: 0.30s   best: 54.7138
2024-09-01 22:29:16,830:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:16,854:INFO:  Epoch 347/650:  train Loss: 58.1073   val Loss: 54.6717   time: 0.27s   best: 54.6717
2024-09-01 22:29:17,126:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:17,150:INFO:  Epoch 348/650:  train Loss: 57.1108   val Loss: 53.9959   time: 0.27s   best: 53.9959
2024-09-01 22:29:17,422:INFO:  Epoch 349/650:  train Loss: 57.1251   val Loss: 54.5697   time: 0.27s   best: 53.9959
2024-09-01 22:29:17,684:INFO:  Epoch 350/650:  train Loss: 57.5298   val Loss: 56.9705   time: 0.26s   best: 53.9959
2024-09-01 22:29:17,972:INFO:  Epoch 351/650:  train Loss: 58.5018   val Loss: 55.5099   time: 0.29s   best: 53.9959
2024-09-01 22:29:18,252:INFO:  Epoch 352/650:  train Loss: 59.4522   val Loss: 64.7395   time: 0.28s   best: 53.9959
2024-09-01 22:29:18,580:INFO:  Epoch 353/650:  train Loss: 62.7976   val Loss: 60.1000   time: 0.33s   best: 53.9959
2024-09-01 22:29:18,864:INFO:  Epoch 354/650:  train Loss: 58.6951   val Loss: 55.8089   time: 0.28s   best: 53.9959
2024-09-01 22:29:19,124:INFO:  Epoch 355/650:  train Loss: 58.4763   val Loss: 57.4560   time: 0.26s   best: 53.9959
2024-09-01 22:29:19,507:INFO:  Epoch 356/650:  train Loss: 58.0474   val Loss: 55.8716   time: 0.38s   best: 53.9959
2024-09-01 22:29:19,759:INFO:  Epoch 357/650:  train Loss: 57.2402   val Loss: 54.4455   time: 0.25s   best: 53.9959
2024-09-01 22:29:20,058:INFO:  Epoch 358/650:  train Loss: 57.9152   val Loss: 56.2531   time: 0.30s   best: 53.9959
2024-09-01 22:29:20,333:INFO:  Epoch 359/650:  train Loss: 57.4726   val Loss: 56.6519   time: 0.27s   best: 53.9959
2024-09-01 22:29:20,642:INFO:  Epoch 360/650:  train Loss: 57.7498   val Loss: 54.8861   time: 0.31s   best: 53.9959
2024-09-01 22:29:20,894:INFO:  Epoch 361/650:  train Loss: 58.6548   val Loss: 55.5346   time: 0.25s   best: 53.9959
2024-09-01 22:29:21,173:INFO:  Epoch 362/650:  train Loss: 57.2665   val Loss: 61.3087   time: 0.28s   best: 53.9959
2024-09-01 22:29:21,424:INFO:  Epoch 363/650:  train Loss: 60.1399   val Loss: 55.4048   time: 0.25s   best: 53.9959
2024-09-01 22:29:21,706:INFO:  Epoch 364/650:  train Loss: 59.4642   val Loss: 62.2499   time: 0.28s   best: 53.9959
2024-09-01 22:29:21,958:INFO:  Epoch 365/650:  train Loss: 61.4121   val Loss: 57.7778   time: 0.25s   best: 53.9959
2024-09-01 22:29:22,239:INFO:  Epoch 366/650:  train Loss: 59.7805   val Loss: 62.3696   time: 0.28s   best: 53.9959
2024-09-01 22:29:22,495:INFO:  Epoch 367/650:  train Loss: 61.5108   val Loss: 59.1761   time: 0.25s   best: 53.9959
2024-09-01 22:29:22,807:INFO:  Epoch 368/650:  train Loss: 60.2497   val Loss: 58.6681   time: 0.31s   best: 53.9959
2024-09-01 22:29:23,062:INFO:  Epoch 369/650:  train Loss: 59.3155   val Loss: 56.3392   time: 0.25s   best: 53.9959
2024-09-01 22:29:23,344:INFO:  Epoch 370/650:  train Loss: 58.5692   val Loss: 54.7629   time: 0.27s   best: 53.9959
2024-09-01 22:29:23,609:INFO:  Epoch 371/650:  train Loss: 57.5357   val Loss: 55.1718   time: 0.26s   best: 53.9959
2024-09-01 22:29:23,884:INFO:  Epoch 372/650:  train Loss: 57.0866   val Loss: 55.6417   time: 0.27s   best: 53.9959
2024-09-01 22:29:24,168:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:24,192:INFO:  Epoch 373/650:  train Loss: 56.8487   val Loss: 53.1170   time: 0.27s   best: 53.1170
2024-09-01 22:29:24,508:INFO:  Epoch 374/650:  train Loss: 56.3200   val Loss: 55.7906   time: 0.31s   best: 53.1170
2024-09-01 22:29:24,823:INFO:  Epoch 375/650:  train Loss: 56.7108   val Loss: 54.7051   time: 0.31s   best: 53.1170
2024-09-01 22:29:25,089:INFO:  Epoch 376/650:  train Loss: 57.1439   val Loss: 56.1545   time: 0.26s   best: 53.1170
2024-09-01 22:29:25,391:INFO:  Epoch 377/650:  train Loss: 57.4669   val Loss: 55.4512   time: 0.30s   best: 53.1170
2024-09-01 22:29:25,663:INFO:  Epoch 378/650:  train Loss: 58.1178   val Loss: 53.7063   time: 0.27s   best: 53.1170
2024-09-01 22:29:25,946:INFO:  Epoch 379/650:  train Loss: 56.8471   val Loss: 56.8455   time: 0.28s   best: 53.1170
2024-09-01 22:29:26,222:INFO:  Epoch 380/650:  train Loss: 58.1685   val Loss: 61.5525   time: 0.27s   best: 53.1170
2024-09-01 22:29:26,524:INFO:  Epoch 381/650:  train Loss: 58.9146   val Loss: 56.0055   time: 0.30s   best: 53.1170
2024-09-01 22:29:26,845:INFO:  Epoch 382/650:  train Loss: 57.2781   val Loss: 54.8197   time: 0.32s   best: 53.1170
2024-09-01 22:29:27,102:INFO:  Epoch 383/650:  train Loss: 56.5830   val Loss: 56.1276   time: 0.25s   best: 53.1170
2024-09-01 22:29:27,383:INFO:  Epoch 384/650:  train Loss: 57.6559   val Loss: 53.8204   time: 0.28s   best: 53.1170
2024-09-01 22:29:27,640:INFO:  Epoch 385/650:  train Loss: 56.6347   val Loss: 56.3729   time: 0.25s   best: 53.1170
2024-09-01 22:29:27,924:INFO:  Epoch 386/650:  train Loss: 57.7698   val Loss: 55.0142   time: 0.28s   best: 53.1170
2024-09-01 22:29:28,191:INFO:  Epoch 387/650:  train Loss: 56.3636   val Loss: 55.7118   time: 0.26s   best: 53.1170
2024-09-01 22:29:28,466:INFO:  Epoch 388/650:  train Loss: 56.4096   val Loss: 54.8993   time: 0.27s   best: 53.1170
2024-09-01 22:29:28,774:INFO:  Epoch 389/650:  train Loss: 56.5051   val Loss: 54.6357   time: 0.30s   best: 53.1170
2024-09-01 22:29:29,049:INFO:  Epoch 390/650:  train Loss: 57.2298   val Loss: 57.0271   time: 0.27s   best: 53.1170
2024-09-01 22:29:29,319:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:29,343:INFO:  Epoch 391/650:  train Loss: 57.1294   val Loss: 52.8340   time: 0.26s   best: 52.8340
2024-09-01 22:29:29,735:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:29,758:INFO:  Epoch 392/650:  train Loss: 55.7744   val Loss: 51.9305   time: 0.38s   best: 51.9305
2024-09-01 22:29:30,030:INFO:  Epoch 393/650:  train Loss: 54.7552   val Loss: 55.3372   time: 0.27s   best: 51.9305
2024-09-01 22:29:30,297:INFO:  Epoch 394/650:  train Loss: 55.9148   val Loss: 56.5980   time: 0.26s   best: 51.9305
2024-09-01 22:29:30,573:INFO:  Epoch 395/650:  train Loss: 57.2196   val Loss: 52.4968   time: 0.27s   best: 51.9305
2024-09-01 22:29:30,883:INFO:  Epoch 396/650:  train Loss: 55.3586   val Loss: 54.6497   time: 0.30s   best: 51.9305
2024-09-01 22:29:31,155:INFO:  Epoch 397/650:  train Loss: 56.2002   val Loss: 58.1664   time: 0.27s   best: 51.9305
2024-09-01 22:29:31,421:INFO:  Epoch 398/650:  train Loss: 58.0631   val Loss: 53.1452   time: 0.26s   best: 51.9305
2024-09-01 22:29:31,697:INFO:  Epoch 399/650:  train Loss: 55.9409   val Loss: 58.9752   time: 0.27s   best: 51.9305
2024-09-01 22:29:31,963:INFO:  Epoch 400/650:  train Loss: 57.9296   val Loss: 52.8566   time: 0.26s   best: 51.9305
2024-09-01 22:29:32,240:INFO:  Epoch 401/650:  train Loss: 56.3579   val Loss: 53.4379   time: 0.27s   best: 51.9305
2024-09-01 22:29:32,506:INFO:  Epoch 402/650:  train Loss: 55.4057   val Loss: 53.6794   time: 0.25s   best: 51.9305
2024-09-01 22:29:32,800:INFO:  Epoch 403/650:  train Loss: 54.7947   val Loss: 53.6615   time: 0.29s   best: 51.9305
2024-09-01 22:29:33,144:INFO:  Epoch 404/650:  train Loss: 55.3580   val Loss: 52.9579   time: 0.34s   best: 51.9305
2024-09-01 22:29:33,411:INFO:  Epoch 405/650:  train Loss: 55.1347   val Loss: 53.2099   time: 0.26s   best: 51.9305
2024-09-01 22:29:33,704:INFO:  Epoch 406/650:  train Loss: 55.4263   val Loss: 55.2890   time: 0.29s   best: 51.9305
2024-09-01 22:29:34,015:INFO:  Epoch 407/650:  train Loss: 55.2886   val Loss: 56.2277   time: 0.30s   best: 51.9305
2024-09-01 22:29:34,293:INFO:  Epoch 408/650:  train Loss: 57.4486   val Loss: 52.9501   time: 0.28s   best: 51.9305
2024-09-01 22:29:34,549:INFO:  Epoch 409/650:  train Loss: 56.7466   val Loss: 55.6205   time: 0.25s   best: 51.9305
2024-09-01 22:29:34,850:INFO:  Epoch 410/650:  train Loss: 56.7964   val Loss: 56.3059   time: 0.29s   best: 51.9305
2024-09-01 22:29:35,148:INFO:  Epoch 411/650:  train Loss: 57.7266   val Loss: 54.0871   time: 0.29s   best: 51.9305
2024-09-01 22:29:35,438:INFO:  Epoch 412/650:  train Loss: 56.7895   val Loss: 53.8211   time: 0.28s   best: 51.9305
2024-09-01 22:29:35,697:INFO:  Epoch 413/650:  train Loss: 55.2061   val Loss: 59.0592   time: 0.25s   best: 51.9305
2024-09-01 22:29:35,983:INFO:  Epoch 414/650:  train Loss: 56.6605   val Loss: 52.4973   time: 0.28s   best: 51.9305
2024-09-01 22:29:36,242:INFO:  Epoch 415/650:  train Loss: 56.0386   val Loss: 55.5835   time: 0.25s   best: 51.9305
2024-09-01 22:29:36,521:INFO:  Epoch 416/650:  train Loss: 54.5673   val Loss: 55.3830   time: 0.27s   best: 51.9305
2024-09-01 22:29:36,801:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:36,824:INFO:  Epoch 417/650:  train Loss: 55.6325   val Loss: 50.9320   time: 0.27s   best: 50.9320
2024-09-01 22:29:37,135:INFO:  Epoch 418/650:  train Loss: 54.5988   val Loss: 51.6379   time: 0.31s   best: 50.9320
2024-09-01 22:29:37,420:INFO:  Epoch 419/650:  train Loss: 54.3019   val Loss: 56.9432   time: 0.27s   best: 50.9320
2024-09-01 22:29:37,677:INFO:  Epoch 420/650:  train Loss: 55.3061   val Loss: 54.0523   time: 0.25s   best: 50.9320
2024-09-01 22:29:37,962:INFO:  Epoch 421/650:  train Loss: 57.1110   val Loss: 55.0069   time: 0.28s   best: 50.9320
2024-09-01 22:29:38,222:INFO:  Epoch 422/650:  train Loss: 56.9341   val Loss: 57.7060   time: 0.25s   best: 50.9320
2024-09-01 22:29:38,510:INFO:  Epoch 423/650:  train Loss: 60.1373   val Loss: 53.0126   time: 0.28s   best: 50.9320
2024-09-01 22:29:38,835:INFO:  Epoch 424/650:  train Loss: 55.2404   val Loss: 59.3977   time: 0.32s   best: 50.9320
2024-09-01 22:29:39,248:INFO:  Epoch 425/650:  train Loss: 56.5675   val Loss: 52.4041   time: 0.40s   best: 50.9320
2024-09-01 22:29:39,554:INFO:  Epoch 426/650:  train Loss: 55.4159   val Loss: 54.3187   time: 0.31s   best: 50.9320
2024-09-01 22:29:39,817:INFO:  Epoch 427/650:  train Loss: 54.2682   val Loss: 52.9985   time: 0.26s   best: 50.9320
2024-09-01 22:29:40,211:INFO:  Epoch 428/650:  train Loss: 56.0897   val Loss: 53.2557   time: 0.39s   best: 50.9320
2024-09-01 22:29:40,504:INFO:  Epoch 429/650:  train Loss: 55.6288   val Loss: 58.3049   time: 0.29s   best: 50.9320
2024-09-01 22:29:40,776:INFO:  Epoch 430/650:  train Loss: 56.3729   val Loss: 51.7101   time: 0.27s   best: 50.9320
2024-09-01 22:29:41,087:INFO:  Epoch 431/650:  train Loss: 54.5280   val Loss: 51.9069   time: 0.31s   best: 50.9320
2024-09-01 22:29:41,340:INFO:  Epoch 432/650:  train Loss: 54.0600   val Loss: 51.4555   time: 0.25s   best: 50.9320
2024-09-01 22:29:41,619:INFO:  Epoch 433/650:  train Loss: 53.2017   val Loss: 51.6391   time: 0.28s   best: 50.9320
2024-09-01 22:29:41,878:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:41,901:INFO:  Epoch 434/650:  train Loss: 53.6215   val Loss: 50.1444   time: 0.25s   best: 50.1444
2024-09-01 22:29:42,183:INFO:  Epoch 435/650:  train Loss: 53.5539   val Loss: 50.8734   time: 0.28s   best: 50.1444
2024-09-01 22:29:42,449:INFO:  Epoch 436/650:  train Loss: 53.2254   val Loss: 52.8472   time: 0.26s   best: 50.1444
2024-09-01 22:29:42,721:INFO:  Epoch 437/650:  train Loss: 53.2429   val Loss: 52.8995   time: 0.27s   best: 50.1444
2024-09-01 22:29:42,987:INFO:  Epoch 438/650:  train Loss: 54.3494   val Loss: 51.4028   time: 0.26s   best: 50.1444
2024-09-01 22:29:43,304:INFO:  Epoch 439/650:  train Loss: 53.3125   val Loss: 51.6762   time: 0.31s   best: 50.1444
2024-09-01 22:29:43,579:INFO:  Epoch 440/650:  train Loss: 53.7137   val Loss: 54.7650   time: 0.27s   best: 50.1444
2024-09-01 22:29:43,858:INFO:  Epoch 441/650:  train Loss: 54.8897   val Loss: 54.5382   time: 0.28s   best: 50.1444
2024-09-01 22:29:44,133:INFO:  Epoch 442/650:  train Loss: 55.2646   val Loss: 52.6028   time: 0.26s   best: 50.1444
2024-09-01 22:29:44,416:INFO:  Epoch 443/650:  train Loss: 54.9419   val Loss: 56.3566   time: 0.27s   best: 50.1444
2024-09-01 22:29:44,703:INFO:  Epoch 444/650:  train Loss: 54.7887   val Loss: 54.2274   time: 0.27s   best: 50.1444
2024-09-01 22:29:44,959:INFO:  Epoch 445/650:  train Loss: 55.8352   val Loss: 52.9912   time: 0.25s   best: 50.1444
2024-09-01 22:29:45,286:INFO:  Epoch 446/650:  train Loss: 54.2459   val Loss: 54.2685   time: 0.32s   best: 50.1444
2024-09-01 22:29:45,553:INFO:  Epoch 447/650:  train Loss: 54.4115   val Loss: 51.6188   time: 0.27s   best: 50.1444
2024-09-01 22:29:45,826:INFO:  Epoch 448/650:  train Loss: 54.7185   val Loss: 53.2720   time: 0.27s   best: 50.1444
2024-09-01 22:29:46,090:INFO:  Epoch 449/650:  train Loss: 55.9126   val Loss: 60.3423   time: 0.26s   best: 50.1444
2024-09-01 22:29:46,363:INFO:  Epoch 450/650:  train Loss: 57.7700   val Loss: 56.0338   time: 0.27s   best: 50.1444
2024-09-01 22:29:46,631:INFO:  Epoch 451/650:  train Loss: 54.6744   val Loss: 53.8435   time: 0.26s   best: 50.1444
2024-09-01 22:29:46,905:INFO:  Epoch 452/650:  train Loss: 54.0034   val Loss: 50.8093   time: 0.27s   best: 50.1444
2024-09-01 22:29:47,226:INFO:  Epoch 453/650:  train Loss: 53.0721   val Loss: 50.9218   time: 0.31s   best: 50.1444
2024-09-01 22:29:47,496:INFO:  Epoch 454/650:  train Loss: 53.4756   val Loss: 50.6658   time: 0.27s   best: 50.1444
2024-09-01 22:29:47,756:INFO:  Epoch 455/650:  train Loss: 52.5463   val Loss: 50.2253   time: 0.26s   best: 50.1444
2024-09-01 22:29:48,026:INFO:  Epoch 456/650:  train Loss: 52.4650   val Loss: 50.2263   time: 0.27s   best: 50.1444
2024-09-01 22:29:48,289:INFO:  Epoch 457/650:  train Loss: 52.2466   val Loss: 50.5162   time: 0.26s   best: 50.1444
2024-09-01 22:29:48,567:INFO:  Epoch 458/650:  train Loss: 52.4703   val Loss: 50.3595   time: 0.28s   best: 50.1444
2024-09-01 22:29:48,820:INFO:  Epoch 459/650:  train Loss: 52.2345   val Loss: 50.3337   time: 0.25s   best: 50.1444
2024-09-01 22:29:49,123:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:49,146:INFO:  Epoch 460/650:  train Loss: 52.4616   val Loss: 50.0507   time: 0.30s   best: 50.0507
2024-09-01 22:29:49,463:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:29:49,487:INFO:  Epoch 461/650:  train Loss: 52.2348   val Loss: 48.5179   time: 0.31s   best: 48.5179
2024-09-01 22:29:49,753:INFO:  Epoch 462/650:  train Loss: 52.0678   val Loss: 49.9420   time: 0.26s   best: 48.5179
2024-09-01 22:29:50,136:INFO:  Epoch 463/650:  train Loss: 51.7699   val Loss: 51.3340   time: 0.38s   best: 48.5179
2024-09-01 22:29:50,394:INFO:  Epoch 464/650:  train Loss: 52.6687   val Loss: 56.0446   time: 0.25s   best: 48.5179
2024-09-01 22:29:50,762:INFO:  Epoch 465/650:  train Loss: 54.5683   val Loss: 50.8502   time: 0.36s   best: 48.5179
2024-09-01 22:29:51,109:INFO:  Epoch 466/650:  train Loss: 54.5983   val Loss: 53.4005   time: 0.35s   best: 48.5179
2024-09-01 22:29:51,424:INFO:  Epoch 467/650:  train Loss: 52.8556   val Loss: 55.7996   time: 0.30s   best: 48.5179
2024-09-01 22:29:51,698:INFO:  Epoch 468/650:  train Loss: 56.0483   val Loss: 49.2241   time: 0.27s   best: 48.5179
2024-09-01 22:29:51,950:INFO:  Epoch 469/650:  train Loss: 55.4256   val Loss: 55.9714   time: 0.25s   best: 48.5179
2024-09-01 22:29:52,232:INFO:  Epoch 470/650:  train Loss: 54.0670   val Loss: 55.4065   time: 0.28s   best: 48.5179
2024-09-01 22:29:52,487:INFO:  Epoch 471/650:  train Loss: 57.9177   val Loss: 58.1846   time: 0.25s   best: 48.5179
2024-09-01 22:29:52,772:INFO:  Epoch 472/650:  train Loss: 58.0182   val Loss: 55.9546   time: 0.28s   best: 48.5179
2024-09-01 22:29:53,028:INFO:  Epoch 473/650:  train Loss: 57.6332   val Loss: 51.6940   time: 0.25s   best: 48.5179
2024-09-01 22:29:53,309:INFO:  Epoch 474/650:  train Loss: 53.6355   val Loss: 55.2460   time: 0.28s   best: 48.5179
2024-09-01 22:29:53,606:INFO:  Epoch 475/650:  train Loss: 54.7750   val Loss: 50.9610   time: 0.30s   best: 48.5179
2024-09-01 22:29:53,889:INFO:  Epoch 476/650:  train Loss: 54.4977   val Loss: 51.5748   time: 0.28s   best: 48.5179
2024-09-01 22:29:54,146:INFO:  Epoch 477/650:  train Loss: 52.1726   val Loss: 52.5686   time: 0.26s   best: 48.5179
2024-09-01 22:29:54,439:INFO:  Epoch 478/650:  train Loss: 53.0473   val Loss: 51.1051   time: 0.29s   best: 48.5179
2024-09-01 22:29:54,703:INFO:  Epoch 479/650:  train Loss: 53.4218   val Loss: 49.6593   time: 0.26s   best: 48.5179
2024-09-01 22:29:54,979:INFO:  Epoch 480/650:  train Loss: 52.8889   val Loss: 54.2927   time: 0.28s   best: 48.5179
2024-09-01 22:29:55,244:INFO:  Epoch 481/650:  train Loss: 52.4980   val Loss: 53.5623   time: 0.26s   best: 48.5179
2024-09-01 22:29:55,553:INFO:  Epoch 482/650:  train Loss: 54.2219   val Loss: 50.8686   time: 0.31s   best: 48.5179
2024-09-01 22:29:55,826:INFO:  Epoch 483/650:  train Loss: 53.2951   val Loss: 53.9845   time: 0.27s   best: 48.5179
2024-09-01 22:29:56,079:INFO:  Epoch 484/650:  train Loss: 52.6439   val Loss: 53.1383   time: 0.25s   best: 48.5179
2024-09-01 22:29:56,360:INFO:  Epoch 485/650:  train Loss: 53.3388   val Loss: 49.6990   time: 0.28s   best: 48.5179
2024-09-01 22:29:56,612:INFO:  Epoch 486/650:  train Loss: 53.7809   val Loss: 55.7633   time: 0.25s   best: 48.5179
2024-09-01 22:29:56,891:INFO:  Epoch 487/650:  train Loss: 53.8381   val Loss: 54.7034   time: 0.28s   best: 48.5179
2024-09-01 22:29:57,142:INFO:  Epoch 488/650:  train Loss: 55.6825   val Loss: 56.4604   time: 0.25s   best: 48.5179
2024-09-01 22:29:57,537:INFO:  Epoch 489/650:  train Loss: 54.6830   val Loss: 51.1442   time: 0.39s   best: 48.5179
2024-09-01 22:29:57,801:INFO:  Epoch 490/650:  train Loss: 54.5079   val Loss: 56.0968   time: 0.26s   best: 48.5179
2024-09-01 22:29:58,067:INFO:  Epoch 491/650:  train Loss: 54.6289   val Loss: 50.0629   time: 0.27s   best: 48.5179
2024-09-01 22:29:58,331:INFO:  Epoch 492/650:  train Loss: 52.1679   val Loss: 49.9305   time: 0.26s   best: 48.5179
2024-09-01 22:29:58,603:INFO:  Epoch 493/650:  train Loss: 51.8838   val Loss: 49.7012   time: 0.27s   best: 48.5179
2024-09-01 22:29:58,864:INFO:  Epoch 494/650:  train Loss: 50.8397   val Loss: 50.0069   time: 0.26s   best: 48.5179
2024-09-01 22:29:59,136:INFO:  Epoch 495/650:  train Loss: 50.8325   val Loss: 51.4309   time: 0.27s   best: 48.5179
2024-09-01 22:29:59,397:INFO:  Epoch 496/650:  train Loss: 52.8426   val Loss: 49.0221   time: 0.26s   best: 48.5179
2024-09-01 22:29:59,711:INFO:  Epoch 497/650:  train Loss: 52.1203   val Loss: 50.3760   time: 0.31s   best: 48.5179
2024-09-01 22:29:59,986:INFO:  Epoch 498/650:  train Loss: 51.5481   val Loss: 55.0811   time: 0.27s   best: 48.5179
2024-09-01 22:30:00,239:INFO:  Epoch 499/650:  train Loss: 54.6717   val Loss: 49.7013   time: 0.25s   best: 48.5179
2024-09-01 22:30:00,645:INFO:  Epoch 500/650:  train Loss: 52.4752   val Loss: 50.3025   time: 0.41s   best: 48.5179
2024-09-01 22:30:00,906:INFO:  Epoch 501/650:  train Loss: 50.9445   val Loss: 51.6378   time: 0.26s   best: 48.5179
2024-09-01 22:30:01,178:INFO:  Epoch 502/650:  train Loss: 51.9827   val Loss: 50.5844   time: 0.27s   best: 48.5179
2024-09-01 22:30:01,440:INFO:  Epoch 503/650:  train Loss: 52.6149   val Loss: 50.3500   time: 0.26s   best: 48.5179
2024-09-01 22:30:01,729:INFO:  Epoch 504/650:  train Loss: 53.3411   val Loss: 51.3731   time: 0.29s   best: 48.5179
2024-09-01 22:30:01,991:INFO:  Epoch 505/650:  train Loss: 53.8457   val Loss: 54.9300   time: 0.26s   best: 48.5179
2024-09-01 22:30:02,262:INFO:  Epoch 506/650:  train Loss: 54.1684   val Loss: 53.2374   time: 0.27s   best: 48.5179
2024-09-01 22:30:02,524:INFO:  Epoch 507/650:  train Loss: 55.8927   val Loss: 52.2125   time: 0.26s   best: 48.5179
2024-09-01 22:30:02,793:INFO:  Epoch 508/650:  train Loss: 52.8340   val Loss: 51.4682   time: 0.27s   best: 48.5179
2024-09-01 22:30:03,055:INFO:  Epoch 509/650:  train Loss: 51.8510   val Loss: 50.4415   time: 0.26s   best: 48.5179
2024-09-01 22:30:03,325:INFO:  Epoch 510/650:  train Loss: 52.1030   val Loss: 53.3063   time: 0.27s   best: 48.5179
2024-09-01 22:30:03,587:INFO:  Epoch 511/650:  train Loss: 54.4347   val Loss: 51.2490   time: 0.26s   best: 48.5179
2024-09-01 22:30:03,900:INFO:  Epoch 512/650:  train Loss: 53.1555   val Loss: 56.4893   time: 0.31s   best: 48.5179
2024-09-01 22:30:04,193:INFO:  Epoch 513/650:  train Loss: 55.3724   val Loss: 53.9906   time: 0.29s   best: 48.5179
2024-09-01 22:30:04,473:INFO:  Epoch 514/650:  train Loss: 55.0188   val Loss: 52.2182   time: 0.28s   best: 48.5179
2024-09-01 22:30:04,729:INFO:  Epoch 515/650:  train Loss: 54.7142   val Loss: 59.0644   time: 0.26s   best: 48.5179
2024-09-01 22:30:05,012:INFO:  Epoch 516/650:  train Loss: 56.4108   val Loss: 48.6644   time: 0.28s   best: 48.5179
2024-09-01 22:30:05,264:INFO:  Epoch 517/650:  train Loss: 53.3676   val Loss: 52.2966   time: 0.25s   best: 48.5179
2024-09-01 22:30:05,543:INFO:  Epoch 518/650:  train Loss: 52.6533   val Loss: 54.4833   time: 0.28s   best: 48.5179
2024-09-01 22:30:05,841:INFO:  Epoch 519/650:  train Loss: 54.2427   val Loss: 49.3841   time: 0.30s   best: 48.5179
2024-09-01 22:30:06,120:INFO:  Epoch 520/650:  train Loss: 51.9919   val Loss: 52.0512   time: 0.28s   best: 48.5179
2024-09-01 22:30:06,373:INFO:  Epoch 521/650:  train Loss: 51.3883   val Loss: 52.0757   time: 0.25s   best: 48.5179
2024-09-01 22:30:06,656:INFO:  Epoch 522/650:  train Loss: 52.7022   val Loss: 49.9785   time: 0.28s   best: 48.5179
2024-09-01 22:30:06,909:INFO:  Epoch 523/650:  train Loss: 52.2880   val Loss: 50.9730   time: 0.25s   best: 48.5179
2024-09-01 22:30:07,201:INFO:  Epoch 524/650:  train Loss: 52.7973   val Loss: 58.2692   time: 0.29s   best: 48.5179
2024-09-01 22:30:07,482:INFO:  Epoch 525/650:  train Loss: 56.7030   val Loss: 52.6636   time: 0.28s   best: 48.5179
2024-09-01 22:30:07,779:INFO:  Epoch 526/650:  train Loss: 53.0512   val Loss: 52.9005   time: 0.30s   best: 48.5179
2024-09-01 22:30:08,057:INFO:  Epoch 527/650:  train Loss: 52.9056   val Loss: 49.7622   time: 0.28s   best: 48.5179
2024-09-01 22:30:08,309:INFO:  Epoch 528/650:  train Loss: 51.3610   val Loss: 49.4595   time: 0.25s   best: 48.5179
2024-09-01 22:30:08,585:INFO:  Epoch 529/650:  train Loss: 51.4669   val Loss: 49.9785   time: 0.28s   best: 48.5179
2024-09-01 22:30:08,837:INFO:  Epoch 530/650:  train Loss: 51.4505   val Loss: 50.8578   time: 0.25s   best: 48.5179
2024-09-01 22:30:09,116:INFO:  Epoch 531/650:  train Loss: 50.4865   val Loss: 51.1239   time: 0.28s   best: 48.5179
2024-09-01 22:30:09,373:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:30:09,397:INFO:  Epoch 532/650:  train Loss: 52.1258   val Loss: 47.8612   time: 0.25s   best: 47.8612
2024-09-01 22:30:09,705:INFO:  Epoch 533/650:  train Loss: 52.2088   val Loss: 48.1758   time: 0.31s   best: 47.8612
2024-09-01 22:30:10,029:INFO:  Epoch 534/650:  train Loss: 52.2740   val Loss: 51.2982   time: 0.32s   best: 47.8612
2024-09-01 22:30:10,407:INFO:  Epoch 535/650:  train Loss: 51.4638   val Loss: 51.4840   time: 0.38s   best: 47.8612
2024-09-01 22:30:10,672:INFO:  Epoch 536/650:  train Loss: 53.4717   val Loss: 49.8594   time: 0.26s   best: 47.8612
2024-09-01 22:30:10,942:INFO:  Epoch 537/650:  train Loss: 53.5117   val Loss: 51.2537   time: 0.27s   best: 47.8612
2024-09-01 22:30:11,204:INFO:  Epoch 538/650:  train Loss: 51.2673   val Loss: 55.5525   time: 0.26s   best: 47.8612
2024-09-01 22:30:11,475:INFO:  Epoch 539/650:  train Loss: 52.7771   val Loss: 50.3352   time: 0.27s   best: 47.8612
2024-09-01 22:30:11,736:INFO:  Epoch 540/650:  train Loss: 54.8085   val Loss: 60.2168   time: 0.26s   best: 47.8612
2024-09-01 22:30:12,128:INFO:  Epoch 541/650:  train Loss: 56.9945   val Loss: 52.2389   time: 0.39s   best: 47.8612
2024-09-01 22:30:12,398:INFO:  Epoch 542/650:  train Loss: 56.5477   val Loss: 57.8502   time: 0.27s   best: 47.8612
2024-09-01 22:30:12,670:INFO:  Epoch 543/650:  train Loss: 54.5984   val Loss: 51.1651   time: 0.26s   best: 47.8612
2024-09-01 22:30:12,944:INFO:  Epoch 544/650:  train Loss: 52.5807   val Loss: 50.5068   time: 0.27s   best: 47.8612
2024-09-01 22:30:13,211:INFO:  Epoch 545/650:  train Loss: 51.8025   val Loss: 49.2143   time: 0.26s   best: 47.8612
2024-09-01 22:30:13,487:INFO:  Epoch 546/650:  train Loss: 50.6439   val Loss: 51.1249   time: 0.27s   best: 47.8612
2024-09-01 22:30:13,760:INFO:  Epoch 547/650:  train Loss: 50.9588   val Loss: 51.0558   time: 0.27s   best: 47.8612
2024-09-01 22:30:14,103:INFO:  Epoch 548/650:  train Loss: 52.8341   val Loss: 49.4964   time: 0.34s   best: 47.8612
2024-09-01 22:30:14,369:INFO:  Epoch 549/650:  train Loss: 51.0177   val Loss: 52.0554   time: 0.27s   best: 47.8612
2024-09-01 22:30:14,648:INFO:  Epoch 550/650:  train Loss: 53.0785   val Loss: 56.1895   time: 0.28s   best: 47.8612
2024-09-01 22:30:14,901:INFO:  Epoch 551/650:  train Loss: 55.1755   val Loss: 55.3419   time: 0.25s   best: 47.8612
2024-09-01 22:30:15,176:INFO:  Epoch 552/650:  train Loss: 53.4831   val Loss: 52.5918   time: 0.27s   best: 47.8612
2024-09-01 22:30:15,248:INFO:  Starting experiment Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)
2024-09-01 22:30:15,250:INFO:  Defining the model
2024-09-01 22:30:15,445:INFO:  Epoch 553/650:  train Loss: 53.2427   val Loss: 50.1458   time: 0.27s   best: 47.8612
2024-09-01 22:30:15,709:INFO:  Epoch 554/650:  train Loss: 51.7904   val Loss: 50.5851   time: 0.26s   best: 47.8612
2024-09-01 22:30:15,776:INFO:  Reading the dataset
2024-09-01 22:30:16,028:INFO:  Epoch 555/650:  train Loss: 51.7470   val Loss: 49.1456   time: 0.32s   best: 47.8612
2024-09-01 22:30:16,290:INFO:  Epoch 556/650:  train Loss: 50.9144   val Loss: 49.5387   time: 0.26s   best: 47.8612
2024-09-01 22:30:16,555:INFO:  Epoch 557/650:  train Loss: 50.4036   val Loss: 51.4866   time: 0.26s   best: 47.8612
2024-09-01 22:30:16,815:INFO:  Epoch 558/650:  train Loss: 51.4744   val Loss: 49.4152   time: 0.26s   best: 47.8612
2024-09-01 22:30:17,086:INFO:  Epoch 559/650:  train Loss: 50.9505   val Loss: 49.5008   time: 0.27s   best: 47.8612
2024-09-01 22:30:17,346:INFO:  Epoch 560/650:  train Loss: 51.7176   val Loss: 48.6376   time: 0.26s   best: 47.8612
2024-09-01 22:30:17,617:INFO:  Epoch 561/650:  train Loss: 50.3200   val Loss: 49.1655   time: 0.27s   best: 47.8612
2024-09-01 22:30:17,879:INFO:  Epoch 562/650:  train Loss: 50.6142   val Loss: 49.6983   time: 0.26s   best: 47.8612
2024-09-01 22:30:18,242:INFO:  Epoch 563/650:  train Loss: 52.0695   val Loss: 52.4047   time: 0.36s   best: 47.8612
2024-09-01 22:30:18,502:INFO:  Epoch 564/650:  train Loss: 55.4942   val Loss: 54.1908   time: 0.26s   best: 47.8612
2024-09-01 22:30:18,782:INFO:  Epoch 565/650:  train Loss: 54.7821   val Loss: 49.6653   time: 0.28s   best: 47.8612
2024-09-01 22:30:19,039:INFO:  Epoch 566/650:  train Loss: 51.1774   val Loss: 49.6914   time: 0.26s   best: 47.8612
2024-09-01 22:30:19,322:INFO:  Epoch 567/650:  train Loss: 50.1146   val Loss: 48.2975   time: 0.28s   best: 47.8612
2024-09-01 22:30:19,574:INFO:  Epoch 568/650:  train Loss: 50.0318   val Loss: 48.9556   time: 0.25s   best: 47.8612
2024-09-01 22:30:19,854:INFO:  Epoch 569/650:  train Loss: 49.6343   val Loss: 51.3441   time: 0.28s   best: 47.8612
2024-09-01 22:30:20,169:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:30:20,204:INFO:  Epoch 570/650:  train Loss: 52.5155   val Loss: 47.6589   time: 0.31s   best: 47.6589
2024-09-01 22:30:20,711:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:30:20,735:INFO:  Epoch 571/650:  train Loss: 49.5650   val Loss: 47.0164   time: 0.34s   best: 47.0164
2024-09-01 22:30:21,016:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:30:21,039:INFO:  Epoch 572/650:  train Loss: 49.7248   val Loss: 46.9690   time: 0.28s   best: 46.9690
2024-09-01 22:30:21,313:INFO:  Epoch 573/650:  train Loss: 49.5328   val Loss: 49.1412   time: 0.27s   best: 46.9690
2024-09-01 22:30:21,566:INFO:  Epoch 574/650:  train Loss: 50.2888   val Loss: 49.9942   time: 0.25s   best: 46.9690
2024-09-01 22:30:21,846:INFO:  Epoch 575/650:  train Loss: 49.9167   val Loss: 51.4469   time: 0.28s   best: 46.9690
2024-09-01 22:30:22,097:INFO:  Epoch 576/650:  train Loss: 50.6755   val Loss: 51.7392   time: 0.25s   best: 46.9690
2024-09-01 22:30:22,440:INFO:  Epoch 577/650:  train Loss: 52.9405   val Loss: 49.0491   time: 0.34s   best: 46.9690
2024-09-01 22:30:22,579:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:22,602:INFO:  Epoch 1/650:  train Loss: 100.2127   val Loss: 96.1372   time: 0.95s   best: 96.1372
2024-09-01 22:30:22,691:INFO:  Epoch 578/650:  train Loss: 52.1445   val Loss: 47.9204   time: 0.25s   best: 46.9690
2024-09-01 22:30:22,848:INFO:  Epoch 2/650:  train Loss: 99.2337   val Loss: 100.1018   time: 0.25s   best: 96.1372
2024-09-01 22:30:22,993:INFO:  Epoch 579/650:  train Loss: 52.6210   val Loss: 51.3074   time: 0.30s   best: 46.9690
2024-09-01 22:30:23,087:INFO:  Epoch 3/650:  train Loss: 99.2925   val Loss: 100.0643   time: 0.24s   best: 96.1372
2024-09-01 22:30:23,256:INFO:  Epoch 580/650:  train Loss: 53.2131   val Loss: 55.7153   time: 0.26s   best: 46.9690
2024-09-01 22:30:23,335:INFO:  Epoch 4/650:  train Loss: 98.9221   val Loss: 97.5314   time: 0.25s   best: 96.1372
2024-09-01 22:30:23,528:INFO:  Epoch 581/650:  train Loss: 54.4399   val Loss: 51.1042   time: 0.27s   best: 46.9690
2024-09-01 22:30:23,575:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:23,599:INFO:  Epoch 5/650:  train Loss: 97.9930   val Loss: 93.4158   time: 0.23s   best: 93.4158
2024-09-01 22:30:23,792:INFO:  Epoch 582/650:  train Loss: 54.0232   val Loss: 52.8264   time: 0.26s   best: 46.9690
2024-09-01 22:30:23,852:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:23,877:INFO:  Epoch 6/650:  train Loss: 96.2969   val Loss: 92.0498   time: 0.25s   best: 92.0498
2024-09-01 22:30:24,065:INFO:  Epoch 583/650:  train Loss: 52.5925   val Loss: 52.6145   time: 0.27s   best: 46.9690
2024-09-01 22:30:24,111:INFO:  Epoch 7/650:  train Loss: 94.7012   val Loss: 92.7676   time: 0.23s   best: 92.0498
2024-09-01 22:30:24,353:INFO:  Epoch 8/650:  train Loss: 93.6351   val Loss: 93.8037   time: 0.24s   best: 92.0498
2024-09-01 22:30:24,430:INFO:  Epoch 584/650:  train Loss: 52.0194   val Loss: 54.1817   time: 0.36s   best: 46.9690
2024-09-01 22:30:24,588:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:24,612:INFO:  Epoch 9/650:  train Loss: 93.6530   val Loss: 91.8457   time: 0.23s   best: 91.8457
2024-09-01 22:30:24,685:INFO:  Epoch 585/650:  train Loss: 55.9660   val Loss: 47.4762   time: 0.25s   best: 46.9690
2024-09-01 22:30:24,858:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:24,883:INFO:  Epoch 10/650:  train Loss: 93.3867   val Loss: 91.0070   time: 0.24s   best: 91.0070
2024-09-01 22:30:24,971:INFO:  Epoch 586/650:  train Loss: 51.9542   val Loss: 53.6084   time: 0.28s   best: 46.9690
2024-09-01 22:30:25,112:INFO:  Epoch 11/650:  train Loss: 92.8705   val Loss: 93.3940   time: 0.23s   best: 91.0070
2024-09-01 22:30:25,232:INFO:  Epoch 587/650:  train Loss: 51.3165   val Loss: 54.4246   time: 0.26s   best: 46.9690
2024-09-01 22:30:25,354:INFO:  Epoch 12/650:  train Loss: 92.6986   val Loss: 93.4356   time: 0.24s   best: 91.0070
2024-09-01 22:30:25,526:INFO:  Epoch 588/650:  train Loss: 53.8367   val Loss: 50.3560   time: 0.28s   best: 46.9690
2024-09-01 22:30:25,584:INFO:  Epoch 13/650:  train Loss: 93.3584   val Loss: 91.2872   time: 0.23s   best: 91.0070
2024-09-01 22:30:25,780:INFO:  Epoch 589/650:  train Loss: 53.9828   val Loss: 56.7376   time: 0.25s   best: 46.9690
2024-09-01 22:30:25,829:INFO:  Epoch 14/650:  train Loss: 92.7465   val Loss: 92.4712   time: 0.24s   best: 91.0070
2024-09-01 22:30:26,059:INFO:  Epoch 15/650:  train Loss: 92.6309   val Loss: 93.3379   time: 0.23s   best: 91.0070
2024-09-01 22:30:26,063:INFO:  Epoch 590/650:  train Loss: 54.3893   val Loss: 51.7549   time: 0.28s   best: 46.9690
2024-09-01 22:30:26,298:INFO:  Epoch 16/650:  train Loss: 93.4007   val Loss: 93.2973   time: 0.24s   best: 91.0070
2024-09-01 22:30:26,362:INFO:  Epoch 591/650:  train Loss: 52.5358   val Loss: 47.2398   time: 0.30s   best: 46.9690
2024-09-01 22:30:26,528:INFO:  Epoch 17/650:  train Loss: 93.1898   val Loss: 93.1931   time: 0.23s   best: 91.0070
2024-09-01 22:30:26,635:INFO:  Epoch 592/650:  train Loss: 50.0243   val Loss: 50.0797   time: 0.27s   best: 46.9690
2024-09-01 22:30:26,770:INFO:  Epoch 18/650:  train Loss: 93.0146   val Loss: 91.0780   time: 0.24s   best: 91.0070
2024-09-01 22:30:26,897:INFO:  Epoch 593/650:  train Loss: 50.5338   val Loss: 50.9800   time: 0.26s   best: 46.9690
2024-09-01 22:30:27,006:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:27,031:INFO:  Epoch 19/650:  train Loss: 92.6931   val Loss: 90.1645   time: 0.23s   best: 90.1645
2024-09-01 22:30:27,170:INFO:  Epoch 594/650:  train Loss: 50.3251   val Loss: 51.7789   time: 0.27s   best: 46.9690
2024-09-01 22:30:27,271:INFO:  Epoch 20/650:  train Loss: 92.4333   val Loss: 90.5434   time: 0.24s   best: 90.1645
2024-09-01 22:30:27,433:INFO:  Epoch 595/650:  train Loss: 53.1381   val Loss: 49.1202   time: 0.26s   best: 46.9690
2024-09-01 22:30:27,502:INFO:  Epoch 21/650:  train Loss: 92.7942   val Loss: 92.3411   time: 0.23s   best: 90.1645
2024-09-01 22:30:27,705:INFO:  Epoch 596/650:  train Loss: 50.4129   val Loss: 49.1493   time: 0.27s   best: 46.9690
2024-09-01 22:30:27,751:INFO:  Epoch 22/650:  train Loss: 92.5741   val Loss: 92.1561   time: 0.25s   best: 90.1645
2024-09-01 22:30:27,970:INFO:  Epoch 597/650:  train Loss: 49.0429   val Loss: 47.5179   time: 0.26s   best: 46.9690
2024-09-01 22:30:27,983:INFO:  Epoch 23/650:  train Loss: 91.7120   val Loss: 91.6809   time: 0.23s   best: 90.1645
2024-09-01 22:30:28,217:INFO:  Epoch 24/650:  train Loss: 91.6046   val Loss: 91.1756   time: 0.23s   best: 90.1645
2024-09-01 22:30:28,250:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:30:28,311:INFO:  Epoch 598/650:  train Loss: 48.6314   val Loss: 46.6596   time: 0.27s   best: 46.6596
2024-09-01 22:30:28,462:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:28,487:INFO:  Epoch 25/650:  train Loss: 91.3370   val Loss: 90.0633   time: 0.24s   best: 90.0633
2024-09-01 22:30:28,591:INFO:  Epoch 599/650:  train Loss: 47.6168   val Loss: 48.8031   time: 0.28s   best: 46.6596
2024-09-01 22:30:28,718:INFO:  Epoch 26/650:  train Loss: 90.6162   val Loss: 90.5658   time: 0.23s   best: 90.0633
2024-09-01 22:30:28,849:INFO:  Epoch 600/650:  train Loss: 50.2100   val Loss: 47.9161   time: 0.26s   best: 46.6596
2024-09-01 22:30:29,004:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:29,029:INFO:  Epoch 27/650:  train Loss: 90.3283   val Loss: 90.0353   time: 0.28s   best: 90.0353
2024-09-01 22:30:29,130:INFO:  Epoch 601/650:  train Loss: 49.7137   val Loss: 48.9893   time: 0.28s   best: 46.6596
2024-09-01 22:30:29,274:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:29,298:INFO:  Epoch 28/650:  train Loss: 90.1043   val Loss: 88.8244   time: 0.24s   best: 88.8244
2024-09-01 22:30:29,393:INFO:  Epoch 602/650:  train Loss: 50.8825   val Loss: 51.4217   time: 0.26s   best: 46.6596
2024-09-01 22:30:29,528:INFO:  Epoch 29/650:  train Loss: 89.7099   val Loss: 88.8573   time: 0.23s   best: 88.8244
2024-09-01 22:30:29,665:INFO:  Epoch 603/650:  train Loss: 51.6480   val Loss: 48.4652   time: 0.27s   best: 46.6596
2024-09-01 22:30:29,777:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:29,802:INFO:  Epoch 30/650:  train Loss: 89.1996   val Loss: 88.1232   time: 0.24s   best: 88.1232
2024-09-01 22:30:29,928:INFO:  Epoch 604/650:  train Loss: 49.6586   val Loss: 51.4799   time: 0.26s   best: 46.6596
2024-09-01 22:30:30,032:INFO:  Epoch 31/650:  train Loss: 88.9061   val Loss: 88.6613   time: 0.23s   best: 88.1232
2024-09-01 22:30:30,196:INFO:  Epoch 605/650:  train Loss: 53.1517   val Loss: 48.6273   time: 0.26s   best: 46.6596
2024-09-01 22:30:30,265:INFO:  Epoch 32/650:  train Loss: 88.6139   val Loss: 88.6709   time: 0.23s   best: 88.1232
2024-09-01 22:30:30,507:INFO:  Epoch 33/650:  train Loss: 89.2574   val Loss: 88.1452   time: 0.24s   best: 88.1232
2024-09-01 22:30:30,522:INFO:  Epoch 606/650:  train Loss: 50.6986   val Loss: 48.6030   time: 0.32s   best: 46.6596
2024-09-01 22:30:30,741:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:30,779:INFO:  Epoch 34/650:  train Loss: 88.3261   val Loss: 87.4084   time: 0.23s   best: 87.4084
2024-09-01 22:30:30,934:INFO:  Epoch 607/650:  train Loss: 51.0706   val Loss: 50.7413   time: 0.41s   best: 46.6596
2024-09-01 22:30:31,027:INFO:  Epoch 35/650:  train Loss: 88.3251   val Loss: 87.5551   time: 0.25s   best: 87.4084
2024-09-01 22:30:31,190:INFO:  Epoch 608/650:  train Loss: 51.3444   val Loss: 48.9164   time: 0.25s   best: 46.6596
2024-09-01 22:30:31,264:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:31,289:INFO:  Epoch 36/650:  train Loss: 87.5678   val Loss: 87.2165   time: 0.23s   best: 87.2165
2024-09-01 22:30:31,481:INFO:  Epoch 609/650:  train Loss: 50.8267   val Loss: 48.8460   time: 0.29s   best: 46.6596
2024-09-01 22:30:31,534:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:31,558:INFO:  Epoch 37/650:  train Loss: 88.2536   val Loss: 86.8225   time: 0.24s   best: 86.8225
2024-09-01 22:30:31,743:INFO:  Epoch 610/650:  train Loss: 52.4410   val Loss: 48.9174   time: 0.26s   best: 46.6596
2024-09-01 22:30:31,797:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:31,831:INFO:  Epoch 38/650:  train Loss: 87.2742   val Loss: 86.4284   time: 0.23s   best: 86.4284
2024-09-01 22:30:32,025:INFO:  Epoch 611/650:  train Loss: 49.3687   val Loss: 52.3032   time: 0.28s   best: 46.6596
2024-09-01 22:30:32,062:INFO:  Epoch 39/650:  train Loss: 87.0002   val Loss: 86.5183   time: 0.23s   best: 86.4284
2024-09-01 22:30:32,280:INFO:  Epoch 612/650:  train Loss: 51.6611   val Loss: 58.1231   time: 0.25s   best: 46.6596
2024-09-01 22:30:32,299:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:32,338:INFO:  Epoch 40/650:  train Loss: 86.5736   val Loss: 85.8124   time: 0.23s   best: 85.8124
2024-09-01 22:30:32,574:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:32,599:INFO:  Epoch 41/650:  train Loss: 85.8984   val Loss: 85.2339   time: 0.23s   best: 85.2339
2024-09-01 22:30:32,642:INFO:  Epoch 613/650:  train Loss: 56.2474   val Loss: 56.0587   time: 0.36s   best: 46.6596
2024-09-01 22:30:32,845:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:32,869:INFO:  Epoch 42/650:  train Loss: 85.5539   val Loss: 84.6374   time: 0.24s   best: 84.6374
2024-09-01 22:30:32,917:INFO:  Epoch 614/650:  train Loss: 56.0982   val Loss: 52.7493   time: 0.27s   best: 46.6596
2024-09-01 22:30:33,111:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:33,137:INFO:  Epoch 43/650:  train Loss: 85.2115   val Loss: 83.6491   time: 0.24s   best: 83.6491
2024-09-01 22:30:33,187:INFO:  Epoch 615/650:  train Loss: 54.1465   val Loss: 51.6306   time: 0.27s   best: 46.6596
2024-09-01 22:30:33,382:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:33,407:INFO:  Epoch 44/650:  train Loss: 83.9004   val Loss: 82.6352   time: 0.24s   best: 82.6352
2024-09-01 22:30:33,473:INFO:  Epoch 616/650:  train Loss: 53.5194   val Loss: 51.7722   time: 0.28s   best: 46.6596
2024-09-01 22:30:33,643:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:33,671:INFO:  Epoch 45/650:  train Loss: 83.2494   val Loss: 81.6402   time: 0.23s   best: 81.6402
2024-09-01 22:30:33,735:INFO:  Epoch 617/650:  train Loss: 55.8691   val Loss: 56.8104   time: 0.26s   best: 46.6596
2024-09-01 22:30:33,911:INFO:  Epoch 46/650:  train Loss: 83.1822   val Loss: 82.4683   time: 0.24s   best: 81.6402
2024-09-01 22:30:34,029:INFO:  Epoch 618/650:  train Loss: 58.4923   val Loss: 57.6286   time: 0.29s   best: 46.6596
2024-09-01 22:30:34,144:INFO:  Epoch 47/650:  train Loss: 83.9542   val Loss: 83.0394   time: 0.23s   best: 81.6402
2024-09-01 22:30:34,284:INFO:  Epoch 619/650:  train Loss: 55.4236   val Loss: 53.6551   time: 0.25s   best: 46.6596
2024-09-01 22:30:34,388:INFO:  Epoch 48/650:  train Loss: 84.0172   val Loss: 83.3720   time: 0.24s   best: 81.6402
2024-09-01 22:30:34,610:INFO:  Epoch 620/650:  train Loss: 54.0380   val Loss: 49.2760   time: 0.32s   best: 46.6596
2024-09-01 22:30:34,621:INFO:  Epoch 49/650:  train Loss: 81.9374   val Loss: 81.7139   time: 0.23s   best: 81.6402
2024-09-01 22:30:34,854:INFO:  Epoch 50/650:  train Loss: 81.2299   val Loss: 82.1603   time: 0.23s   best: 81.6402
2024-09-01 22:30:34,865:INFO:  Epoch 621/650:  train Loss: 50.6780   val Loss: 48.3214   time: 0.25s   best: 46.6596
2024-09-01 22:30:35,136:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:35,147:INFO:  Epoch 622/650:  train Loss: 50.3103   val Loss: 47.0665   time: 0.28s   best: 46.6596
2024-09-01 22:30:35,161:INFO:  Epoch 51/650:  train Loss: 82.5954   val Loss: 80.8373   time: 0.28s   best: 80.8373
2024-09-01 22:30:35,400:INFO:  Epoch 623/650:  train Loss: 48.9676   val Loss: 47.6655   time: 0.25s   best: 46.6596
2024-09-01 22:30:35,409:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:35,434:INFO:  Epoch 52/650:  train Loss: 81.2008   val Loss: 79.6001   time: 0.24s   best: 79.6001
2024-09-01 22:30:35,675:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:35,691:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_ce9b.pt
2024-09-01 22:30:35,697:INFO:  Epoch 53/650:  train Loss: 80.0741   val Loss: 79.2791   time: 0.23s   best: 79.2791
2024-09-01 22:30:35,719:INFO:  Epoch 624/650:  train Loss: 49.5849   val Loss: 45.6492   time: 0.28s   best: 45.6492
2024-09-01 22:30:35,945:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:35,969:INFO:  Epoch 54/650:  train Loss: 79.8837   val Loss: 78.7711   time: 0.24s   best: 78.7711
2024-09-01 22:30:35,979:INFO:  Epoch 625/650:  train Loss: 48.8337   val Loss: 47.3706   time: 0.26s   best: 45.6492
2024-09-01 22:30:36,203:INFO:  Epoch 55/650:  train Loss: 80.1786   val Loss: 78.9503   time: 0.23s   best: 78.7711
2024-09-01 22:30:36,267:INFO:  Epoch 626/650:  train Loss: 49.5350   val Loss: 46.2910   time: 0.29s   best: 45.6492
2024-09-01 22:30:36,554:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:36,579:INFO:  Epoch 56/650:  train Loss: 79.2480   val Loss: 77.9466   time: 0.24s   best: 77.9466
2024-09-01 22:30:36,604:INFO:  Epoch 627/650:  train Loss: 49.2923   val Loss: 46.5915   time: 0.33s   best: 45.6492
2024-09-01 22:30:36,818:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:36,843:INFO:  Epoch 57/650:  train Loss: 78.8046   val Loss: 77.5892   time: 0.23s   best: 77.5892
2024-09-01 22:30:36,858:INFO:  Epoch 628/650:  train Loss: 48.8450   val Loss: 47.2663   time: 0.25s   best: 45.6492
2024-09-01 22:30:37,088:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:37,113:INFO:  Epoch 58/650:  train Loss: 78.7360   val Loss: 77.2708   time: 0.24s   best: 77.2708
2024-09-01 22:30:37,139:INFO:  Epoch 629/650:  train Loss: 48.6255   val Loss: 46.9833   time: 0.28s   best: 45.6492
2024-09-01 22:30:37,354:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:37,379:INFO:  Epoch 59/650:  train Loss: 78.7221   val Loss: 76.9851   time: 0.24s   best: 76.9851
2024-09-01 22:30:37,394:INFO:  Epoch 630/650:  train Loss: 47.3087   val Loss: 49.2779   time: 0.25s   best: 45.6492
2024-09-01 22:30:37,642:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:37,670:INFO:  Epoch 631/650:  train Loss: 47.9011   val Loss: 51.5754   time: 0.27s   best: 45.6492
2024-09-01 22:30:37,670:INFO:  Epoch 60/650:  train Loss: 78.0630   val Loss: 76.2729   time: 0.26s   best: 76.2729
2024-09-01 22:30:37,906:INFO:  Epoch 61/650:  train Loss: 77.9297   val Loss: 76.5687   time: 0.23s   best: 76.2729
2024-09-01 22:30:37,924:INFO:  Epoch 632/650:  train Loss: 48.9456   val Loss: 56.0647   time: 0.25s   best: 45.6492
2024-09-01 22:30:38,154:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:38,181:INFO:  Epoch 62/650:  train Loss: 77.2740   val Loss: 75.9968   time: 0.24s   best: 75.9968
2024-09-01 22:30:38,205:INFO:  Epoch 633/650:  train Loss: 52.8801   val Loss: 48.6482   time: 0.28s   best: 45.6492
2024-09-01 22:30:38,418:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:38,453:INFO:  Epoch 63/650:  train Loss: 76.8658   val Loss: 75.6278   time: 0.23s   best: 75.6278
2024-09-01 22:30:38,459:INFO:  Epoch 634/650:  train Loss: 53.1842   val Loss: 60.7927   time: 0.25s   best: 45.6492
2024-09-01 22:30:38,686:INFO:  Epoch 64/650:  train Loss: 76.9743   val Loss: 76.3756   time: 0.23s   best: 75.6278
2024-09-01 22:30:38,844:INFO:  Epoch 635/650:  train Loss: 55.7839   val Loss: 48.8538   time: 0.38s   best: 45.6492
2024-09-01 22:30:38,918:INFO:  Epoch 65/650:  train Loss: 77.3679   val Loss: 76.0167   time: 0.23s   best: 75.6278
2024-09-01 22:30:39,108:INFO:  Epoch 636/650:  train Loss: 54.7093   val Loss: 60.6316   time: 0.26s   best: 45.6492
2024-09-01 22:30:39,160:INFO:  Epoch 66/650:  train Loss: 78.5932   val Loss: 77.1401   time: 0.24s   best: 75.6278
2024-09-01 22:30:39,379:INFO:  Epoch 637/650:  train Loss: 54.8241   val Loss: 50.7006   time: 0.27s   best: 45.6492
2024-09-01 22:30:39,394:INFO:  Epoch 67/650:  train Loss: 78.6275   val Loss: 77.3651   time: 0.23s   best: 75.6278
2024-09-01 22:30:39,636:INFO:  Epoch 68/650:  train Loss: 77.8349   val Loss: 76.2635   time: 0.24s   best: 75.6278
2024-09-01 22:30:39,737:INFO:  Epoch 638/650:  train Loss: 52.8959   val Loss: 50.7089   time: 0.36s   best: 45.6492
2024-09-01 22:30:39,872:INFO:  Epoch 69/650:  train Loss: 77.8434   val Loss: 76.5047   time: 0.24s   best: 75.6278
2024-09-01 22:30:40,027:INFO:  Epoch 639/650:  train Loss: 52.0466   val Loss: 52.7081   time: 0.29s   best: 45.6492
2024-09-01 22:30:40,182:INFO:  Epoch 70/650:  train Loss: 76.9091   val Loss: 76.6296   time: 0.31s   best: 75.6278
2024-09-01 22:30:40,304:INFO:  Epoch 640/650:  train Loss: 50.9901   val Loss: 52.3385   time: 0.27s   best: 45.6492
2024-09-01 22:30:40,419:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:40,447:INFO:  Epoch 71/650:  train Loss: 76.0195   val Loss: 74.7400   time: 0.23s   best: 74.7400
2024-09-01 22:30:40,611:INFO:  Epoch 641/650:  train Loss: 50.6019   val Loss: 50.4450   time: 0.30s   best: 45.6492
2024-09-01 22:30:40,803:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:40,827:INFO:  Epoch 72/650:  train Loss: 75.1195   val Loss: 74.4567   time: 0.35s   best: 74.4567
2024-09-01 22:30:41,076:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:41,077:INFO:  Epoch 642/650:  train Loss: 51.6461   val Loss: 46.0704   time: 0.46s   best: 45.6492
2024-09-01 22:30:41,099:INFO:  Epoch 73/650:  train Loss: 75.1363   val Loss: 73.0496   time: 0.24s   best: 73.0496
2024-09-01 22:30:41,373:INFO:  Epoch 74/650:  train Loss: 74.3301   val Loss: 73.5042   time: 0.27s   best: 73.0496
2024-09-01 22:30:41,376:INFO:  Epoch 643/650:  train Loss: 49.4614   val Loss: 48.2571   time: 0.29s   best: 45.6492
2024-09-01 22:30:41,615:INFO:  Epoch 75/650:  train Loss: 75.7246   val Loss: 74.0179   time: 0.24s   best: 73.0496
2024-09-01 22:30:41,638:INFO:  Epoch 644/650:  train Loss: 50.0476   val Loss: 49.0248   time: 0.26s   best: 45.6492
2024-09-01 22:30:41,860:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:41,884:INFO:  Epoch 76/650:  train Loss: 74.4294   val Loss: 72.6926   time: 0.24s   best: 72.6926
2024-09-01 22:30:41,910:INFO:  Epoch 645/650:  train Loss: 48.9827   val Loss: 53.0828   time: 0.27s   best: 45.6492
2024-09-01 22:30:42,135:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:42,160:INFO:  Epoch 77/650:  train Loss: 74.0714   val Loss: 72.4993   time: 0.25s   best: 72.4993
2024-09-01 22:30:42,173:INFO:  Epoch 646/650:  train Loss: 51.3267   val Loss: 51.0627   time: 0.26s   best: 45.6492
2024-09-01 22:30:42,392:INFO:  Epoch 78/650:  train Loss: 74.6517   val Loss: 73.5905   time: 0.23s   best: 72.4993
2024-09-01 22:30:42,445:INFO:  Epoch 647/650:  train Loss: 52.1001   val Loss: 49.1245   time: 0.27s   best: 45.6492
2024-09-01 22:30:42,636:INFO:  Epoch 79/650:  train Loss: 74.4299   val Loss: 72.8588   time: 0.24s   best: 72.4993
2024-09-01 22:30:42,750:INFO:  Epoch 648/650:  train Loss: 50.9323   val Loss: 50.9071   time: 0.30s   best: 45.6492
2024-09-01 22:30:42,879:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:42,903:INFO:  Epoch 80/650:  train Loss: 74.2092   val Loss: 72.3639   time: 0.24s   best: 72.3639
2024-09-01 22:30:43,025:INFO:  Epoch 649/650:  train Loss: 50.8346   val Loss: 52.0119   time: 0.27s   best: 45.6492
2024-09-01 22:30:43,149:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:43,173:INFO:  Epoch 81/650:  train Loss: 74.2573   val Loss: 72.2911   time: 0.24s   best: 72.2911
2024-09-01 22:30:43,288:INFO:  Epoch 650/650:  train Loss: 52.9878   val Loss: 47.8031   time: 0.26s   best: 45.6492
2024-09-01 22:30:43,290:INFO:  -----> Training complete in 3m 5s   best validation loss: 45.6492
 
2024-09-01 22:30:43,448:INFO:  Epoch 82/650:  train Loss: 74.5296   val Loss: 73.6053   time: 0.27s   best: 72.2911
2024-09-01 22:30:43,694:INFO:  Epoch 83/650:  train Loss: 75.6678   val Loss: 73.5171   time: 0.24s   best: 72.2911
2024-09-01 22:30:43,924:INFO:  Epoch 84/650:  train Loss: 74.6511   val Loss: 72.9750   time: 0.23s   best: 72.2911
2024-09-01 22:30:44,210:INFO:  Epoch 85/650:  train Loss: 74.3406   val Loss: 72.8358   time: 0.29s   best: 72.2911
2024-09-01 22:30:44,445:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:44,468:INFO:  Epoch 86/650:  train Loss: 73.8862   val Loss: 72.1916   time: 0.23s   best: 72.1916
2024-09-01 22:30:44,708:INFO:  Epoch 87/650:  train Loss: 73.4264   val Loss: 72.2582   time: 0.24s   best: 72.1916
2024-09-01 22:30:44,938:INFO:  Epoch 88/650:  train Loss: 73.9514   val Loss: 73.5965   time: 0.23s   best: 72.1916
2024-09-01 22:30:45,178:INFO:  Epoch 89/650:  train Loss: 74.6654   val Loss: 72.8162   time: 0.24s   best: 72.1916
2024-09-01 22:30:45,449:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:45,477:INFO:  Epoch 90/650:  train Loss: 72.5145   val Loss: 71.2843   time: 0.26s   best: 71.2843
2024-09-01 22:30:45,726:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:45,749:INFO:  Epoch 91/650:  train Loss: 73.3758   val Loss: 70.6810   time: 0.24s   best: 70.6810
2024-09-01 22:30:46,053:INFO:  Epoch 92/650:  train Loss: 72.0142   val Loss: 70.7488   time: 0.30s   best: 70.6810
2024-09-01 22:30:46,299:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:46,322:INFO:  Epoch 93/650:  train Loss: 72.1660   val Loss: 70.2493   time: 0.24s   best: 70.2493
2024-09-01 22:30:46,552:INFO:  Epoch 94/650:  train Loss: 71.9046   val Loss: 70.3218   time: 0.23s   best: 70.2493
2024-09-01 22:30:46,792:INFO:  Epoch 95/650:  train Loss: 72.3237   val Loss: 70.8890   time: 0.24s   best: 70.2493
2024-09-01 22:30:47,021:INFO:  Epoch 96/650:  train Loss: 72.2787   val Loss: 70.8039   time: 0.23s   best: 70.2493
2024-09-01 22:30:47,260:INFO:  Epoch 97/650:  train Loss: 72.4079   val Loss: 70.9381   time: 0.24s   best: 70.2493
2024-09-01 22:30:47,489:INFO:  Epoch 98/650:  train Loss: 72.0819   val Loss: 70.7790   time: 0.23s   best: 70.2493
2024-09-01 22:30:47,740:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:47,764:INFO:  Epoch 99/650:  train Loss: 71.1665   val Loss: 70.1154   time: 0.25s   best: 70.1154
2024-09-01 22:30:48,000:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:48,022:INFO:  Epoch 100/650:  train Loss: 71.7868   val Loss: 69.4776   time: 0.23s   best: 69.4776
2024-09-01 22:30:48,262:INFO:  Epoch 101/650:  train Loss: 71.7081   val Loss: 69.9144   time: 0.24s   best: 69.4776
2024-09-01 22:30:48,492:INFO:  Epoch 102/650:  train Loss: 71.8669   val Loss: 69.5544   time: 0.23s   best: 69.4776
2024-09-01 22:30:48,737:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:48,762:INFO:  Epoch 103/650:  train Loss: 71.0750   val Loss: 69.4562   time: 0.24s   best: 69.4562
2024-09-01 22:30:48,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:49,019:INFO:  Epoch 104/650:  train Loss: 71.0230   val Loss: 69.2760   time: 0.23s   best: 69.2760
2024-09-01 22:30:49,259:INFO:  Epoch 105/650:  train Loss: 70.6520   val Loss: 69.4772   time: 0.24s   best: 69.2760
2024-09-01 22:30:49,489:INFO:  Epoch 106/650:  train Loss: 71.1629   val Loss: 69.4807   time: 0.23s   best: 69.2760
2024-09-01 22:30:49,761:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:49,783:INFO:  Epoch 107/650:  train Loss: 71.0399   val Loss: 69.0041   time: 0.27s   best: 69.0041
2024-09-01 22:30:50,014:INFO:  Epoch 108/650:  train Loss: 70.2120   val Loss: 69.0826   time: 0.23s   best: 69.0041
2024-09-01 22:30:50,254:INFO:  Epoch 109/650:  train Loss: 70.6927   val Loss: 69.1689   time: 0.24s   best: 69.0041
2024-09-01 22:30:50,484:INFO:  Epoch 110/650:  train Loss: 71.0679   val Loss: 69.0479   time: 0.23s   best: 69.0041
2024-09-01 22:30:50,723:INFO:  Epoch 111/650:  train Loss: 71.7716   val Loss: 70.0357   time: 0.24s   best: 69.0041
2024-09-01 22:30:50,952:INFO:  Epoch 112/650:  train Loss: 71.7630   val Loss: 69.3451   time: 0.23s   best: 69.0041
2024-09-01 22:30:51,182:INFO:  Epoch 113/650:  train Loss: 71.3858   val Loss: 69.6631   time: 0.23s   best: 69.0041
2024-09-01 22:30:51,423:INFO:  Epoch 114/650:  train Loss: 71.2718   val Loss: 69.4392   time: 0.24s   best: 69.0041
2024-09-01 22:30:51,652:INFO:  Epoch 115/650:  train Loss: 71.7542   val Loss: 70.3089   time: 0.23s   best: 69.0041
2024-09-01 22:30:51,902:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:51,926:INFO:  Epoch 116/650:  train Loss: 71.0197   val Loss: 68.7456   time: 0.24s   best: 68.7456
2024-09-01 22:30:52,157:INFO:  Epoch 117/650:  train Loss: 71.3879   val Loss: 70.2113   time: 0.23s   best: 68.7456
2024-09-01 22:30:52,404:INFO:  Epoch 118/650:  train Loss: 72.3761   val Loss: 70.2575   time: 0.25s   best: 68.7456
2024-09-01 22:30:52,636:INFO:  Epoch 119/650:  train Loss: 70.9014   val Loss: 69.3944   time: 0.23s   best: 68.7456
2024-09-01 22:30:52,877:INFO:  Epoch 120/650:  train Loss: 69.7171   val Loss: 68.8696   time: 0.24s   best: 68.7456
2024-09-01 22:30:53,143:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:53,166:INFO:  Epoch 121/650:  train Loss: 70.1206   val Loss: 68.6015   time: 0.26s   best: 68.6015
2024-09-01 22:30:53,432:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:53,455:INFO:  Epoch 122/650:  train Loss: 70.1446   val Loss: 67.9549   time: 0.26s   best: 67.9549
2024-09-01 22:30:53,712:INFO:  Epoch 123/650:  train Loss: 69.5932   val Loss: 68.0664   time: 0.26s   best: 67.9549
2024-09-01 22:30:53,996:INFO:  Epoch 124/650:  train Loss: 70.2070   val Loss: 68.2208   time: 0.28s   best: 67.9549
2024-09-01 22:30:54,248:INFO:  Epoch 125/650:  train Loss: 69.9480   val Loss: 68.3831   time: 0.25s   best: 67.9549
2024-09-01 22:30:54,513:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:54,536:INFO:  Epoch 126/650:  train Loss: 70.5039   val Loss: 67.3517   time: 0.26s   best: 67.3517
2024-09-01 22:30:54,798:INFO:  Epoch 127/650:  train Loss: 70.1424   val Loss: 67.6869   time: 0.26s   best: 67.3517
2024-09-01 22:30:55,050:INFO:  Epoch 128/650:  train Loss: 69.7668   val Loss: 68.2158   time: 0.25s   best: 67.3517
2024-09-01 22:30:55,312:INFO:  Epoch 129/650:  train Loss: 70.1727   val Loss: 69.0536   time: 0.26s   best: 67.3517
2024-09-01 22:30:55,563:INFO:  Epoch 130/650:  train Loss: 70.0438   val Loss: 68.0333   time: 0.25s   best: 67.3517
2024-09-01 22:30:55,828:INFO:  Epoch 131/650:  train Loss: 70.1965   val Loss: 67.8696   time: 0.26s   best: 67.3517
2024-09-01 22:30:56,081:INFO:  Epoch 132/650:  train Loss: 69.2024   val Loss: 68.0489   time: 0.25s   best: 67.3517
2024-09-01 22:30:56,342:INFO:  Epoch 133/650:  train Loss: 69.7770   val Loss: 68.3784   time: 0.26s   best: 67.3517
2024-09-01 22:30:56,594:INFO:  Epoch 134/650:  train Loss: 70.3003   val Loss: 67.4327   time: 0.25s   best: 67.3517
2024-09-01 22:30:56,857:INFO:  Epoch 135/650:  train Loss: 70.8859   val Loss: 69.1888   time: 0.26s   best: 67.3517
2024-09-01 22:30:57,109:INFO:  Epoch 136/650:  train Loss: 70.9318   val Loss: 69.2474   time: 0.25s   best: 67.3517
2024-09-01 22:30:57,372:INFO:  Epoch 137/650:  train Loss: 70.2656   val Loss: 69.2975   time: 0.26s   best: 67.3517
2024-09-01 22:30:57,625:INFO:  Epoch 138/650:  train Loss: 70.6051   val Loss: 68.2935   time: 0.25s   best: 67.3517
2024-09-01 22:30:57,890:INFO:  Epoch 139/650:  train Loss: 70.6526   val Loss: 69.1634   time: 0.26s   best: 67.3517
2024-09-01 22:30:58,142:INFO:  Epoch 140/650:  train Loss: 69.7926   val Loss: 67.9579   time: 0.25s   best: 67.3517
2024-09-01 22:30:58,403:INFO:  Epoch 141/650:  train Loss: 70.0850   val Loss: 68.7825   time: 0.26s   best: 67.3517
2024-09-01 22:30:58,655:INFO:  Epoch 142/650:  train Loss: 69.7203   val Loss: 68.2902   time: 0.25s   best: 67.3517
2024-09-01 22:30:58,943:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:30:58,965:INFO:  Epoch 143/650:  train Loss: 68.8885   val Loss: 66.9161   time: 0.28s   best: 66.9161
2024-09-01 22:30:59,218:INFO:  Epoch 144/650:  train Loss: 69.9489   val Loss: 67.9968   time: 0.25s   best: 66.9161
2024-09-01 22:30:59,478:INFO:  Epoch 145/650:  train Loss: 69.9997   val Loss: 69.5185   time: 0.26s   best: 66.9161
2024-09-01 22:30:59,733:INFO:  Epoch 146/650:  train Loss: 70.3961   val Loss: 68.9238   time: 0.25s   best: 66.9161
2024-09-01 22:31:00,008:INFO:  Epoch 147/650:  train Loss: 69.7921   val Loss: 68.6570   time: 0.27s   best: 66.9161
2024-09-01 22:31:00,260:INFO:  Epoch 148/650:  train Loss: 69.3775   val Loss: 68.3777   time: 0.25s   best: 66.9161
2024-09-01 22:31:00,521:INFO:  Epoch 149/650:  train Loss: 69.1381   val Loss: 67.9686   time: 0.26s   best: 66.9161
2024-09-01 22:31:00,777:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:00,800:INFO:  Epoch 150/650:  train Loss: 69.6802   val Loss: 66.6951   time: 0.25s   best: 66.6951
2024-09-01 22:31:01,061:INFO:  Epoch 151/650:  train Loss: 69.3038   val Loss: 66.8153   time: 0.26s   best: 66.6951
2024-09-01 22:31:01,313:INFO:  Epoch 152/650:  train Loss: 69.0735   val Loss: 67.8777   time: 0.25s   best: 66.6951
2024-09-01 22:31:01,574:INFO:  Epoch 153/650:  train Loss: 69.0745   val Loss: 67.2477   time: 0.26s   best: 66.6951
2024-09-01 22:31:01,829:INFO:  Epoch 154/650:  train Loss: 69.2119   val Loss: 67.8948   time: 0.25s   best: 66.6951
2024-09-01 22:31:02,111:INFO:  Epoch 155/650:  train Loss: 70.3741   val Loss: 69.0430   time: 0.28s   best: 66.6951
2024-09-01 22:31:02,363:INFO:  Epoch 156/650:  train Loss: 71.2128   val Loss: 69.2948   time: 0.25s   best: 66.6951
2024-09-01 22:31:02,624:INFO:  Epoch 157/650:  train Loss: 71.2423   val Loss: 68.4926   time: 0.26s   best: 66.6951
2024-09-01 22:31:02,875:INFO:  Epoch 158/650:  train Loss: 69.4849   val Loss: 68.2234   time: 0.25s   best: 66.6951
2024-09-01 22:31:03,135:INFO:  Epoch 159/650:  train Loss: 68.8999   val Loss: 68.0839   time: 0.26s   best: 66.6951
2024-09-01 22:31:03,387:INFO:  Epoch 160/650:  train Loss: 68.6249   val Loss: 66.7808   time: 0.25s   best: 66.6951
2024-09-01 22:31:03,647:INFO:  Epoch 161/650:  train Loss: 69.9288   val Loss: 68.7380   time: 0.26s   best: 66.6951
2024-09-01 22:31:03,902:INFO:  Epoch 162/650:  train Loss: 70.9359   val Loss: 69.7684   time: 0.25s   best: 66.6951
2024-09-01 22:31:04,189:INFO:  Epoch 163/650:  train Loss: 70.0367   val Loss: 67.2407   time: 0.29s   best: 66.6951
2024-09-01 22:31:04,441:INFO:  Epoch 164/650:  train Loss: 69.0967   val Loss: 67.4317   time: 0.25s   best: 66.6951
2024-09-01 22:31:04,705:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:04,728:INFO:  Epoch 165/650:  train Loss: 68.3206   val Loss: 66.6395   time: 0.26s   best: 66.6395
2024-09-01 22:31:04,994:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:05,016:INFO:  Epoch 166/650:  train Loss: 68.2700   val Loss: 66.4022   time: 0.25s   best: 66.4022
2024-09-01 22:31:05,268:INFO:  Epoch 167/650:  train Loss: 68.3048   val Loss: 67.7137   time: 0.25s   best: 66.4022
2024-09-01 22:31:05,532:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:05,555:INFO:  Epoch 168/650:  train Loss: 69.1160   val Loss: 66.3596   time: 0.26s   best: 66.3596
2024-09-01 22:31:05,811:INFO:  Epoch 169/650:  train Loss: 69.3700   val Loss: 67.1490   time: 0.26s   best: 66.3596
2024-09-01 22:31:06,071:INFO:  Epoch 170/650:  train Loss: 68.9291   val Loss: 68.1116   time: 0.26s   best: 66.3596
2024-09-01 22:31:06,324:INFO:  Epoch 171/650:  train Loss: 68.7134   val Loss: 67.3660   time: 0.25s   best: 66.3596
2024-09-01 22:31:06,584:INFO:  Epoch 172/650:  train Loss: 69.4157   val Loss: 67.8205   time: 0.26s   best: 66.3596
2024-09-01 22:31:06,836:INFO:  Epoch 173/650:  train Loss: 68.1742   val Loss: 66.3946   time: 0.25s   best: 66.3596
2024-09-01 22:31:07,096:INFO:  Epoch 174/650:  train Loss: 68.3062   val Loss: 66.6912   time: 0.26s   best: 66.3596
2024-09-01 22:31:07,353:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:07,376:INFO:  Epoch 175/650:  train Loss: 67.9869   val Loss: 65.1697   time: 0.25s   best: 65.1697
2024-09-01 22:31:07,639:INFO:  Epoch 176/650:  train Loss: 68.0455   val Loss: 66.3837   time: 0.26s   best: 65.1697
2024-09-01 22:31:07,894:INFO:  Epoch 177/650:  train Loss: 68.3137   val Loss: 66.2412   time: 0.25s   best: 65.1697
2024-09-01 22:31:08,154:INFO:  Epoch 178/650:  train Loss: 68.4180   val Loss: 66.0928   time: 0.26s   best: 65.1697
2024-09-01 22:31:08,407:INFO:  Epoch 179/650:  train Loss: 67.9423   val Loss: 66.1080   time: 0.25s   best: 65.1697
2024-09-01 22:31:08,740:INFO:  Epoch 180/650:  train Loss: 67.1538   val Loss: 65.5693   time: 0.33s   best: 65.1697
2024-09-01 22:31:08,997:INFO:  Epoch 181/650:  train Loss: 68.0547   val Loss: 65.6056   time: 0.25s   best: 65.1697
2024-09-01 22:31:09,257:INFO:  Epoch 182/650:  train Loss: 68.0155   val Loss: 65.4461   time: 0.26s   best: 65.1697
2024-09-01 22:31:09,508:INFO:  Epoch 183/650:  train Loss: 67.8720   val Loss: 66.1736   time: 0.25s   best: 65.1697
2024-09-01 22:31:09,771:INFO:  Epoch 184/650:  train Loss: 68.0410   val Loss: 65.5050   time: 0.26s   best: 65.1697
2024-09-01 22:31:10,023:INFO:  Epoch 185/650:  train Loss: 68.5749   val Loss: 66.3564   time: 0.25s   best: 65.1697
2024-09-01 22:31:10,284:INFO:  Epoch 186/650:  train Loss: 69.2139   val Loss: 67.1099   time: 0.26s   best: 65.1697
2024-09-01 22:31:10,536:INFO:  Epoch 187/650:  train Loss: 70.1959   val Loss: 69.6012   time: 0.25s   best: 65.1697
2024-09-01 22:31:10,797:INFO:  Epoch 188/650:  train Loss: 71.8703   val Loss: 69.1739   time: 0.26s   best: 65.1697
2024-09-01 22:31:11,048:INFO:  Epoch 189/650:  train Loss: 70.0574   val Loss: 68.7297   time: 0.25s   best: 65.1697
2024-09-01 22:31:11,310:INFO:  Epoch 190/650:  train Loss: 68.9681   val Loss: 67.3309   time: 0.26s   best: 65.1697
2024-09-01 22:31:11,561:INFO:  Epoch 191/650:  train Loss: 68.0536   val Loss: 67.0747   time: 0.25s   best: 65.1697
2024-09-01 22:31:11,837:INFO:  Epoch 192/650:  train Loss: 67.5172   val Loss: 65.2561   time: 0.27s   best: 65.1697
2024-09-01 22:31:12,088:INFO:  Epoch 193/650:  train Loss: 68.1237   val Loss: 67.4711   time: 0.25s   best: 65.1697
2024-09-01 22:31:12,349:INFO:  Epoch 194/650:  train Loss: 68.0313   val Loss: 66.0706   time: 0.26s   best: 65.1697
2024-09-01 22:31:12,606:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:12,629:INFO:  Epoch 195/650:  train Loss: 67.3785   val Loss: 64.9973   time: 0.25s   best: 64.9973
2024-09-01 22:31:12,890:INFO:  Epoch 196/650:  train Loss: 67.7439   val Loss: 65.1983   time: 0.26s   best: 64.9973
2024-09-01 22:31:13,142:INFO:  Epoch 197/650:  train Loss: 68.1270   val Loss: 66.8928   time: 0.25s   best: 64.9973
2024-09-01 22:31:13,408:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:13,430:INFO:  Epoch 198/650:  train Loss: 67.9525   val Loss: 64.9010   time: 0.26s   best: 64.9010
2024-09-01 22:31:13,691:INFO:  Epoch 199/650:  train Loss: 68.3041   val Loss: 66.8913   time: 0.26s   best: 64.9010
2024-09-01 22:31:13,945:INFO:  Epoch 200/650:  train Loss: 68.7951   val Loss: 65.7999   time: 0.25s   best: 64.9010
2024-09-01 22:31:14,206:INFO:  Epoch 201/650:  train Loss: 69.2340   val Loss: 68.1928   time: 0.26s   best: 64.9010
2024-09-01 22:31:14,479:INFO:  Epoch 202/650:  train Loss: 69.5651   val Loss: 67.5597   time: 0.27s   best: 64.9010
2024-09-01 22:31:14,740:INFO:  Epoch 203/650:  train Loss: 68.6320   val Loss: 68.1869   time: 0.26s   best: 64.9010
2024-09-01 22:31:14,992:INFO:  Epoch 204/650:  train Loss: 68.3106   val Loss: 66.4114   time: 0.25s   best: 64.9010
2024-09-01 22:31:15,253:INFO:  Epoch 205/650:  train Loss: 67.2697   val Loss: 66.0159   time: 0.26s   best: 64.9010
2024-09-01 22:31:15,504:INFO:  Epoch 206/650:  train Loss: 67.3308   val Loss: 65.6558   time: 0.25s   best: 64.9010
2024-09-01 22:31:15,767:INFO:  Epoch 207/650:  train Loss: 66.6784   val Loss: 65.7178   time: 0.26s   best: 64.9010
2024-09-01 22:31:16,023:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:16,046:INFO:  Epoch 208/650:  train Loss: 66.6800   val Loss: 64.1031   time: 0.25s   best: 64.1031
2024-09-01 22:31:16,379:INFO:  Epoch 209/650:  train Loss: 66.4290   val Loss: 65.0301   time: 0.33s   best: 64.1031
2024-09-01 22:31:16,633:INFO:  Epoch 210/650:  train Loss: 67.8418   val Loss: 64.4134   time: 0.25s   best: 64.1031
2024-09-01 22:31:16,898:INFO:  Epoch 211/650:  train Loss: 67.6623   val Loss: 67.2468   time: 0.26s   best: 64.1031
2024-09-01 22:31:17,148:INFO:  Epoch 212/650:  train Loss: 68.1568   val Loss: 64.9503   time: 0.25s   best: 64.1031
2024-09-01 22:31:17,410:INFO:  Epoch 213/650:  train Loss: 67.1896   val Loss: 65.6660   time: 0.26s   best: 64.1031
2024-09-01 22:31:17,660:INFO:  Epoch 214/650:  train Loss: 68.3726   val Loss: 66.1183   time: 0.25s   best: 64.1031
2024-09-01 22:31:17,947:INFO:  Epoch 215/650:  train Loss: 67.9556   val Loss: 66.3126   time: 0.29s   best: 64.1031
2024-09-01 22:31:18,197:INFO:  Epoch 216/650:  train Loss: 67.7524   val Loss: 66.2332   time: 0.25s   best: 64.1031
2024-09-01 22:31:18,459:INFO:  Epoch 217/650:  train Loss: 68.5012   val Loss: 66.3652   time: 0.26s   best: 64.1031
2024-09-01 22:31:18,736:INFO:  Epoch 218/650:  train Loss: 68.1941   val Loss: 66.1529   time: 0.28s   best: 64.1031
2024-09-01 22:31:18,997:INFO:  Epoch 219/650:  train Loss: 66.6242   val Loss: 64.6749   time: 0.26s   best: 64.1031
2024-09-01 22:31:19,247:INFO:  Epoch 220/650:  train Loss: 66.9382   val Loss: 64.6460   time: 0.25s   best: 64.1031
2024-09-01 22:31:19,507:INFO:  Epoch 221/650:  train Loss: 66.4205   val Loss: 66.1575   time: 0.26s   best: 64.1031
2024-09-01 22:31:19,765:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:19,788:INFO:  Epoch 222/650:  train Loss: 66.5374   val Loss: 63.6213   time: 0.25s   best: 63.6213
2024-09-01 22:31:20,049:INFO:  Epoch 223/650:  train Loss: 65.8733   val Loss: 64.5512   time: 0.26s   best: 63.6213
2024-09-01 22:31:20,310:INFO:  Epoch 224/650:  train Loss: 66.2472   val Loss: 63.8205   time: 0.26s   best: 63.6213
2024-09-01 22:31:20,561:INFO:  Epoch 225/650:  train Loss: 65.4647   val Loss: 63.7136   time: 0.25s   best: 63.6213
2024-09-01 22:31:20,847:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:20,870:INFO:  Epoch 226/650:  train Loss: 65.9859   val Loss: 63.3734   time: 0.28s   best: 63.3734
2024-09-01 22:31:21,122:INFO:  Epoch 227/650:  train Loss: 66.1830   val Loss: 63.7185   time: 0.25s   best: 63.3734
2024-09-01 22:31:21,383:INFO:  Epoch 228/650:  train Loss: 67.0358   val Loss: 66.1796   time: 0.26s   best: 63.3734
2024-09-01 22:31:21,634:INFO:  Epoch 229/650:  train Loss: 67.6234   val Loss: 64.2986   time: 0.25s   best: 63.3734
2024-09-01 22:31:21,898:INFO:  Epoch 230/650:  train Loss: 66.8705   val Loss: 63.8565   time: 0.26s   best: 63.3734
2024-09-01 22:31:22,150:INFO:  Epoch 231/650:  train Loss: 66.4449   val Loss: 64.0414   time: 0.25s   best: 63.3734
2024-09-01 22:31:22,411:INFO:  Epoch 232/650:  train Loss: 66.5393   val Loss: 63.6739   time: 0.26s   best: 63.3734
2024-09-01 22:31:22,662:INFO:  Epoch 233/650:  train Loss: 66.7415   val Loss: 64.7880   time: 0.25s   best: 63.3734
2024-09-01 22:31:22,928:INFO:  Epoch 234/650:  train Loss: 67.4697   val Loss: 63.6897   time: 0.27s   best: 63.3734
2024-09-01 22:31:23,180:INFO:  Epoch 235/650:  train Loss: 66.9933   val Loss: 66.6754   time: 0.25s   best: 63.3734
2024-09-01 22:31:23,441:INFO:  Epoch 236/650:  train Loss: 68.7679   val Loss: 64.8894   time: 0.26s   best: 63.3734
2024-09-01 22:31:23,692:INFO:  Epoch 237/650:  train Loss: 68.0151   val Loss: 65.1831   time: 0.25s   best: 63.3734
2024-09-01 22:31:23,955:INFO:  Epoch 238/650:  train Loss: 67.5750   val Loss: 66.1672   time: 0.26s   best: 63.3734
2024-09-01 22:31:24,206:INFO:  Epoch 239/650:  train Loss: 67.6389   val Loss: 66.0932   time: 0.25s   best: 63.3734
2024-09-01 22:31:24,467:INFO:  Epoch 240/650:  train Loss: 67.8201   val Loss: 65.4212   time: 0.26s   best: 63.3734
2024-09-01 22:31:24,718:INFO:  Epoch 241/650:  train Loss: 67.6906   val Loss: 67.4173   time: 0.25s   best: 63.3734
2024-09-01 22:31:24,983:INFO:  Epoch 242/650:  train Loss: 67.5372   val Loss: 64.5177   time: 0.26s   best: 63.3734
2024-09-01 22:31:25,234:INFO:  Epoch 243/650:  train Loss: 65.8361   val Loss: 63.7936   time: 0.25s   best: 63.3734
2024-09-01 22:31:25,495:INFO:  Epoch 244/650:  train Loss: 65.8501   val Loss: 63.6173   time: 0.26s   best: 63.3734
2024-09-01 22:31:25,748:INFO:  Epoch 245/650:  train Loss: 65.5787   val Loss: 64.6435   time: 0.25s   best: 63.3734
2024-09-01 22:31:26,010:INFO:  Epoch 246/650:  train Loss: 66.7255   val Loss: 63.6647   time: 0.26s   best: 63.3734
2024-09-01 22:31:26,261:INFO:  Epoch 247/650:  train Loss: 65.7742   val Loss: 64.4650   time: 0.25s   best: 63.3734
2024-09-01 22:31:26,522:INFO:  Epoch 248/650:  train Loss: 66.4767   val Loss: 63.8387   time: 0.26s   best: 63.3734
2024-09-01 22:31:26,774:INFO:  Epoch 249/650:  train Loss: 67.6607   val Loss: 65.1826   time: 0.25s   best: 63.3734
2024-09-01 22:31:27,042:INFO:  Epoch 250/650:  train Loss: 68.9345   val Loss: 67.0893   time: 0.27s   best: 63.3734
2024-09-01 22:31:27,294:INFO:  Epoch 251/650:  train Loss: 67.2345   val Loss: 65.6159   time: 0.25s   best: 63.3734
2024-09-01 22:31:27,555:INFO:  Epoch 252/650:  train Loss: 65.9386   val Loss: 64.2972   time: 0.26s   best: 63.3734
2024-09-01 22:31:27,810:INFO:  Epoch 253/650:  train Loss: 65.9622   val Loss: 64.3931   time: 0.25s   best: 63.3734
2024-09-01 22:31:28,075:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:28,097:INFO:  Epoch 254/650:  train Loss: 65.8093   val Loss: 63.1039   time: 0.26s   best: 63.1039
2024-09-01 22:31:28,351:INFO:  Epoch 255/650:  train Loss: 64.6357   val Loss: 63.6948   time: 0.25s   best: 63.1039
2024-09-01 22:31:28,617:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:28,641:INFO:  Epoch 256/650:  train Loss: 65.2166   val Loss: 62.6581   time: 0.26s   best: 62.6581
2024-09-01 22:31:28,900:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:28,924:INFO:  Epoch 257/650:  train Loss: 64.5929   val Loss: 62.3310   time: 0.25s   best: 62.3310
2024-09-01 22:31:29,186:INFO:  Epoch 258/650:  train Loss: 65.5519   val Loss: 62.5168   time: 0.26s   best: 62.3310
2024-09-01 22:31:29,439:INFO:  Epoch 259/650:  train Loss: 65.3280   val Loss: 63.3350   time: 0.25s   best: 62.3310
2024-09-01 22:31:29,700:INFO:  Epoch 260/650:  train Loss: 66.1425   val Loss: 63.6456   time: 0.26s   best: 62.3310
2024-09-01 22:31:29,955:INFO:  Epoch 261/650:  train Loss: 66.6323   val Loss: 63.2081   time: 0.25s   best: 62.3310
2024-09-01 22:31:30,216:INFO:  Epoch 262/650:  train Loss: 67.1286   val Loss: 63.2767   time: 0.26s   best: 62.3310
2024-09-01 22:31:30,546:INFO:  Epoch 263/650:  train Loss: 67.6625   val Loss: 65.7589   time: 0.33s   best: 62.3310
2024-09-01 22:31:30,850:INFO:  Epoch 264/650:  train Loss: 67.8021   val Loss: 65.3739   time: 0.30s   best: 62.3310
2024-09-01 22:31:31,141:INFO:  Epoch 265/650:  train Loss: 67.0396   val Loss: 66.3568   time: 0.29s   best: 62.3310
2024-09-01 22:31:31,395:INFO:  Epoch 266/650:  train Loss: 66.3109   val Loss: 64.5431   time: 0.25s   best: 62.3310
2024-09-01 22:31:31,658:INFO:  Epoch 267/650:  train Loss: 64.7240   val Loss: 62.3859   time: 0.26s   best: 62.3310
2024-09-01 22:31:31,914:INFO:  Epoch 268/650:  train Loss: 64.2982   val Loss: 64.1725   time: 0.26s   best: 62.3310
2024-09-01 22:31:32,181:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:32,203:INFO:  Epoch 269/650:  train Loss: 64.7329   val Loss: 61.9711   time: 0.26s   best: 61.9711
2024-09-01 22:31:32,460:INFO:  Epoch 270/650:  train Loss: 63.6383   val Loss: 62.6775   time: 0.26s   best: 61.9711
2024-09-01 22:31:32,726:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:32,749:INFO:  Epoch 271/650:  train Loss: 63.7131   val Loss: 61.3509   time: 0.26s   best: 61.3509
2024-09-01 22:31:33,004:INFO:  Epoch 272/650:  train Loss: 63.8217   val Loss: 63.4119   time: 0.25s   best: 61.3509
2024-09-01 22:31:33,267:INFO:  Epoch 273/650:  train Loss: 63.9233   val Loss: 61.4023   time: 0.26s   best: 61.3509
2024-09-01 22:31:33,521:INFO:  Epoch 274/650:  train Loss: 63.6862   val Loss: 62.2096   time: 0.25s   best: 61.3509
2024-09-01 22:31:33,791:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:33,813:INFO:  Epoch 275/650:  train Loss: 63.9264   val Loss: 61.1105   time: 0.26s   best: 61.1105
2024-09-01 22:31:34,102:INFO:  Epoch 276/650:  train Loss: 64.2718   val Loss: 62.8884   time: 0.29s   best: 61.1105
2024-09-01 22:31:34,356:INFO:  Epoch 277/650:  train Loss: 64.8916   val Loss: 62.2765   time: 0.25s   best: 61.1105
2024-09-01 22:31:34,618:INFO:  Epoch 278/650:  train Loss: 64.1450   val Loss: 64.1346   time: 0.26s   best: 61.1105
2024-09-01 22:31:34,872:INFO:  Epoch 279/650:  train Loss: 64.9516   val Loss: 62.7734   time: 0.25s   best: 61.1105
2024-09-01 22:31:35,134:INFO:  Epoch 280/650:  train Loss: 63.6485   val Loss: 61.5177   time: 0.26s   best: 61.1105
2024-09-01 22:31:35,394:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:35,416:INFO:  Epoch 281/650:  train Loss: 63.1238   val Loss: 60.5032   time: 0.25s   best: 60.5032
2024-09-01 22:31:35,757:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:35,781:INFO:  Epoch 282/650:  train Loss: 63.2034   val Loss: 60.1816   time: 0.33s   best: 60.1816
2024-09-01 22:31:36,041:INFO:  Epoch 283/650:  train Loss: 63.3588   val Loss: 60.6758   time: 0.26s   best: 60.1816
2024-09-01 22:31:36,305:INFO:  Epoch 284/650:  train Loss: 62.6422   val Loss: 61.2311   time: 0.26s   best: 60.1816
2024-09-01 22:31:36,565:INFO:  Epoch 285/650:  train Loss: 62.7532   val Loss: 61.1291   time: 0.26s   best: 60.1816
2024-09-01 22:31:36,857:INFO:  Epoch 286/650:  train Loss: 62.6563   val Loss: 60.5187   time: 0.29s   best: 60.1816
2024-09-01 22:31:37,178:INFO:  Epoch 287/650:  train Loss: 62.2151   val Loss: 60.5713   time: 0.32s   best: 60.1816
2024-09-01 22:31:37,515:INFO:  Epoch 288/650:  train Loss: 62.3523   val Loss: 60.3427   time: 0.33s   best: 60.1816
2024-09-01 22:31:37,791:INFO:  Epoch 289/650:  train Loss: 62.3022   val Loss: 60.8160   time: 0.28s   best: 60.1816
2024-09-01 22:31:38,048:INFO:  Epoch 290/650:  train Loss: 62.5984   val Loss: 60.5243   time: 0.26s   best: 60.1816
2024-09-01 22:31:38,326:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:38,348:INFO:  Epoch 291/650:  train Loss: 62.7647   val Loss: 58.9215   time: 0.27s   best: 58.9215
2024-09-01 22:31:38,600:INFO:  Epoch 292/650:  train Loss: 62.2224   val Loss: 59.2839   time: 0.25s   best: 58.9215
2024-09-01 22:31:38,863:INFO:  Epoch 293/650:  train Loss: 63.1898   val Loss: 60.2160   time: 0.26s   best: 58.9215
2024-09-01 22:31:39,114:INFO:  Epoch 294/650:  train Loss: 63.4471   val Loss: 59.8874   time: 0.25s   best: 58.9215
2024-09-01 22:31:39,386:INFO:  Epoch 295/650:  train Loss: 62.6057   val Loss: 60.1985   time: 0.27s   best: 58.9215
2024-09-01 22:31:39,637:INFO:  Epoch 296/650:  train Loss: 62.6062   val Loss: 60.1146   time: 0.25s   best: 58.9215
2024-09-01 22:31:39,900:INFO:  Epoch 297/650:  train Loss: 63.1096   val Loss: 59.5110   time: 0.26s   best: 58.9215
2024-09-01 22:31:40,152:INFO:  Epoch 298/650:  train Loss: 62.9960   val Loss: 60.9285   time: 0.25s   best: 58.9215
2024-09-01 22:31:40,413:INFO:  Epoch 299/650:  train Loss: 62.7422   val Loss: 59.5211   time: 0.26s   best: 58.9215
2024-09-01 22:31:40,665:INFO:  Epoch 300/650:  train Loss: 62.9969   val Loss: 61.9221   time: 0.25s   best: 58.9215
2024-09-01 22:31:40,927:INFO:  Epoch 301/650:  train Loss: 63.6384   val Loss: 59.0808   time: 0.26s   best: 58.9215
2024-09-01 22:31:41,179:INFO:  Epoch 302/650:  train Loss: 63.0061   val Loss: 61.8605   time: 0.25s   best: 58.9215
2024-09-01 22:31:41,452:INFO:  Epoch 303/650:  train Loss: 63.3373   val Loss: 62.4198   time: 0.27s   best: 58.9215
2024-09-01 22:31:41,704:INFO:  Epoch 304/650:  train Loss: 63.4020   val Loss: 62.2671   time: 0.25s   best: 58.9215
2024-09-01 22:31:41,967:INFO:  Epoch 305/650:  train Loss: 64.2067   val Loss: 60.5569   time: 0.26s   best: 58.9215
2024-09-01 22:31:42,218:INFO:  Epoch 306/650:  train Loss: 62.7878   val Loss: 60.8223   time: 0.25s   best: 58.9215
2024-09-01 22:31:42,542:INFO:  Epoch 307/650:  train Loss: 64.5197   val Loss: 59.1312   time: 0.32s   best: 58.9215
2024-09-01 22:31:42,806:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:42,829:INFO:  Epoch 308/650:  train Loss: 61.5215   val Loss: 58.6571   time: 0.26s   best: 58.6571
2024-09-01 22:31:43,081:INFO:  Epoch 309/650:  train Loss: 62.5525   val Loss: 59.6789   time: 0.25s   best: 58.6571
2024-09-01 22:31:43,344:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:43,367:INFO:  Epoch 310/650:  train Loss: 61.6561   val Loss: 58.1070   time: 0.26s   best: 58.1070
2024-09-01 22:31:43,620:INFO:  Epoch 311/650:  train Loss: 61.2508   val Loss: 58.8124   time: 0.25s   best: 58.1070
2024-09-01 22:31:43,891:INFO:  Epoch 312/650:  train Loss: 61.8964   val Loss: 60.0513   time: 0.27s   best: 58.1070
2024-09-01 22:31:44,150:INFO:  Epoch 313/650:  train Loss: 61.6300   val Loss: 58.5554   time: 0.26s   best: 58.1070
2024-09-01 22:31:44,423:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:44,446:INFO:  Epoch 314/650:  train Loss: 61.1241   val Loss: 57.9733   time: 0.27s   best: 57.9733
2024-09-01 22:31:44,698:INFO:  Epoch 315/650:  train Loss: 60.8469   val Loss: 58.4938   time: 0.25s   best: 57.9733
2024-09-01 22:31:45,039:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:45,069:INFO:  Epoch 316/650:  train Loss: 61.0666   val Loss: 57.5267   time: 0.34s   best: 57.5267
2024-09-01 22:31:45,329:INFO:  Epoch 317/650:  train Loss: 60.4161   val Loss: 58.1714   time: 0.25s   best: 57.5267
2024-09-01 22:31:45,593:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:45,616:INFO:  Epoch 318/650:  train Loss: 61.1747   val Loss: 57.1782   time: 0.26s   best: 57.1782
2024-09-01 22:31:45,879:INFO:  Epoch 319/650:  train Loss: 60.7071   val Loss: 57.7049   time: 0.26s   best: 57.1782
2024-09-01 22:31:46,135:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:46,157:INFO:  Epoch 320/650:  train Loss: 60.1351   val Loss: 57.0107   time: 0.25s   best: 57.0107
2024-09-01 22:31:46,418:INFO:  Epoch 321/650:  train Loss: 60.8339   val Loss: 57.7769   time: 0.26s   best: 57.0107
2024-09-01 22:31:46,669:INFO:  Epoch 322/650:  train Loss: 60.0194   val Loss: 57.6714   time: 0.25s   best: 57.0107
2024-09-01 22:31:46,929:INFO:  Epoch 323/650:  train Loss: 61.1161   val Loss: 57.4766   time: 0.26s   best: 57.0107
2024-09-01 22:31:47,180:INFO:  Epoch 324/650:  train Loss: 61.1890   val Loss: 58.9978   time: 0.25s   best: 57.0107
2024-09-01 22:31:47,440:INFO:  Epoch 325/650:  train Loss: 60.8662   val Loss: 57.9930   time: 0.26s   best: 57.0107
2024-09-01 22:31:47,692:INFO:  Epoch 326/650:  train Loss: 61.7429   val Loss: 58.3421   time: 0.25s   best: 57.0107
2024-09-01 22:31:47,955:INFO:  Epoch 327/650:  train Loss: 60.8647   val Loss: 58.5971   time: 0.26s   best: 57.0107
2024-09-01 22:31:48,206:INFO:  Epoch 328/650:  train Loss: 61.6874   val Loss: 58.2451   time: 0.25s   best: 57.0107
2024-09-01 22:31:48,468:INFO:  Epoch 329/650:  train Loss: 61.6517   val Loss: 60.1298   time: 0.26s   best: 57.0107
2024-09-01 22:31:48,719:INFO:  Epoch 330/650:  train Loss: 62.3645   val Loss: 57.5441   time: 0.25s   best: 57.0107
2024-09-01 22:31:48,979:INFO:  Epoch 331/650:  train Loss: 60.4423   val Loss: 58.2544   time: 0.26s   best: 57.0107
2024-09-01 22:31:49,230:INFO:  Epoch 332/650:  train Loss: 60.6560   val Loss: 58.4695   time: 0.25s   best: 57.0107
2024-09-01 22:31:49,490:INFO:  Epoch 333/650:  train Loss: 61.7865   val Loss: 59.2040   time: 0.26s   best: 57.0107
2024-09-01 22:31:49,748:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:49,771:INFO:  Epoch 334/650:  train Loss: 60.5554   val Loss: 56.8068   time: 0.25s   best: 56.8068
2024-09-01 22:31:50,045:INFO:  Epoch 335/650:  train Loss: 60.1705   val Loss: 57.2000   time: 0.27s   best: 56.8068
2024-09-01 22:31:50,300:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:50,323:INFO:  Epoch 336/650:  train Loss: 60.5139   val Loss: 56.8048   time: 0.25s   best: 56.8048
2024-09-01 22:31:50,589:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:50,611:INFO:  Epoch 337/650:  train Loss: 60.3536   val Loss: 56.6779   time: 0.26s   best: 56.6779
2024-09-01 22:31:50,863:INFO:  Epoch 338/650:  train Loss: 59.6961   val Loss: 56.9659   time: 0.25s   best: 56.6779
2024-09-01 22:31:51,124:INFO:  Epoch 339/650:  train Loss: 61.0530   val Loss: 58.4162   time: 0.26s   best: 56.6779
2024-09-01 22:31:51,375:INFO:  Epoch 340/650:  train Loss: 60.6446   val Loss: 58.1172   time: 0.25s   best: 56.6779
2024-09-01 22:31:51,635:INFO:  Epoch 341/650:  train Loss: 62.1884   val Loss: 57.3912   time: 0.26s   best: 56.6779
2024-09-01 22:31:51,894:INFO:  Epoch 342/650:  train Loss: 61.2447   val Loss: 59.0950   time: 0.26s   best: 56.6779
2024-09-01 22:31:52,154:INFO:  Epoch 343/650:  train Loss: 62.3338   val Loss: 59.1529   time: 0.26s   best: 56.6779
2024-09-01 22:31:52,405:INFO:  Epoch 344/650:  train Loss: 61.6497   val Loss: 57.9219   time: 0.25s   best: 56.6779
2024-09-01 22:31:52,666:INFO:  Epoch 345/650:  train Loss: 62.5084   val Loss: 61.5435   time: 0.26s   best: 56.6779
2024-09-01 22:31:52,917:INFO:  Epoch 346/650:  train Loss: 63.1997   val Loss: 60.3331   time: 0.25s   best: 56.6779
2024-09-01 22:31:53,177:INFO:  Epoch 347/650:  train Loss: 63.0789   val Loss: 59.6578   time: 0.26s   best: 56.6779
2024-09-01 22:31:53,428:INFO:  Epoch 348/650:  train Loss: 63.7454   val Loss: 60.5927   time: 0.25s   best: 56.6779
2024-09-01 22:31:53,692:INFO:  Epoch 349/650:  train Loss: 62.6649   val Loss: 58.1270   time: 0.26s   best: 56.6779
2024-09-01 22:31:53,949:INFO:  Epoch 350/650:  train Loss: 61.9646   val Loss: 60.4908   time: 0.26s   best: 56.6779
2024-09-01 22:31:54,209:INFO:  Epoch 351/650:  train Loss: 61.9164   val Loss: 59.5117   time: 0.26s   best: 56.6779
2024-09-01 22:31:54,461:INFO:  Epoch 352/650:  train Loss: 61.5629   val Loss: 58.6483   time: 0.25s   best: 56.6779
2024-09-01 22:31:54,721:INFO:  Epoch 353/650:  train Loss: 59.8453   val Loss: 56.7328   time: 0.26s   best: 56.6779
2024-09-01 22:31:54,976:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:54,999:INFO:  Epoch 354/650:  train Loss: 60.3208   val Loss: 56.5166   time: 0.25s   best: 56.5166
2024-09-01 22:31:55,260:INFO:  Epoch 355/650:  train Loss: 60.2845   val Loss: 56.7821   time: 0.26s   best: 56.5166
2024-09-01 22:31:55,526:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:55,549:INFO:  Epoch 356/650:  train Loss: 59.7556   val Loss: 56.4206   time: 0.25s   best: 56.4206
2024-09-01 22:31:55,803:INFO:  Epoch 357/650:  train Loss: 59.7364   val Loss: 59.3687   time: 0.25s   best: 56.4206
2024-09-01 22:31:56,087:INFO:  Epoch 358/650:  train Loss: 61.6893   val Loss: 57.1652   time: 0.28s   best: 56.4206
2024-09-01 22:31:56,339:INFO:  Epoch 359/650:  train Loss: 61.2828   val Loss: 57.7879   time: 0.25s   best: 56.4206
2024-09-01 22:31:56,601:INFO:  Epoch 360/650:  train Loss: 62.8782   val Loss: 58.6708   time: 0.26s   best: 56.4206
2024-09-01 22:31:56,853:INFO:  Epoch 361/650:  train Loss: 61.8427   val Loss: 58.1167   time: 0.25s   best: 56.4206
2024-09-01 22:31:57,120:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:57,142:INFO:  Epoch 362/650:  train Loss: 60.7646   val Loss: 55.8209   time: 0.26s   best: 55.8209
2024-09-01 22:31:57,476:INFO:  Epoch 363/650:  train Loss: 60.6842   val Loss: 58.9937   time: 0.33s   best: 55.8209
2024-09-01 22:31:57,743:INFO:  Epoch 364/650:  train Loss: 59.0778   val Loss: 55.9139   time: 0.27s   best: 55.8209
2024-09-01 22:31:58,027:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:58,048:INFO:  Epoch 365/650:  train Loss: 58.4779   val Loss: 55.4445   time: 0.27s   best: 55.4445
2024-09-01 22:31:58,313:INFO:  Epoch 366/650:  train Loss: 59.6292   val Loss: 57.7863   time: 0.26s   best: 55.4445
2024-09-01 22:31:58,578:INFO:  Epoch 367/650:  train Loss: 59.5146   val Loss: 56.6272   time: 0.26s   best: 55.4445
2024-09-01 22:31:58,833:INFO:  Epoch 368/650:  train Loss: 58.7380   val Loss: 56.3376   time: 0.25s   best: 55.4445
2024-09-01 22:31:59,099:INFO:  Epoch 369/650:  train Loss: 58.8665   val Loss: 55.8073   time: 0.26s   best: 55.4445
2024-09-01 22:31:59,355:INFO:  Epoch 370/650:  train Loss: 58.6035   val Loss: 56.1807   time: 0.25s   best: 55.4445
2024-09-01 22:31:59,620:INFO:  Epoch 371/650:  train Loss: 57.7549   val Loss: 56.5893   time: 0.26s   best: 55.4445
2024-09-01 22:31:59,883:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:31:59,906:INFO:  Epoch 372/650:  train Loss: 59.1886   val Loss: 54.9377   time: 0.25s   best: 54.9377
2024-09-01 22:32:00,171:INFO:  Epoch 373/650:  train Loss: 58.8434   val Loss: 56.7592   time: 0.26s   best: 54.9377
2024-09-01 22:32:00,427:INFO:  Epoch 374/650:  train Loss: 58.6251   val Loss: 55.5190   time: 0.25s   best: 54.9377
2024-09-01 22:32:00,692:INFO:  Epoch 375/650:  train Loss: 58.9304   val Loss: 55.8345   time: 0.26s   best: 54.9377
2024-09-01 22:32:00,948:INFO:  Epoch 376/650:  train Loss: 59.1404   val Loss: 55.7267   time: 0.25s   best: 54.9377
2024-09-01 22:32:01,212:INFO:  Epoch 377/650:  train Loss: 58.2772   val Loss: 56.1108   time: 0.26s   best: 54.9377
2024-09-01 22:32:01,468:INFO:  Epoch 378/650:  train Loss: 58.8529   val Loss: 55.5275   time: 0.25s   best: 54.9377
2024-09-01 22:32:01,732:INFO:  Epoch 379/650:  train Loss: 59.4060   val Loss: 58.1498   time: 0.26s   best: 54.9377
2024-09-01 22:32:01,996:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:02,019:INFO:  Epoch 380/650:  train Loss: 60.2555   val Loss: 54.8756   time: 0.25s   best: 54.8756
2024-09-01 22:32:02,290:INFO:  Epoch 381/650:  train Loss: 59.1295   val Loss: 56.9987   time: 0.27s   best: 54.8756
2024-09-01 22:32:02,547:INFO:  Epoch 382/650:  train Loss: 59.8500   val Loss: 55.2289   time: 0.25s   best: 54.8756
2024-09-01 22:32:02,817:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:02,840:INFO:  Epoch 383/650:  train Loss: 57.9099   val Loss: 54.5276   time: 0.26s   best: 54.5276
2024-09-01 22:32:03,104:INFO:  Epoch 384/650:  train Loss: 57.0360   val Loss: 55.5358   time: 0.26s   best: 54.5276
2024-09-01 22:32:03,368:INFO:  Epoch 385/650:  train Loss: 57.6266   val Loss: 55.1474   time: 0.26s   best: 54.5276
2024-09-01 22:32:03,692:INFO:  Epoch 386/650:  train Loss: 59.2585   val Loss: 57.6158   time: 0.32s   best: 54.5276
2024-09-01 22:32:03,970:INFO:  Epoch 387/650:  train Loss: 60.5383   val Loss: 58.1373   time: 0.28s   best: 54.5276
2024-09-01 22:32:04,243:INFO:  Epoch 388/650:  train Loss: 60.2306   val Loss: 56.9594   time: 0.26s   best: 54.5276
2024-09-01 22:32:04,500:INFO:  Epoch 389/650:  train Loss: 61.0368   val Loss: 55.4488   time: 0.25s   best: 54.5276
2024-09-01 22:32:04,764:INFO:  Epoch 390/650:  train Loss: 60.3212   val Loss: 60.6357   time: 0.26s   best: 54.5276
2024-09-01 22:32:05,022:INFO:  Epoch 391/650:  train Loss: 63.9208   val Loss: 60.0017   time: 0.25s   best: 54.5276
2024-09-01 22:32:05,363:INFO:  Epoch 392/650:  train Loss: 60.8128   val Loss: 57.7232   time: 0.34s   best: 54.5276
2024-09-01 22:32:05,626:INFO:  Epoch 393/650:  train Loss: 59.3678   val Loss: 55.9694   time: 0.26s   best: 54.5276
2024-09-01 22:32:05,889:INFO:  Epoch 394/650:  train Loss: 59.2788   val Loss: 57.8818   time: 0.26s   best: 54.5276
2024-09-01 22:32:06,141:INFO:  Epoch 395/650:  train Loss: 59.3676   val Loss: 55.6242   time: 0.25s   best: 54.5276
2024-09-01 22:32:06,401:INFO:  Epoch 396/650:  train Loss: 59.5892   val Loss: 57.9701   time: 0.26s   best: 54.5276
2024-09-01 22:32:06,653:INFO:  Epoch 397/650:  train Loss: 59.7377   val Loss: 57.5187   time: 0.25s   best: 54.5276
2024-09-01 22:32:06,913:INFO:  Epoch 398/650:  train Loss: 59.7349   val Loss: 57.1659   time: 0.26s   best: 54.5276
2024-09-01 22:32:07,165:INFO:  Epoch 399/650:  train Loss: 59.1252   val Loss: 55.7880   time: 0.25s   best: 54.5276
2024-09-01 22:32:07,426:INFO:  Epoch 400/650:  train Loss: 60.6540   val Loss: 56.5912   time: 0.26s   best: 54.5276
2024-09-01 22:32:07,677:INFO:  Epoch 401/650:  train Loss: 59.9082   val Loss: 57.0299   time: 0.25s   best: 54.5276
2024-09-01 22:32:07,940:INFO:  Epoch 402/650:  train Loss: 60.7315   val Loss: 58.0213   time: 0.26s   best: 54.5276
2024-09-01 22:32:08,191:INFO:  Epoch 403/650:  train Loss: 60.2014   val Loss: 56.9287   time: 0.25s   best: 54.5276
2024-09-01 22:32:08,453:INFO:  Epoch 404/650:  train Loss: 61.7594   val Loss: 58.5272   time: 0.26s   best: 54.5276
2024-09-01 22:32:08,704:INFO:  Epoch 405/650:  train Loss: 61.5796   val Loss: 59.0988   time: 0.25s   best: 54.5276
2024-09-01 22:32:08,978:INFO:  Epoch 406/650:  train Loss: 59.9945   val Loss: 56.8616   time: 0.27s   best: 54.5276
2024-09-01 22:32:09,229:INFO:  Epoch 407/650:  train Loss: 60.2059   val Loss: 58.7015   time: 0.25s   best: 54.5276
2024-09-01 22:32:09,490:INFO:  Epoch 408/650:  train Loss: 60.8627   val Loss: 55.4889   time: 0.26s   best: 54.5276
2024-09-01 22:32:09,827:INFO:  Epoch 409/650:  train Loss: 59.6490   val Loss: 57.7665   time: 0.34s   best: 54.5276
2024-09-01 22:32:10,092:INFO:  Epoch 410/650:  train Loss: 59.4669   val Loss: 56.8989   time: 0.26s   best: 54.5276
2024-09-01 22:32:10,378:INFO:  Epoch 411/650:  train Loss: 59.8268   val Loss: 57.3711   time: 0.28s   best: 54.5276
2024-09-01 22:32:10,634:INFO:  Epoch 412/650:  train Loss: 60.9526   val Loss: 55.3844   time: 0.25s   best: 54.5276
2024-09-01 22:32:10,899:INFO:  Epoch 413/650:  train Loss: 58.6914   val Loss: 55.4005   time: 0.26s   best: 54.5276
2024-09-01 22:32:11,155:INFO:  Epoch 414/650:  train Loss: 58.6000   val Loss: 55.5947   time: 0.25s   best: 54.5276
2024-09-01 22:32:11,419:INFO:  Epoch 415/650:  train Loss: 58.4932   val Loss: 54.8823   time: 0.26s   best: 54.5276
2024-09-01 22:32:11,675:INFO:  Epoch 416/650:  train Loss: 57.4202   val Loss: 55.9043   time: 0.25s   best: 54.5276
2024-09-01 22:32:11,948:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:11,971:INFO:  Epoch 417/650:  train Loss: 57.1241   val Loss: 53.7412   time: 0.26s   best: 53.7412
2024-09-01 22:32:12,227:INFO:  Epoch 418/650:  train Loss: 57.8613   val Loss: 55.7973   time: 0.25s   best: 53.7412
2024-09-01 22:32:12,503:INFO:  Epoch 419/650:  train Loss: 56.9492   val Loss: 55.2122   time: 0.27s   best: 53.7412
2024-09-01 22:32:12,837:INFO:  Epoch 420/650:  train Loss: 57.9667   val Loss: 57.1515   time: 0.33s   best: 53.7412
2024-09-01 22:32:13,103:INFO:  Epoch 421/650:  train Loss: 59.1201   val Loss: 57.8193   time: 0.26s   best: 53.7412
2024-09-01 22:32:13,358:INFO:  Epoch 422/650:  train Loss: 61.1630   val Loss: 58.6825   time: 0.25s   best: 53.7412
2024-09-01 22:32:13,623:INFO:  Epoch 423/650:  train Loss: 59.7772   val Loss: 54.8331   time: 0.26s   best: 53.7412
2024-09-01 22:32:13,910:INFO:  Epoch 424/650:  train Loss: 56.6218   val Loss: 54.4378   time: 0.28s   best: 53.7412
2024-09-01 22:32:14,166:INFO:  Epoch 425/650:  train Loss: 57.9126   val Loss: 55.2528   time: 0.25s   best: 53.7412
2024-09-01 22:32:14,432:INFO:  Epoch 426/650:  train Loss: 57.7000   val Loss: 53.9684   time: 0.26s   best: 53.7412
2024-09-01 22:32:14,687:INFO:  Epoch 427/650:  train Loss: 56.8265   val Loss: 55.5874   time: 0.25s   best: 53.7412
2024-09-01 22:32:14,957:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:14,979:INFO:  Epoch 428/650:  train Loss: 57.6550   val Loss: 52.9973   time: 0.26s   best: 52.9973
2024-09-01 22:32:15,264:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:15,287:INFO:  Epoch 429/650:  train Loss: 56.3244   val Loss: 52.7033   time: 0.28s   best: 52.7033
2024-09-01 22:32:15,552:INFO:  Epoch 430/650:  train Loss: 56.7649   val Loss: 54.7720   time: 0.26s   best: 52.7033
2024-09-01 22:32:15,804:INFO:  Epoch 431/650:  train Loss: 58.0139   val Loss: 54.8875   time: 0.25s   best: 52.7033
2024-09-01 22:32:16,066:INFO:  Epoch 432/650:  train Loss: 58.3055   val Loss: 53.7594   time: 0.26s   best: 52.7033
2024-09-01 22:32:16,317:INFO:  Epoch 433/650:  train Loss: 58.0060   val Loss: 56.0679   time: 0.25s   best: 52.7033
2024-09-01 22:32:16,601:INFO:  Epoch 434/650:  train Loss: 60.1693   val Loss: 57.3129   time: 0.28s   best: 52.7033
2024-09-01 22:32:16,851:INFO:  Epoch 435/650:  train Loss: 60.4785   val Loss: 55.8470   time: 0.25s   best: 52.7033
2024-09-01 22:32:17,112:INFO:  Epoch 436/650:  train Loss: 60.7151   val Loss: 56.9621   time: 0.26s   best: 52.7033
2024-09-01 22:32:17,363:INFO:  Epoch 437/650:  train Loss: 58.4733   val Loss: 53.5177   time: 0.25s   best: 52.7033
2024-09-01 22:32:17,624:INFO:  Epoch 438/650:  train Loss: 57.2459   val Loss: 53.5833   time: 0.26s   best: 52.7033
2024-09-01 22:32:17,877:INFO:  Epoch 439/650:  train Loss: 56.1858   val Loss: 53.4479   time: 0.25s   best: 52.7033
2024-09-01 22:32:18,139:INFO:  Epoch 440/650:  train Loss: 55.4208   val Loss: 53.8019   time: 0.26s   best: 52.7033
2024-09-01 22:32:18,394:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:18,417:INFO:  Epoch 441/650:  train Loss: 56.0547   val Loss: 52.1201   time: 0.25s   best: 52.1201
2024-09-01 22:32:18,683:INFO:  Epoch 442/650:  train Loss: 54.9367   val Loss: 53.5414   time: 0.27s   best: 52.1201
2024-09-01 22:32:18,934:INFO:  Epoch 443/650:  train Loss: 55.3087   val Loss: 53.2006   time: 0.25s   best: 52.1201
2024-09-01 22:32:19,195:INFO:  Epoch 444/650:  train Loss: 57.3261   val Loss: 53.5152   time: 0.26s   best: 52.1201
2024-09-01 22:32:19,446:INFO:  Epoch 445/650:  train Loss: 56.4104   val Loss: 54.8606   time: 0.25s   best: 52.1201
2024-09-01 22:32:19,706:INFO:  Epoch 446/650:  train Loss: 56.9926   val Loss: 54.8714   time: 0.26s   best: 52.1201
2024-09-01 22:32:19,959:INFO:  Epoch 447/650:  train Loss: 57.8659   val Loss: 56.8411   time: 0.25s   best: 52.1201
2024-09-01 22:32:20,219:INFO:  Epoch 448/650:  train Loss: 59.2759   val Loss: 58.3238   time: 0.26s   best: 52.1201
2024-09-01 22:32:20,470:INFO:  Epoch 449/650:  train Loss: 62.3126   val Loss: 60.6240   time: 0.25s   best: 52.1201
2024-09-01 22:32:20,757:INFO:  Epoch 450/650:  train Loss: 61.1711   val Loss: 58.1782   time: 0.29s   best: 52.1201
2024-09-01 22:32:21,009:INFO:  Epoch 451/650:  train Loss: 59.4116   val Loss: 56.5044   time: 0.25s   best: 52.1201
2024-09-01 22:32:21,269:INFO:  Epoch 452/650:  train Loss: 58.0277   val Loss: 55.5241   time: 0.26s   best: 52.1201
2024-09-01 22:32:21,520:INFO:  Epoch 453/650:  train Loss: 56.7022   val Loss: 53.2015   time: 0.25s   best: 52.1201
2024-09-01 22:32:21,781:INFO:  Epoch 454/650:  train Loss: 57.2713   val Loss: 55.2633   time: 0.26s   best: 52.1201
2024-09-01 22:32:22,035:INFO:  Epoch 455/650:  train Loss: 56.1439   val Loss: 53.3782   time: 0.25s   best: 52.1201
2024-09-01 22:32:22,329:INFO:  Epoch 456/650:  train Loss: 55.2060   val Loss: 52.2278   time: 0.29s   best: 52.1201
2024-09-01 22:32:22,600:INFO:  Epoch 457/650:  train Loss: 55.6383   val Loss: 54.8555   time: 0.27s   best: 52.1201
2024-09-01 22:32:22,900:INFO:  Epoch 458/650:  train Loss: 56.8085   val Loss: 53.6701   time: 0.30s   best: 52.1201
2024-09-01 22:32:23,170:INFO:  Epoch 459/650:  train Loss: 57.2333   val Loss: 54.0797   time: 0.27s   best: 52.1201
2024-09-01 22:32:23,431:INFO:  Epoch 460/650:  train Loss: 60.2891   val Loss: 56.4775   time: 0.26s   best: 52.1201
2024-09-01 22:32:23,702:INFO:  Epoch 461/650:  train Loss: 57.4836   val Loss: 53.2446   time: 0.27s   best: 52.1201
2024-09-01 22:32:23,967:INFO:  Epoch 462/650:  train Loss: 55.8985   val Loss: 52.9213   time: 0.26s   best: 52.1201
2024-09-01 22:32:24,241:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:24,264:INFO:  Epoch 463/650:  train Loss: 55.3683   val Loss: 51.7508   time: 0.27s   best: 51.7508
2024-09-01 22:32:24,536:INFO:  Epoch 464/650:  train Loss: 54.4661   val Loss: 51.8785   time: 0.27s   best: 51.7508
2024-09-01 22:32:24,808:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:24,831:INFO:  Epoch 465/650:  train Loss: 53.6407   val Loss: 51.0710   time: 0.27s   best: 51.0710
2024-09-01 22:32:25,124:INFO:  Epoch 466/650:  train Loss: 54.0764   val Loss: 51.2312   time: 0.29s   best: 51.0710
2024-09-01 22:32:25,383:INFO:  Epoch 467/650:  train Loss: 54.3821   val Loss: 51.5436   time: 0.26s   best: 51.0710
2024-09-01 22:32:25,652:INFO:  Epoch 468/650:  train Loss: 53.9405   val Loss: 51.4498   time: 0.27s   best: 51.0710
2024-09-01 22:32:25,921:INFO:  Epoch 469/650:  train Loss: 53.0539   val Loss: 51.7466   time: 0.27s   best: 51.0710
2024-09-01 22:32:26,192:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:26,215:INFO:  Epoch 470/650:  train Loss: 54.5889   val Loss: 50.7233   time: 0.27s   best: 50.7233
2024-09-01 22:32:26,478:INFO:  Epoch 471/650:  train Loss: 55.8908   val Loss: 53.3493   time: 0.25s   best: 50.7233
2024-09-01 22:32:26,745:INFO:  Epoch 472/650:  train Loss: 55.9294   val Loss: 52.6871   time: 0.26s   best: 50.7233
2024-09-01 22:32:27,014:INFO:  Epoch 473/650:  train Loss: 55.1544   val Loss: 52.1714   time: 0.26s   best: 50.7233
2024-09-01 22:32:27,278:INFO:  Epoch 474/650:  train Loss: 56.1556   val Loss: 56.0632   time: 0.26s   best: 50.7233
2024-09-01 22:32:27,531:INFO:  Epoch 475/650:  train Loss: 58.0196   val Loss: 53.1901   time: 0.25s   best: 50.7233
2024-09-01 22:32:27,792:INFO:  Epoch 476/650:  train Loss: 57.3295   val Loss: 56.6577   time: 0.26s   best: 50.7233
2024-09-01 22:32:28,047:INFO:  Epoch 477/650:  train Loss: 57.6032   val Loss: 53.2627   time: 0.25s   best: 50.7233
2024-09-01 22:32:28,320:INFO:  Epoch 478/650:  train Loss: 56.4299   val Loss: 54.8190   time: 0.27s   best: 50.7233
2024-09-01 22:32:28,571:INFO:  Epoch 479/650:  train Loss: 56.1920   val Loss: 53.4534   time: 0.25s   best: 50.7233
2024-09-01 22:32:28,832:INFO:  Epoch 480/650:  train Loss: 59.1503   val Loss: 54.2169   time: 0.26s   best: 50.7233
2024-09-01 22:32:29,086:INFO:  Epoch 481/650:  train Loss: 58.8005   val Loss: 54.5518   time: 0.25s   best: 50.7233
2024-09-01 22:32:29,346:INFO:  Epoch 482/650:  train Loss: 60.7938   val Loss: 54.8221   time: 0.26s   best: 50.7233
2024-09-01 22:32:29,597:INFO:  Epoch 483/650:  train Loss: 57.0613   val Loss: 51.7504   time: 0.25s   best: 50.7233
2024-09-01 22:32:29,859:INFO:  Epoch 484/650:  train Loss: 54.8468   val Loss: 52.0613   time: 0.26s   best: 50.7233
2024-09-01 22:32:30,112:INFO:  Epoch 485/650:  train Loss: 55.6774   val Loss: 51.2262   time: 0.25s   best: 50.7233
2024-09-01 22:32:30,373:INFO:  Epoch 486/650:  train Loss: 54.5354   val Loss: 52.2901   time: 0.26s   best: 50.7233
2024-09-01 22:32:30,624:INFO:  Epoch 487/650:  train Loss: 54.6753   val Loss: 52.2854   time: 0.25s   best: 50.7233
2024-09-01 22:32:30,884:INFO:  Epoch 488/650:  train Loss: 54.5348   val Loss: 51.4586   time: 0.26s   best: 50.7233
2024-09-01 22:32:31,146:INFO:  Epoch 489/650:  train Loss: 53.4205   val Loss: 50.7566   time: 0.26s   best: 50.7233
2024-09-01 22:32:31,410:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:31,433:INFO:  Epoch 490/650:  train Loss: 53.2027   val Loss: 50.5011   time: 0.26s   best: 50.5011
2024-09-01 22:32:31,685:INFO:  Epoch 491/650:  train Loss: 55.1414   val Loss: 53.5353   time: 0.25s   best: 50.5011
2024-09-01 22:32:31,947:INFO:  Epoch 492/650:  train Loss: 56.0315   val Loss: 54.1319   time: 0.26s   best: 50.5011
2024-09-01 22:32:32,198:INFO:  Epoch 493/650:  train Loss: 55.5385   val Loss: 54.1719   time: 0.25s   best: 50.5011
2024-09-01 22:32:32,457:INFO:  Epoch 494/650:  train Loss: 55.8323   val Loss: 52.4319   time: 0.26s   best: 50.5011
2024-09-01 22:32:32,709:INFO:  Epoch 495/650:  train Loss: 57.7175   val Loss: 53.9394   time: 0.25s   best: 50.5011
2024-09-01 22:32:32,970:INFO:  Epoch 496/650:  train Loss: 55.8966   val Loss: 54.7461   time: 0.26s   best: 50.5011
2024-09-01 22:32:33,221:INFO:  Epoch 497/650:  train Loss: 57.9103   val Loss: 55.7840   time: 0.25s   best: 50.5011
2024-09-01 22:32:33,481:INFO:  Epoch 498/650:  train Loss: 57.7532   val Loss: 53.5856   time: 0.26s   best: 50.5011
2024-09-01 22:32:33,732:INFO:  Epoch 499/650:  train Loss: 54.9617   val Loss: 53.5680   time: 0.25s   best: 50.5011
2024-09-01 22:32:34,127:INFO:  Epoch 500/650:  train Loss: 55.6962   val Loss: 53.1213   time: 0.39s   best: 50.5011
2024-09-01 22:32:34,377:INFO:  Epoch 501/650:  train Loss: 55.9963   val Loss: 54.0818   time: 0.25s   best: 50.5011
2024-09-01 22:32:34,637:INFO:  Epoch 502/650:  train Loss: 56.4027   val Loss: 55.0502   time: 0.26s   best: 50.5011
2024-09-01 22:32:34,888:INFO:  Epoch 503/650:  train Loss: 57.4556   val Loss: 55.6287   time: 0.25s   best: 50.5011
2024-09-01 22:32:35,149:INFO:  Epoch 504/650:  train Loss: 57.3296   val Loss: 53.1045   time: 0.26s   best: 50.5011
2024-09-01 22:32:35,400:INFO:  Epoch 505/650:  train Loss: 56.0671   val Loss: 53.6124   time: 0.25s   best: 50.5011
2024-09-01 22:32:35,660:INFO:  Epoch 506/650:  train Loss: 56.2163   val Loss: 54.8193   time: 0.26s   best: 50.5011
2024-09-01 22:32:35,913:INFO:  Epoch 507/650:  train Loss: 56.6100   val Loss: 52.2323   time: 0.25s   best: 50.5011
2024-09-01 22:32:36,174:INFO:  Epoch 508/650:  train Loss: 55.5713   val Loss: 53.0900   time: 0.26s   best: 50.5011
2024-09-01 22:32:36,424:INFO:  Epoch 509/650:  train Loss: 55.2533   val Loss: 52.1006   time: 0.25s   best: 50.5011
2024-09-01 22:32:36,685:INFO:  Epoch 510/650:  train Loss: 55.9351   val Loss: 53.9933   time: 0.26s   best: 50.5011
2024-09-01 22:32:36,935:INFO:  Epoch 511/650:  train Loss: 56.6816   val Loss: 54.8838   time: 0.25s   best: 50.5011
2024-09-01 22:32:37,195:INFO:  Epoch 512/650:  train Loss: 59.8854   val Loss: 57.6324   time: 0.26s   best: 50.5011
2024-09-01 22:32:37,446:INFO:  Epoch 513/650:  train Loss: 59.1974   val Loss: 52.1125   time: 0.25s   best: 50.5011
2024-09-01 22:32:37,707:INFO:  Epoch 514/650:  train Loss: 55.2361   val Loss: 53.5791   time: 0.26s   best: 50.5011
2024-09-01 22:32:37,960:INFO:  Epoch 515/650:  train Loss: 56.0812   val Loss: 52.6694   time: 0.25s   best: 50.5011
2024-09-01 22:32:38,221:INFO:  Epoch 516/650:  train Loss: 53.5561   val Loss: 52.2114   time: 0.26s   best: 50.5011
2024-09-01 22:32:38,471:INFO:  Epoch 517/650:  train Loss: 53.7033   val Loss: 51.4203   time: 0.25s   best: 50.5011
2024-09-01 22:32:38,732:INFO:  Epoch 518/650:  train Loss: 54.5965   val Loss: 53.2953   time: 0.26s   best: 50.5011
2024-09-01 22:32:38,982:INFO:  Epoch 519/650:  train Loss: 55.2916   val Loss: 53.5511   time: 0.25s   best: 50.5011
2024-09-01 22:32:39,243:INFO:  Epoch 520/650:  train Loss: 55.1892   val Loss: 52.5279   time: 0.26s   best: 50.5011
2024-09-01 22:32:39,493:INFO:  Epoch 521/650:  train Loss: 54.9388   val Loss: 52.0457   time: 0.25s   best: 50.5011
2024-09-01 22:32:39,754:INFO:  Epoch 522/650:  train Loss: 53.7939   val Loss: 50.9271   time: 0.26s   best: 50.5011
2024-09-01 22:32:40,013:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:40,035:INFO:  Epoch 523/650:  train Loss: 53.5796   val Loss: 49.9282   time: 0.25s   best: 49.9282
2024-09-01 22:32:40,301:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:40,324:INFO:  Epoch 524/650:  train Loss: 53.3254   val Loss: 49.6726   time: 0.26s   best: 49.6726
2024-09-01 22:32:40,584:INFO:  Epoch 525/650:  train Loss: 52.3298   val Loss: 51.2257   time: 0.26s   best: 49.6726
2024-09-01 22:32:40,835:INFO:  Epoch 526/650:  train Loss: 55.3805   val Loss: 50.4652   time: 0.25s   best: 49.6726
2024-09-01 22:32:41,095:INFO:  Epoch 527/650:  train Loss: 53.7001   val Loss: 51.1477   time: 0.26s   best: 49.6726
2024-09-01 22:32:41,347:INFO:  Epoch 528/650:  train Loss: 52.9076   val Loss: 51.0939   time: 0.25s   best: 49.6726
2024-09-01 22:32:41,615:INFO:  Epoch 529/650:  train Loss: 58.2790   val Loss: 55.7323   time: 0.26s   best: 49.6726
2024-09-01 22:32:41,866:INFO:  Epoch 530/650:  train Loss: 60.4137   val Loss: 54.8347   time: 0.25s   best: 49.6726
2024-09-01 22:32:42,129:INFO:  Epoch 531/650:  train Loss: 62.8251   val Loss: 53.0170   time: 0.26s   best: 49.6726
2024-09-01 22:32:42,381:INFO:  Epoch 532/650:  train Loss: 58.5748   val Loss: 53.8134   time: 0.25s   best: 49.6726
2024-09-01 22:32:42,642:INFO:  Epoch 533/650:  train Loss: 57.7713   val Loss: 53.8800   time: 0.26s   best: 49.6726
2024-09-01 22:32:42,893:INFO:  Epoch 534/650:  train Loss: 55.2409   val Loss: 52.1413   time: 0.25s   best: 49.6726
2024-09-01 22:32:43,154:INFO:  Epoch 535/650:  train Loss: 53.8564   val Loss: 52.7908   time: 0.26s   best: 49.6726
2024-09-01 22:32:43,404:INFO:  Epoch 536/650:  train Loss: 53.7146   val Loss: 50.5977   time: 0.25s   best: 49.6726
2024-09-01 22:32:43,665:INFO:  Epoch 537/650:  train Loss: 54.9922   val Loss: 54.5494   time: 0.26s   best: 49.6726
2024-09-01 22:32:43,918:INFO:  Epoch 538/650:  train Loss: 56.3081   val Loss: 53.4241   time: 0.25s   best: 49.6726
2024-09-01 22:32:44,179:INFO:  Epoch 539/650:  train Loss: 56.7197   val Loss: 55.6949   time: 0.26s   best: 49.6726
2024-09-01 22:32:44,430:INFO:  Epoch 540/650:  train Loss: 59.6671   val Loss: 53.5101   time: 0.25s   best: 49.6726
2024-09-01 22:32:44,692:INFO:  Epoch 541/650:  train Loss: 56.5916   val Loss: 53.6368   time: 0.26s   best: 49.6726
2024-09-01 22:32:44,943:INFO:  Epoch 542/650:  train Loss: 57.0217   val Loss: 52.1327   time: 0.25s   best: 49.6726
2024-09-01 22:32:45,204:INFO:  Epoch 543/650:  train Loss: 53.3068   val Loss: 51.3523   time: 0.26s   best: 49.6726
2024-09-01 22:32:45,455:INFO:  Epoch 544/650:  train Loss: 52.7601   val Loss: 52.1121   time: 0.25s   best: 49.6726
2024-09-01 22:32:45,716:INFO:  Epoch 545/650:  train Loss: 54.7002   val Loss: 52.3924   time: 0.26s   best: 49.6726
2024-09-01 22:32:45,970:INFO:  Epoch 546/650:  train Loss: 54.1714   val Loss: 52.3403   time: 0.25s   best: 49.6726
2024-09-01 22:32:46,231:INFO:  Epoch 547/650:  train Loss: 56.1928   val Loss: 53.2414   time: 0.26s   best: 49.6726
2024-09-01 22:32:46,483:INFO:  Epoch 548/650:  train Loss: 53.3806   val Loss: 52.0672   time: 0.25s   best: 49.6726
2024-09-01 22:32:46,755:INFO:  Epoch 549/650:  train Loss: 54.3358   val Loss: 51.7648   time: 0.27s   best: 49.6726
2024-09-01 22:32:47,006:INFO:  Epoch 550/650:  train Loss: 53.6709   val Loss: 52.0479   time: 0.25s   best: 49.6726
2024-09-01 22:32:47,267:INFO:  Epoch 551/650:  train Loss: 53.9502   val Loss: 50.5240   time: 0.26s   best: 49.6726
2024-09-01 22:32:47,518:INFO:  Epoch 552/650:  train Loss: 53.1488   val Loss: 51.9351   time: 0.25s   best: 49.6726
2024-09-01 22:32:47,785:INFO:  Epoch 553/650:  train Loss: 56.5062   val Loss: 52.9116   time: 0.26s   best: 49.6726
2024-09-01 22:32:48,040:INFO:  Epoch 554/650:  train Loss: 54.8112   val Loss: 52.2737   time: 0.25s   best: 49.6726
2024-09-01 22:32:48,305:INFO:  Epoch 555/650:  train Loss: 53.5757   val Loss: 52.2152   time: 0.26s   best: 49.6726
2024-09-01 22:32:48,561:INFO:  Epoch 556/650:  train Loss: 56.3782   val Loss: 51.8218   time: 0.25s   best: 49.6726
2024-09-01 22:32:48,826:INFO:  Epoch 557/650:  train Loss: 55.8958   val Loss: 53.4327   time: 0.26s   best: 49.6726
2024-09-01 22:32:49,082:INFO:  Epoch 558/650:  train Loss: 56.4338   val Loss: 51.8698   time: 0.25s   best: 49.6726
2024-09-01 22:32:49,346:INFO:  Epoch 559/650:  train Loss: 60.8467   val Loss: 54.4530   time: 0.26s   best: 49.6726
2024-09-01 22:32:49,619:INFO:  Epoch 560/650:  train Loss: 57.0278   val Loss: 53.3899   time: 0.27s   best: 49.6726
2024-09-01 22:32:49,895:INFO:  Epoch 561/650:  train Loss: 57.4013   val Loss: 53.4170   time: 0.27s   best: 49.6726
2024-09-01 22:32:50,154:INFO:  Epoch 562/650:  train Loss: 56.6375   val Loss: 53.2187   time: 0.25s   best: 49.6726
2024-09-01 22:32:50,419:INFO:  Epoch 563/650:  train Loss: 56.7101   val Loss: 53.9694   time: 0.26s   best: 49.6726
2024-09-01 22:32:50,675:INFO:  Epoch 564/650:  train Loss: 57.7109   val Loss: 52.1633   time: 0.25s   best: 49.6726
2024-09-01 22:32:50,940:INFO:  Epoch 565/650:  train Loss: 54.0150   val Loss: 51.3575   time: 0.26s   best: 49.6726
2024-09-01 22:32:51,196:INFO:  Epoch 566/650:  train Loss: 55.0813   val Loss: 50.0517   time: 0.25s   best: 49.6726
2024-09-01 22:32:51,460:INFO:  Epoch 567/650:  train Loss: 52.3446   val Loss: 52.0170   time: 0.26s   best: 49.6726
2024-09-01 22:32:51,784:INFO:  Epoch 568/650:  train Loss: 53.8745   val Loss: 50.3767   time: 0.32s   best: 49.6726
2024-09-01 22:32:52,044:INFO:  Epoch 569/650:  train Loss: 52.9392   val Loss: 49.9520   time: 0.26s   best: 49.6726
2024-09-01 22:32:52,304:INFO:  Epoch 570/650:  train Loss: 54.6197   val Loss: 54.8973   time: 0.26s   best: 49.6726
2024-09-01 22:32:52,583:INFO:  Epoch 571/650:  train Loss: 58.0169   val Loss: 54.7356   time: 0.28s   best: 49.6726
2024-09-01 22:32:52,878:INFO:  Epoch 572/650:  train Loss: 55.0565   val Loss: 52.4423   time: 0.29s   best: 49.6726
2024-09-01 22:32:53,129:INFO:  Epoch 573/650:  train Loss: 53.9347   val Loss: 51.5148   time: 0.25s   best: 49.6726
2024-09-01 22:32:53,390:INFO:  Epoch 574/650:  train Loss: 52.4469   val Loss: 50.0043   time: 0.26s   best: 49.6726
2024-09-01 22:32:53,641:INFO:  Epoch 575/650:  train Loss: 53.6212   val Loss: 52.3239   time: 0.25s   best: 49.6726
2024-09-01 22:32:53,945:INFO:  Epoch 576/650:  train Loss: 53.0642   val Loss: 51.2103   time: 0.30s   best: 49.6726
2024-09-01 22:32:54,197:INFO:  Epoch 577/650:  train Loss: 54.0793   val Loss: 53.2694   time: 0.25s   best: 49.6726
2024-09-01 22:32:54,457:INFO:  Epoch 578/650:  train Loss: 52.0595   val Loss: 51.3126   time: 0.26s   best: 49.6726
2024-09-01 22:32:54,713:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:54,736:INFO:  Epoch 579/650:  train Loss: 55.5966   val Loss: 49.2779   time: 0.25s   best: 49.2779
2024-09-01 22:32:54,996:INFO:  Epoch 580/650:  train Loss: 53.5709   val Loss: 53.8409   time: 0.26s   best: 49.2779
2024-09-01 22:32:55,247:INFO:  Epoch 581/650:  train Loss: 56.7973   val Loss: 53.7554   time: 0.25s   best: 49.2779
2024-09-01 22:32:55,509:INFO:  Epoch 582/650:  train Loss: 56.2710   val Loss: 54.6498   time: 0.26s   best: 49.2779
2024-09-01 22:32:55,761:INFO:  Epoch 583/650:  train Loss: 57.1235   val Loss: 51.2885   time: 0.25s   best: 49.2779
2024-09-01 22:32:56,060:INFO:  Epoch 584/650:  train Loss: 54.4923   val Loss: 51.5084   time: 0.30s   best: 49.2779
2024-09-01 22:32:56,312:INFO:  Epoch 585/650:  train Loss: 56.9796   val Loss: 54.0748   time: 0.25s   best: 49.2779
2024-09-01 22:32:56,573:INFO:  Epoch 586/650:  train Loss: 56.4909   val Loss: 54.0233   time: 0.26s   best: 49.2779
2024-09-01 22:32:56,825:INFO:  Epoch 587/650:  train Loss: 54.8142   val Loss: 50.2201   time: 0.25s   best: 49.2779
2024-09-01 22:32:57,087:INFO:  Epoch 588/650:  train Loss: 56.3831   val Loss: 52.4452   time: 0.26s   best: 49.2779
2024-09-01 22:32:57,338:INFO:  Epoch 589/650:  train Loss: 54.9692   val Loss: 54.1367   time: 0.25s   best: 49.2779
2024-09-01 22:32:57,599:INFO:  Epoch 590/650:  train Loss: 55.8845   val Loss: 51.7420   time: 0.26s   best: 49.2779
2024-09-01 22:32:57,850:INFO:  Epoch 591/650:  train Loss: 54.3114   val Loss: 52.1765   time: 0.25s   best: 49.2779
2024-09-01 22:32:58,133:INFO:  Epoch 592/650:  train Loss: 55.4191   val Loss: 49.9245   time: 0.28s   best: 49.2779
2024-09-01 22:32:58,383:INFO:  Epoch 593/650:  train Loss: 53.1700   val Loss: 50.1993   time: 0.25s   best: 49.2779
2024-09-01 22:32:58,644:INFO:  Epoch 594/650:  train Loss: 52.3689   val Loss: 49.6864   time: 0.26s   best: 49.2779
2024-09-01 22:32:58,894:INFO:  Epoch 595/650:  train Loss: 53.4238   val Loss: 50.2007   time: 0.25s   best: 49.2779
2024-09-01 22:32:59,155:INFO:  Epoch 596/650:  train Loss: 51.9544   val Loss: 51.8584   time: 0.26s   best: 49.2779
2024-09-01 22:32:59,410:DEBUG:  Saving checkpoint to /scratch/cl5503/cost_model_lstm_autoencoder/weights/best_model_Debug lstm autoencoder perm50 with 0.05 dataset (0.05 dropout)_6d2e.pt
2024-09-01 22:32:59,431:INFO:  Epoch 597/650:  train Loss: 53.8995   val Loss: 47.6000   time: 0.25s   best: 47.6000
2024-09-01 22:32:59,696:INFO:  Epoch 598/650:  train Loss: 51.0721   val Loss: 49.3752   time: 0.26s   best: 47.6000
2024-09-01 22:32:59,960:INFO:  Epoch 599/650:  train Loss: 52.1743   val Loss: 51.0458   time: 0.26s   best: 47.6000
2024-09-01 22:33:00,220:INFO:  Epoch 600/650:  train Loss: 55.0619   val Loss: 50.4914   time: 0.26s   best: 47.6000
2024-09-01 22:33:00,512:INFO:  Epoch 601/650:  train Loss: 52.2405   val Loss: 50.6984   time: 0.29s   best: 47.6000
2024-09-01 22:33:00,765:INFO:  Epoch 602/650:  train Loss: 52.2316   val Loss: 50.0501   time: 0.25s   best: 47.6000
2024-09-01 22:33:01,027:INFO:  Epoch 603/650:  train Loss: 53.0521   val Loss: 52.3581   time: 0.26s   best: 47.6000
2024-09-01 22:33:01,278:INFO:  Epoch 604/650:  train Loss: 53.1768   val Loss: 51.6748   time: 0.25s   best: 47.6000
2024-09-01 22:33:01,537:INFO:  Epoch 605/650:  train Loss: 53.6979   val Loss: 49.9381   time: 0.26s   best: 47.6000
2024-09-01 22:33:01,789:INFO:  Epoch 606/650:  train Loss: 51.6708   val Loss: 48.6699   time: 0.25s   best: 47.6000
2024-09-01 22:33:02,051:INFO:  Epoch 607/650:  train Loss: 51.7038   val Loss: 49.9995   time: 0.26s   best: 47.6000
2024-09-01 22:33:02,302:INFO:  Epoch 608/650:  train Loss: 51.9684   val Loss: 48.4880   time: 0.25s   best: 47.6000
2024-09-01 22:33:02,562:INFO:  Epoch 609/650:  train Loss: 52.6308   val Loss: 50.1651   time: 0.26s   best: 47.6000
2024-09-01 22:33:02,813:INFO:  Epoch 610/650:  train Loss: 56.4929   val Loss: 55.7244   time: 0.25s   best: 47.6000
2024-09-01 22:33:03,073:INFO:  Epoch 611/650:  train Loss: 57.5669   val Loss: 53.8979   time: 0.26s   best: 47.6000
2024-09-01 22:33:03,323:INFO:  Epoch 612/650:  train Loss: 55.1837   val Loss: 51.3033   time: 0.25s   best: 47.6000
2024-09-01 22:33:03,584:INFO:  Epoch 613/650:  train Loss: 57.0427   val Loss: 56.5722   time: 0.26s   best: 47.6000
2024-09-01 22:33:03,834:INFO:  Epoch 614/650:  train Loss: 58.9503   val Loss: 52.9560   time: 0.25s   best: 47.6000
2024-09-01 22:33:04,097:INFO:  Epoch 615/650:  train Loss: 56.8803   val Loss: 54.7005   time: 0.26s   best: 47.6000
2024-09-01 22:33:04,348:INFO:  Epoch 616/650:  train Loss: 55.4699   val Loss: 51.8361   time: 0.25s   best: 47.6000
2024-09-01 22:33:04,608:INFO:  Epoch 617/650:  train Loss: 55.0256   val Loss: 52.3299   time: 0.26s   best: 47.6000
2024-09-01 22:33:04,859:INFO:  Epoch 618/650:  train Loss: 55.2109   val Loss: 50.1310   time: 0.25s   best: 47.6000
2024-09-01 22:33:05,119:INFO:  Epoch 619/650:  train Loss: 54.0336   val Loss: 51.9342   time: 0.26s   best: 47.6000
2024-09-01 22:33:05,381:INFO:  Epoch 620/650:  train Loss: 53.2583   val Loss: 51.0870   time: 0.26s   best: 47.6000
2024-09-01 22:33:05,642:INFO:  Epoch 621/650:  train Loss: 53.5027   val Loss: 49.6486   time: 0.26s   best: 47.6000
2024-09-01 22:33:05,893:INFO:  Epoch 622/650:  train Loss: 52.6539   val Loss: 50.5878   time: 0.25s   best: 47.6000
2024-09-01 22:33:06,156:INFO:  Epoch 623/650:  train Loss: 52.7514   val Loss: 50.8514   time: 0.26s   best: 47.6000
2024-09-01 22:33:06,411:INFO:  Epoch 624/650:  train Loss: 55.5631   val Loss: 51.5749   time: 0.25s   best: 47.6000
2024-09-01 22:33:06,673:INFO:  Epoch 625/650:  train Loss: 54.6535   val Loss: 54.8204   time: 0.26s   best: 47.6000
2024-09-01 22:33:06,923:INFO:  Epoch 626/650:  train Loss: 56.0674   val Loss: 48.3415   time: 0.25s   best: 47.6000
2024-09-01 22:33:07,184:INFO:  Epoch 627/650:  train Loss: 55.6084   val Loss: 54.9017   time: 0.26s   best: 47.6000
2024-09-01 22:33:07,434:INFO:  Epoch 628/650:  train Loss: 55.2172   val Loss: 50.9432   time: 0.25s   best: 47.6000
2024-09-01 22:33:07,695:INFO:  Epoch 629/650:  train Loss: 55.1032   val Loss: 53.7706   time: 0.26s   best: 47.6000
2024-09-01 22:33:07,945:INFO:  Epoch 630/650:  train Loss: 57.2807   val Loss: 52.3279   time: 0.25s   best: 47.6000
2024-09-01 22:33:08,209:INFO:  Epoch 631/650:  train Loss: 54.2880   val Loss: 51.7599   time: 0.26s   best: 47.6000
2024-09-01 22:33:08,469:INFO:  Epoch 632/650:  train Loss: 54.8567   val Loss: 49.3755   time: 0.26s   best: 47.6000
2024-09-01 22:33:08,731:INFO:  Epoch 633/650:  train Loss: 53.1837   val Loss: 50.0175   time: 0.26s   best: 47.6000
2024-09-01 22:33:08,981:INFO:  Epoch 634/650:  train Loss: 54.3430   val Loss: 51.8232   time: 0.25s   best: 47.6000
2024-09-01 22:33:09,242:INFO:  Epoch 635/650:  train Loss: 52.4036   val Loss: 49.2331   time: 0.26s   best: 47.6000
2024-09-01 22:33:09,492:INFO:  Epoch 636/650:  train Loss: 53.2605   val Loss: 48.1730   time: 0.25s   best: 47.6000
2024-09-01 22:33:09,753:INFO:  Epoch 637/650:  train Loss: 51.8628   val Loss: 49.1080   time: 0.26s   best: 47.6000
2024-09-01 22:33:10,009:INFO:  Epoch 638/650:  train Loss: 51.9235   val Loss: 51.2036   time: 0.26s   best: 47.6000
2024-09-01 22:33:10,272:INFO:  Epoch 639/650:  train Loss: 52.9378   val Loss: 48.4590   time: 0.26s   best: 47.6000
2024-09-01 22:33:10,533:INFO:  Epoch 640/650:  train Loss: 52.2667   val Loss: 50.9291   time: 0.26s   best: 47.6000
2024-09-01 22:33:10,795:INFO:  Epoch 641/650:  train Loss: 56.8456   val Loss: 59.2736   time: 0.26s   best: 47.6000
2024-09-01 22:33:11,048:INFO:  Epoch 642/650:  train Loss: 64.9820   val Loss: 62.4279   time: 0.25s   best: 47.6000
2024-09-01 22:33:11,334:INFO:  Epoch 643/650:  train Loss: 64.9102   val Loss: 60.4468   time: 0.29s   best: 47.6000
2024-09-01 22:33:11,586:INFO:  Epoch 644/650:  train Loss: 60.7649   val Loss: 55.3327   time: 0.25s   best: 47.6000
2024-09-01 22:33:11,846:INFO:  Epoch 645/650:  train Loss: 56.7996   val Loss: 55.6043   time: 0.26s   best: 47.6000
2024-09-01 22:33:12,099:INFO:  Epoch 646/650:  train Loss: 56.8735   val Loss: 55.2742   time: 0.25s   best: 47.6000
2024-09-01 22:33:12,359:INFO:  Epoch 647/650:  train Loss: 57.7748   val Loss: 56.9899   time: 0.26s   best: 47.6000
2024-09-01 22:33:12,610:INFO:  Epoch 648/650:  train Loss: 58.9568   val Loss: 58.6771   time: 0.25s   best: 47.6000
2024-09-01 22:33:12,870:INFO:  Epoch 649/650:  train Loss: 61.1238   val Loss: 53.9545   time: 0.26s   best: 47.6000
2024-09-01 22:33:13,122:INFO:  Epoch 650/650:  train Loss: 56.8394   val Loss: 56.6272   time: 0.25s   best: 47.6000
2024-09-01 22:33:13,122:INFO:  -----> Training complete in 2m 51s   best validation loss: 47.6000
 
